{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_keyword(keywords, sentence):\n",
    "    for kw in keywords:\n",
    "        if kw in sentence:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def contain_citation(sentence):\n",
    "    matches = re.findall(\"\\[[0-9,]*\\]|\\([a-zA-Z.& ]+,[^A-Za-z0-9_]?[0-9]*[a-zA-Z0-9.&,; ]*\\)|[A-Z][a-zA-Z. ]* \\([0-9]*\\)\", sentence)\n",
    "    return len(matches)\n",
    "\n",
    "def contain_year(sentence):\n",
    "    matches = re.findall(\"19[0-9]{2}|20[0-9]{2}\", sentence)\n",
    "    return len(matches)\n",
    "\n",
    "def patch_sent_tokenize(sentences):\n",
    "    out = []\n",
    "    i = 0\n",
    "    while i < len(sentences):\n",
    "        if i>0 and sentences[i-1][-4:] == \" et.\" and sentences[i][:2] == \"al\":\n",
    "            out[-1] += \" \" + sentences[i]\n",
    "        elif i>0 and (sentences[i-1][-4:] == \" al.\" or sentences[i-1]==\"al.\"):\n",
    "            out[-1] += \" \" + sentences[i]\n",
    "        elif i>0 and sentences[i-1][-4:] == \"e.g.\":\n",
    "            out[-1] += \" \" + sentences[i]\n",
    "        elif i>0 and sentences[i-1][-4:] == \"i.e.\":\n",
    "            out[-1] += \" \" + sentences[i]\n",
    "        else:\n",
    "            out.append(sentences[i])\n",
    "        i += 1\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_work_file = \"related_work.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11020it [00:13, 818.04it/s]\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "paragraphs = []\n",
    "paper_ids = []\n",
    "with open(related_work_file,\"r\") as f_pdf:\n",
    "    #with open(output_file,\"w\") as wf:\n",
    "    for line in tqdm(f_pdf):\n",
    "        related_work_dict = json.loads(line)\n",
    "        for pi, para in enumerate(related_work_dict[\"related_work\"]):\n",
    "            cite_span_texts = set([citation[\"text\"] for citation in para[\"cite_spans\"]])\n",
    "            sentences = []\n",
    "            citation_counts = []\n",
    "            tags = []\n",
    "            tag = \"\"\n",
    "            for si, sentence in enumerate(patch_sent_tokenize(sent_tokenize(para[\"text\"]))):\n",
    "                sentence = re.sub(\"([^\\x00-\\x7F])+\",\"\",sentence)\n",
    "                citation_count = 0\n",
    "                for citation in cite_span_texts:\n",
    "                    if citation in sentence:\n",
    "                        citation_count+=1\n",
    "                if citation_count == 0: # Try to extract citation for the second time, in case S2ORC did not find them out.\n",
    "                    citation_count = contain_citation(sentence)\n",
    "                if citation_count == 0:\n",
    "                    citation_count = contain_year(sentence)\n",
    "                sentences.append(sentence)\n",
    "            paragraphs.append(sentences)\n",
    "            paper_ids.append(related_work_dict[\"paper_id\"] + \"_\" + str(pi+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = {}\n",
    "for paper_id, str_seq in zip(paper_ids, paragraphs):\n",
    "    paperid, paraid = paper_id.split(\"_\")\n",
    "    this_doc = docs.get(paperid,[])\n",
    "    decorated_str_seq = [\"[BOS] \"+seq for seq in str_seq]\n",
    "    this_doc.append(decorated_str_seq)\n",
    "    docs[paperid] = this_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"unlabeled_related_work/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "for doc_id, doc in docs.items():\n",
    "    with open(path+doc_id+\".txt\",\"w\") as f:\n",
    "        for paragraph in doc:\n",
    "            for line in paragraph:\n",
    "                f.write(line+\"\\n\")\n",
    "            f.write(\"\\n\")\n",
    "    with open(path+doc_id+\".ann\",\"w\") as f:\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
