{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b01c70f1",
   "metadata": {},
   "source": [
    "# An example code that tags any related work section texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4184214b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "#from util import *\n",
    "from data_util import scientific_sent_tokenize\n",
    "from joint_tagger import CorwaTagger\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b0abd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c7326be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "class ArgumentParser:\n",
    "    def __init__(self):\n",
    "        self.args = {}\n",
    "    \n",
    "    def add_argument(self,*args,**kwargs):\n",
    "        name = args[0][2:]\n",
    "        self.args[name] = kwargs.get(\"default\",None)\n",
    "        \n",
    "    def parse_args(self):\n",
    "        args = Args()\n",
    "        for k,v in self.args.items():\n",
    "            setattr(args, k, v)\n",
    "        return args\n",
    "    def __str__(self):\n",
    "        return self.args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67192c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "argparser = ArgumentParser()\n",
    "argparser.add_argument('--repfile', type=str, default = \"allenai/scibert_scivocab_uncased\", help=\"Word embedding file\")\n",
    "argparser.add_argument('--dropout', type=float, default=0, help=\"embedding_dropout rate\")\n",
    "argparser.add_argument('--bert_dim', type=int, default=768, help=\"bert_dimension\")\n",
    "argparser.add_argument('--MAX_SENT_LEN', type=int, default=512)\n",
    "argparser.add_argument('--checkpoint', type=str, default = \"/data/XiangciLi/checkpoints/joint_tagger/joint_tagger_train_scibert_final.model\")\n",
    "argparser.add_argument('--batch_size', type=int, default=32) # roberta-large: 2; bert: 8\n",
    "args = argparser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "511d7355",
   "metadata": {},
   "outputs": [],
   "source": [
    "related_work_text = \"\"\"\n",
    "Extractive Related Work Generation. Early related work generation systems employed the extractive summarization approach. Hoang and Kan (2010) pioneered the task, developing rules to select sentences following a topic hierarchy tree that was assumed to be given as input. Hu and Wan (2014) grouped sentences into topic-biased clusters with PLSA, modeled sentence importance with SVR, and applied a global optimization framework to select sentences. Chen and Zhuge (2019) se-lected sentences from papers that co-cited the same cited papers as the target paper in order to cover a minimum Steiner tree constructed from the paper's keywords. Wang et al. (2019) extracted Cited Text Spans (CTS), the matched text spans in the cited paper that are most related to a given citation. However, these extractive approaches aim to maximally cover the citation texts with the extracted sentences, thus mostly ignoring the reference type citations that are concise and abstractive ( §3.1.3). \n",
    "Abstractive Related Work Generation. Recently, Xing et al. (2020) extend the pointergenerator (See et al., 2017) to take two text inputs, allowing them to recover a masked citation sentence given its neighboring context sentences. Ge et al. (2021) encode the citation context, cited paper's abstract, and citation network and train their model with multiple objectives: sentence salience score regression of the cited paper's abstract, functional role classification of the citation sentence, and citation sentence generation. Chen et al. (2021) propose a relation-aware, multi-document encoder to generate a related work paragraph given a set of cited papers. Luu et al. (2021) fine-tune GPT2 (Radford et al., 2019) on scientific texts and explore several techniques for representing documents, such as using extracted named entities. All of the works described above focus on the generation aspect, while neglecting dataset collection; their datasets are mostly extracted automatically. Moreover, the datasets are not reused, though they are publicly available, because these works all use slightly different problem definitions, and thus the models are not directly comparable (Li and Ouyang, 2022) . In this work, we focus on collecting a dataset that is widely applicable to various related work generation settings, rather than proposing another incomparable approach. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff675322",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_paragraphs = {}\n",
    "count = 0\n",
    "for para in related_work_text.split(\"\\n\"):\n",
    "    if para:\n",
    "        generated_paragraphs[\"generated_\"+str(count)] = \" \".join(scientific_sent_tokenize(para))\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90ebbb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_related_work_json = {\n",
    "    \"paper_id\": \"generated\",\n",
    "    \"bib_entries\": {},\n",
    "    \"body_text\": []\n",
    "}\n",
    "for para_id, paragraph in generated_paragraphs.items():\n",
    "    pseudo_related_work_json[\"body_text\"].append({\n",
    "        \"section\": para_id,\n",
    "        \"text\": paragraph,\n",
    "        \"cite_spans\": [],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b7913c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_tagger_tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "additional_special_tokens = {'additional_special_tokens': ['[BOS]']}\n",
    "joint_tagger_tokenizer.add_special_tokens(additional_special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48dd95eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded!\n"
     ]
    }
   ],
   "source": [
    "tagger = CorwaTagger(joint_tagger_tokenizer, device, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23dab7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                | 0/1 [00:00<?, ?it/s]/home/xxl190027/anaconda3/envs/led/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2137: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.66it/s]\n",
      "2it [00:00, 584.21it/s]\n"
     ]
    }
   ],
   "source": [
    "all_span_citation_mappings = tagger.run_prediction(generated_paragraphs, {\"generated\": pseudo_related_work_json})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff77b3e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'generated_1',\n",
       "  'paragraph': \"[BOS] Abstractive Related Work Generation. [BOS] Recently, Xing et al. (2020) extend the pointergenerator (See et al., 2017) to take two text inputs, allowing them to recover a masked citation sentence given its neighboring context sentences. [BOS] Ge et al. (2021) encode the citation context, cited paper's abstract, and citation network and train their model with multiple objectives: sentence salience score regression of the cited paper's abstract, functional role classification of the citation sentence, and citation sentence generation. [BOS] Chen et al. (2021) propose a relation-aware, multi-document encoder to generate a related work paragraph given a set of cited papers. [BOS] Luu et al. (2021) fine-tune GPT2 (Radford et al., 2019) on scientific texts and explore several techniques for representing documents, such as using extracted named entities. [BOS] All of the works described above focus on the generation aspect, while neglecting dataset collection; their datasets are mostly extracted automatically. [BOS] Moreover, the datasets are not reused, though they are publicly available, because these works all use slightly different problem definitions, and thus the models are not directly comparable (Li and Ouyang, 2022) . [BOS] In this work, we focus on collecting a dataset that is widely applicable to various related work generation settings, rather than proposing another incomparable approach.\",\n",
       "  'discourse_tags': ['Transition',\n",
       "   'Single_summ',\n",
       "   'Single_summ',\n",
       "   'Single_summ',\n",
       "   'Single_summ',\n",
       "   'Transition',\n",
       "   'Narrative_cite',\n",
       "   'Reflection'],\n",
       "  'span_citation_mapping': [{'token_start': 11,\n",
       "    'token_end': 53,\n",
       "    'char_start': 59,\n",
       "    'char_end': 242,\n",
       "    'span_type': 'Dominant',\n",
       "    'span_citation_mapping': {'Dominant': {'Xing et al. (2020)': None},\n",
       "     'Reference': {'(See et al., 2017)': None}}},\n",
       "   {'token_start': 54,\n",
       "    'token_end': 109,\n",
       "    'char_start': 249,\n",
       "    'char_end': 544,\n",
       "    'span_type': 'Dominant',\n",
       "    'span_citation_mapping': {'Dominant': {'Ge et al. (2021)': None},\n",
       "     'Reference': {}}},\n",
       "   {'token_start': 110,\n",
       "    'token_end': 141,\n",
       "    'char_start': 551,\n",
       "    'char_end': 684,\n",
       "    'span_type': 'Dominant',\n",
       "    'span_citation_mapping': {'Dominant': {'Chen et al. (2021)': None},\n",
       "     'Reference': {}}},\n",
       "   {'token_start': 142,\n",
       "    'token_end': 185,\n",
       "    'char_start': 691,\n",
       "    'char_end': 865,\n",
       "    'span_type': 'Dominant',\n",
       "    'span_citation_mapping': {'Dominant': {'Luu et al. (2021)': None,\n",
       "      '(Radford et al., 2019)': None},\n",
       "     'Reference': {}}},\n",
       "   {'token_start': 227,\n",
       "    'token_end': 252,\n",
       "    'char_start': 1120,\n",
       "    'char_end': 1243,\n",
       "    'span_type': 'Reference',\n",
       "    'span_citation_mapping': {'Dominant': {},\n",
       "     'Reference': {'(Li and Ouyang, 2022)': None}}}]},\n",
       " {'id': 'generated_0',\n",
       "  'paragraph': \"[BOS] Extractive Related Work Generation. [BOS] Early related work generation systems employed the extractive summarization approach. [BOS] Hoang and Kan (2010) pioneered the task, developing rules to select sentences following a topic hierarchy tree that was assumed to be given as input. [BOS] Hu and Wan (2014) grouped sentences into topic-biased clusters with PLSA, modeled sentence importance with SVR, and applied a global optimization framework to select sentences. [BOS] Chen and Zhuge (2019) se-lected sentences from papers that co-cited the same cited papers as the target paper in order to cover a minimum Steiner tree constructed from the paper's keywords. [BOS] Wang et al. (2019) extracted Cited Text Spans (CTS), the matched text spans in the cited paper that are most related to a given citation. [BOS] However, these extractive approaches aim to maximally cover the citation texts with the extracted sentences, thus mostly ignoring the reference type citations that are concise and abstractive ( 3.1.3).\",\n",
       "  'discourse_tags': ['Transition',\n",
       "   'Transition',\n",
       "   'Single_summ',\n",
       "   'Single_summ',\n",
       "   'Single_summ',\n",
       "   'Single_summ',\n",
       "   'Single_summ'],\n",
       "  'span_citation_mapping': [{'token_start': 23,\n",
       "    'token_end': 54,\n",
       "    'char_start': 140,\n",
       "    'char_end': 289,\n",
       "    'span_type': 'Dominant',\n",
       "    'span_citation_mapping': {'Dominant': {'Hoang and Kan (2010)': None},\n",
       "     'Reference': {}}},\n",
       "   {'token_start': 55,\n",
       "    'token_end': 88,\n",
       "    'char_start': 296,\n",
       "    'char_end': 472,\n",
       "    'span_type': 'Dominant',\n",
       "    'span_citation_mapping': {'Dominant': {'Hu and Wan (2014)': None},\n",
       "     'Reference': {}}},\n",
       "   {'token_start': 89,\n",
       "    'token_end': 132,\n",
       "    'char_start': 479,\n",
       "    'char_end': 668,\n",
       "    'span_type': 'Dominant',\n",
       "    'span_citation_mapping': {'Dominant': {'Chen and Zhuge (2019)': None},\n",
       "     'Reference': {}}},\n",
       "   {'token_start': 133,\n",
       "    'token_end': 168,\n",
       "    'char_start': 675,\n",
       "    'char_end': 826,\n",
       "    'span_type': 'Dominant',\n",
       "    'span_citation_mapping': {'Dominant': {'Wang et al. (2019)': None},\n",
       "     'Reference': {}}}]}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_span_citation_mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf9ed06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
