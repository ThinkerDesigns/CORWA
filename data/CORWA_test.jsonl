{"id": "201740743_2", "paragraph": "[BOS] Training with noisy data: designs methods to generate noise in the text, mainly focusing on syntactic noise and semantic noise.\n[BOS] (Sperber et al., 2017 ) proposes a noise model based on automatic speech recognizer (ASR) error types, which consists of substitutions, deletions and insertions.\n[BOS] Their noise model samples the positions of words that should be altered in the source sentence.\n[BOS] Even training with synthetic noise data brings a large improvement in translating noisy data, Belinkov and Bisk (2017) shows that models mainly perform well on the same kind of noise that is introduced at training time, and they mostly fail to generalize to text with other kinds of noise.\n[BOS] Similar findings were outlined in and Anastasopoulos (2019) , which evaluated MT systems on natural and natural-like grammatical noise, specifically on English produced by non-native speakers.\n[BOS] Natural noise appears to be richer and more complex compared to synthetic noise, making it challenging to manually design a comprehensive set of noise to approximate real world settings.\n[BOS] In our work, we follow (Vaibhav et al., 2019) and synthesize the noisy text through back-translation.\n[BOS] There is no need to manually control the distribution of noise.\n[BOS] In terms of multi-task learning for machine translation, Tu et al. (2017) proposes to add a reconstructor on top of the decoder.\n[BOS] The auxiliary objective is to reconstruct the source sentence from the hidden layers of the translation decoder.\n[BOS] This encourages the decoder to embed complete source information, which helps improve the translation performance.\n[BOS] This approach was found to be helpful in low-resource MT scenarios also by Niu et al. (2019) .\n[BOS] Anastasopoulos and Chiang (2018) proposes a tied multitask learning model architecture to improve the speech translation task.\n[BOS] The intuition is that, speech transcription as an intermediate task, should improve the performance of speech translation if the speech translation is based on both the input speech and its transcription.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Transition", "Reflection", "Reflection", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 26, "token_end": 80, "char_start": 140, "char_end": 403, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Sperber et al., 2017": "21657379"}, "Reference": {}}}, {"token_start": 81, "token_end": 138, "char_start": 410, "char_end": 699, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Belinkov and Bisk (2017)": "3513372"}, "Reference": {}}}, {"token_start": 145, "token_end": 176, "char_start": 744, "char_end": 898, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Anastasopoulos (2019)": "165163810"}, "Reference": {}}}, {"token_start": 213, "token_end": 237, "char_start": 1111, "char_end": 1199, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vaibhav et al., 2019)": "67856759"}}}, {"token_start": 251, "token_end": 318, "char_start": 1276, "char_end": 1644, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Tu et al. (2017)": "15830483"}, "Reference": {}}}, {"token_start": 319, "token_end": 343, "char_start": 1651, "char_end": 1743, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Niu et al. (2019)": "53218398"}, "Reference": {}}}, {"token_start": 345, "token_end": 404, "char_start": 1752, "char_end": 2089, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Anastasopoulos and Chiang (2018)": "3351788"}, "Reference": {}}}]}
{"id": "201740743_1", "paragraph": "[BOS] Denoising text: Sakaguchi et al. (2017) proposes semi-character level recurrent neural network (scRNN) to correct words with scrambling characters.\n[BOS] Each word is represented as a vector with elements corresponding to the characters' position.\n[BOS] Heigold et al. (2018) investigates the robustness of character-based word embeddings in machine translation against word scrambling and random noise.\n[BOS] The experiments show that the noise has a larger influence on character-based models than BPE-based models.\n[BOS] To minimize the influence of word structure, Belinkov and Bisk (2017) proposes to represent word as its average character embeddings, which is invariant to these kinds of noise.\n[BOS] The proposed method enables the MT system to be more robust to scrambling noise even training the model with clean text.\n[BOS] Instead of handling noise at the word level, we try to recover the clean text from the noisy one at the sentence level.\n[BOS] Besides noise like word scrambling, the sentence level denoising could potentially better deal with more complex noise like grammatical errors.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection", "Transition"], "span_citation_mapping": [{"token_start": 2, "token_end": 51, "char_start": 6, "char_end": 253, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Sakaguchi et al. (2017)": "11217839"}, "Reference": {}}}, {"token_start": 52, "token_end": 105, "char_start": 260, "char_end": 523, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Heigold et al. (2018)": "19009822"}, "Reference": {}}}, {"token_start": 106, "token_end": 167, "char_start": 530, "char_end": 834, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Belinkov and Bisk (2017)": "3513372"}, "Reference": {}}}]}
{"id": "201740743_0", "paragraph": "[BOS] Here, we discuss how the MT community handles the noise problem.\n[BOS] In general, there are mainly two kinds of approaches: the first attempts to denoise text, and the second proposes training with noisy texts.\n\n", "discourse_tags": ["Reflection", "Transition"], "span_citation_mapping": []}
{"id": "184482991_4", "paragraph": "[BOS] The description of the current task is presented in detail in Zampieri et al. (2019b) , which clearly provides the context and underlying problem statement for this paper.\n\n", "discourse_tags": ["Single_summ"], "span_citation_mapping": [{"token_start": 13, "token_end": 38, "char_start": 68, "char_end": 177, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zampieri et al. (2019b)": "84843035"}, "Reference": {}}}]}
{"id": "184482991_3", "paragraph": "[BOS] These help in creating segment wise features to capture them separately.\n[BOS] Finally, methods in identifying profanity vs. hate speech is talked by Malmasi and Zampieri (2018) .\n[BOS] This work highlighted the challenges of distinguishing between profanity, and threatening language which may not actually contain profane language.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 17, "token_end": 68, "char_start": 94, "char_end": 339, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Malmasi and Zampieri (2018)": "3936688"}, "Reference": {}}}]}
{"id": "184482991_2", "paragraph": "[BOS] A proposal of typology of abusive language sub-tasks is presented in Waseem et al. (2017) where the author talks about how an offensive tweet can be categorized into four segments -Explicit, Implicit, Directed and Generalized abuse.\n\n", "discourse_tags": ["Single_summ"], "span_citation_mapping": [{"token_start": 3, "token_end": 54, "char_start": 8, "char_end": 238, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Waseem et al. (2017)": "8821211"}, "Reference": {}}}]}
{"id": "184482991_1", "paragraph": "[BOS] Papers published in the last two years include the surveys by Schmidt and Wiegand (2017) and Fortuna and Nunes (2018) where the authors extract features from the text like sentiment, linguistic features, utilize different lexical resources to tag an offensive tweet, and another paper by presenting the Hate Speech Detection data set used in Malmasi and Zampieri (2017) where the authors perform a three way classification -Hate Speech, Offensive and None.\n[BOS] By classifying these, the authors talk about specific patterns related to offensive terms.\n[BOS] It is found that the usage of cuss words like b*tch and n*gga is fond in both offensive and casual setting, while f*ggot and n*gger were predominantly used in hateful contexts.\n[BOS] One of the major takeaways was that lexical methods are effective to identify potentially offensive terms, but are inaccurate at identifying hate speech Other work include: ElSherief et al. (2018) ; Gambck and Sikdar (2017); Zhang et al. (2018) .\n\n", "discourse_tags": ["Multi_summ", "Multi_summ", "Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 13, "token_end": 55, "char_start": 68, "char_end": 271, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Schmidt and Wiegand (2017)": "9626793", "Fortuna and Nunes (2018)": "52184457"}, "Reference": {}}}, {"token_start": 62, "token_end": 115, "char_start": 309, "char_end": 559, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Malmasi and Zampieri (2017)": "19182892"}, "Reference": {}}}, {"token_start": 188, "token_end": 226, "char_start": 864, "char_end": 993, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Gamb\u00e4ck and Sikdar (2017);": null, "Zhang et al. (2018)": "46939253"}}}]}
{"id": "184482991_0", "paragraph": "[BOS] This issue has gathered a lot of attention over the past few years with various types of hate speech detection models.\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "201058633_2", "paragraph": "[BOS] Recognizing textual entailment The application of causes and effects to new situations has a strong connection to notions of entailment-ROPES tries to get systems to understand what is entailed by an expository paragraph.\n[BOS] The setup is fundamentally different, however: instead of giving systems pairs of sentences to classify as entailed or not, as in the traditional formulation (Dagan et al., 2006; Bowman et al., 2015 , inter alia), we give systems questions whose answers require understanding the entailment.\n\n", "discourse_tags": ["Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 54, "token_end": 93, "char_start": 292, "char_end": 446, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dagan et al., 2006;": null, "Bowman et al., 2015": "14604520"}}}]}
{"id": "201058633_1", "paragraph": "[BOS] The most closely related datasets to ROPES are ShARC (Saeidi et al., 2018) , OpenBookQA (Mihaylov et al., 2018) , and QuaRel (Tafjord et al., 2019) .\n[BOS] ShARC shares the same goal of understanding causes and effects (in terms of specified rules), but frames it as a dialogue where the system has to also generate questions to gain complete information.\n[BOS] OpenBookQA, similar to ROPES, requires reading scientific facts, but it is focused on a retrieval problem where a system must find the right fact for a question (and some additional common sense fact), whereas ROPES targets reading a given, complex passage of text, with no retrieval involved.\n[BOS] QuaRel is also focused on reasoning about situational effects in a question-answering setting, but the \"causes\" are all pre-specified, not read from a background passage, so the setting is limited.\n\n", "discourse_tags": ["Multi_summ", "Multi_summ", "Multi_summ", "Multi_summ"], "span_citation_mapping": [{"token_start": 8, "token_end": 195, "char_start": 43, "char_end": 865, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Saeidi et al., 2018)": "52165754", "(Mihaylov et al., 2018)": "52183757", "(Tafjord et al., 2019)": "53748665"}, "Reference": {}}}]}
{"id": "201058633_0", "paragraph": "[BOS] Reading comprehension There are many reading comprehension datasets (Richardson et al., 2013; Rajpurkar et al., 2016; Kwiatkowski et al., 2019; Dua et al., 2019) , the majority of which principally require understanding local predicateargument structure in a passage of text.\n[BOS] The success of recent models suggests that machines are becoming capable of this level of understanding.\n[BOS] ROPES challenges reading comprehension models to handle more difficult phenomena: understanding the implications of a passage of text.\n[BOS] ROPES is also particularly related to datasets focusing on \"multi-hop reasoning\" (Yang et al., 2018; Khashabi et al., 2018) , as by construction answering questions in ROPES requires connecting information from multiple parts of a given passage.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Multi_summ"], "span_citation_mapping": [{"token_start": 7, "token_end": 47, "char_start": 43, "char_end": 167, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Richardson et al., 2013;": "2100831", "Rajpurkar et al., 2016;": "11816014", "Kwiatkowski et al., 2019;": "86611921", "Dua et al., 2019)": "67855846"}}}, {"token_start": 118, "token_end": 160, "char_start": 600, "char_end": 785, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Yang et al., 2018;": "52822214", "Khashabi et al., 2018)": "5112038"}, "Reference": {}}}]}
{"id": "202537221_3", "paragraph": "[BOS] In this section, we first formally define the goal of adversarial defense and then introduce the proposed framework DISP, learning to discriminate perturbations, for blocking adversarial attacks.\n[BOS] Problem Statement.\n[BOS] Given an NLP model F (X), where X = {t 1 , .\n[BOS] .\n[BOS] .\n[BOS] , t N } is the input text of N tokens while t i indicates the i-th token.\n[BOS] A malicious attacker can add a few inconspicuous perturbations into the input text and generate an adversarial example X a so that F (X) = F (X a ) with unsatisfactory prediction performance.\n[BOS] For example, a perturbation can be an insertion, a deletion of a character in a token, a replacement of a token with its synonym.\n[BOS] In this paper, we aim to block adversarial attacks for general text classification models.\n[BOS] More specifically, we seek to preserve the model performances by recovering original input text and universally improve the robustness of any text classification model.\n[BOS] Figure 1 illustrates the overall schema of the proposed framework.\n[BOS] DISP consists of three components, (1) a perturbation discriminator, (2) an embedding estimator, and (3) a token embedding corpus with the corresponding small world graphs G. In the training phase, DISP constructs a corpus D from the original corpus for training the perturbation discriminator so that it is capable of recognizing the perturbed tokens.\n[BOS] The corpus of token embeddings C is then applied to train the embedding estimator to recover the removed tokens after establishing the small world graphs G of the embedding corpus.\n[BOS] In the prediction phase, for each token in testing data, the perturbation discriminator predicts if the token is perturbed.\n[BOS] For each potential perturbation that is potentially perturbed, the embedding estimator generates an approximate embedding vector and retrieve the token with the closest distance in the embedding space for token recovery.\n[BOS] Finally, the recovered testing data can be applied for prediction.\n[BOS] Note that the prediction model can be any NLP model.Moreover, DISP is a general framework for blocking adversarial attacks, so the model selection for the discriminator and estimator can also be flexible.\n\n", "discourse_tags": ["Reflection", "Transition", "Transition", "Other", "Other", "Transition", "Transition", "Transition", "Reflection", "Reflection", "Reflection", "Reflection", "Reflection", "Reflection", "Transition", "Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "202537221_2", "paragraph": "[BOS] Spelling correction (Mays et al., 1991; Islam and Inkpen, 2009 ) and grammar error correction (Sakaguchi et al., 2017) are useful tools which can block editorial adversarial attacks, such as swap and insertion.\n[BOS] However, they cannot handle cases where word-level attacks that do not cause spelling and grammar errors.\n[BOS] In our paper, we propose a general schema to block both word-level and character-level attacks.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 21, "char_start": 6, "char_end": 68, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mays et al., 1991;": null, "Islam and Inkpen, 2009": null}}}, {"token_start": 23, "token_end": 35, "char_start": 75, "char_end": 124, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sakaguchi et al., 2017)": "7833469"}}}]}
{"id": "202537221_1", "paragraph": "[BOS] However, limited efforts have been done on adversarial defense in the NLP fields.\n[BOS] Texts as discrete data are sensitive to the perturbations and cannot transplant most of the defense techniques from the image processing domain such as Gaussian denoising with autoencoders (Meng and Chen, 2017; Gu and Rigazio, 2014) .\n[BOS] Adversarial training is the prevailing counter-measure to build a robust model (Goodfellow et al., 2015; Iyyer et al., 2018; Marzinotto et al., 2019; Cheng et al., 2019; by mixing adversarial examples with the original ones during training the model.\n[BOS] However, these adversarial examples can be detected and deactivated by a genetic algorithm (Alzantot et al., 2018) .\n[BOS] This method also requires retraining, which can be time and cost consuming for large-scale models.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 44, "token_end": 65, "char_start": 246, "char_end": 326, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Meng and Chen, 2017;": null, "Gu and Rigazio, 2014)": "15538683"}}}, {"token_start": 79, "token_end": 117, "char_start": 401, "char_end": 503, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Goodfellow et al., 2015;": "6706414", "Iyyer et al., 2018;": "4956100", "Marzinotto et al., 2019;": "174800075"}}}, {"token_start": 147, "token_end": 159, "char_start": 665, "char_end": 706, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Alzantot et al., 2018)": "5076191"}}}]}
{"id": "202537221_0", "paragraph": "[BOS] Adversarial examples crafted by malicious attackers expose the vulnerability of deep neural networks when they are applied to down-streaming tasks, such as image recognition, speech processing, and text classifications (Wang et al., 2019; Goodfellow et al., 2015; Nguyen et al., 2015; Moosavi-Dezfooli et al., 2017) .\n[BOS] For adversarial attacks, white-box attacks have full access to the target model while black-box attacks can only explore the models by observing the outputs with limited trials.\n[BOS] Ebrahimi et al. (2017) propose a gradient-based white-box model to attack character-level classifiers via an atomic flip operation.\n[BOS] Small character-level transformations, such as swap, deletion, and insertion, are applied on critical tokens identified with a scoring strategy (Gao et al., 2018) or gradient-based computation (Liang et al., 2017) .\n[BOS] Samanta and Mehta (2017) ; Alzantot et al. (2018) replace words with semantically and syntactically similar adversarial examples.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Single_summ", "Multi_summ", "Multi_summ"], "span_citation_mapping": [{"token_start": 31, "token_end": 79, "char_start": 162, "char_end": 321, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wang et al., 2019;": "67773053", "Goodfellow et al., 2015;": "6706414", "Nguyen et al., 2015;": "206592585", "Moosavi-Dezfooli et al., 2017)": "11558223"}}}, {"token_start": 116, "token_end": 147, "char_start": 514, "char_end": 645, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ebrahimi et al. (2017)": "21698802"}, "Reference": {}}}, {"token_start": 171, "token_end": 181, "char_start": 779, "char_end": 814, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Gao et al., 2018)": "4858173"}}}, {"token_start": 182, "token_end": 194, "char_start": 818, "char_end": 865, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Liang et al., 2017)": "10642653"}}}, {"token_start": 196, "token_end": 226, "char_start": 874, "char_end": 1003, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Samanta and Mehta (2017)": "38134825", "Alzantot et al. (2018)": "5076191"}, "Reference": {}}}]}
{"id": "195847865_3", "paragraph": "[BOS] Most of the existing work discussed so far is based on the PDTB dataset, which targets formal texts like news, making it less suitable for our task which is centered around informal dialogue.\n[BOS] Related work on discourse relation annotation in a dialogue corpus is limited (Stent, 2000; Tonelli et al., 2010) .\n[BOS] For example Tonelli et al. (2010) annotated the Luna corpus, 3 which does not include English annotations.\n[BOS] To our knowledge there is no English dialogue-based corpus with implicit discourse relation labels, as such research specifically targeting a discourse relation identification model for social open-domain dialogue remains unexplored.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 43, "token_end": 66, "char_start": 220, "char_end": 317, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Stent, 2000;": "6568223", "Tonelli et al., 2010)": "2367993"}}}, {"token_start": 70, "token_end": 93, "char_start": 338, "char_end": 432, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Tonelli et al. (2010)": "2367993"}, "Reference": {}}}]}
{"id": "195847865_2", "paragraph": "[BOS] This work is further extended by methods for selecting high-quality samples (Rutherford and Xue, 2015; Xu et al., 2018; Braud and Denis, 2014; Wang et al., 2012) .\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 9, "token_end": 46, "char_start": 47, "char_end": 167, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rutherford and Xue, 2015;": "6428039", "Xu et al., 2018;": "53083237", "Braud and Denis, 2014;": "1470863", "Wang et al., 2012)": "259234"}}}]}
{"id": "195847865_1", "paragraph": "[BOS] To utilize machine learning models for this task, larger datasets would provide a bigger optimization space (Li and Nenkova, 2014) .\n[BOS] Marcu and Echihabi (2002) is the first work to generate artificial samples to extend the dataset by using rules to convert explicit discourse relation pairs into implicit pairs by dropping the connectives.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ"], "span_citation_mapping": [{"token_start": 12, "token_end": 28, "char_start": 63, "char_end": 136, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Li and Nenkova, 2014)": "11624810"}}}, {"token_start": 30, "token_end": 69, "char_start": 145, "char_end": 350, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Marcu and Echihabi (2002)": "210363"}, "Reference": {}}}]}
{"id": "195847865_0", "paragraph": "[BOS] The release of the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) makes research on machine learning based implicit discourse relation recognition possible.\n[BOS] Most previous work is based on linguistic and semantic features such as word pairs and brown cluster pair representation (Pitler et al., 2008; Lin et al., 2009) or rulebased systems (Wellner et al., 2006) .\n[BOS] Recent work has proposed neural network based models with attention or advanced representations, such as CNN (Qin et al., 2016) , attention on neural tensor network (Guo et al., 2018) , and memory networks (Jia et al., 2018) .\n[BOS] Advanced representations may help to achieve higher performance (Bai and Zhao, 2018) .\n[BOS] Some methods also consider context paragraphs and inter-paragraph dependency (Dai and Huang, 2018) .\n\n", "discourse_tags": ["Single_summ", "Narrative_cite", "Narrative_cite", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 6, "token_end": 35, "char_start": 25, "char_end": 168, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Prasad et al., 2008)": "13374927"}, "Reference": {}}}, {"token_start": 50, "token_end": 71, "char_start": 258, "char_end": 335, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Pitler et al., 2008;": "464400", "Lin et al., 2009)": "1421908"}}}, {"token_start": 72, "token_end": 84, "char_start": 339, "char_end": 379, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wellner et al., 2006)": "15893207"}}}, {"token_start": 102, "token_end": 111, "char_start": 493, "char_end": 515, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Qin et al., 2016)": "7284112"}}}, {"token_start": 112, "token_end": 125, "char_start": 518, "char_end": 571, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Guo et al., 2018)": "52013559"}}}, {"token_start": 127, "token_end": 137, "char_start": 578, "char_end": 612, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Jia et al., 2018)": "51876089"}}}, {"token_start": 139, "token_end": 154, "char_start": 621, "char_end": 705, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bai and Zhao, 2018)": "49744397"}}}, {"token_start": 163, "token_end": 174, "char_start": 764, "char_end": 812, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dai and Huang, 2018)": "4939018"}}}]}
{"id": "196206239_0", "paragraph": "[BOS] Previous work has shown that modeling locality benefits SANs for certain tasks.\n[BOS] Luong et al. (2015) proposed a Gaussian-based local attention with a predictable position; Sperber et al. (2018) differently applied a local method with variable window size for acoustic task; Yang et al. (2018) investigated the affect of the dynamical local Gaussian bias by combining these two approaches for the translation task.\n[BOS] Different from these methods using a learnable local scope, Yang et al. (2019b) and Wu et al. (2019) restricted the attention area with fixed size by borrowing the concept of convolution into SANs.\n[BOS] Although both these methods yield considerable improvements, they to some extent discard long-distance dependencies and the global information.\n[BOS] On the contrary, other researchers observed that global feature fusion is one of the salient advantages of SANs.\n[BOS] Shen et al. (2018) and Yu et al. (2018) succeeded to employ SANs on capturing global context for their downstream NLP tasks.\n[BOS] Recent works also suggested that such the contextual information can improve word sense disambiguation (Zhang et al., 2017a) , dependency parsing (Choi et al., 2017) and semantic modeling (Yang et al., 2019a) .\n[BOS] For exploring the contribution of them, our work integrates both the local and global information under a unified framework.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Multi_summ", "Multi_summ", "Transition", "Multi_summ", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 16, "token_end": 35, "char_start": 92, "char_end": 181, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Luong et al. (2015)": "1998416"}, "Reference": {}}}, {"token_start": 36, "token_end": 57, "char_start": 183, "char_end": 283, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Sperber et al. (2018)": null}, "Reference": {}}}, {"token_start": 58, "token_end": 84, "char_start": 285, "char_end": 424, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Yang et al. (2018)": "53081403"}, "Reference": {}}}, {"token_start": 89, "token_end": 153, "char_start": 460, "char_end": 778, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Yang et al. (2019b)": "102351629", "Wu et al. (2019)": "59310641"}, "Reference": {}}}, {"token_start": 175, "token_end": 205, "char_start": 904, "char_end": 1028, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Shen et al. (2018)": "4564356", "Yu et al. (2018)": "4842909"}, "Reference": {}}}, {"token_start": 217, "token_end": 230, "char_start": 1112, "char_end": 1159, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhang et al., 2017a)": "13733385"}}}, {"token_start": 231, "token_end": 241, "char_start": 1162, "char_end": 1200, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Choi et al., 2017)": "7037110"}}}, {"token_start": 242, "token_end": 254, "char_start": 1205, "char_end": 1243, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yang et al., 2019a)": null}}}]}
{"id": "174802358_2", "paragraph": "[BOS] Neural Network Based: Neural network based approaches do not rely on hand-crafted rules, but instead use an encoder-decoder architecture which can be trained in an end-to-end fashion to automatically generate questions from text.\n[BOS] Several neural network based approaches (Du et al., 2017; Kumar et al., 2018a,b) have been proposed for automatic question generation from text.\n[BOS] Du et al. (2017) propose a sequence to sequence model for automatic question generation from English text.\n[BOS] Kumar et al. (2018a) use a rich set of linguistic features and encode pivotal answers predicted using a pointer network based model to automatically generate a question for the encoded answer.\n[BOS] All existing models optimize a crossentropy based loss function, that suffers from exposure bias (Ranzato et al., 2016) .\n[BOS] Further, existing methods do not directly address the problem of handling important rare words and word repetition in QG.\n[BOS] Kumar et al. (2018b) propose a reinforcement learning based framework which addresses the problem of exposure bias, word repetition and rare words.\n[BOS] Tang et al. (2017) and Wang et al. (2017) propose a joint model to address QG and the question answering problem together.\n[BOS] All prior work on QG assumed access to a sufficiently large number of training instances for a language.\n[BOS] We relax this assumption in our work as we only have access to a small question answering dataset in the primary language.\n[BOS] We show how we can improve QG performance on the primary language by leveraging a larger question answering dataset in a secondary language.\n[BOS] (Similarly in spirit, cross-lingual transfer learning based approaches have been recently proposed for other NLP tasks such as machine translation (Schuster et al., 2019; Lample and Conneau, 2019 In Algorithm 1, we outline our training procedure and Figure 2 illustrates the overall architecture of our QG system.\n[BOS] Our cross-lingual QG model consists of two encoders and two decoders specific to each language.\n[BOS] We also enforce shared layers in both the encoder and the decoder whose weights are updated using data in both languages.\n[BOS] (This weight sharing is discussed in more detail in Section 3.3.)\n[BOS] For the encoder and decoder layers, we use the newly released Transformer (Vaswani et al., 2017) model that has shown great success compared to recurrent neural network-based models in neural machine translation.\n[BOS] Encoders and decoders consist of a stack of four identical layers, of which two layers are independently trained and two are trained in a shared manner.\n[BOS] Each layer of the transformer consists of a multi-headed selfattention model followed by a position-wise fully connected feed-forward network.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Single_summ", "Single_summ", "Narrative_cite", "Transition", "Single_summ", "Multi_summ", "Transition", "Reflection", "Reflection", "Reflection", "Reflection", "Reflection", "Reflection", "Reflection", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 50, "token_end": 69, "char_start": 250, "char_end": 319, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Du et al., 2017;": "2172129"}}}, {"token_start": 83, "token_end": 104, "char_start": 393, "char_end": 499, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Du et al. (2017)": "2172129"}, "Reference": {}}}, {"token_start": 105, "token_end": 141, "char_start": 506, "char_end": 698, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kumar et al. (2018a)": "3830722"}, "Reference": {}}}, {"token_start": 157, "token_end": 169, "char_start": 788, "char_end": 824, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ranzato et al., 2016)": "7147309"}}}, {"token_start": 194, "token_end": 222, "char_start": 961, "char_end": 1108, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kumar et al. (2018b)": "52011757"}, "Reference": {}}}, {"token_start": 223, "token_end": 253, "char_start": 1115, "char_end": 1237, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Tang et al. (2017)": "37738077", "Wang et al. (2017)": "1356821"}, "Reference": {}}}, {"token_start": 350, "token_end": 371, "char_start": 1758, "char_end": 1826, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Schuster et al., 2019;": "53110354", "Lample and Conneau, 2019": null}}}, {"token_start": 469, "token_end": 480, "char_start": 2315, "char_end": 2349, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vaswani et al., 2017)": "13756489"}}}]}
{"id": "174802358_1", "paragraph": "[BOS] Rule-based: Rule-based approaches (Heilman, 2011) mainly rely on manually curated rules for transforming a declarative sentence into an interrogative sentence.\n[BOS] The quality of the questions generated using rule-based systems highly depends on the quality of the handcrafted rules.\n[BOS] Manually curating a large number of rules for a new language is a tedious and challenging task.\n[BOS] More recently, Zheng et al. (2018) propose a template-based technique to construct questions from Chinese text, where they rank generated questions using a neural model and select the topranked question as the final output.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 6, "token_end": 79, "char_start": 18, "char_end": 393, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Heilman, 2011)": "2387668"}, "Reference": {}}}, {"token_start": 83, "token_end": 124, "char_start": 415, "char_end": 623, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zheng et al. (2018)": "53098184"}, "Reference": {}}}]}
{"id": "174802358_0", "paragraph": "[BOS] Prior work in QG from text can be classified into two broad categories.\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "203905453_1", "paragraph": "[BOS] Paraphrasing can be performed using neural networks with an encoder-decoder configuration, including sequence to sequence (S2S) (Luong et al., 2015) and generative models (Bowman et al., 2016) allow for control of the output distribution of the data generation (Yan et al., 2015; Hu et al., 2018) .\n[BOS] Unlike the typical paraphrasing task we care about the lexical diversity and novelty of the generated output.\n[BOS] This has been a concern in paraphrase generation: a generator that only produces trivial outputs can still perform fairly well in terms of typical paraphrasing evaluation metrics, despite the output being of little use.\n[BOS] Alternative metrics have been proposed to encourage more diverse outputs (Shima and Mitamura, 2011) .\n[BOS] Typically evaluation of paraphrasing or text generation tasks is performed by using a similarity metric (usually some variant of BLEU (Papineni et al., 2002) ) calculated against a held-out set (Prakash et al., 2016; Rajeswar et al., 2017; Yu et al., 2017) .\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 19, "token_end": 36, "char_start": 107, "char_end": 154, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Luong et al., 2015)": "1998416"}}}, {"token_start": 37, "token_end": 48, "char_start": 159, "char_end": 198, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bowman et al., 2016)": "748227"}}}, {"token_start": 57, "token_end": 74, "char_start": 251, "char_end": 302, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yan et al., 2015;": "7577075", "Hu et al., 2018)": "20981275"}}}, {"token_start": 145, "token_end": 158, "char_start": 695, "char_end": 752, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Shima and Mitamura, 2011)": null}}}, {"token_start": 175, "token_end": 194, "char_start": 847, "char_end": 918, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Papineni et al., 2002)": "11080756"}}}, {"token_start": 198, "token_end": 227, "char_start": 942, "char_end": 1017, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Prakash et al., 2016;": "9385494", "Rajeswar et al., 2017;": "397556", "Yu et al., 2017)": "3439214"}}}]}
{"id": "203905453_0", "paragraph": "[BOS] Neural networks have revolutionized the field of text generation, in machine translation (Sutskever et al., 2014) , summarization (See et al., 2017) and image captioning (You et al., 2016) .\n[BOS] However, conditional text generation has been relatively less studied as compared to conditional image generation and poses some unique problems.\n[BOS] One of the issues is the non-differentiability of the sampled text that limits the applicability of a global discriminator in end-to-end training.\n[BOS] The problem has been relatively addressed by using CNNs for generation (Rajeswar et al., 2017) , policy gradient reinforcement learning methods including SeqGAN (Yu et al., 2017) , LeakGAN (Guo et al., 2018) , or using latent representation like Gumbel softmax ((Jang et al., 2016) ).\n[BOS] Many of these approaches suffer from high training variance, mode collapse or cannot be evaluated beyond a qualitative analysis.\n[BOS] Many models have been proposed for text generation.\n[BOS] Seq2seq models are standard encoderdecoder models widely used in text applications like machine translation (Luong et al., 2015) and paraphrasing (Prakash et al., 2016) .\n[BOS] Variational Auto-Encoder (VAE) models are another important family (Kingma and Welling, 2013) and they consist of an encoder that maps each sample to a latent representation and a decoder that generates samples from the latent space.\n[BOS] The advantage of these models is the variational component and its potential to add diversity to the generated data.\n[BOS] They have been shown to work well for text generation (Bowman et al., 2016) .\n[BOS] Conditional VAE (CVAE) (Kingma et al., 2014) was proposed to improve over seq2seq models for generating more diverse and relevant text.\n[BOS] CVAE based models (Serban et al., 2017; Shen et al., 2017; Zhou and Wang, 2018) incorporate stochastic latent variables that represents the generated text, and append the output of VAE as an additional input to decoder.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Narrative_cite", "Transition", "Transition", "Narrative_cite", "Narrative_cite", "Transition", "Narrative_cite", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 14, "token_end": 26, "char_start": 75, "char_end": 119, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sutskever et al., 2014)": "7961699"}}}, {"token_start": 27, "token_end": 37, "char_start": 122, "char_end": 154, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(See et al., 2017)": null}}}, {"token_start": 38, "token_end": 50, "char_start": 159, "char_end": 194, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(You et al., 2016)": "3120635"}}}, {"token_start": 115, "token_end": 129, "char_start": 559, "char_end": 602, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rajeswar et al., 2017)": "397556"}}}, {"token_start": 136, "token_end": 146, "char_start": 662, "char_end": 686, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yu et al., 2017)": "3439214"}}}, {"token_start": 147, "token_end": 157, "char_start": 689, "char_end": 715, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Guo et al., 2018)": "3389583"}}}, {"token_start": 163, "token_end": 177, "char_start": 754, "char_end": 789, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"((Jang et al., 2016)": "2428314"}}}, {"token_start": 228, "token_end": 239, "char_start": 1080, "char_end": 1120, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Luong et al., 2015)": "1998416"}}}, {"token_start": 240, "token_end": 252, "char_start": 1125, "char_end": 1160, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Prakash et al., 2016)": "9385494"}}}, {"token_start": 254, "token_end": 276, "char_start": 1169, "char_end": 1262, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 331, "token_end": 342, "char_start": 1570, "char_end": 1607, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bowman et al., 2016)": "748227"}}}, {"token_start": 344, "token_end": 360, "char_start": 1616, "char_end": 1660, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kingma et al., 2014)": null}}}, {"token_start": 378, "token_end": 404, "char_start": 1758, "char_end": 1837, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Serban et al., 2017;": "14857825", "Shen et al., 2017;": "9346726", "Zhou and Wang, 2018)": "3033303"}}}]}
{"id": "202558976_0", "paragraph": "[BOS] Pretrained language models Pretrained language models based on an LSTM (Peters et al., 2018a; Howard and Ruder, 2018 ) and a Transformer (Radford et al., 2018; Devlin et al., 2018) have been proposed.\n[BOS] Recent work (Peters et al., 2018b) suggests that-all else being equal-an LSTM outperforms the Transformer in terms of downstream performance.\n[BOS] For this reason, we use a variant of the LSTM as our language model.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 12, "token_end": 31, "char_start": 69, "char_end": 124, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Peters et al., 2018a;": "3626819", "Howard and Ruder, 2018": "40100965"}}}, {"token_start": 32, "token_end": 51, "char_start": 129, "char_end": 186, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Devlin et al., 2018)": "52967399"}}}, {"token_start": 58, "token_end": 87, "char_start": 225, "char_end": 354, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Peters et al., 2018b)": "52098907"}, "Reference": {}}}]}
{"id": "202763233_0", "paragraph": "[BOS] In this section, we briefly review recent advancement in encoder-decoder models and CVAE-based models for response generation.\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "201679049_3", "paragraph": "[BOS] The UniMorph 2.0 data-set (Kirov et al., 2018) provides resources for morphosyntactic analysis across 111 different languages.\n[BOS] The work described here uses the tag set from UniMorph.\n\n", "discourse_tags": ["Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 3, "token_end": 33, "char_start": 10, "char_end": 132, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Kirov et al., 2018)": null}, "Reference": {}}}]}
{"id": "201679049_2", "paragraph": "[BOS] In order to make embeddings useful across languages, we need a method for aligning embedding spaces across languages.\n[BOS] Ruder et al. (2017) provide an excellent survey of methods for aligning embedding spaces.\n[BOS] Mikolov et al. (2013a) introduce a translation matrix for aligning embeddings spaces in different languages and show how this is useful for machine translation purposes.\n[BOS] We adopt this approach to do alignment at the verse level.\n[BOS] Alignment with contextual embeddings is more complicated, since the embeddings are dynamic by their very nature (different across different contexts).\n[BOS] In order to align these dynamic embeddings, Schuster et al. (2019) introduce a number of methods, however they all require either a supervised dictionary for each language, or access to the MUSE framework for alignment, neither of which we assume in our work.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Reflection", "Transition", "Single_summ"], "span_citation_mapping": [{"token_start": 24, "token_end": 44, "char_start": 130, "char_end": 219, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ruder et al. (2017)": null}, "Reference": {}}}, {"token_start": 45, "token_end": 78, "char_start": 226, "char_end": 395, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Mikolov et al. (2013a)": "1966640"}, "Reference": {}}}, {"token_start": 123, "token_end": 171, "char_start": 648, "char_end": 883, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Schuster et al. (2019)": "67856005"}, "Reference": {}}}]}
{"id": "201679049_1", "paragraph": "[BOS] Both of these properties -sensitivity to context and the ability to capture sub-word informationmake contextual embeddings suitable for the task at hand.\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "201679049_0", "paragraph": "[BOS] The core idea of using the Bible as parallel data in low-resource settings is largely inspired by previous work.\n[BOS] The Bible has been used as a means of alignment for cross-lingual projection, both for POS tagging (Agic et al., 2015) and for dependency parsing (Agic et al., 2016) , as well as for base noun-phrase bracketing, named-entity tagging, and morphological analysis (Yarowsky et al., 2001 ) with promising results.\n[BOS] Peters et al. (2018) introduce ELMo embeddings, contextual word embeddings which incorporate character-level information using a CNN.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Single_summ"], "span_citation_mapping": [{"token_start": 46, "token_end": 57, "char_start": 212, "char_end": 243, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Agic et al., 2015)": "18824729"}}}, {"token_start": 59, "token_end": 70, "char_start": 252, "char_end": 290, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Agic et al., 2016)": null}}}, {"token_start": 88, "token_end": 99, "char_start": 363, "char_end": 408, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yarowsky et al., 2001": "15279538"}}}, {"token_start": 105, "token_end": 130, "char_start": 441, "char_end": 574, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Peters et al. (2018)": "3626819"}, "Reference": {}}}]}
{"id": "67855860_3", "paragraph": "[BOS] We hope our work motivates further development of these methods, resulting in attention variants that both improve predictive performance and provide insights into model predictions.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "67855860_2", "paragraph": "[BOS] More specific to attention mechanisms, recent promising work has proposed more principled attention variants designed explicitly for interpretability; these may provide greater transparency by imposing hard, sparse attention.\n[BOS] Such instantiations explicitly select (modest) subsets of inputs to be considered when making a prediction, which are then by construction responsible for model output (Lei et al., 2016; Peters et al., 2018) .\n[BOS] Structured attention models (Kim et al., 2017) provide a generalized framework for describing and fitting attention variants with explicit probabilistic semantics.\n[BOS] Tying attention weights to human-provided rationales is another potentially promising avenue (Bao et al., 2018) .\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 41, "token_end": 80, "char_start": 269, "char_end": 445, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lei et al., 2016;": "7205805", "Peters et al., 2018)": "53604866"}}}, {"token_start": 82, "token_end": 108, "char_start": 454, "char_end": 617, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Kim et al., 2017)": "6961760"}, "Reference": {}}}, {"token_start": 109, "token_end": 133, "char_start": 624, "char_end": 735, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bao et al., 2018)": "52113465"}}}]}
{"id": "67855860_1", "paragraph": "[BOS] Comparing such importance measures to attention scores may provide additional insights into the working of attention based models (Ghaeini et al., 2018) .\n[BOS] Another novel line of work in this direction involves explicitly identifying explanations of black-box predictions via a causal framework (Alvarez-Melis and Jaakkola, 2017).\n[BOS] We also note that there has been complementary work demonstrating low correlation between human attention and induced attention weights (Pappas and Popescu-Belis, 2016) .\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 17, "token_end": 30, "char_start": 113, "char_end": 158, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ghaeini et al., 2018)": "51979567"}}}, {"token_start": 41, "token_end": 68, "char_start": 221, "char_end": 340, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 79, "token_end": 101, "char_start": 413, "char_end": 515, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Pappas and Popescu-Belis, 2016)": "12664997"}}}]}
{"id": "67855860_0", "paragraph": "[BOS] We have focused on attention mechanisms and the question of whether they afford transparency, but a number of interesting strategies unrelated to attention mechanisms have been recently proposed to provide insights into neural NLP models.\n[BOS] These include approaches that measure feature importance based on gradient information (Ross et al., 2017; Sundararajan et al., 2017) (aligned with the gradient-based measures that we have used here), and methods based on representation erasure , in which dimensions are removed and then the resultant change in output is recorded (similar to our experiments with removing tokens from inputs, albeit we do this at the input layer).\n\n", "discourse_tags": ["Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 45, "token_end": 70, "char_start": 281, "char_end": 384, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ross et al., 2017;": "7053611", "Sundararajan et al., 2017)": "16747630"}}}]}
{"id": "189928048_3", "paragraph": "[BOS] The current state of the art on GED is based on augmenting neural approaches with artificially generated training data.\n[BOS] showed improved GED performance using the bi-LSTM sequence labeler, by generating artificial errors in two different ways: 1) learning frequent error patterns from error-annotated corpora and applying these to error-free text; 2) using a statistical MT approach to \"translate\" correct text to its incorrect counterpart using parallel corpora.\n[BOS] Recently, Kasewa et al. (2018) applied the latter approach using a neural MT system instead, and achieved a new state of the art on GED using the neural model of Rei (2017) .\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ"], "span_citation_mapping": [{"token_start": 91, "token_end": 132, "char_start": 491, "char_end": 655, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kasewa et al. (2018)": "52896498"}, "Reference": {"Rei (2017)": "16386838"}}}]}
{"id": "189928048_2", "paragraph": "[BOS] Recently, Rei and Yannakoudakis (2016) presented the first approach towards neural GED, training a sequence labeling model based on word embeddings processed by a bidirectional LSTM (bi-LSTM), outputting a probability distribution over labels informed by the entire sentence as context.\n[BOS] This approach achieves strong results when trained and evaluated on in-domain data, but shows weaker generalization performance on outof-domain data.\n[BOS] extended this model to include character embeddings in order to capture morphological similarities such as word endings.\n[BOS] Rei (2017) subsequently added a secondary LM objective to the neural sequence labeling architecture, operating on both word and character-level embeddings.\n[BOS] This was found to be particularly useful for GED -introducing an LM objective allows the network to learn more generic features about language and composition.\n[BOS] At the same time, investigated the effectiveness of a number of auxiliary (morpho-syntactic) training objectives for the task of GED, finding that predicting part-ofspeech tags, grammatical relations or error types as auxiliary tasks yields improvements in performance over the single-task GED objective (though not as high as when utilizing an LM objective).\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 87, "char_start": 6, "char_end": 448, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Rei and Yannakoudakis (2016)": "1521197"}, "Reference": {}}}, {"token_start": 108, "token_end": 164, "char_start": 582, "char_end": 903, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Rei (2017)": "16386838"}, "Reference": {}}}]}
{"id": "189928048_1", "paragraph": "[BOS] As a distinct task, GEC has been formulated as a nave-bayes classification (Rozovskaya et al., 2013 (Rozovskaya et al., , 2014 Rozovskaya and Roth, 2016) or a monolingual (statistical or neural) machine translation (MT) problem (where uncorrected text is treated as the source \"language\" and the corrected text as its target counterpart) (Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014; Rozovskaya and Roth, 2016; Yuan and Briscoe, 2016) .\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 14, "token_end": 52, "char_start": 55, "char_end": 159, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rozovskaya et al., 2013": "1632958", "(Rozovskaya et al., , 2014": "1547333", "Rozovskaya and Roth, 2016)": "18563136"}}}, {"token_start": 54, "token_end": 131, "char_start": 165, "char_end": 456, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Felice et al., 2014;": "16548363", "Junczys-Dowmunt and Grundkiewicz, 2014;": "18318073", "Rozovskaya and Roth, 2016;": "18563136", "Yuan and Briscoe, 2016)": "16766006"}}}]}
{"id": "189928048_0", "paragraph": "[BOS] In this section, we describe previous work on GED and on the related task of GEC.\n[BOS] While error correction systems can be used for error detection, previous work has shown that standalone error detection models can be complementary to error correction ones, and can be used to further improve performance on GEC .\n[BOS] Early approaches to GED and GEC relied upon handwritten rules and error grammars (e.g. Foster and Vogel (2004) ), while later work focused on supervised learning from error-annotated corpora using feature engineering approaches and often utilizing maximum entropy-based classifiers (e.g. Chodorow et al. (2007) ; De Felice and Pulman (2008) ).\n[BOS] A large range of work has focused on the development of systems targeting specific error types, such as preposition (Tetreault and Chodorow, 2008; Chodorow et al., 2007) , article usage (Han et al., 2004 (Han et al., , 2006 , and verb form errors (Lee and Seneff, 2008) .\n[BOS] Among others, errortype agnostic approaches have focused on generating synthetic ungrammatical data to augment the available training sets, or learning from native English datasets; for example, Foster and Andersen (2009) investigate rule-based error generation methods, while Gamon (2010) trains a language model (LM) on a large, general domain corpus, from which features (e.g. word likelihoods) are derived for use in error classification.\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite", "Narrative_cite", "Multi_summ"], "span_citation_mapping": [{"token_start": 73, "token_end": 90, "char_start": 374, "char_end": 440, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Foster and Vogel (2004)": null}}}, {"token_start": 110, "token_end": 140, "char_start": 568, "char_end": 670, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Chodorow et al. (2007)": "1445945", "Felice and Pulman (2008)": "11574043"}}}, {"token_start": 162, "token_end": 184, "char_start": 784, "char_end": 849, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Tetreault and Chodorow, 2008;": "17110547", "Chodorow et al., 2007)": "1445945"}}}, {"token_start": 185, "token_end": 202, "char_start": 852, "char_end": 903, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Han et al., 2004": "10368042", "(Han et al., , 2006": "44557034"}}}, {"token_start": 204, "token_end": 215, "char_start": 910, "char_end": 949, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lee and Seneff, 2008)": "14222290"}}}, {"token_start": 251, "token_end": 264, "char_start": 1153, "char_end": 1227, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 266, "token_end": 306, "char_start": 1235, "char_end": 1400, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Gamon (2010)": null}, "Reference": {}}}]}
{"id": "202539806_1", "paragraph": "[BOS] Overall, the broad finding has been that it is important to either be provided with or learn to choose pivotal answer spans to ask questions about from an input passage.\n[BOS] Founded on this observation, our system facilitates users with an option to either choose answer spans from the pre-populated set of named entities and noun phrases or manually select custom answer spans interactively.\n[BOS] Our system, ParaQG, presented in this paper uses a novel four-stage procedure: (1) text review, (2) pivotal answer selection (3) automatic question generation pertaining to the selected answer, and (4) filtering and grouping questions based on confidence scores and different facets of the selected answer.\n\n", "discourse_tags": ["Transition", "Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "202539806_0", "paragraph": "[BOS] Automatically generating questions and answers from text is a challenging task.\n[BOS] This task can be traced back to 1976 when Wolfe (1976) presented their system AUTOQUEST, which examined the generation of Wh-questions from single sentences.\n[BOS] This was followed by several pattern matching (Hirschman et al., 1999) and linear regression (Ng et al., 2000) based models.\n[BOS] These approaches are heavily dependent on either rules or question templates, and require deep linguistic knowledge, yet are not exhaustive enough.\n[BOS] Recent successes in neural machine translation (Sutskever et al., 2014; have helped address these issues by letting deep neural nets learn the implicit rules from data.\n[BOS] This approach has inspired application of sequence-to-sequence learning to automated question generation.\n[BOS] Serban et al. (2016) proposed an attention-based Luong et al., 2015) approach to question generation from a pre-defined template of knowledge base triples (subject, relation, object).\n[BOS] We proposed multi-hop question generation (Kumar et al., 2019a ) from knowledge graphs using transformers (Vaswani et al., 2017) .\n[BOS] Du et al. (2017) proposed an attention-based sequence learning approach to question generation.\n[BOS] Most existing work focuses on generating questions from text without concerning itself with answ er generation.\n[BOS] In our previous work (Kumar et al., 2018), we presented a pointer networkbased model that predicts candidate answers and generates a question by providing a pivotal answer as an input to the decoder.\n[BOS] Our model for question generation combines a rich set of linguistic features, pointer network-based answer selection, and an improved decoder, and is able to generate questions that are relatively more relevant to the given sentence than the questions generated without the answer signal.\n\n", "discourse_tags": ["Transition", "Single_summ", "Narrative_cite", "Transition", "Narrative_cite", "Transition", "Single_summ", "Narrative_cite", "Single_summ", "Transition", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 24, "token_end": 48, "char_start": 134, "char_end": 249, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 54, "token_end": 65, "char_start": 285, "char_end": 326, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hirschman et al., 1999)": "15197674"}}}, {"token_start": 66, "token_end": 76, "char_start": 331, "char_end": 366, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ng et al., 2000)": null}}}, {"token_start": 109, "token_end": 121, "char_start": 561, "char_end": 611, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 158, "token_end": 201, "char_start": 828, "char_end": 1011, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {"Luong et al., 2015)": "1998416"}}}, {"token_start": 204, "token_end": 219, "char_start": 1030, "char_end": 1082, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kumar et al., 2019a": "202740548"}}}, {"token_start": 223, "token_end": 234, "char_start": 1111, "char_end": 1146, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 236, "token_end": 255, "char_start": 1155, "char_end": 1250, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Du et al. (2017)": "2172129"}, "Reference": {}}}, {"token_start": 278, "token_end": 314, "char_start": 1396, "char_end": 1574, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "120360213_0", "paragraph": "[BOS] Speech recognition errors, unknown sentence boundaries and disfluencies are three major problems addressed by previous work on parsing speech.\n[BOS] In this work, we focus on the problem of disfluency detection in parsing human-transcribed speech, where we assume that sentence boundaries are given and there are no word recognition errors.\n[BOS] This section reviews approaches that add special mechanisms to parsers to handle disfluencies as well as specialized disfluency detection models.\n\n", "discourse_tags": ["Transition", "Transition", "Transition"], "span_citation_mapping": []}
{"id": "174799399_1", "paragraph": "[BOS] Deep models have been explored in the context of neural machine translation since the emergence of RNN-based models.\n[BOS] To ease optimization, researchers tried to reduce the number of non-linear transitions (Zhou et al.,\n\n", "discourse_tags": ["Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 36, "token_end": 45, "char_start": 193, "char_end": 228, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "174799399_0", "paragraph": "[BOS] Deep Models.\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "192546007_2", "paragraph": "[BOS] This work is unique in a number of aspects.\n[BOS] First, we examine the robustness of uni-and bidirectional self-attentive model as compared to recurrent neural networks.\n[BOS] And, we devise novel attack methods that take advantage of the embedding distance to maximize semantic similarity between real and adversarial examples.\n[BOS] Last but not least, we provide detail observations of the inter-nal variations of different models under attack and theoretical analysis regarding their levels of robustness.\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "192546007_1", "paragraph": "[BOS] In addition, the concept of adversarial attacks has also been explored in more complex NLP tasks.\n[BOS] For example, Jia and Liang (2017) attempt to craft adversarial input to a question answering system by inserting irrelevant sentences at the end of a paragraph.\n[BOS] Cheng et al. (2018) develop an algorithm for attacking seq2seq models with specific constraints on the content of the adversarial examples.\n[BOS] Belinkov and Bisk (2018) compare typos and artificial noise as adversarial input to machine translation models.\n[BOS] Also, Iyyer et al. (2018) propose a paraphrase generator model learned from back-translation data to generate legitimate paraphrases of a sentence as adversaries.\n[BOS] However, the semantic similarity is not guaranteed.\n[BOS] In terms of comparisons between LSTM and Transformers, Tang et al. (2018) show that multiheaded attention is a critical factor in Transformer when learning long distance linguistic relations.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 26, "token_end": 55, "char_start": 123, "char_end": 270, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Jia and Liang (2017)": "7228830"}, "Reference": {}}}, {"token_start": 56, "token_end": 85, "char_start": 277, "char_end": 416, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Cheng et al. (2018)": "3689056"}, "Reference": {}}}, {"token_start": 86, "token_end": 110, "char_start": 423, "char_end": 534, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Belinkov and Bisk (2018)": "3513372"}, "Reference": {}}}, {"token_start": 113, "token_end": 157, "char_start": 547, "char_end": 761, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Iyyer et al. (2018)": "4956100"}, "Reference": {}}}, {"token_start": 158, "token_end": 194, "char_start": 768, "char_end": 959, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Tang et al. (2018)": "52100282"}, "Reference": {}}}]}
{"id": "192546007_0", "paragraph": "[BOS] Robustness of neural network models has been a prominent research topic since Szegedy et al. (2013) discovered that CNN-based image classification models are vulnerable to adversarial examples.\n[BOS] However, attempts to examine the robustness of NLP models are relatively few and far between.\n[BOS] Previous work on attacking neural NLP models include using Fast Gradient Sign Method (Goodfellow et al., 2015) to perturb the embedding of RNN-based classifiers (Papernot et al., 2016; , but they have difficulties mapping from continuous embedding space to discrete input space.\n[BOS] Ebrahimi et al. (2018) propose the 'HotFilp' method that replaces the word or character with the largest difference in the Jacobian matrix.\n[BOS] Li et al. (2016) employ reinforcement learning to find the optimal words to delete in order to fool the classifier.\n[BOS] More recently, Yang et al. (2018) propose a greedy method to construct adversarial examples by solving a discrete optimization problem.\n[BOS] They show superior performance than previous work in terms of attack success rate, but the greedy edits usually degrade the readability or significantly change the semantics.\n[BOS] Zhao et al. (2018) utilize generative adversarial networks (GAN) to generate adversarial attacks against black-box models for applications including image classification, textual entailment, and machine translation.\n[BOS] Alzantot et al. (2018) propose to use a pre-compiled list of semantically similar words to alleviate this issue, but leads to lower successful rate as shown in our experiments.\n[BOS] We thus include the latest greedy and list-based approaches in our comparisons.\n\n", "discourse_tags": ["Single_summ", "Transition", "Narrative_cite", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 14, "token_end": 38, "char_start": 84, "char_end": 199, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 69, "token_end": 83, "char_start": 365, "char_end": 416, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Goodfellow et al., 2015)": "6706414"}}}, {"token_start": 87, "token_end": 101, "char_start": 442, "char_end": 489, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 118, "token_end": 151, "char_start": 591, "char_end": 730, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ebrahimi et al. (2018)": "21698802"}, "Reference": {}}}, {"token_start": 152, "token_end": 177, "char_start": 737, "char_end": 852, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Li et al. (2016)": "13017314"}, "Reference": {}}}, {"token_start": 181, "token_end": 235, "char_start": 874, "char_end": 1175, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Yang et al. (2018)": "44081093"}, "Reference": {}}}, {"token_start": 236, "token_end": 275, "char_start": 1182, "char_end": 1397, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhao et al. (2018)": "3513418"}, "Reference": {}}}, {"token_start": 276, "token_end": 314, "char_start": 1404, "char_end": 1580, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Alzantot et al. (2018)": "5076191"}, "Reference": {}}}]}
{"id": "57721315_5", "paragraph": "[BOS] Query-based abstractive summarization has been rarely studied.\n[BOS] Nema et al. (2017) proposed an attentional encoder-decoder model, and Saha et al. (2018) reported that it performed worse than BiDAF on DuoRC.\n[BOS] Hasselqvist et al. (2017) proposed a pointer-generator based model; however, it does not consider copying words from the question and multiple passages.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 15, "token_end": 30, "char_start": 75, "char_end": 139, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Nema et al. (2017)": "5673925"}, "Reference": {}}}, {"token_start": 32, "token_end": 53, "char_start": 145, "char_end": 217, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Saha et al. (2018)": "5071138"}, "Reference": {}}}, {"token_start": 54, "token_end": 87, "char_start": 224, "char_end": 376, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hasselqvist et al. (2017)": "21581059"}, "Reference": {}}}]}
{"id": "57721315_4", "paragraph": "[BOS] Abstractive summarization.\n[BOS] Current state-ofthe-art models use pointer-generator mechanisms (See et al., 2017) .\n[BOS] In particular, content selection approaches, which decide what to summarize, have recently been used with abstractive models.\n[BOS] Most methods select content at the sentence level (Hsu et al., 2018; Chen and Bansal, 2018) and the word level (Pasunuru and Bansal, 2018; Gehrmann et al., 2018) ; our model incorporates content selection at the passage level in the combined attention.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 16, "token_end": 28, "char_start": 74, "char_end": 121, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(See et al., 2017)": null}}}, {"token_start": 55, "token_end": 76, "char_start": 275, "char_end": 353, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hsu et al., 2018;": "21723747", "Chen and Bansal, 2018)": "44129061"}}}, {"token_start": 78, "token_end": 100, "char_start": 362, "char_end": 423, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Pasunuru and Bansal, 2018;": "4940548", "Gehrmann et al., 2018)": "52144157"}}}]}
{"id": "57721315_3", "paragraph": "[BOS] RC with unanswerable question identification.\n[BOS] The previous work of ( Levy et al., 2017; ) outputs a no-answer score depending on the probability of all answer spans.\n[BOS] Hu et al. (2018) proposed an answer verifier to compare the answer sentence with the question.\n[BOS] Sun et al. (2018a) proposed a unified model that jointly learns an RC model and an answer verifier.\n[BOS] Our model introduces a classifier on the basis of question-passages matching, which is not dependent on the generated answer, unlike the previous methods.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 17, "token_end": 40, "char_start": 81, "char_end": 177, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Levy et al., 2017;": "793385"}, "Reference": {}}}, {"token_start": 41, "token_end": 63, "char_start": 184, "char_end": 278, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hu et al. (2018)": "52041587"}, "Reference": {}}}, {"token_start": 64, "token_end": 89, "char_start": 285, "char_end": 384, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Sun et al. (2018a)": "53116060"}, "Reference": {}}}]}
{"id": "57721315_2", "paragraph": "[BOS] Multi-passage RC.\n[BOS] The simplest approach is to concatenate the passages and find the answer from the concatenated one as in (Wang et al., 2017) .\n[BOS] Earlier pipeline models find a small number of relevant passages with a TF-IDF based ranker and pass them to a neural reader , while more recent pipeline models use a neural re-ranker to more accurately select the relevant passages Nishida et al., 2018) .\n[BOS] Also, non-pipelined models (including ours) consider all the provided passages and find the answer by comparing scores between passages .\n[BOS] The most recent models make a proper trade-off between efficiency and accuracy .\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 13, "token_end": 35, "char_start": 58, "char_end": 154, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wang et al., 2017)": "12501880"}}}, {"token_start": 71, "token_end": 91, "char_start": 330, "char_end": 416, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Nishida et al., 2018)": "52145665"}}}]}
{"id": "57721315_1", "paragraph": "[BOS] Controllable text generation.\n[BOS] Many studies have been carried out in the framework of style transfer, which is the task of rephrasing the text so that it contains specific styles such as sentiment.\n[BOS] Recent work uses artificial tokens (Sennrich et al., 2016; Johnson et al., 2017) , variational auto-encoders (Hu et al., 2017) , adversarial training (Fu et al., 2018; Tsvetkov et al., 2018) , or prior knowledge (Li et al., 2018b) to separate the content and style on the encoder side.\n[BOS] On the decoder side, conditional language modeling has been used to generate output sentence with the target style.\n[BOS] In addition to style transfer, output length control with conditional language modeling has been well studied (Kikuchi et al., 2016; Takeno et al., 2017; Fan et al., 2018) .\n[BOS] Our style-controllable RC relies on conditional language modeling on the decoder side.\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite", "Transition", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 44, "token_end": 63, "char_start": 232, "char_end": 295, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sennrich et al., 2016;": "845121", "Johnson et al., 2017)": "6053988"}}}, {"token_start": 64, "token_end": 77, "char_start": 298, "char_end": 341, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hu et al., 2017)": "20981275"}}}, {"token_start": 78, "token_end": 98, "char_start": 344, "char_end": 405, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Fu et al., 2018;": "6484065", "Tsvetkov et al., 2018)": "13959787"}}}, {"token_start": 100, "token_end": 122, "char_start": 411, "char_end": 500, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Li et al., 2018b)": "4937880"}}}, {"token_start": 150, "token_end": 186, "char_start": 660, "char_end": 800, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kikuchi et al., 2016;": "11157751", "Takeno et al., 2017;": "27834461", "Fan et al., 2018)": "22716243"}}}]}
{"id": "57721315_0", "paragraph": "[BOS] To the best of our knowledge, there are no datasets for providing answers in natural language with multiple styles except MS MARCO 2.1, although there are some datasets that provide abstractive answers.\n[BOS] DuReader , a Chinese multi-document RC dataset, provides the top-10 ranked entire documents from Baidu Search and Zhidao.\n[BOS] Many of the answers are long and relatively far from the source documents compared with those from MS MARCO.\n[BOS] NarrativeQA (Kocisk et al., 2018) proposed a dataset about stories or summaries of books or movie scripts.\n[BOS] The documents are long, averaging 62,528 (659) words in stories (summaries), while the answers are relatively short, averaging 4.73 words.\n[BOS] Moreover, DuoRC (Saha et al., 2018) and CoQA (Reddy et al., 2018) contain abstractive answers; most of the answers are short phrases.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Single_summ", "Single_summ", "Multi_summ"], "span_citation_mapping": [{"token_start": 93, "token_end": 153, "char_start": 458, "char_end": 709, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Kocisk\u00fd et al., 2018)": "2593903"}, "Reference": {}}}, {"token_start": 156, "token_end": 193, "char_start": 726, "char_end": 849, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Saha et al., 2018)": "5071138", "(Reddy et al., 2018)": "52055325"}, "Reference": {}}}]}
{"id": "102350939_5", "paragraph": "[BOS] The architecture of the attention-based NMT.\n\n", "discourse_tags": ["Other"], "span_citation_mapping": []}
{"id": "102350939_4", "paragraph": "[BOS] .\n[BOS] .\n[BOS] .\n\n", "discourse_tags": ["Transition", "Transition", "Transition"], "span_citation_mapping": []}
{"id": "102350939_3", "paragraph": "[BOS] Adversarial Networks have achieved great success in some areas (Ganin et al., 2016; Goodfellow et al., 2014) .\n[BOS] Inspired by these work, we also employ a domain discriminator to extract some domain invariant features which has already shown its effectiveness in some related NLP tasks.\n[BOS] Chen et al. (2017) use a classifier to exploit the shared information between different Chinese word segment criteria.\n[BOS] Gui et al. (2017) tries to learn common features of the out-domain data and indomain data through adversarial discriminator for the part-of-speech tagging problem.\n[BOS] Kim et al. (2017) train a cross-lingual model with languageadversarial training to generate the general information across different languages for the POS tagging problem.\n[BOS] All these work try to utilize a discriminator to distinguish invariant features across the divergence.\n\n", "discourse_tags": ["Narrative_cite", "Reflection", "Single_summ", "Single_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 12, "token_end": 20, "char_start": 69, "char_end": 88, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 21, "token_end": 30, "char_start": 90, "char_end": 114, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Goodfellow et al., 2014)": "1033682"}}}, {"token_start": 64, "token_end": 86, "char_start": 302, "char_end": 420, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Chen et al. (2017)": "61274"}, "Reference": {}}}, {"token_start": 87, "token_end": 125, "char_start": 427, "char_end": 590, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Gui et al. (2017)": "38194723"}, "Reference": {}}}, {"token_start": 126, "token_end": 160, "char_start": 597, "char_end": 768, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kim et al. (2017)": "9489563"}, "Reference": {}}}]}
{"id": "102350939_2", "paragraph": "[BOS] To make use of out-of-domain parallel data, Luong and Manning (2015) first train an NMT model with a large amount of out-of-domain data, then fine tune the model with in-domain data.\n[BOS] Wang et al. (2017a) select sentence pairs from the outof-domain data set according to their similarity to the in-domain data and then add them to the indomain training data.\n[BOS] Chu et al. (2017) construct the training data set for the NMT model by combining out-of-domain data with the over-sampled in-domain data.\n[BOS] Wang et al. (2017b) combine the in-domain and out-of-domain data together as the training data but apply instance weighting to get a weight for each sentence pair in the out-of-domain data which is used in the parameter updating during back propagation.\n[BOS] Britz et al. (2017) employ a common encoder to encode the sentences from both the in-domain and out-of-domain data and meanwhile add a discriminator to the encoder to make sure that only domain-invariant information is transferred to the decoder.\n[BOS] They focus on the situation that the quantity of the out-of-domain data is almost the same as the in-domain data while our method can handle more generic situations and there is no specific demand for the ratio of the quantity between the in-domain and out-ofdomain data.\n[BOS] Besides, our method employs a private encoder-decoder for each domain which can hold the domain-specific features.\n[BOS] In addition to the common encoder, Zeng et al. (2018) further introduce a domain-specific encoder to each domain together with a domain-specific classifier to ensure the features extracted by the domain-specific encoder is proper.\n[BOS] Compared to our method, they focus on the encoder and do not distinguish the information in the decoder.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection", "Reflection", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 14, "token_end": 51, "char_start": 50, "char_end": 188, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Luong and Manning (2015)": null}, "Reference": {}}}, {"token_start": 52, "token_end": 93, "char_start": 195, "char_end": 368, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wang et al. (2017a)": "9990193"}, "Reference": {}}}, {"token_start": 94, "token_end": 129, "char_start": 375, "char_end": 512, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Chu et al. (2017)": "15201884"}, "Reference": {}}}, {"token_start": 130, "token_end": 186, "char_start": 519, "char_end": 772, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wang et al. (2017b)": "1054586"}, "Reference": {}}}, {"token_start": 187, "token_end": 240, "char_start": 779, "char_end": 1025, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Britz et al. (2017)": "30042437"}, "Reference": {}}}, {"token_start": 330, "token_end": 368, "char_start": 1466, "char_end": 1661, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zeng et al. (2018)": "53079221"}, "Reference": {}}}]}
{"id": "102350939_1", "paragraph": "[BOS] To exploit in-domain monolingual data, Glehre et al. (2015) train a RNNLM on the target side monolingual data first and then use it in decoding.\n[BOS] Domhan and Hieber (2017) further extend this work by training the RNNLM part and translation part jointly.\n[BOS] Sennrich et al. (2015a) propose to conduct back translation for the monolingual target data so as to generate the corresponding parallel data.\n[BOS] Zhang and Zong (2016) employs the self-learning algorithm to generate the synthetic large-scale parallel data for NMT training.\n[BOS] To introduce meta information, Chen et al. (2016) use the topic or category information of the input text to assistant the decoder and Kobus et al. (2017) extend the generic NMT models, which are trained on a diverse set of data to, specific domains with the specialized terminology and style.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Multi_summ"], "span_citation_mapping": [{"token_start": 11, "token_end": 39, "char_start": 45, "char_end": 150, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"G\u00fcl\u00e7ehre et al. (2015)": "15352384"}, "Reference": {}}}, {"token_start": 40, "token_end": 64, "char_start": 157, "char_end": 263, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Domhan and Hieber (2017)": "19164342"}, "Reference": {}}}, {"token_start": 65, "token_end": 95, "char_start": 270, "char_end": 412, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Sennrich et al. (2015a)": "15600925"}, "Reference": {}}}, {"token_start": 96, "token_end": 123, "char_start": 419, "char_end": 546, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhang and Zong (2016)": "17667087"}, "Reference": {}}}, {"token_start": 129, "token_end": 150, "char_start": 584, "char_end": 683, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Chen et al. (2016)": "17078659"}, "Reference": {}}}, {"token_start": 151, "token_end": 186, "char_start": 688, "char_end": 846, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kobus et al. (2017)": "7497218"}, "Reference": {}}}]}
{"id": "102350939_0", "paragraph": "[BOS] The task of domain adaptation for NMT is to translate a text in-domain for which only a small number of parallel sentences is available.\n[BOS] The main idea of the work for domain adaptation is to introduce external information to help in-domain translation which may include in-domain monolingual data, meta information or out-of-domain parallel data.\n\n", "discourse_tags": ["Transition", "Transition"], "span_citation_mapping": []}
{"id": "126181046_5", "paragraph": "[BOS] On translating named entities, Currey et al. (2017) augment the training data by copying target-side sentences to the source-side, resulting in augmented training corpora where the source and the target sides contain identical sentences.\n[BOS] The augmented data is shown to improve translation performance, especially for proper nouns and other words that are identical in the source and target languages.\n\n", "discourse_tags": ["Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 3, "token_end": 75, "char_start": 9, "char_end": 412, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Currey et al. (2017)": "40575489"}, "Reference": {}}}]}
{"id": "126181046_4", "paragraph": "[BOS] Our method is similar in the sense that external translations of source phrases are leveraged.\n[BOS] However, their tasks are different.\n[BOS] In particular, these methods regard one-to-many translation lexicons as a suggestion.\n[BOS] In contrast, our task aims to constrain NMT translation through one-to-one prespecified translations.\n[BOS] Lexical translations can be used to generate code-switched source sentences during training, but we do not modify NMT models by integrating translation lexicons.\n[BOS] In addition, our data augmentation method is more flexible, because it is model-free.\n[BOS] Alkhouli et al. (2018) simulate a dictionaryguided translation task to evaluate NMT's alignment extraction.\n[BOS] A one-to-one word translation dictionary is used to guide NMT decoding.\n[BOS] In their method, a dictionary entry is limited to only one word on both the source and target sides.\n[BOS] In addition, a pre-specified translation can come into effect only if the corresponding source-side word is successfully aligned during decoding.\n\n", "discourse_tags": ["Reflection", "Transition", "Transition", "Reflection", "Reflection", "Reflection", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 119, "token_end": 210, "char_start": 609, "char_end": 1053, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Alkhouli et al. (2018)": "52190658"}, "Reference": {}}}]}
{"id": "126181046_3", "paragraph": "[BOS] Using probabilistic lexicons.\n[BOS] Aiming at making use of one-to-many phrasal translations, the following work is remotely related to our work.\n[BOS] Tang et al. (2016) use a phrase memory to provide extra information for their NMT encoder, dynamically switching between word generation and phrase generation during decoding.\n[BOS] Wang et al. (2017a) use SMT to recommend prediction for NMT, which contains not only translation operations of a SMT phrase table, but also alignment information and coverage information.\n[BOS] Arthur et al. (2016) incorporate discrete lexicons by converting lexicon probabilities into predictive probabilities and linearly interpolating them with NMT probability distributions.\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 33, "token_end": 65, "char_start": 158, "char_end": 333, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Tang et al. (2016)": "18192067"}, "Reference": {}}}, {"token_start": 66, "token_end": 105, "char_start": 340, "char_end": 527, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wang et al. (2017a)": "5565551"}, "Reference": {}}}, {"token_start": 106, "token_end": 136, "char_start": 534, "char_end": 718, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Arthur et al. (2016)": "10086161"}, "Reference": {}}}]}
{"id": "126181046_2", "paragraph": "[BOS] In contrast to their methods, our method does not make changes to the decoder, and therefore decoding speed remains unchanged.\n[BOS] Translation fidelity of pre-specified source words is achieved through a combination of training and decoding procedure, where replaced source-side words still contain their target-side meaning.\n[BOS] As a soft method of inserting pre-specified translation, our method does not guarantee that all lexical constraints are satisfied during decoding, but has better overall translation quality compared to their method.\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "126181046_1", "paragraph": "[BOS] Lexical constraints.\n[BOS] Hokamp and Liu (2017) propose an altered beam search algorithm, namely grid beam search, which takes target-side prespecified translations as lexical constraints during beam search.\n[BOS] A potential problem of this method is that translation fidelity is not specifically considered, since there is no indication of a matching source of each pre-specific translation.\n[BOS] In addition, decoding speed is significantly reduced (Post and Vilar, 2018) .\n[BOS] Hasler et al. (2018) use alignment to gain target-side constraints' corresponding source words, simultaneously use finitestate machines and multi-stack (Anderson et al., 2016) decoding to guide beam search.\n[BOS] Post and Vilar (2018) give a fast version of Hokamp and Liu (2017) , which limits the decoding complexity linearly by altering the beam search algorithm through dynamic beam allocation.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Narrative_cite", "Multi_summ", "Multi_summ"], "span_citation_mapping": [{"token_start": 6, "token_end": 74, "char_start": 33, "char_end": 400, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hokamp and Liu (2017)": "15281983"}, "Reference": {}}}, {"token_start": 83, "token_end": 91, "char_start": 460, "char_end": 482, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Post and Vilar, 2018)": "4936344"}}}, {"token_start": 93, "token_end": 137, "char_start": 491, "char_end": 697, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hasler et al. (2018)": "13663540"}, "Reference": {"(Anderson et al., 2016)": "9662636"}}}, {"token_start": 138, "token_end": 176, "char_start": 704, "char_end": 889, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Post and Vilar (2018)": "4936344"}, "Reference": {"Hokamp and Liu (2017)": "15281983"}}}]}
{"id": "126181046_0", "paragraph": "[BOS] Using placeholders.\n[BOS] Luong et al. (2014) use annotated unk tags to present the unk symbols in training corpora, where the correspondence between source and target unk symbols are obtained from word alignment (Brown et al., 1993) .\n[BOS] Output unk tags are replaced through a post-processing stage by looking up a pre-specified dictionary or copying the corresponding source word.\n[BOS] Crego et al. (2016) extended unk tags symbol to specific symbols that can present name entities.\n[BOS] Wang et al. (2017b) and use a similar method.\n[BOS] This method is limited when constrain NMT with pre-specified translations consisting of more general words, due to the loss of word meaning when representing them with placeholder tags.\n[BOS] In contrast to their work, word meaning is fully kept in modified source in our work.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Multi_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 7, "token_end": 82, "char_start": 32, "char_end": 391, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Luong et al. (2014)": "1245593"}, "Reference": {"(Brown et al., 1993)": null}}}, {"token_start": 83, "token_end": 105, "char_start": 398, "char_end": 494, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Crego et al. (2016)": "16641238"}, "Reference": {}}}, {"token_start": 106, "token_end": 155, "char_start": 501, "char_end": 738, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wang et al. (2017b)": "32936404"}, "Reference": {}}}]}
{"id": "202538463_2", "paragraph": "[BOS] Imitation learning algorithms are a great fit for training agents in simulated environments: access to ground-truth information about the environments allows optimal actions to be computed in many situations.\n[BOS] The \"teacher\" in standard imitation learning algorithms (Daum III et al., 2009; Ross et al., 2011; Ross and Bagnell, 2014; Chang et al., 2015; Sun et al., 2017; Sharaf and Daum III, 2017) does not take into consideration the agent's capability and behavior.\n[BOS] He et al. (2012) present a coaching method where the teacher gradually increases the complexity of its demonstrations over time.\n[BOS] Welleck et al. (2019) propose an \"unlikelihood\" objective, which, similar to our curiosity-encouraging objective, penalizes likelihoods of candidate negative actions to avoid mistake repetition.\n[BOS] Our approach takes into account the agent's past and future behavior to determine actions that are most and least beneficial to them, combining the advantages of both model-based and progress-estimating methods (Wang et al., 2018; Ma et al., 2019a,b) .\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 41, "token_end": 93, "char_start": 238, "char_end": 408, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Daum\u00e9 III et al., 2009;": "704519", "Ross et al., 2011;": "103456", "Ross and Bagnell, 2014;": "1685197", "Chang et al., 2015;": "16610517", "Sun et al., 2017;": "6008303"}}}, {"token_start": 107, "token_end": 133, "char_start": 485, "char_end": 613, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 134, "token_end": 178, "char_start": 620, "char_end": 814, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 209, "token_end": 236, "char_start": 988, "char_end": 1071, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "202538463_1", "paragraph": "[BOS] Recent navigation tasks in photo-realistic simulators have accelerated research on teaching agents to execute human instructions.\n[BOS] Nevertheless, modeling human assistance in these problems remains simplistic (Table 1) : they either do not incorporate the ability to request additional help while executing tasks (Misra et al., 2014 (Misra et al., , 2017 Anderson et al., 2018b; Chen et al., 2019; Das et al., 2018; Misra et al., 2018; Wijmans et al., 2019; Qi et al., 2019) , or mimic human verbal assistance with primitive, highly scripted language (Nguyen et al., 2019; Chevalier-Boisvert et al., 2019) .\n[BOS] HANNA improves the realisticity of the VNLA setup (Nguyen et al., 2019) by using fully natural language instructions.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 40, "token_end": 117, "char_start": 243, "char_end": 484, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Anderson et al., 2018b;": null, "Chen et al., 2019;": "54078068", "Das et al., 2018;": "35985986", "Qi et al., 2019)": null}}}, {"token_start": 119, "token_end": 153, "char_start": 490, "char_end": 615, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Nguyen et al., 2019;": null, "Chevalier-Boisvert et al., 2019)": null}}}, {"token_start": 163, "token_end": 175, "char_start": 663, "char_end": 695, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "202538463_0", "paragraph": "[BOS] Simulated environments provide an inexpensive platform for fast prototyping and evaluating new ideas before deploying them into the real world.\n[BOS] Video-game and physics simulators are standard benchmarks in reinforcement learning (Todorov et al., 2012; Mnih et al., 2013; Kempka et al., 2016; Brockman et al., 2016; Vinyals et al., 2017) .\n[BOS] Nevertheless, these environments under-represent the complexity of the world.\n[BOS] Realistic simulators play an important role in sim-to-real approaches, in which an agent is trained with arbitrarily many samples provided by the simulators, then transferred to real settings using sample-efficient transfer learning techniques (Kalashnikov et al., 2018; Andrychowicz et al., 2018; Karttunen et al., 2019) .\n[BOS] While modern techniques are capable of simulating images that can convince human perception (Karras et al., 2017 (Karras et al., , 2018 , simulating language interaction remains challenging.\n[BOS] There are efforts in building complex interactive text-based worlds (Ct et al., 2018; Urbanek et al., 2019) but the lack of a graphical component makes them not suitable for visually grounded learning.\n[BOS] On the other hand, experimentation on real humans and robots, despite expensive and time-consuming, are important for understanding the true complexity of real-world scenarios (Chai et al., 2018 (Chai et al., , 2016 Rybski et al., 2007; Mohan and Laird, 2014; Liu et al., 2016; She et al., 2014) .\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Narrative_cite", "Narrative_cite", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 35, "token_end": 81, "char_start": 217, "char_end": 347, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Todorov et al., 2012;": "5230692"}}}, {"token_start": 132, "token_end": 169, "char_start": 638, "char_end": 761, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Andrychowicz et al., 2018;": "51894399"}}}, {"token_start": 172, "token_end": 202, "char_start": 776, "char_end": 905, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 215, "token_end": 238, "char_start": 997, "char_end": 1074, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(C\u00f4t\u00e9 et al., 2018;": null}}}, {"token_start": 260, "token_end": 334, "char_start": 1194, "char_end": 1470, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chai et al., 2018": "51609464", "(Chai et al., , 2016": "5014724", "Rybski et al., 2007;": "2645651", "Liu et al., 2016;": "5014724"}}}]}
{"id": "196181072_5", "paragraph": "[BOS] In the parsing community, Flannery and Mori (2015) propose to annotate partially labeled target-domain data with active learning for cross-domain Japanese dependency parsing.\n[BOS] Similarly, Joshi et al. (2018) annotate a few dozen partially labeled target-domain sentences with a few brackets for cross-domain constituent parsing.\n[BOS] Both results report large improvement and show the usefulness of even small amount of target-domain annotation, showing the great potential of semi-supervised domain adaptation for parsing.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 7, "token_end": 36, "char_start": 32, "char_end": 180, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Flannery and Mori (2015)": "18357997"}, "Reference": {}}}, {"token_start": 39, "token_end": 103, "char_start": 198, "char_end": 534, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Joshi et al. (2018)": "21712653"}, "Reference": {}}}]}
{"id": "196181072_4", "paragraph": "[BOS] Semi-supervised domain adaptation assumes there exist some (usually very small-scale) labeled target-domain data, which can be used to directly learn the domain-specific distributions or features.\n[BOS] Daum III (2007) propose a simple yet effective feature augmentation approach that performs well on a number of sequence labeling tasks.\n[BOS] The idea is to distinguish domain-specific and general features by making a copy of each feature for each domain plus a shared (general) pseudo domain.\n[BOS] Finkel and Manning (2009) further propose a hierarchical Bayesian extension of this idea.\n[BOS] As pointed by Finkel and Manning (2009) , those two works can be understood as MTL under the traditional discrete-feature ML framework.\n[BOS] Kim et al. (2017) propose a neural mixture of experts approach for cross-domain intent classification and slot tagging.\n[BOS] Different from the unsupervised method of Guo et al. (2018) , they use a small amount of target-domain labeled data to train an attention module for the computation of example-to-domain distances.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 40, "token_end": 96, "char_start": 209, "char_end": 502, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Daum\u00e9 III (2007)": "5360764"}, "Reference": {}}}, {"token_start": 97, "token_end": 115, "char_start": 509, "char_end": 598, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Finkel and Manning (2009)": "14045921"}, "Reference": {}}}, {"token_start": 119, "token_end": 146, "char_start": 619, "char_end": 740, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Finkel and Manning (2009)": "14045921"}}}, {"token_start": 147, "token_end": 171, "char_start": 747, "char_end": 866, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kim et al. (2017)": "39130572"}, "Reference": {}}}, {"token_start": 175, "token_end": 213, "char_start": 892, "char_end": 1069, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Guo et al. (2018)": "52178689"}, "Reference": {}}}]}
{"id": "196181072_3", "paragraph": "[BOS] The multi-source domain adaptation problem assumes there are labeled datasets for multiple source domains.\n[BOS] Given a target domain, the challenge is how to effectively combine knowledge in the source domains.\n[BOS] McClosky et al. (2010) first raise this scenario for constituent parsing.\n[BOS] They employ a regression model to predict crossdomain performance, and then use the values to combine parsing models independently trained on each source domain.\n[BOS] Guo et al. (2018) employ a similar idea of mixture of experts under the neural MTL framework, and conduct experiments on sentiment classification and POS tagging tasks.\n[BOS] They employ meta-training to learn to compute the point-to-set distance between a target-domain example and a source domain.\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 39, "token_end": 84, "char_start": 225, "char_end": 466, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"McClosky et al. (2010)": "10585087"}, "Reference": {}}}, {"token_start": 85, "token_end": 146, "char_start": 473, "char_end": 772, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Guo et al. (2018)": "52178689"}, "Reference": {}}}]}
{"id": "196181072_2", "paragraph": "[BOS] Source-domain data selection is another interesting research direction.\n[BOS] Given a target domain, the idea is to automatically select a most relevant subset from the source-domain training data to train the parsing model, instead of using all the labeled data (Plank and van Noord, 2011; Khan et al., 2013) .\n\n", "discourse_tags": ["Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 23, "token_end": 65, "char_start": 122, "char_end": 315, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Plank and van Noord, 2011;": "7362580", "Khan et al., 2013)": "7403488"}}}]}
{"id": "196181072_1", "paragraph": "[BOS] Unsupervised domain adaptation.\n[BOS] Due to the lack of sufficient labeled data, most previous works focuses on unsupervised domain adapta-tion, assuming there is only labeled data for the source domain.\n[BOS] Researchers make great effort to learn useful features from large-scale unlabeled target-domain data, which is usually much easier to collect.\n[BOS] As a typical semi-supervised approach, self-training is shown to be very useful for cross-domain constituent parsing (McClosky et al., 2006) and dependency parsing (Yu et al., 2015) .\n[BOS] There are also many failed works on applying self-training for in-domain and crossdomain dependency parsing.\n[BOS] Sagae and Tsujii (2007) apply co-training to the CoNLL-2007 cross-domain dependency parsing task and report positive gains (Nivre et al., 2007) .\n[BOS] In contrast, Dredze et al. (2007) experiment with many domain adaptation approaches with no success on the same datasets and suggest the major obstacle comes from the divergent annotation guideline adopted by the target-domain evaluation data.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Narrative_cite", "Transition", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 75, "token_end": 100, "char_start": 405, "char_end": 506, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(McClosky et al., 2006)": "1825866"}}}, {"token_start": 101, "token_end": 111, "char_start": 511, "char_end": 547, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yu et al., 2015)": null}}}, {"token_start": 135, "token_end": 173, "char_start": 671, "char_end": 814, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Sagae and Tsujii (2007)": "2768696"}, "Reference": {"(Nivre et al., 2007)": "1585700"}}}, {"token_start": 178, "token_end": 220, "char_start": 836, "char_end": 1066, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Dredze et al. (2007)": "5811151"}, "Reference": {}}}]}
{"id": "196181072_0", "paragraph": "[BOS] Domain adaptation has been a crucial and challenging research topic in both NLP and ML fields.\n[BOS] Due to the vast scope of related research, we try to give a brief (and far from complete) review on some representative approaches of high relevance with syntactic parsing.\n\n", "discourse_tags": ["Transition", "Reflection"], "span_citation_mapping": []}
{"id": "173990267_1", "paragraph": "[BOS] This information can be incorporated into an SRL system in several different ways.\n[BOS] Swayamdipta et al. (2018) use span information from constituency parse trees as an additional training target in a multi-task learning approach, similar to one of the approaches we evaluate here.\n[BOS] Roth and Lapata (2016) use an LSTM model to represent the dependency paths between predicates and arguments and feed the output as the input features to their SRL system.\n[BOS] Marcheggiani and Titov (2017) use Graph Convolutional Network (Niepert et al., 2016) to encode the dependency parsing trees into their LSTM-based SRL system.\n[BOS] Xia et al. (2019) represent dependency parses using position-based categorical features of tree structures in a neural model.\n[BOS] Strubell et al. (2018) use dependency trees as a supervision signal to train one of attention heads in a self-attentive neural model.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 18, "token_end": 60, "char_start": 95, "char_end": 290, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Swayamdipta et al. (2018)": "52144168"}, "Reference": {}}}, {"token_start": 61, "token_end": 96, "char_start": 297, "char_end": 467, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Roth and Lapata (2016)": "5779419"}, "Reference": {}}}, {"token_start": 97, "token_end": 136, "char_start": 474, "char_end": 631, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Marcheggiani and Titov (2017)": "16839291"}, "Reference": {"(Niepert et al., 2016)": "1430801"}}}, {"token_start": 137, "token_end": 163, "char_start": 638, "char_end": 763, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Xia et al. (2019)": "59561763"}, "Reference": {}}}, {"token_start": 164, "token_end": 195, "char_start": 770, "char_end": 903, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Strubell et al. (2018)": "5068376"}, "Reference": {}}}]}
{"id": "173990267_0", "paragraph": "[BOS] Semantic Role Labeling (SRL) generally refers to the PropBank style of annotation (Palmer et al., 2005) .\n[BOS] Broadly speaking, prior work on SRL makes use of syntactic information in two different ways.\n[BOS] Carreras and Mrquez (2005) ; Pradhan et al. (2013) incorporate constituent-structure span-based information, while Haji et al. (2009) incorporate dependency-structure information.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Multi_summ"], "span_citation_mapping": [{"token_start": 13, "token_end": 26, "char_start": 59, "char_end": 109, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Palmer et al., 2005)": "2486369"}}}, {"token_start": 47, "token_end": 74, "char_start": 218, "char_end": 325, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Carreras and M\u00e0rquez (2005)": "16509032", "Pradhan et al. (2013)": "14515377"}, "Reference": {}}}, {"token_start": 76, "token_end": 90, "char_start": 333, "char_end": 397, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Haji\u010d et al. (2009)": "9210201"}, "Reference": {}}}]}
{"id": "208264685_0", "paragraph": "[BOS] There are several previous attempts of incorporating knowledge from other NLP tasks into NMT.\n[BOS] Early work incorporated word sense disambiguation (WSD) into existing machine translation pipelines (Chan et al., 2007; Carpuat and Wu, 2007; Vickrey et al., 2005) .\n[BOS] Recently, Liu et al. (2018) demonstrated that existing NMT systems have significant problems properly translating ambiguous words.\n[BOS] They proposed to use WSD to enhance the system's ability to capture contextual knowledge in translation.\n[BOS] Their work showed improvement on sentences with contextual information, but this method does not apply to sentences which do not have strong contextual information.\n[BOS] Rios et al. (2017) pass sense embeddings as additional input to NMT, extracting lexical chains based on sense embeddings from the document and integrating it into the NMT model.\n[BOS] Their method improved lexical choice, especially for rare word senses, but did not improve the overall translation performance as measured by BLEU.\n[BOS] Pu et al. (2018) incorporate weakly supervised word sense disambiguation into NMT to improve translation quality and accuracy of ambiguous words.\n[BOS] However, these works focused on cases where there is only one correct sense for the source sentences.\n[BOS] This differs from our goal, which is to tackle cases where both sentiments are correct interpretations of the source sentence.\n[BOS] He et al. (2010) used machine translation to learn lexical prior knowledge of English sentiment lexicons and incorporated the prior knowledge into latent Dirichlet allocation (LDA), where sentiment labels are considered as topics for sentiment analysis.\n[BOS] In contrast, our work incorporates lexical information from sentiment analysis directly into the NMT process.\n[BOS] Sennrich et al. (2016) attempt to control politeness of the translations via incorporating side constraints.\n[BOS] Similar to our approach, they also have a two-stage pipeline where they first automatically annotate the T-V distinction of the target sentences in the training set and then they add the annotations as special tokens at the end of the source text.\n[BOS] The attentional encoder-decoder framework is then trained to learn to pay attention to the side constraints during training.\n[BOS] However, there are several differences between our work and theirs: 1) instead of politeness, we control the sentiment of the translations; 2) instead of annotating Original He is so proud that nobody likes him.\n[BOS] AddLabel neg He is so proud that nobody likes him.\n[BOS] InsertLabel He is so neg proud that nobody likes him.\n[BOS] the politeness (in our case the sentiment) using linguistic rules, we train a BERT classifier to do automatic sentiment labeling; 3) instead of having only sentence-level annotation, we have sentiment annotation for the specific sentiment ambiguous lexicons; 4) instead of always adding the special politeness token at the end of the source sentence, we explored adding the special tokens at the front as well as right next to the corresponding sentiment ambiguous word; 5) we also propose a methodValence Sensitive Embedding -to better control the sentiment of the translations.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection", "Single_summ", "Reflection", "Single_summ", "Single_summ", "Transition", "Reflection", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 33, "token_end": 62, "char_start": 176, "char_end": 269, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chan et al., 2007;": "14598745", "Carpuat and Wu, 2007;": "135295", "Vickrey et al., 2005)": "7241107"}}}, {"token_start": 66, "token_end": 135, "char_start": 288, "char_end": 690, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Liu et al. (2018)": "4410027"}, "Reference": {}}}, {"token_start": 136, "token_end": 200, "char_start": 697, "char_end": 1028, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Rios et al. (2017)": "529114"}, "Reference": {}}}, {"token_start": 201, "token_end": 248, "char_start": 1035, "char_end": 1288, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Pu et al. (2018)": null}, "Reference": {}}}, {"token_start": 273, "token_end": 317, "char_start": 1428, "char_end": 1681, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"He et al. (2010)": "3859767"}, "Reference": {}}}, {"token_start": 337, "token_end": 409, "char_start": 1804, "char_end": 2166, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Sennrich et al. (2016)": "845121"}, "Reference": {}}}]}
{"id": "196174574_1", "paragraph": "[BOS] Some recent work improves the performance of neural abstractive models on document summarization task from various aspects.\n[BOS] To better grasp the essential meaning for summarization, propose not only to pay attention to specific regions and content of input documents with attention models, but also distract them to traverse between different content.\n[BOS] Tan et al. (2017) propose a graph-based attention mechanism in a hierarchical encoder-decoder framework to generate multi-sentence summary.\n[BOS] Gehrmann et al. (2018) presents a content selection model for summarization that identifies phrases within a document that are likely included in its summary.\n[BOS] To produce more informative summaries, (Gu et al., 2016) is the first to show that the copy mechanism (Vinyals et al., 2015) can alleviate the OutOf-Vocabulary problem by copying words from the source documents.\n[BOS] See et al. (2017) rebuild this pointer-generator network and incorporate an additional coverage mechanism into the decoder.\n[BOS] Li et al. (2018b) notice the necessity of explicit information selection and they build a gated global information filter and local sentence selection mechanism.\n[BOS] Moreover, reinforcement learning (RL) approaches have been shown to further improve performance on these tasks (Celikyilmaz et al., 2018; Li et al., 2018a) .\n[BOS] Pasunuru and Bansal (2018) develop a loss-function based on whether salient segments are included in a summary.\n[BOS] However, the optimization of RL-based models can be difficult to tune and slow to train.\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Narrative_cite", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 63, "token_end": 91, "char_start": 369, "char_end": 508, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Tan et al. (2017)": "26698484"}, "Reference": {}}}, {"token_start": 92, "token_end": 123, "char_start": 515, "char_end": 673, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Gehrmann et al. (2018)": "52144157"}, "Reference": {}}}, {"token_start": 130, "token_end": 173, "char_start": 719, "char_end": 891, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Gu et al., 2016)": "8174613"}, "Reference": {"(Vinyals et al., 2015)": "5692837"}}}, {"token_start": 174, "token_end": 198, "char_start": 898, "char_end": 1021, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"See et al. (2017)": null}, "Reference": {}}}, {"token_start": 199, "token_end": 228, "char_start": 1028, "char_end": 1189, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Li et al. (2018b)": "53082908"}, "Reference": {}}}, {"token_start": 231, "token_end": 267, "char_start": 1206, "char_end": 1351, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Celikyilmaz et al., 2018;": "4406182", "Li et al., 2018a)": "4451272"}}}, {"token_start": 269, "token_end": 295, "char_start": 1360, "char_end": 1471, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Pasunuru and Bansal (2018)": "4940548"}, "Reference": {}}}]}
{"id": "196174574_0", "paragraph": "[BOS] With the development of seq2seq model on neural translation task, more and more researchers take note of its great potential in text summarization area (Fan et al., 2017; Ling and Rush, 2017; Cheng and Lapata, 2016) , especially for abstractive methods.\n[BOS] Rush et al. (2015) is the first to apply seq2seq model with attention mechanism to abstractive summarization and achieve promising improvement.\n[BOS] Nallapati et al. (2016) modify the basic model with RNN-based encoder and decoder and propose several techniques.\n[BOS] further propose to improve the novelty of generated summaries and design a distractionbased attentional model.\n[BOS] Li et al. (2017) creatively incorporate the variational auto-encoder into the seq2seq model to learn the latent structure information.\n[BOS] However, these models are nearly designed for abstractive sentence summarization, which focus on encoding and mining salient information on sentence-level and lead to unsatisfactory performances for document summarization.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 26, "token_end": 52, "char_start": 134, "char_end": 221, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Fan et al., 2017;": "22716243", "Ling and Rush, 2017;": "27970287", "Cheng and Lapata, 2016)": "1499080"}}}, {"token_start": 60, "token_end": 90, "char_start": 266, "char_end": 409, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 91, "token_end": 117, "char_start": 416, "char_end": 529, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Nallapati et al. (2016)": "8928715"}, "Reference": {}}}, {"token_start": 136, "token_end": 164, "char_start": 653, "char_end": 787, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Li et al. (2017)": "1508909"}, "Reference": {}}}]}
{"id": "91184639_4", "paragraph": "[BOS] Gradient descent is used for inference in several settings, e.g., structured prediction energy networks (Belanger and McCallum, 2016) , image generation applications (Mordvintsev et al., 2015; Gatys et al., 2015) , finding adversarial examples (Goodfellow et al., 2015) , learning paragraph embeddings (Le and Mikolov, 2014) , and machine translation (Hoang et al., 2017) .\n[BOS] Gradient descent has started to be replaced by inference networks in some of these settings, such as image transformation (Johnson et al., 2016; Li and Wand, 2016) .\n[BOS] Our results provide more evidence that gradient descent can be replaced by inference networks or improved through combination with them.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 17, "token_end": 31, "char_start": 72, "char_end": 139, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Belanger and McCallum, 2016)": "6366436"}}}, {"token_start": 32, "token_end": 55, "char_start": 142, "char_end": 218, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mordvintsev et al., 2015;": null, "Gatys et al., 2015)": "13914930"}}}, {"token_start": 56, "token_end": 70, "char_start": 221, "char_end": 275, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Goodfellow et al., 2015)": "6706414"}}}, {"token_start": 71, "token_end": 83, "char_start": 278, "char_end": 330, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Le and Mikolov, 2014)": "2407601"}}}, {"token_start": 85, "token_end": 96, "char_start": 337, "char_end": 377, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hoang et al., 2017)": null}}}, {"token_start": 116, "token_end": 132, "char_start": 487, "char_end": 549, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Johnson et al., 2016;": "980236", "Li and Wand, 2016)": "18781152"}}}]}
{"id": "91184639_3", "paragraph": "[BOS] There are also connections between inference networks and amortized inference (Srikumar et al., 2012) as well as methods for neural knowledge distillation and model compression (Hinton et al., 2015; Ba and Caruana, 2014; .\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 7, "token_end": 24, "char_start": 41, "char_end": 107, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Srikumar et al., 2012)": "2595749"}}}, {"token_start": 29, "token_end": 53, "char_start": 131, "char_end": 225, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hinton et al., 2015;": "7200347"}}}]}
{"id": "91184639_2", "paragraph": "[BOS] We focused on structured prediction in this paper, but inference networks are useful in other settings as well.\n[BOS] For example, it is common to use a particular type of inference network to approximate posterior inference in neural approaches to latent-variable probabilistic modeling, such as variational autoencoders (Kingma and Welling, 2013) .\n[BOS] In that setting, Kim et al. (2018) have found benefit with instance-specific updating of inference network parameters, which is related to our instance-level fine-tuning.\n\n", "discourse_tags": ["Reflection", "Narrative_cite", "Single_summ"], "span_citation_mapping": [{"token_start": 30, "token_end": 66, "char_start": 153, "char_end": 354, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kingma and Welling, 2013)": null}}}, {"token_start": 72, "token_end": 104, "char_start": 380, "char_end": 533, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kim et al. (2018)": "334803"}, "Reference": {}}}]}
{"id": "91184639_1", "paragraph": "[BOS] We focused in this paper on sequence labeling, in which CRFs with neural network potentials have emerged as a state-of-the-art approach (Lample et al., 2016; Ma and Hovy, 2016; Strubell et al., 2017; Yang et al., 2018) .\n[BOS] Our results suggest that inference networks can provide a feasible way to speed up test-time inference over Viterbi without much loss in performance.\n[BOS] The benefits of inference networks may be coming in part from multi-task training; Edunov et al. (2018) similarly found benefit from combining tokenlevel and sequence-level losses.\n\n", "discourse_tags": ["Narrative_cite", "Reflection", "Single_summ"], "span_citation_mapping": [{"token_start": 13, "token_end": 63, "char_start": 62, "char_end": 224, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lample et al., 2016;": "6042994", "Ma and Hovy, 2016;": "10489017", "Strubell et al., 2017;": "21428581", "Yang et al., 2018)": "48352299"}}}, {"token_start": 110, "token_end": 131, "char_start": 472, "char_end": 569, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Edunov et al. (2018)": "3718988"}, "Reference": {}}}]}
{"id": "91184639_0", "paragraph": "[BOS] The most closely related prior work is that of Tu and Gimpel (2018) , who experimented with RNN inference networks for sequence labeling.\n[BOS] We compared three architectural families, showed the relationship between optimal architectures and downstream tasks, compared inference networks to gradient descent, and proposed novel variations.\n\n", "discourse_tags": ["Multi_summ", "Reflection"], "span_citation_mapping": [{"token_start": 11, "token_end": 32, "char_start": 53, "char_end": 143, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Tu and Gimpel (2018)": "3790787"}, "Reference": {}}}]}
{"id": "202539031_4", "paragraph": "[BOS] Evaluating models on out-of-domain examples built by applying minor perturbations to existing examples has also been the subject of recent study (Szegedy et al., 2014; Belinkov and Bisk, 2018; Carlini and Wagner, 2018; Glockner et al., 2018) .\n[BOS] The domain shifts we consider involve larger changes to the input distribution, built to uncover higher-level flaws in existing models.\n\n", "discourse_tags": ["Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 13, "token_end": 62, "char_start": 59, "char_end": 247, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Szegedy et al., 2014;": "604334", "Glockner et al., 2018)": "19204066"}}}]}
{"id": "202539031_3", "paragraph": "[BOS] A related task is preventing models from using particular problematic dataset features, which is often studied from the perspective of fairness (Zhao et al., 2017; Burns et al., 2018) .\n[BOS] A popular approach is to use an adversary to remove information about a target feature, often gender or ethnicity, from a model's internal representations (Edwards and Storkey, 2016; Kim et al., 2019) .\n[BOS] In contrast, the biases we consider are related to features that are essential to the overall task, so they cannot simply be ignored.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 16, "token_end": 39, "char_start": 100, "char_end": 189, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhao et al., 2017;": "1389483"}}}, {"token_start": 59, "token_end": 85, "char_start": 299, "char_end": 398, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Edwards and Storkey, 2016;": "4986726", "Kim et al., 2019)": "56895575"}}}]}
{"id": "202539031_2", "paragraph": "[BOS] Recent work has focused on biases that come from ignoring parts of the input (e.g., guessing the answer to a question before seeing the evidence).\n[BOS] Solutions include generative objectives to force models to understand all the input (Lewis and Fan, 2019) , carefully designed model architecture (Agrawal et al., 2018; , or adversarial removal of class-indicative features from model's internal representations (Ramakrishnan et al., 2018; Zhang et al., 2018a; Belinkov et al., 2019; Grand and Belinkov, 2019) .\n[BOS] In contrast, we consider biases beyond partial-input cases (Feng et al., 2019) , and show our method is superior on VQA-CP.\n[BOS] Concurrently, He et al. (2019) also suggested using a product-of-experts ensemble to train unbiased models, but we consider a wider variety of ensembling approaches and test on additional domains.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Reflection", "Single_summ"], "span_citation_mapping": [{"token_start": 38, "token_end": 56, "char_start": 177, "char_end": 264, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lewis and Fan, 2019)": "86534652"}}}, {"token_start": 57, "token_end": 70, "char_start": 267, "char_end": 326, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 73, "token_end": 124, "char_start": 333, "char_end": 517, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ramakrishnan et al., 2018;": "52946763", "Zhang et al., 2018a;": "9424845", "Belinkov et al., 2019;": "195218513", "Grand and Belinkov, 2019)": "195218513"}}}, {"token_start": 131, "token_end": 146, "char_start": 551, "char_end": 604, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Feng et al., 2019)": "155092969"}}}, {"token_start": 162, "token_end": 201, "char_start": 670, "char_end": 852, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"He et al. (2019)": null}, "Reference": {}}}]}
{"id": "202539031_1", "paragraph": "[BOS] Recent dataset construction protocols have tried to avoid certain kinds of bias.\n[BOS] For example, both CoQA (Reddy et al., 2019) and QuAC ) take steps to prevent annotators from using words that occur in the context passage, VQA 2.0 (Goyal et al., 2018) selects examples to limit the effectiveness of question-only models, and others have filtered examples solvable by simple baselines (Yang et al., 2018; Zhang et al., 2018b; Zellers et al., 2018) .\n[BOS] While reducing bias is important, developing ways to prevent models from using known biases will allow us to continue to leverage existing datasets, and update our methods as our understanding of what biases we want to avoid evolve.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Transition"], "span_citation_mapping": [{"token_start": 19, "token_end": 51, "char_start": 106, "char_end": 231, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Reddy et al., 2019)": "52055325"}, "Reference": {}}}, {"token_start": 52, "token_end": 77, "char_start": 233, "char_end": 329, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Goyal et al., 2018)": "8081284"}, "Reference": {}}}, {"token_start": 80, "token_end": 112, "char_start": 342, "char_end": 456, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yang et al., 2018;": "52822214", "Zhang et al., 2018b;": null, "Zellers et al., 2018)": "52019251"}}}]}
{"id": "202539031_0", "paragraph": "[BOS] Researchers have raised concerns about bias in many datasets.\n[BOS] For example, many joint natu-1 github.com/chrisc36/debias ral language processing and vision datasets can be partially solved by models that ignore the vision aspect of the task (Jabri et al., 2016; Anand et al., 2018; Caglayan et al., 2019) .\n[BOS] Some questions in recent multi-hop QA datasets (Yang et al., 2018; Welbl et al., 2018) can be solved by single-hop models (Chen and Durrett, 2019; Min et al., 2019) .\n[BOS] Additional examples include story completion (Schwartz et al., 2017) and multiple choice questions (Clark et al., 2016 .\n[BOS] Recognizing that bias is a concern in diverse domains, our work is the first to perform an evaluation across multiple datasets spanning language and vision.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 48, "token_end": 81, "char_start": 222, "char_end": 315, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Jabri et al., 2016;": "11328415"}}}, {"token_start": 87, "token_end": 108, "char_start": 349, "char_end": 410, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yang et al., 2018;": "52822214", "Welbl et al., 2018)": "9192723"}}}, {"token_start": 112, "token_end": 133, "char_start": 428, "char_end": 488, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chen and Durrett, 2019;": "139103297", "Min et al., 2019)": "174801764"}}}, {"token_start": 138, "token_end": 148, "char_start": 525, "char_end": 565, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Schwartz et al., 2017)": "1994584"}}}, {"token_start": 149, "token_end": 159, "char_start": 570, "char_end": 615, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Clark et al., 2016": "1255845"}}}]}
{"id": "209063721_2", "paragraph": "[BOS] Visual QA systems: aim to answer questions based on contexts from images (Antol et al., 2015; .\n[BOS] Recently, neural modules (Kottur et al., 2018) are proposed to address specific challenges to VQA such as visual co-reference resolutions, etc.\n[BOS] Our work extends the idea of neural modules for Episodic Memory QA by implementing modules that can take graph paths as input for answer decoding.\n[BOS] Jiang et al. (2018) proposes visual memex QA which tackles similar problem domains given a dataset collected around photo albums.\n[BOS] Instead of relying on meta information and multi-modal content of a photo album, our work explicitly utilizes semantic and structural contexts from memory and knowledge graphs.\n[BOS] Another recent line of work for VQA includes graph based visual learning (Hudson and Manning, 2019) , which aims to represent each image with a sub-graph of visual contexts.\n[BOS] While graph-based VQA operates on a graph constructed from a single scene, Episodic Memory QA operates on a large-scale memory graph with knowledge nodes.\n[BOS] We therefore propose memory graph networks to handle ambiguous candidate nodes, a main contribution of the proposed work.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Reflection", "Single_summ", "Reflection", "Single_summ", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 23, "char_start": 6, "char_end": 98, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 28, "token_end": 58, "char_start": 118, "char_end": 251, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Kottur et al., 2018)": "52167938"}, "Reference": {}}}, {"token_start": 86, "token_end": 113, "char_start": 411, "char_end": 540, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Jiang et al. (2018)": null}, "Reference": {}}}, {"token_start": 146, "token_end": 185, "char_start": 730, "char_end": 903, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Hudson and Manning, 2019)": "152282269"}, "Reference": {}}}]}
{"id": "209063721_1", "paragraph": "[BOS] Machine Reading Comprehension (MRC) systems: aim at predicting answers given evidence documents, typically in length of a few paragraphs (Seo et al., 2017; Rajpurkar et al., 2016 Rajpurkar et al., , 2018 Cao et al., 2019; tau Yih et al., 2015) .\n[BOS] Several recent work address multi-hop reasoning within multiple documents (Yang et al., 2018; Welbl et al., 2018; Bauer et al., 2018; Clark et al., 2018) or conversational settings (Choi et al., 2018; Reddy et al., 2018) , which require often complex reasoning tools.\n[BOS] Unlike in MRC systems that typically rely on language understanding, we effectively utilize structural properties of memory graph to traverse and highlight specific attributes or nodes that are required to answer questions.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 18, "token_end": 68, "char_start": 103, "char_end": 249, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Seo et al., 2017;": "8535316", "Rajpurkar et al., 2016": "11816014", "Rajpurkar et al., , 2018": "47018994", "Cao et al., 2019;": "52116920", "tau Yih et al., 2015)": "18309765"}}}, {"token_start": 74, "token_end": 111, "char_start": 286, "char_end": 411, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yang et al., 2018;": "52822214", "Welbl et al., 2018;": "9192723", "Bauer et al., 2018;": "52290656", "Clark et al., 2018)": "3922816"}}}, {"token_start": 112, "token_end": 131, "char_start": 415, "char_end": 478, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Choi et al., 2018;": null, "Reddy et al., 2018)": "52055325"}}}]}
{"id": "209063721_0", "paragraph": "[BOS] Memory Networks: Weston et al. (2014) ; Sukhbaatar et al. (2016) propose Memory Networks with explicit memory slots to contain auxiliary multi-input, now widely used in many QA and MRC tasks for its transitive reasoning capability.\n[BOS] Traditional limitations are that memory slots for storing answer candidates are fixed in size, and naively increasing the slot size typically decreases the precision.\n[BOS] Several work extend this line of research, for example by allowing for dynamic update of memory slots given streams of input (Kumar et al., 2016; Tran et al., 2016; Xu et al., 2019) , reinforcement learning based retention control (Jung et al., 2018) , etc.\n[BOS] By allowing for storing graph nodes as memory slots and for slot expansion via graph traversals, our proposed Memory Graph Networks (MGN) effectively bypass the issues.\n[BOS] Structured QA systems: often answer questions based on large-scale common fact knowledge graphs (Bordes et al., 2015; tau Yih et al., 2015; Xu et al., 2016; Jain, 2016; Yin et al., 2016; Dubey et al., 2018) , typically via an entity linking system and a QA model for predicting graph operations through template matching approaches, etc.\n[BOS] Our approach is inspired by this line of work, and we utilize the proposed module networks and the MGN walker model to address unique challenges to Episodic Memory QA.\n\n", "discourse_tags": ["Multi_summ", "Transition", "Narrative_cite", "Reflection", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 53, "char_start": 6, "char_end": 237, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Sukhbaatar et al. (2016)": "1399322"}, "Reference": {}}}, {"token_start": 95, "token_end": 127, "char_start": 488, "char_end": 598, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kumar et al., 2016;": "2319779", "Tran et al., 2016;": "11637332", "Xu et al., 2019)": "174800228"}}}, {"token_start": 128, "token_end": 141, "char_start": 601, "char_end": 667, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Jung et al., 2018)": "54566346"}}}, {"token_start": 182, "token_end": 238, "char_start": 879, "char_end": 1062, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bordes et al., 2015;": "9605730", "tau Yih et al., 2015;": "18309765", "Xu et al., 2016;": "139787", "Jain, 2016;": "18780529", "Yin et al., 2016;": "557620", "Dubey et al., 2018)": "6263477"}}}]}
{"id": "208031414_2", "paragraph": "[BOS] Data Selection: Not all the data points from the source domain are equally important for target domain transfer.\n[BOS] Irrelevant source data points only add noise and overfit the training model.\n[BOS] Recent work from Ruder and Plank, applied Bayesian optimization to learn a scoring function to rank the source data points.\n[BOS] Data selection method was also used by Tsvetkov et al. to learn the curriculum for task-specific word representation learning, and by Axelrod et al. ; Duh et al. for machine translation using a neural language model.\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ", "Multi_summ"], "span_citation_mapping": [{"token_start": 41, "token_end": 61, "char_start": 225, "char_end": 331, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 62, "token_end": 110, "char_start": 338, "char_end": 554, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "208031414_1", "paragraph": "[BOS] Curriculum Learning: Curriculum Learning (Bengio et al., 2009 ) deals with the question of how to use prior knowledge about the difficulty of the training examples, to boost the rate of learning and the performance of the final model.\n[BOS] The ranking or weighting of the training examples is used to guide the order of presentation of examples to the learner.\n[BOS] The idea is to build a curriculum of progressively harder samples in order to significantly accelerate a neural network's train-ing.\n[BOS] While curriculum learning can leverage label information (loss of the model, training progress) (Weinshall and Amir, 2018) to guide data selection, this work assumes no or few labeled data in the new domain.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 5, "token_end": 15, "char_start": 27, "char_end": 67, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bengio et al., 2009": null}}}, {"token_start": 98, "token_end": 124, "char_start": 513, "char_end": 635, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Weinshall and Amir, 2018)": "54463963"}}}]}
{"id": "208031414_0", "paragraph": "[BOS] Our method is inspired by the work on curriculum learning and recent work on data selection for transfer learning.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "202769494_3", "paragraph": "[BOS] End-to-end task-oriented dialogue: Taskoriented dialogue systems have evolved from traditional modularized pipeline architectures (Rudnicky et al., 1999; to recent end-to-end neural frameworks (Eric and Manning, 2017a,b;  Yes, I would like the phone number please.\n[BOS] .\n[BOS] Our work is an innovative combination of ellipsis and co-reference resolution and the end-to-end task-oriented dialogue.\n\n", "discourse_tags": ["Narrative_cite", "Other", "Reflection"], "span_citation_mapping": [{"token_start": 20, "token_end": 33, "char_start": 101, "char_end": 158, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 36, "token_end": 53, "char_start": 170, "char_end": 225, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "202769494_2", "paragraph": "[BOS] Ellipsis and co-reference resolution in QA and Dialogue: The methods mentioned above do not generalize well to dialogues because they normally require a large amount of well-annotated contextual data with syntactic norms and candidate antecedents.\n[BOS] In recent years, a few studies try to solve ellipsis / co-reference resolution tailored for dialogue or QA tasks.\n[BOS] Kumar and Joshi (2016) train a semantic sequence model to learn semantic patterns and a syntactic sequence model to learn linguistic patterns to tackle with the non-sentential (incomplete) questions in a question answering system.\n[BOS] Zheng et al. (2018) builds a seq2seq neural network model for short texts to identify and recover ellipsis.\n[BOS] However, these methods are still limited to short texts or one-shot dialogues.\n[BOS] Our work is the first attempt to provide both solution and dataset for ellipsis and co-reference resolution in multi-turn dialogues.\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ", "Single_summ", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 72, "token_end": 115, "char_start": 380, "char_end": 610, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kumar and Joshi (2016)": "16222659"}, "Reference": {}}}, {"token_start": 116, "token_end": 141, "char_start": 617, "char_end": 724, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zheng et al. (2018)": null}, "Reference": {}}}]}
{"id": "202769494_1", "paragraph": "[BOS] Co-reference resolution: Co-reference resolution is mainly concerned with two sub-tasks, referring expressions (i.e., mentions) detection, and entity candidate ranking.\n[BOS] Uryupina and Moschitti (2013) propose a rule-based approach for coreference detection which employs parse tree features with an SVM model.\n[BOS] Peng et al. (2015) improve the performance of mention detection by applying a binary classififier on their feature set.\n[BOS] In recent years, applying deep neural networks to the co-reference resolution has gained great success.\n[BOS] Clark and Manning (2016) apply reinforcement learning on mention-ranking coreference resolution.\n[BOS] Lee et al. (2017) introduce the first end-to-end co-reference resolution model.\n[BOS] Lee et al. (2018) present a high-order co-reference resolution model with coarse-to-fine inference.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Transition", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 39, "token_end": 71, "char_start": 181, "char_end": 319, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Uryupina and Moschitti (2013)": "15723339"}, "Reference": {}}}, {"token_start": 72, "token_end": 98, "char_start": 326, "char_end": 445, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Peng et al. (2015)": "17010034"}, "Reference": {}}}, {"token_start": 119, "token_end": 137, "char_start": 562, "char_end": 658, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Clark and Manning (2016)": "2012188"}, "Reference": {}}}, {"token_start": 138, "token_end": 159, "char_start": 665, "char_end": 744, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Lee et al. (2017)": "1222212"}, "Reference": {}}}, {"token_start": 160, "token_end": 185, "char_start": 751, "char_end": 850, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Lee et al. (2018)": null}, "Reference": {}}}]}
{"id": "202769494_0", "paragraph": "[BOS] Ellipsis recovery: The earliest work on ellipsis as far as we know is the PUNDIT system (Palmer et al., 1986) which discusses the communication between the syntactic, semantic and pragmatic modules that is necessary for making implicit linguistic information explicit.\n[BOS] Dalrymple et al. (1991) and Shieber et al. (1996) 1 The new dataset and the code of our proposed system are available at https://multinlp.github.io/GECOR/ establish a set of linguistic theories in the ellipsis recovery of English verb phrases.\n[BOS] Nielsen (2003) first proposes an end-to-end computable system to perform English verb phrase ellipsis recovery on the original input text.\n[BOS] Liu et al. (2016) propose to decompose the resolution of the verb phrase ellipsis into three sub-tasks: target detection, antecedent head resolution, and antecedent boundary detection.\n\n", "discourse_tags": ["Single_summ", "Narrative_cite", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 19, "token_end": 53, "char_start": 80, "char_end": 274, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Palmer et al., 1986)": "5249956"}, "Reference": {}}}, {"token_start": 54, "token_end": 74, "char_start": 281, "char_end": 330, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Dalrymple et al. (1991)": null, "Shieber et al. (1996)": null}}}, {"token_start": 121, "token_end": 149, "char_start": 531, "char_end": 669, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Nielsen (2003)": null}, "Reference": {}}}, {"token_start": 150, "token_end": 186, "char_start": 676, "char_end": 860, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Liu et al. (2016)": "8024511"}, "Reference": {}}}]}
{"id": "67856013_2", "paragraph": "[BOS] The state-of-the-art system on GEC task is achieved by (Ge et al., 2018) , which are based on the sequence-to-sequence framework and fluency boost learning and inference mechanism.\n[BOS] However, the usage of the non-public CLC corpus (Nicholls, 2003) and self-collected non-public error-corrected sentence pairs from Lang-8 made their training data 3.6 times larger than the others and their results hard to compare.\n\n", "discourse_tags": ["Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 3, "token_end": 46, "char_start": 10, "char_end": 186, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Ge et al., 2018)": "49564245"}, "Reference": {}}}, {"token_start": 53, "token_end": 66, "char_start": 219, "char_end": 257, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Nicholls, 2003)": null}}}]}
{"id": "67856013_1", "paragraph": "[BOS] Recently, neural machine translation approaches have been shown to be very powerful.\n[BOS] (Yannakoudakis et al., 2017) developed a neural sequence-labeling model for error detection to calculate the probability of each token in a sentence as being correct or incorrect, and then use the error detecting model's result as a feature to re-rank the N best hypotheses.\n[BOS] (Ji et al., 2017) proposed a hybrid neural model incorporating both the word and character-level information.\n[BOS] (Chollampatt and Ng, 2018 ) used a multilayer convolutional encoder-decoder neural network and outperforms all prior neural and statistical based systems on this task.\n[BOS] tried deep RNN (Barone et al., 2017) and transformer (Vaswani et al., 2017) encoderdecoder models and got a higher result by using transformer and a set of model-independent methods for neural GEC.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 17, "token_end": 76, "char_start": 97, "char_end": 371, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Yannakoudakis et al., 2017)": "10289085"}, "Reference": {}}}, {"token_start": 77, "token_end": 100, "char_start": 378, "char_end": 487, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Ji et al., 2017)": "235645"}, "Reference": {}}}, {"token_start": 101, "token_end": 133, "char_start": 494, "char_end": 661, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Chollampatt and Ng, 2018": "19236015"}, "Reference": {}}}, {"token_start": 135, "token_end": 147, "char_start": 674, "char_end": 704, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 148, "token_end": 159, "char_start": 709, "char_end": 743, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vaswani et al., 2017)": "13756489"}}}]}
{"id": "67856013_0", "paragraph": "[BOS] Early published works in GEC develop specific classifiers for different error types and then use them to build hybrid systems.\n[BOS] Later, leveraging the progress of statistical machine translation(SMT) and large-scale error corrected data, GEC systems are further improved treated as a translation problem.\n[BOS] SMT systems can remember phrase-based correction pairs, but they are hard to generalize beyond what was seen in training.\n[BOS] The CoNLL-14 shared task overview paper (Ng et al., 2014) provides a comparative evaluation of approaches.\n[BOS] (Rozovskaya and Roth, 2016) detailed classification and machine translation approaches to grammatical error correction problems, and combined the strengths for both methods.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 86, "token_end": 109, "char_start": 453, "char_end": 555, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Ng et al., 2014)": null}, "Reference": {}}}, {"token_start": 110, "token_end": 141, "char_start": 562, "char_end": 735, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Rozovskaya and Roth, 2016)": "18563136"}, "Reference": {}}}]}
{"id": "202779863_0", "paragraph": "[BOS] The exploration on document summarization can be broadly divided into extractive and abstractive summarization.\n[BOS] The extractive methods (Nallapati et al., 2017; Jadhav and Rajan, 2018; Shi Article: poundland has been been forced to pull decorative plastic easter eggs from their shelves over fears children may choke -because they look like cadbury mini eggs .\n[BOS] trading standards officials in buckinghamshire and surrey raised the alarm over the chinese made decorations , as they were ' likely to contravene food imitation safety rules ' .\n[BOS] the eggs have now been withdrawn nationwide ahead of the easter break .\n[BOS] scroll down for video .\n[BOS] poundland has been been forced to pull decorative plastic easter eggs from their shelves over fears they may choke -because they look like cadbury mini eggs -lrb-pictured is the poundland version -rrb-.\n[BOS] the eggs bear a striking similarity to the sugar-coated chocolate treats with a brown ' speckle ' designed to make it look like a quail 's egg -lrb-cadbury mini eggs are pictured -rrb-.\n[BOS] .\n[BOS] .\n[BOS] . '\n[BOS] parents should also be wary of similar products being offered for sale over the easter period at other stores or online . '\n[BOS] Reference Summary: Trading standards officials in buckinghamshire and surrey raised alarm.\n[BOS] Officers said they were 'likely to contravene food imitation safety rules'.\n[BOS] The eggs bear a striking similarity to the sugar-coated chocolate treats.\n[BOS] PGN: Poundland has been forced to pull decorative plastic easter eggs from their shelves over fears children may chokebecause they look like cadbury mini eggs.\n[BOS] The eggs have now been withdrawn nationwide ahead of the easter break.\n[BOS] The eggs have now been withdrawn nationwide ahead of the easter break.\n[BOS] PGN+Coverage: Trading standards officials in buckinghamshire and surrey raised the alarm over the chinese made decorations , as they were ' likely to contravene food imitation safety rules ' the eggs have now been withdrawn nationwide ahead of the easter break .\n[BOS] the eggs bear a striking similarity to the sugar-coated chocolate treats with a brown ' speckle ' designed to make it look like a quail 's egg .\n[BOS] + ARU: Eggs bear a striking similarity to the sugar-coated chocolate treats with a brown 'speckle' designed to make it look like a quail's egg.\n[BOS] The eggs bear a striking similarity to the sugar-coated chocolate treats with a brown 'speckle' designed to make it look like a quail's egg.\n[BOS] + Variance loss: Trading standards officials in buckinghamshire and surrey raised the alarm over the chinese made decorations, as they were 'likely to contravene food imitation safety rules'.\n[BOS] The eggs have now been withdrawn nationwide ahead of the easter break.\n[BOS] The eggs bear a striking similarity to the sugar-coated chocolate treats with a brown 'speckle'.\n[BOS] (Rush et al., 2015; Nallapati et al., 2016; See et al., 2017; Chen and Bansal, 2018) generates summaries word-by-word after digesting the main content of the document.\n[BOS] Outof-vocabulary(OOV), repetition, and saliency are three conspicuous problems need to be well solved in abstractive document summarization.\n[BOS] Some works (Nallapati et al., 2016; See et al., 2017; Paulus et al., 2018) handle the OOV problem by introducing the pointer network.\n[BOS] See et al. (2017) introduces a coverage mechanism, which is a variant of the coverage vector (Tu et al., 2016) from Neural Machine Translation, to eliminate repetitions.\n[BOS] However, there are just a few studies on saliency problem (Tan et al., 2017; Shi et al., 2018; Gehrmann et al., 2018) .\n[BOS] To obtain more salient in-formation, Chen et al. (2016) proposes a new attention mechanism to distract them in the decoding step to better grasp the overall meaning of input documents.\n[BOS] We optimize attention using an attention refinement unit under the novel variance loss supervision.\n[BOS] As far as we know, we are the first to propose explicit losses to refine the attention model in abstractive document summarization tasks.\n[BOS] Recently many models (Paulus et al., 2018; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Zhou et al., 2018; Jiang and Bansal, 2018) have emerged taking advantage of reinforcement learning (RL) to solve the discrepancy issue in seq2seq model and have yielded the state-of-the-art performance.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Other", "Other", "Other", "Other", "Other", "Other", "Other", "Other", "Other", "Other", "Other", "Other", "Other", "Other", "Other", "Other", "Other", "Other", "Other", "Transition", "Transition", "Other", "Multi_summ", "Transition", "Narrative_cite", "Single_summ", "Narrative_cite", "Single_summ", "Reflection", "Reflection", "Narrative_cite"], "span_citation_mapping": [{"token_start": 22, "token_end": 44, "char_start": 124, "char_end": 194, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Nallapati et al., 2017;": "6405271"}}}, {"token_start": 646, "token_end": 695, "char_start": 2902, "char_end": 3069, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Rush et al., 2015;": "1918428", "Nallapati et al., 2016;": "8928715", "See et al., 2017;": null, "Chen and Bansal, 2018)": "44129061"}, "Reference": {}}}, {"token_start": 727, "token_end": 754, "char_start": 3223, "char_end": 3297, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Nallapati et al., 2016;": "8928715", "See et al., 2017;": null, "Paulus et al., 2018)": "21850704"}}}, {"token_start": 766, "token_end": 803, "char_start": 3363, "char_end": 3532, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"See et al. (2017)": null}, "Reference": {"(Tu et al., 2016)": "146843"}}}, {"token_start": 813, "token_end": 839, "char_start": 3580, "char_end": 3656, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Tan et al., 2017;": "26698484", "Shi et al., 2018;": "53230962", "Gehrmann et al., 2018)": "52144157"}}}, {"token_start": 849, "token_end": 879, "char_start": 3702, "char_end": 3849, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Chen et al. (2016)": "14288483"}, "Reference": {}}}, {"token_start": 924, "token_end": 968, "char_start": 4106, "char_end": 4240, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Paulus et al., 2018;": "21850704", "Celikyilmaz et al., 2018;": "4406182", "Chen and Bansal, 2018;": "44129061", "Zhou et al., 2018;": "49656757", "Jiang and Bansal, 2018)": "52198214"}}}]}
{"id": "207910555_1", "paragraph": "[BOS] Rush (2015) have applied Seq2Seq model with attention mechanism for text summarization.\n[BOS] Prakash (2016) employ a residual net in Seq2Seq model to generate paraphrases.\n[BOS] Cao (2017) combine a copying decoder and a generative decoder for paraphrase generation.\n[BOS] Cao(2018) try to utilize template information to help text summarization, however, the template is vague in that paper.\n[BOS] We hope to utilize the special structure of question and extract the template explicitly from questions.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 21, "char_start": 6, "char_end": 93, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 22, "token_end": 42, "char_start": 100, "char_end": 178, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 43, "token_end": 61, "char_start": 185, "char_end": 273, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Cao (2017)": "13531903"}, "Reference": {}}}, {"token_start": 62, "token_end": 87, "char_start": 280, "char_end": 399, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Cao(2018)": "51878811"}, "Reference": {}}}]}
{"id": "207910555_0", "paragraph": "[BOS] There are two lines of research for paraphrase generation including knowledge based ones and neural network based ones.\n[BOS] Some researchers provide rules (Bhagat and Hovy, 2013) or corpus including knowledge (Fader et al., 2013; Ganitkevitch et al., 2013; Pavlick et al., 2015) .\n[BOS] Other researchers try to make use of templates (Berant and Liang, 2014) , semantic information (Kozlowski et al., 2003) and thesaurus (Hassan et al., 2007) for paraphrase generation.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 27, "token_end": 38, "char_start": 157, "char_end": 186, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bhagat and Hovy, 2013)": null}}}, {"token_start": 39, "token_end": 70, "char_start": 190, "char_end": 286, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Fader et al., 2013;": "8893912", "Ganitkevitch et al., 2013;": "6067240", "Pavlick et al., 2015)": "9711750"}}}, {"token_start": 79, "token_end": 88, "char_start": 332, "char_end": 366, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Berant and Liang, 2014)": "1336493"}}}, {"token_start": 89, "token_end": 101, "char_start": 369, "char_end": 414, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kozlowski et al., 2003)": "3099330"}}}, {"token_start": 102, "token_end": 114, "char_start": 419, "char_end": 450, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hassan et al., 2007)": "10296658"}}}]}
{"id": "196202909_3", "paragraph": "[BOS] The related works have achieved good results for text adversarial attacks, but there is still much room for improvement regarding the percentage of modifications, attacking success rate, maintenance on lexical as well as grammatical correctness and semantic similarity, etc.\n[BOS] Based on the synonyms substitution strategy, we propose a novel blackbox attack method called PWWS for the NLP classification tasks and contribute to the field of adversarial machine learning.\n\n", "discourse_tags": ["Transition", "Reflection"], "span_citation_mapping": []}
{"id": "196202909_2", "paragraph": "[BOS] As the above methods all need to calculate the gradient with access to the model structure, model parameters, and the feature set of the inputs, they are classified as white-box attacks.\n[BOS] To achieve attack under a black-box setting, which assumes no access to the details of the model or the feature representation of the inputs, Alzantot et al. (2018) propose to use a population-based optimization algorithm.\n[BOS] Gao et al. (2018) present a DeepWordBug algorithm to generate small perturbations in the character-level for black-box attack.\n[BOS] They sort the tokens based on the importance evaluated by four functions, and make random token transformations such as substitution and deletion with the constraint of edit distance.\n[BOS] Ebrahimi et al. (2018) also propose a token transformation method, and it is based on the gradients of the one-hot input vectors.\n[BOS] The downside of the character-level perturbations is that they usually lead to lexical errors, which hurts the readability and can easily be perceived by humans.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 40, "token_end": 87, "char_start": 199, "char_end": 421, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Alzantot et al. (2018)": "5076191"}, "Reference": {}}}, {"token_start": 88, "token_end": 148, "char_start": 428, "char_end": 744, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Gao et al. (2018)": "4858173"}, "Reference": {}}}, {"token_start": 149, "token_end": 214, "char_start": 751, "char_end": 1048, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ebrahimi et al. (2018)": "21698802"}, "Reference": {}}}]}
{"id": "196202909_1", "paragraph": "[BOS] Therefore, subsequent research are mainly based on the word substitution strategy so as to avoid artificial fabrications and achieve automatic generations.\n[BOS] The key difference of these subsequent methods is on how they generate substitute words.\n[BOS] Samanta and Mehta (2017) propose to build a candidate pool that includes synonyms, typos and genre specific keywords.\n[BOS] They adopt Fast Gradient Sign Method (FGSM) (Goodfellow et al., 2015) to pick a candidate word for replacement.\n[BOS] Papernot et al. (2016b) perturb a word vector by calculating forward derivative (Papernot et al., 2016a) and map the perturbed word vector to a closest word in the word embedding space.\n[BOS] Yang et al. (2018) derive two methods, Greedy Attack based on perturbation, and Gumbel Attack based on scalable learning.\n[BOS] Aiming to restore the interpretability of adversarial attacks based on word substitution strategy, Sato et al. (2018) restrict the direction of perturbations towards existing words in the input embedding space.\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ", "Narrative_cite", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 43, "token_end": 69, "char_start": 263, "char_end": 380, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Samanta and Mehta (2017)": "38134825"}, "Reference": {}}}, {"token_start": 72, "token_end": 90, "char_start": 398, "char_end": 456, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Goodfellow et al., 2015)": "6706414"}}}, {"token_start": 99, "token_end": 142, "char_start": 505, "char_end": 690, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Papernot et al. (2016b)": "12390290"}, "Reference": {"(Papernot et al., 2016a)": "7004303"}}}, {"token_start": 143, "token_end": 169, "char_start": 697, "char_end": 818, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Yang et al. (2018)": "44081093"}, "Reference": {}}}, {"token_start": 170, "token_end": 207, "char_start": 825, "char_end": 1035, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Sato et al. (2018)": "13684516"}, "Reference": {}}}]}
{"id": "196202909_0", "paragraph": "[BOS] We first provide a brief review on related works for attacking text classification models.\n[BOS] Liang et al. (2018) propose to find appropriate words for insertion, deletion and replacement by calculating the word frequency and the highest gradient magnitude of the cost function.\n[BOS] But their method involves considerable human participation in crafting the adversarial examples.\n[BOS] To maintain semantic similarity and avoid human detection, it requires human efforts such as searching related facts online for insertion.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 19, "token_end": 92, "char_start": 103, "char_end": 535, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Liang et al. (2018)": "10642653"}, "Reference": {}}}]}
{"id": "195218742_0", "paragraph": "[BOS] Zero-shot translation is of considerable concern among the multilingual translation community.\n[BOS] By sharing network parameters across languages, ZS was proven feasible for universal multilingual MT (Ha et al., 2016; Johnson et al., 2016) .\n[BOS] There are many variations of multilingual models geared towards zero-shot translation.\n[BOS] Lu et al. (2018) proposed to explicitly define a recurrent layer with a fixed number of states as \"Interlingua\" which resembles our attention-pooling models.\n[BOS] However, they compromise the model compactness by having separate encoder-decoder per language, which linearly increases the model size across languages.\n[BOS] On the other hand, Platanios et al. (2018) shares all parameters, but utilized a parameter generator to generate specific parameters for the LSTMs in each language pair using language embeddings.\n[BOS] The closest to our work is probably Arivazhagan et al. (2019) the model into a common encoding space by taking the mean-pooling of the encoder states and minimize the cosine similarity between the source and the target sentence encodings.\n[BOS] In comparison, our approach is more generalized because the decoder is also taken into account during regularization, which is shown by our results on the IWSLT benchmark 9 .\n[BOS] Also, we proposed stronger representation-forcing since the cosine similarity minimizes the angle between two representational vectors, while the MSE forces them to be exactly equal.\n[BOS] In addition, zero-resource techniques which generate artificial data for the missing directions have been proposed as an alternative to zero-shot translation (Chen et al., 2018; Al-Shedivat and Parikh, 2019; Chen et al., 2017) .\n[BOS] The main disadvantage, however, is the requirement of computationally expensive sampling during training which makes the algorithm less scalable to the number of languages.\n[BOS] In our work, we focus on minimally affecting the training paradigm of universal multilingual NMT.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection", "Reflection", "Narrative_cite", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 30, "token_end": 49, "char_start": 182, "char_end": 247, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ha et al., 2016;": "5234044", "Johnson et al., 2016)": "6053988"}}}, {"token_start": 68, "token_end": 128, "char_start": 349, "char_end": 666, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 134, "token_end": 169, "char_start": 692, "char_end": 868, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Platanios et al. (2018)": "52100117"}, "Reference": {}}}, {"token_start": 176, "token_end": 220, "char_start": 902, "char_end": 1113, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Arivazhagan et al. (2019)": "81978057"}, "Reference": {}}}, {"token_start": 290, "token_end": 340, "char_start": 1503, "char_end": 1716, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chen et al., 2018;": null, "Al-Shedivat and Parikh, 2019;": "102353391", "Chen et al., 2017)": "2166461"}}}]}
{"id": "174797942_2", "paragraph": "[BOS] Several models have been presented for DAG parsing (Sagae and Tsujii, 2008; Ribeyre et al., 2014; Tokgz and Glsen, 2015; Hershcovich et al., 2017) .\n[BOS] Wang et al. (2018) proposed a similar transition-based parsing model for SDP; they modified the possible transitions of the ArcEager algorithm (Nivre and Scholz, 2004b) to create multi-headed graphs.\n[BOS] We are, to the best of our knowledge, first to explore reinforcement learning for DAG parsing.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 8, "token_end": 48, "char_start": 45, "char_end": 152, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sagae and Tsujii, 2008;": "8836054", "Ribeyre et al., 2014;": "2300025", "Tokg\u00f6z and G\u00fclsen, 2015;": null}}}, {"token_start": 50, "token_end": 98, "char_start": 161, "char_end": 360, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wang et al. (2018)": "19118353"}, "Reference": {"(Nivre and Scholz, 2004b)": "643522"}}}]}
{"id": "174797942_1", "paragraph": "[BOS] There have been several attempts to train transition-based parsers with reinforcement learning: Chan (2009) applied SARSA (Baird III, 1999) to an Arc-Standard model, using SARSA updates to fine-tune a model that was pre-trained using a feed-forward neural network.\n[BOS] Fried and Klein (2018) , more recently, presented experiments with applying policy gradient training to several constituency parsers, including the RNNG transition-based parser (Dyer et al., 2016) .\n[BOS] In their experiments, however, the models trained with policy gradient did not always perform better than the models trained with supervised learning.\n[BOS] We hypothesize this is due to credit assignment being difficult in transition-based parsing.\n[BOS] Iterative refinement approaches have been proposed in the context of sentence generation (Lee et al., 2018) .\n[BOS] Our proposed model explores multiple transition paths at once and avoids making risky decisions in the initial transitions, in part inspired by such iterative refinement techniques.\n[BOS] We also pre-train our model with supervised learning to avoid sampling from irrelevant states at the early stages of policy gradient training.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Reflection", "Narrative_cite", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 18, "token_end": 62, "char_start": 102, "char_end": 270, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 63, "token_end": 131, "char_start": 277, "char_end": 632, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Fried and Klein (2018)": "44158569"}, "Reference": {"(Dyer et al., 2016)": "1949831"}}}, {"token_start": 159, "token_end": 169, "char_start": 807, "char_end": 845, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lee et al., 2018)": "3438497"}}}]}
{"id": "174797942_0", "paragraph": "[BOS] There are generally two kinds of dependency parsing algorithms, namely transition-based parsing algorithms (McDonald and Nivre, 2007; Kiperwasser and Goldberg, 2016; Ballesteros et al., 2015) and graph-based ones (McDonald and Pereira, 2006; Zhang and Clark, 2008; Galley and Manning, 2009; Zhang et al., 2017) .\n[BOS] In graphbased parsing, a model is trained to score all possible dependency arcs between words, and decoding algorithms are subsequently applied to find the most likely dependency graph.\n[BOS] The Eisner algorithm (Eisner, 1996) and the Chu-Liu-Edmonds algorithm are often used for finding the most likely dependency trees, whereas the AD 3 algorithm (Martins et al., 2011 ) is used for finding SDP graphs that form DAGs in Peng et al. (2017) and Peng et al. (2018) .\n[BOS] During training, the loss is computed after decoding, leading the models to reflect a structured loss.\n[BOS] The advantage of graphbased algorithms is that there is no real error propagation to the extent the decoding algorithms are global inference algorithm, but this also means that reinforcement learning is not obviously applicable to graph-based parsing.\n[BOS] In transition-based parsing, the model is typically taught to follow a gold transition path to obtain a perfect dependency graph during training.\n[BOS] This training paradigm has the limitation that the model only ever gets to see states that are on gold transition paths, and error propagation is therefore likely to happen when the parser predicts wrong transitions leading to unseen states (McDonald and Nivre, 2007; Goldberg and Nivre, 2013) .\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Narrative_cite", "Transition", "Transition", "Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 13, "token_end": 44, "char_start": 77, "char_end": 197, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(McDonald and Nivre, 2007;": "1900468", "Kiperwasser and Goldberg, 2016;": "1642392", "Ballesteros et al., 2015)": "256149"}}}, {"token_start": 45, "token_end": 79, "char_start": 202, "char_end": 316, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(McDonald and Pereira, 2006;": "802998", "Zhang and Clark, 2008;": "15533677", "Galley and Manning, 2009;": "2869969", "Zhang et al., 2017)": "14081838"}}}, {"token_start": 115, "token_end": 124, "char_start": 521, "char_end": 552, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Eisner, 1996)": "3262717"}}}, {"token_start": 147, "token_end": 158, "char_start": 660, "char_end": 696, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Martins et al., 2011": "1779773"}}}, {"token_start": 168, "token_end": 186, "char_start": 740, "char_end": 789, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Peng et al. (2017)": "15939234"}}}, {"token_start": 301, "token_end": 334, "char_start": 1442, "char_end": 1610, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(McDonald and Nivre, 2007;": "1900468", "Goldberg and Nivre, 2013)": "815755"}}}]}
{"id": "189762214_12", "paragraph": "[BOS] For ease of exposition, we describe E 3 for a single turn in the conversation.\n[BOS] To make the references concrete in the following sections, we use as an example the inputs and outputs from Figure 1 .\n[BOS] This example describes a turn in a conversation in which the system helps the user determine whether they need to pay UK taxes on their pension.\n\n", "discourse_tags": ["Reflection", "Reflection", "Other"], "span_citation_mapping": []}
{"id": "189762214_11", "paragraph": "[BOS] The key challenges in CMR are to identify implicit rules present in the document, understand which rules are necessary to answer the question, and inquire about necessary rules that are not entailed by the conversation history by asking follow-up questions.\n[BOS] The three core modules of E 3 , the extraction, entailment, and decision modules, combine to address these challenges.\n[BOS] Figure 2 illustrates the components of E 3 .\n\n", "discourse_tags": ["Transition", "Other", "Other"], "span_citation_mapping": []}
{"id": "189762214_10", "paragraph": "[BOS] In order to answer the user's question, the system must ask the user a series of follow-up questions to determine whether the user satisfies the set of decision rules.\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "189762214_9", "paragraph": "[BOS] w 5 i f P i v D s f y 9 a C k 8 + c w h 8 4 n z / J n Y 8 m < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" C U 4 D P r y C j v 9 j 5 0 rules.\n[BOS] The user presents a scenario describing their situation, and asks the system an underspecified question.\n\n", "discourse_tags": ["Other", "Transition"], "span_citation_mapping": []}
{"id": "189762214_8", "paragraph": "[BOS] w 5 i f P i v D s f y 9 a C k 8 + c w h 8 4 n z / J n Y 8 m < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" C U 4 D P r y C j v 9 j 5 0\n\n", "discourse_tags": ["Other"], "span_citation_mapping": []}
{"id": "189762214_7", "paragraph": "[BOS] w 5 i f P i v D s f y 9 a C k 8 + c w h 8 4 n z / J n Y 8 m < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" C U 4 D P r y C j v 9 j 5 0\n\n", "discourse_tags": ["Other"], "span_citation_mapping": []}
{"id": "189762214_6", "paragraph": "[BOS] w 5 i f P i v D s f y 9 a C k 8 + c w h 8 4 n z / J n Y 8 m < / l a t e x i t > x H,1 < l a t e x i t s h a 1 _ b a s e 6 4 = \" C U 4 D P r y C j v 9 j 5 0\n\n", "discourse_tags": ["Other"], "span_citation_mapping": []}
{"id": "189762214_5", "paragraph": "[BOS] w 5 i f P i v D s f y 9 a C k 8 + c w h 8 4 n z / J n Y 8 m < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" C U 4 D P r y C j v 9 j 5 0\n\n", "discourse_tags": ["Other"], "span_citation_mapping": []}
{"id": "189762214_4", "paragraph": "[BOS] w 5 i f P i v D s f y 9 a C k 8 + c w h 8 4 n z / J n Y 8 m < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" C U 4 D P r y C j v 9 j 5 0\n\n", "discourse_tags": ["Other"], "span_citation_mapping": []}
{"id": "189762214_3", "paragraph": "[BOS] w 5 i f P i v D s f y 9 a C k 8 + c w h 8 4 n z / J n Y 8 m < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" C U 4 D P r y C j v 9 j 5 0\n\n", "discourse_tags": ["Other"], "span_citation_mapping": []}
{"id": "189762214_2", "paragraph": "[BOS] Extracting latent rules from text.\n[BOS] There is a long history of work on extracting knowledge automatically from text (Moulin and Rousseau, 1992) .\n[BOS] Relation extraction typically assumes that there is a fixed ontology onto which extracted knowledge falls (Mintz et al., 2009; Riedel et al., 2013) .\n[BOS] Other works forgo the ontology by using, for example, natural language (Angeli and Manning, 2014; Angeli et al., 2015) .\n[BOS] These extractions from text are subsequently used for inference over a knowledge base (Bordes et al., 2013; Dettmers et al., 2018; Lin et al., 2018) and rationalizing model predictions (Lei et al., 2016) .\n[BOS] Our work is more similar with the latter type in which knowledge extracted are not confined to a fixed ontology and instead differ on a document basis.\n[BOS] In addition, the rules extracted by our model are used for inference over natural language documents.\n[BOS] Finally, these rules provide rationalization for the model's decision making, in the sense that the user can visualize what rules the model extracted and which rules are entailed by previous turns.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite", "Narrative_cite", "Narrative_cite", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 19, "token_end": 33, "char_start": 103, "char_end": 154, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Moulin and Rousseau, 1992)": null}}}, {"token_start": 47, "token_end": 67, "char_start": 243, "char_end": 310, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mintz et al., 2009;": "10910955", "Riedel et al., 2013)": "2687019"}}}, {"token_start": 71, "token_end": 100, "char_start": 331, "char_end": 437, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Angeli and Manning, 2014;": "2854390", "Angeli et al., 2015)": "6015236"}}}, {"token_start": 111, "token_end": 141, "char_start": 500, "char_end": 594, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bordes et al., 2013;": null, "Dettmers et al., 2018;": "4328400", "Lin et al., 2018)": "52143467"}}}, {"token_start": 142, "token_end": 154, "char_start": 599, "char_end": 649, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lei et al., 2016)": "7205805"}}}]}
{"id": "189762214_1", "paragraph": "[BOS] Dialogue systems.\n[BOS] One traditional approach for designing dialogue systems divides the task into language understanding/state-tracking Zhong et al., 2018) , reasoning/policy learning (Su et al., 2016) , and response generation (Wen et al., 2015) .\n[BOS] The models for each of these subtasks are then combined to form a full dialogue system (Young et al., 2013; .\n[BOS] The previous best system for ShARC (Saeidi et al., 2018 ) similarly breaks the CMR task into subtasks and combines handdesigned sub-models for decision classification, entailment, and follow-up generation.\n[BOS] In contrast, the core reasoning (e.g. non-editor) components of E 3 are jointly trained, and does not require complex hand-designed features.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 17, "token_end": 30, "char_start": 108, "char_end": 165, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Zhong et al., 2018)": "43959086"}}}, {"token_start": 31, "token_end": 43, "char_start": 168, "char_end": 211, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Su et al., 2016)": "8443958"}}}, {"token_start": 45, "token_end": 55, "char_start": 218, "char_end": 256, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wen et al., 2015)": "739696"}}}, {"token_start": 71, "token_end": 81, "char_start": 331, "char_end": 371, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 85, "token_end": 128, "char_start": 385, "char_end": 586, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Saeidi et al., 2018": "52165754"}, "Reference": {}}}]}
{"id": "189762214_0", "paragraph": "[BOS] Dialogue tasks.\n[BOS] Recently, there has been growing interest in question answering (QA) in a dialogue setting (Choi et al., 2018; Reddy et al., 2019) .\n[BOS] CMR (Saeidi et al., 2018) differs from dialogue QA in the domain covered (regulatory text vs Wikipedia).\n[BOS] A consequence of this is that CMR requires the interpretation of complex decision rules in order to answer high-level questions, whereas dialogue QA typically contains questions whose answers are directly extractable from the text.\n[BOS] In addition, CMR requires the formulation of free-form follow-up questions in order to identify whether the user satisfies decision rules, whereas dialogue QA does not.\n[BOS] There has also been significant work on task-oriented dialogue, where the system must inquire about miss-ing information in order to help the user achieve a goal (Williams et al., 2013; Henderson et al., 2014; Young et al., 2013) .\n[BOS] However, these tasks are typically constrained to a fixed ontology (e.g. restaurant reservation), instead of a latent ontology specified via natural language documents.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Single_summ", "Single_summ", "Single_summ", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 14, "token_end": 40, "char_start": 73, "char_end": 158, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Choi et al., 2018;": "52057510", "Reddy et al., 2019)": "52055325"}}}, {"token_start": 42, "token_end": 140, "char_start": 167, "char_end": 684, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Saeidi et al., 2018)": "52165754"}, "Reference": {}}}, {"token_start": 155, "token_end": 196, "char_start": 765, "char_end": 920, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Williams et al., 2013;": null, "Henderson et al., 2014;": "1294169", "Young et al., 2013)": null}}}]}
{"id": "184483086_3", "paragraph": "[BOS] Previous editions of related workshops are TA-COS 1 , Abusive Language Online 2 , and TRAC 3 and related shared tasks are GermEval (Wiegand et al., 2018) and TRAC (Kumar et al., 2018) .\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 28, "token_end": 41, "char_start": 128, "char_end": 159, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 42, "token_end": 51, "char_start": 164, "char_end": 189, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kumar et al., 2018)": "59336626"}}}]}
{"id": "184483086_2", "paragraph": "[BOS] A proposal of typology of abusive language sub-tasks is presented in (Waseem et al., 2017) .\n[BOS] For studies on languages other than English see (Su et al., 2017) on Chinese and (Fier et al., 2017) on Slovene.\n[BOS] Finally, for recent discussion on identifying profanity vs. hate speech see .\n[BOS] This work highlighted the challenges of distinguishing between profanity, and threatening language which may not actually contain profane language.\n\n", "discourse_tags": ["Single_summ", "Narrative_cite", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 3, "token_end": 27, "char_start": 8, "char_end": 96, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Waseem et al., 2017)": "8821211"}, "Reference": {}}}, {"token_start": 33, "token_end": 47, "char_start": 130, "char_end": 181, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Su et al., 2017)": "32076269"}}}, {"token_start": 48, "token_end": 62, "char_start": 186, "char_end": 217, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Fi\u0161er et al., 2017)": "28550367"}}}]}
{"id": "184483086_1", "paragraph": "[BOS] We were guided by the work of (Zhang et al., 2018) who used a CNN+GRU based approach for a similar task.\n[BOS] We use an approach which was influenced by this work but used an LSTM based approach.\n\n", "discourse_tags": ["Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 31, "char_start": 6, "char_end": 110, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Zhang et al., 2018)": "46939253"}, "Reference": {}}}]}
{"id": "184483086_0", "paragraph": "[BOS] Papers which have been published in the last two years include the surveys by (Schmidt and Wiegand, 2017) and (Fortuna and Nunes, 2018) , the paper by presenting the Hate Speech Detection dataset used in (Malmasi and Zampieri, 2017 ) and a few other recent papers such as (ElSherief et al., 2018; Gambck and Sikdar, 2017; Zhang et al., 2018) .\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 16, "token_end": 25, "char_start": 84, "char_end": 111, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Schmidt and Wiegand, 2017)": "9626793"}}}, {"token_start": 26, "token_end": 35, "char_start": 116, "char_end": 141, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Fortuna and Nunes, 2018)": "52184457"}}}, {"token_start": 41, "token_end": 58, "char_start": 172, "char_end": 237, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Malmasi and Zampieri, 2017": "19182892"}}}, {"token_start": 63, "token_end": 94, "char_start": 256, "char_end": 347, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(ElSherief et al., 2018;": "4809781", "Gamb\u00e4ck and Sikdar, 2017;": null, "Zhang et al., 2018)": "46939253"}}}]}
{"id": "196183935_4", "paragraph": "[BOS] Finally, recent work by Leonandya et al. (2018) also explores neural architectures for the block stacking task we used in section 4.1.\n[BOS] The authors recognize the need for additional inductive bias, and introduce this bias by creating additional synthetic supervised data with artificial language, creating a transfer learning-style setup.\n[BOS] This is in contrast to our unsupervised pre-training method that does not need language for the additional data.\n[BOS] Even with their stronger data assumptions, their online accuracy evaluation reaches just 23%, compared to our result of 28.5%, providing independent verification of the difficulty of this task for neural networks.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 4, "token_end": 66, "char_start": 15, "char_end": 349, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Leonandya et al. (2018)": "52290717"}, "Reference": {}}}]}
{"id": "196183935_3", "paragraph": "[BOS] Another related line of research uses language to guide learning about an environment (Branavan et al., 2012; Srivastava et al., 2017; Andreas et al., 2018; Hancock et al., 2018) .\n[BOS] These papers use language to learn about an environment more efficiently, which can be seen as a kind of inverse to our work, where we use environment knowledge to learn language more efficiently.\n\n", "discourse_tags": ["Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 8, "token_end": 50, "char_start": 44, "char_end": 184, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Branavan et al., 2012;": "8781666", "Srivastava et al., 2017;": "30444239", "Andreas et al., 2018;": "37390552", "Hancock et al., 2018)": "13696741"}}}]}
{"id": "196183935_2", "paragraph": "[BOS] Model-based reinforcement learning is another area of work that improves data-efficiency by learning from observations of an environment Deisenroth et al., 2013; Kaiser et al., 2019) .\n[BOS] It differs from the current work in which aspect of the environment it seeks to capture: in model-based RL the goal is to model which states will result from taking a particular action, but in this work we aim to learn patterns in what actions tend to be chosen by a knowledgeable actor.\n\n", "discourse_tags": ["Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 18, "token_end": 41, "char_start": 98, "char_end": 188, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Deisenroth et al., 2013;": null, "Kaiser et al., 2019)": null}}}]}
{"id": "196183935_1", "paragraph": "[BOS] Our approach also relates to recent work on learning artificial languages by simulating agents interacting in an environment (Mordatch and Abbeel, 2018; Das et al., 2017; Kottur et al., 2017, i.a.)\n[BOS] .\n[BOS] Our environment learning procedure could be viewed as a language learning game where the encoder is a speaker and the decoder is a listener.\n[BOS] The speaker must create a \"language\" a that allows the decoder to complete a task.\n[BOS] Many of these papers have found that it is possible to induce representations that align semantically with language humans use, as explored in detail in Andreas and Klein (2017) .\n[BOS] Our analysis in Section 7 is based on the method from this work.\n\n", "discourse_tags": ["Narrative_cite", "Reflection", "Reflection", "Reflection", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 10, "token_end": 51, "char_start": 50, "char_end": 203, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mordatch and Abbeel, 2018;": "13548281", "Das et al., 2017;": "1448723"}}}, {"token_start": 111, "token_end": 133, "char_start": 509, "char_end": 631, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Andreas and Klein (2017)": "5623056"}}}]}
{"id": "196183935_0", "paragraph": "[BOS] Many other works use autoencoders to form representations in an unsupervised or semi-supervised way.\n[BOS] Variants such as denoising autoencoders (Vincent et al., 2008) and variational autoencoders (Kingma and Welling, 2013) have been used for various vision and language tasks.\n[BOS] In the area of semantic grounding, Koisk et al. (2016) perform semi-supervised semantic parsing using an autoencoder where the latent state takes the form of language.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Single_summ"], "span_citation_mapping": [{"token_start": 25, "token_end": 37, "char_start": 130, "char_end": 175, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vincent et al., 2008)": "207168299"}}}, {"token_start": 38, "token_end": 51, "char_start": 180, "char_end": 231, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kingma and Welling, 2013)": null}}}, {"token_start": 62, "token_end": 99, "char_start": 292, "char_end": 459, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "128350532_3", "paragraph": "[BOS] Theoretically, employing GANs for training cross-lingual word embedding is also a promising way to avoid the use of bilingual evidence.\n[BOS] As far as we know, Barone (2016) was the first attempt at this approach, but the performance of their model is not competitive.\n[BOS] Zhang et al. (2017) enforce the mapping matrix to be orthogonal during the adversarial training and achieve a good performance on bilingual lexicon induction.\n[BOS] The main drawback of their approach is that the vocabularies of their training data are small, and the performance Method en-de en-es en-fi en-it Supervised Mikolov et al. (2013b) 35.0 27.3 25.9 34.9 Faruqui and Dyer (2014) 37.1 26.8 27.6 38.4 Dinu et al. (2015) 38.9 30.4 29.1 37.7 Xing et al. (2015) 41 Table 3 : Results of bilingual lexicon induction (accuracy % P@1) on BLI-2 dataset, all the results of previous methods come from the paper of Artetxe et al. (2018a) of their models drops significantly when they use large training data.\n[BOS] The recent model proposed by Lample et al. (2018) is so far the most successful and becomes competitive with previous supervised approaches through a strong CSLS-based refinement to the core mapping matrix trained by GANs.\n[BOS] Even in this case, though, without refinement, the core mappings are not as good as hoped for some distant language pairs.\n[BOS] More recently, Chen and Cardie (2018) extends the work of Lample et al. (2018) from bilingual setting to multi-lingual setting, instead of training crosslingual word embeddings for only one language pair, their approach allows us to train crosslingual word embeddings for many language pairs at the same time.\n[BOS] Another recent piece of work which is similar to Lample et al. (2018) comes from Xu et al. (2018) .\n[BOS] Their approach can be divided into 2 steps: first, using Wasserstein GAN (Arjovsky et al., 2017) to train a preliminary mapping between two monolingual distribution and then minimizing the Sinkhorn Distance across distributions.\n[BOS] Although their method performs better than Lample et al. (2018) in several tasks, the improvement mainly comes from the second step, showing that the problem of how to train a better preliminary mapping has not been resolved.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Single_summ", "Narrative_cite", "Single_summ", "Single_summ", "Multi_summ", "Narrative_cite", "Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 4, "token_end": 40, "char_start": 21, "char_end": 180, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 59, "token_end": 88, "char_start": 282, "char_end": 440, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhang et al. (2017)": "26873455"}, "Reference": {}}}, {"token_start": 124, "token_end": 197, "char_start": 593, "char_end": 748, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Mikolov et al. (2013b)": "1966640", "Dinu et al. (2015)": "17910711", "Xing et al. (2015)": "3144258"}}}, {"token_start": 231, "token_end": 241, "char_start": 895, "char_end": 917, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Artetxe et al. (2018a)": "21728524"}}}, {"token_start": 259, "token_end": 324, "char_start": 1024, "char_end": 1346, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 328, "token_end": 391, "char_start": 1368, "char_end": 1662, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Chen and Cardie (2018)": "52099904"}, "Reference": {}}}, {"token_start": 401, "token_end": 409, "char_start": 1718, "char_end": 1738, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 411, "token_end": 466, "char_start": 1750, "char_end": 2003, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {"Xu et al. (2018)": "52186890"}}}, {"token_start": 473, "token_end": 511, "char_start": 2053, "char_end": 2235, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "128350532_2", "paragraph": "[BOS] Recently, efforts have concentrated on how to limit or avoid reliance on dictionaries.\n[BOS] Good results were achieved with some drastically minimal techniques.\n[BOS] Zhang et al. (2016) achieved good results at bilingual POS tagging, but not bilingual lexicon induction, using only ten word pairs to build a coarse orthonormal mapping between source and target monolingual embeddings.\n[BOS] The work of Smith et al. (2017) has shown that a singular value decomposition (SVD) method can produce a competitive cross-lingual mapping by using identical character strings across languages.\n[BOS] Artetxe et al. (2017 Artetxe et al. ( , 2018b proposed a self-learning framework, which iteratively trains its cross-lingual mapping by using dictionaries trained in previous rounds.\n[BOS] The initial dictionary of the self-learning can be reduced to 25 word pairs or even only a list of numerals and still have competitive performance.\n[BOS] Furthermore, Artetxe et al. (2018a) extend their self-learning framework to unsupervised models, and build the state-ofthe-art for bilingual lexicon induction.\n[BOS] Instead of using a pre-build dictionary for initialization, they sort the value of the word vectors in both the source and the target distribution, treat two vectors that have similar permutations as possible translations and use them as the initialization dictionary.\n[BOS] Additionally, their unsupervised framework also includes many optimization augmentations, such as stochastic dictionary induction, symmetric reweighting, among others.\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ", "Single_summ", "Multi_summ", "Transition", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 30, "token_end": 70, "char_start": 174, "char_end": 392, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhang et al. (2016)": "158652"}, "Reference": {}}}, {"token_start": 74, "token_end": 109, "char_start": 411, "char_end": 592, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Smith et al. (2017)": "11591887"}, "Reference": {}}}, {"token_start": 110, "token_end": 153, "char_start": 599, "char_end": 781, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Artetxe et al. (2017": "13335042", "Artetxe et al. ( , 2018b": "4334731"}, "Reference": {}}}, {"token_start": 186, "token_end": 266, "char_start": 955, "char_end": 1376, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Artetxe et al. (2018a)": "21728524"}, "Reference": {}}}]}
{"id": "128350532_1", "paragraph": "[BOS] Instead of training shared embedding space directly, the work of Mikolov et al. (2013b) has shown that we can also combine two monolin-gual spaces by applying a linear mapping matrix.\n[BOS] The matrix is trained by minimizing the sum of squared Euclidean distances between source and target words of a dictionary.\n[BOS] This simple approach has been improved upon in several ways: using canonical correlation analysis to map source and target embeddings (Faruqui and Dyer, 2014) ; or by forcing the mapping matrix to be orthogonal (Xing et al., 2015) .\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Multi_summ"], "span_citation_mapping": [{"token_start": 13, "token_end": 66, "char_start": 71, "char_end": 319, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Mikolov et al. (2013b)": "1966640"}, "Reference": {}}}, {"token_start": 79, "token_end": 99, "char_start": 393, "char_end": 484, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Faruqui and Dyer, 2014)": "3792324"}, "Reference": {}}}, {"token_start": 102, "token_end": 118, "char_start": 493, "char_end": 556, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Xing et al., 2015)": "3144258"}, "Reference": {}}}]}
{"id": "128350532_0", "paragraph": "[BOS] Sharing a word embedding space across different languages has proven useful for many crosslingual tasks, such as machine translation (Zou et al., 2013) and cross-lingual dependency parsing (Jiang et al., 2015 (Jiang et al., , 2016 Ammar et al., 2016a) .\n[BOS] Generally, such spaces can be trained directly from bilingual sentence aligned or document aligned text (Hermann and Blunsom, 2014; Chandar A P et al., 2014; Sgaard et al., 2015; Vuli and Moens, 2013) .\n[BOS] However the performance of directly trained models is limited by their vocabulary size.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 22, "token_end": 33, "char_start": 119, "char_end": 157, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zou et al., 2013)": "931054"}}}, {"token_start": 34, "token_end": 64, "char_start": 162, "char_end": 257, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Jiang et al., 2015": "18634877", "(Jiang et al., , 2016": "7067409", "Ammar et al., 2016a)": "2868247"}}}, {"token_start": 75, "token_end": 119, "char_start": 318, "char_end": 466, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Chandar A P et al., 2014;": "217774", "S\u00f8gaard et al., 2015;": "4311046"}}}]}
{"id": "202789367_2", "paragraph": "[BOS] Lucy and Gauthier (2017) investigate perceptual properties of distributional embeddings and suggest that part-whole properties like has legs are well encoded by embeddings.\n[BOS] This may help explain why the simple word-based MLP models perform well without other sources of context.\n[BOS] Rei et al. (2018) introduce an effective neural architecture for learning word-embedding based models for graded lexical entailment.\n[BOS] Prior work (Bulat et al., 2016; Fagarasan et al., 2015) utilizes embeddings to predict real-world perceptual proper- The bench's support is wooden.\n[BOS] ties, and we expect an approach that leverages this will help solve this task, but we leave it to future work.\n\n", "discourse_tags": ["Single_summ", "Transition", "Single_summ", "Multi_summ", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 33, "char_start": 6, "char_end": 178, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 55, "token_end": 81, "char_start": 297, "char_end": 429, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Rei et al. (2018)": "43957586"}, "Reference": {}}}, {"token_start": 82, "token_end": 112, "char_start": 436, "char_end": 551, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Bulat et al., 2016;": "8463304", "Fagarasan et al., 2015)": "16303338"}, "Reference": {}}}]}
{"id": "202789367_1", "paragraph": "[BOS] Past work has also evaluated commonsense capabilities in neural models.\n[BOS] Pavlick and Callison-Burch (2016) investigate the related problem of entailment in adjective-nouns, and show surprising negative results for neural NLI models.\n[BOS] Wang et al. (2018) showed that models based on distributional semantics without explicit external knowledge perform poorly at predicting physical plausibility of actions.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 15, "token_end": 50, "char_start": 84, "char_end": 243, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Pavlick and Callison-Burch (2016)": "115505"}, "Reference": {}}}, {"token_start": 51, "token_end": 79, "char_start": 250, "char_end": 420, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wang et al. (2018)": "4731156"}, "Reference": {}}}]}
{"id": "202789367_0", "paragraph": "[BOS] Other researchers have constructed datasets investigating similar ideas in commonsense reasoning.\n[BOS] Forbes and Choi (2017) develop a dataset and methods for inferring physical commonsense knowledge from verb usage, showing it is possible to learn the physical implications of unseen verbs from a small seed set.\n[BOS] Zhang et al. (2017) create a large dataset for general commonsense inference in the form of premise-hypothesis pairs, equipped with ordinal labels ranging from \"impossible\" to \"very likely\".\n[BOS] We adopt much of their methodology but for a targeted subset of commonsense reasoning.\n[BOS] The SemEval 2018 Task 10 on Capturing Discriminative Attributes (Krebs et al., 2018) describes a similar lexical reasoning task involving triplets of words, though it focuses on finding attributes that distinguish two concepts, while in our work the adjective may well apply to both part and whole.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Reflection", "Single_summ"], "span_citation_mapping": [{"token_start": 16, "token_end": 57, "char_start": 110, "char_end": 321, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Forbes and Choi (2017)": "10543068"}, "Reference": {}}}, {"token_start": 58, "token_end": 96, "char_start": 328, "char_end": 516, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhang et al. (2017)": "1461182"}, "Reference": {}}}, {"token_start": 124, "token_end": 157, "char_start": 646, "char_end": 844, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Krebs et al., 2018)": "44068699"}, "Reference": {}}}]}
{"id": "201070022_4", "paragraph": "[BOS] As mentioned in the introduction, our approach takes inspiration from \"Build it Break it\" approaches which have been successfully tried in other domains (Ruef et al., 2016; Ettinger et al., 2017) .\n[BOS] Those approaches advocate finding faults in systems by having humans look for insecurities (in software) or prediction failures (in models), but do not advocate an automated approach as we do here.\n[BOS] Our work is also closely connected to the \"Mechanical Turker Descent\" algorithm detailed in (Yang et al., 2018) where language to action pairs were collected from crowdworkers by incentivizing them with a game-with-a-purpose technique: a crowdworker receives a bonus if their contribution results in better models than another crowdworker.\n[BOS] We did not gamify our approach in this way, but still our approach has commonalities in the round-based improvement of models through crowdworker interaction.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 14, "token_end": 46, "char_start": 77, "char_end": 201, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ruef et al., 2016;": "2306413"}}}, {"token_start": 96, "token_end": 159, "char_start": 457, "char_end": 753, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Yang et al., 2018)": "4862861"}, "Reference": {}}}]}
{"id": "201070022_3", "paragraph": "[BOS] The broad class of adversarial training is currently a hot topic in machine learning (Goodfellow et al., 2014) .\n[BOS] Use cases include training image generators (Brock et al., 2018) as well as image classifiers to be robust to adversarial examples (Liu et al., 2019) .\n[BOS] These methods find the breaking examples algorithmically, rather than by using humans breakers as we do.\n[BOS] Applying the same approaches to NLP tends to be more challenging because, unlike for images, even small changes to a sentence can cause a large change in the meaning of that sentence, which a human can detect but a lower quality model cannot.\n[BOS] Nevertheless algorithmic approaches have been attempted, for example in text classification (Ebrahimi et al., 2018) , machine translation (Belinkov and Bisk, 2018) , dialogue generation tasks (Li et al., 2017) and reading comprehension (Jia and Liang, 2017 ).\n[BOS] The latter was particularly effective at proposing a more difficult version of the popular SQuAD dataset.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Transition", "Transition", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 6, "token_end": 27, "char_start": 25, "char_end": 116, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Goodfellow et al., 2014)": "1033682"}}}, {"token_start": 32, "token_end": 44, "char_start": 143, "char_end": 189, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 47, "token_end": 65, "char_start": 201, "char_end": 274, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Liu et al., 2019)": "70346871"}}}, {"token_start": 147, "token_end": 160, "char_start": 715, "char_end": 758, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 161, "token_end": 173, "char_start": 761, "char_end": 806, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 174, "token_end": 185, "char_start": 809, "char_end": 852, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Li et al., 2017)": "98180"}}}, {"token_start": 186, "token_end": 194, "char_start": 857, "char_end": 899, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Jia and Liang, 2017": "7228830"}}}]}
{"id": "201070022_2", "paragraph": "[BOS] Many approaches have been taken to solve these tasks -from linear regression and SVMs to deep learning (Noever, 2018) .\n[BOS] The best performing systems in each of the competitions mentioned above (for aggression and toxic comment classification) used deep learning approaches such as LSTMs and CNNs (Kumar et al., 2018; Google, 2018) .\n[BOS] In this work we consider a large-pretrained transformer model which has been shown to perform well on many downstream NLP tasks (Devlin et al., 2018) .\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 19, "token_end": 27, "char_start": 95, "char_end": 123, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Noever, 2018)": "52921164"}}}, {"token_start": 50, "token_end": 73, "char_start": 259, "char_end": 341, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kumar et al., 2018;": "59336626", "Google, 2018)": null}}}, {"token_start": 81, "token_end": 109, "char_start": 377, "char_end": 499, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "201070022_1", "paragraph": "[BOS] To this end, various datasets have been created to benchmark progress in the field.\n[BOS] In hate speech detection, recently Davidson et al. (2017) compiled and released a dataset of over 24,000 tweets labeled as containing hate speech, offensive language, or neither.\n[BOS] The TRAC shared task on Aggression Identification, a dataset of over 15,000 Facebook comments labeled with varying levels of aggression, was released as part of a competition (Kumar et al., 2018) .\n[BOS] In order to benchmark toxic comment detection, The Wikipedia Toxic Comments dataset (which we study in this work) was collected and extracted from Wikipedia Talk pages and featured in a Kaggle competition (Wulczyn et al., 2017; Google, 2018) .\n[BOS] Each of these benchmarks examine only single-turn utterances, outside of the context in which the language appeared.\n[BOS] In this work we recommend that future systems should move beyond classification of singular utterances and use contextual information to help identify offensive language.\n\n", "discourse_tags": ["Transition", "Single_summ", "Narrative_cite", "Multi_summ", "Multi_summ", "Reflection"], "span_citation_mapping": [{"token_start": 26, "token_end": 58, "char_start": 131, "char_end": 274, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 71, "token_end": 98, "char_start": 350, "char_end": 476, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kumar et al., 2018)": "59336626"}}}, {"token_start": 108, "token_end": 176, "char_start": 532, "char_end": 851, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Wulczyn et al., 2017;": "6060248", "Google, 2018)": null}, "Reference": {}}}]}
{"id": "201070022_0", "paragraph": "[BOS] The task of detecting offensive language has been studied across a variety of content classes.\n[BOS] Perhaps the most commonly studied class is hate speech, but work has also covered bullying, aggression, and toxic comments (Zampieri et al., 2019) .\n\n", "discourse_tags": ["Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 36, "token_end": 55, "char_start": 189, "char_end": 253, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zampieri et al., 2019)": "84843035"}}}]}
{"id": "202572810_1", "paragraph": "[BOS] Question Generation for QA As the dual task of QA, QG has been often proposed for improving QA.\n[BOS] Some works have directly used QG in QA models' pipeline (Duan et al., 2017; Dong et al., 2017; Lewis and Fan, 2019) .\n[BOS] Some other works enabled semi-supervised QA with the help of QG.\n[BOS] Tang et al. (2017) applied the \"dual learning\" algorithm (He et al., 2016) to learn QA and QG jointly with unlabeled texts.\n[BOS] and Tang et al. (2018) followed the GAN (Goodfellow et al., 2014) paradigm, taking QG as a generator and QA as a discriminator, to utilize unlabeled data.\n[BOS] Sachan and Xing (2018) proposed a self-training cycle between QA and QG.\n[BOS] However, these works either reduced the ground-truth data size or simplified the span-prediction QA task to answer sentence selection.\n[BOS] Dhingra et al. (2018) collected 3.2M cloze-style QA pairs to pre-train a QA model, then fine-tune with the full groundtruth data which improved a BiDAF-QA baseline.\n[BOS] In our paper, we follow the back-translation (Sennrich et al., 2016) strategy to generate new QA pairs by our best QG model to augment SQuAD training set.\n[BOS] Further, we introduce a data filter to remove poorly generated examples and a mixing mini-batch training strategy to more effectively use the synthetic data.\n[BOS] Similar methods have also been applied in some very recent concurrent works (Dong et al., 2019; Alberti et al., 2019) on SQuADv2.0.\n[BOS] The main difference is that we also propose to generate new questions from existing articles without introducing new articles.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Single_summ", "Single_summ", "Single_summ", "Transition", "Single_summ", "Reflection", "Reflection", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 29, "token_end": 59, "char_start": 138, "char_end": 223, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Duan et al., 2017;": "37738077"}}}, {"token_start": 77, "token_end": 110, "char_start": 303, "char_end": 426, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Tang et al. (2017)": "37738077"}, "Reference": {}}}, {"token_start": 112, "token_end": 152, "char_start": 437, "char_end": 587, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Tang et al. (2018)": "44071286"}, "Reference": {}}}, {"token_start": 153, "token_end": 173, "char_start": 594, "char_end": 666, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Sachan and Xing (2018)": "44130298"}, "Reference": {}}}, {"token_start": 200, "token_end": 248, "char_start": 814, "char_end": 978, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 256, "token_end": 269, "char_start": 1013, "char_end": 1053, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sennrich et al., 2016)": "15600925"}}}, {"token_start": 328, "token_end": 348, "char_start": 1369, "char_end": 1427, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Alberti et al., 2019)": "189762081"}}}]}
{"id": "202572810_0", "paragraph": "[BOS] Question Generation Early QG studies focused on using rule-based methods to transform statements to questions (Heilman and Smith, 2010; Lindberg et al., 2013; Labutov et al., 2015) .\n[BOS] Recent works adopted the attention-based sequenceto-sequence neural model (Bahdanau et al., 2014) for QG tasks, taking answer sentence as input and outputting the question (Du et al., 2017; , which proved to be better than rulebased methods.\n[BOS] Since human-labeled questions are often relevant to a longer context, later works leveraged information from the whole paragraph for QG, either by extracting additional information from the paragraph Song et al., 2018; Liu et al., 2019) or by directly taking the whole paragraph as input Kim et al., 2018; .\n[BOS] A very recent concurrent work applied the large-scale language model pre-training strategy for QG and also achieved a new state-of-the-art performance (Dong et al., 2019) .\n[BOS] However, the above models were trained with teacher forcing only.\n[BOS] To address the exposure bias problem, some works applied reinforcement learning taking evaluation metrics (e.g., BLEU) as rewards (Song et al., 2017; Kumar et al., 2018) .\n[BOS] Yuan et al. (2017) proposed to use a language model's perplexity (R P P L ) and a QA model's accuracy (R QA ) as two rewards but failed to get significant improvement.\n[BOS] Their second reward is similar to our QAP reward except that we use QA probability rather than accuracy as the probability distribution is more smooth.\n[BOS] Hosking and Riedel (2019) compared a set of different rewards, including R P P L and R QA , and claimed none of them improved the quality of generated questions.\n[BOS] For QG evaluation, even though some previous works conducted human evaluations, most of them still relied on traditional metrics (e.g., BLEU).\n[BOS] However, Nema and Khapra (2018) pointed out the existing metrics do not correlate with human judgment about answerability, so they proposed \"Q-metrics\" that mixed traditional metrics with an \"answerability\" score.\n[BOS] In our work, we will show QG results on traditional metrics, Q-metrics, as well as human evaluation, and also propose a QA-based QG evaluation.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Narrative_cite", "Narrative_cite", "Transition", "Narrative_cite", "Single_summ", "Single_summ", "Single_summ", "Transition", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 16, "token_end": 45, "char_start": 82, "char_end": 186, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 51, "token_end": 70, "char_start": 220, "char_end": 292, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 75, "token_end": 92, "char_start": 307, "char_end": 383, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 134, "token_end": 154, "char_start": 601, "char_end": 679, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Song et al., 2018;": "44178763"}}}, {"token_start": 160, "token_end": 169, "char_start": 712, "char_end": 747, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 179, "token_end": 213, "char_start": 799, "char_end": 927, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 238, "token_end": 269, "char_start": 1065, "char_end": 1177, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Song et al., 2017;": "24005757"}}}, {"token_start": 271, "token_end": 344, "char_start": 1186, "char_end": 1511, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Yuan et al. (2017)": "7562142"}, "Reference": {}}}, {"token_start": 345, "token_end": 382, "char_start": 1518, "char_end": 1679, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 418, "token_end": 462, "char_start": 1844, "char_end": 2048, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "86611921_4", "paragraph": "[BOS] There are a number of reading comprehension benchmarks based on multiple choice tests (Mihaylov et al., 2018; Richardson et al., 2013; Lai et al., 2017) .\n[BOS] The TriviaQA data set (Joshi et al., 2017) contains questions and answers taken from trivia quizzes found online.\n[BOS] A number of Clozestyle tasks have also been proposed (Hermann et al., 2015; Hill et al., 2015; Paperno et al., 2016; Onishi et al., 2016) .\n[BOS] We believe that all of these tasks are related to, but distinct from, answering information-seeking questions.\n[BOS] We also believe that, because a solution to NQ will have genuine utility, it is better equipped as a benchmark for NLU.\n\n", "discourse_tags": ["Narrative_cite", "Multi_summ", "Narrative_cite", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 7, "token_end": 40, "char_start": 28, "char_end": 158, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mihaylov et al., 2018;": "52183757", "Richardson et al., 2013;": "2100831", "Lai et al., 2017)": "6826032"}}}, {"token_start": 43, "token_end": 71, "char_start": 171, "char_end": 280, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Joshi et al., 2017)": "26501419"}, "Reference": {}}}, {"token_start": 75, "token_end": 116, "char_start": 299, "char_end": 424, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hermann et al., 2015;": "6203757", "Hill et al., 2015;": "14915449", "Paperno et al., 2016;": "2381275", "Onishi et al., 2016)": "5761781"}}}]}
{"id": "86611921_3", "paragraph": "[BOS] The WikiQA (Yang et al., 2015) and MS Marco (Nguyen et al., 2016) data sets contain queries sampled from the Bing search engine.\n[BOS] WikiQA contains only 3,047 questions.\n[BOS] MS Marco contains 100,000 questions with freeform answers.\n[BOS] For each question, the annotator is presented with 10 passages returned by the search engine, and is asked to generate an answer to the query, or to say that the answer is not contained within the passages.\n[BOS] Free-form text answers allow more flexibility in providing abstractive answers, but lead to difficulties in evaluation (BLEU score [Papineni et al., 2002 ] is used).\n[BOS] MS Marco's authors do not discuss issues of variability or report quality metrics for their annotations.\n[BOS] From our experience, these issues are critical.\n[BOS] DuReader ) is a Chinese language data set containing queries from Baidu search logs.\n[BOS] Like NQ, DuReader contains real user queries; it requires systems to read entire documents to find answers; and it identifies acceptable variability in answers.\n[BOS] However, as with MS Marco, DuReader is reliant on BLEU for answer scoring, and systems already outperform a humans according to this metric.\n\n", "discourse_tags": ["Multi_summ", "Multi_summ", "Multi_summ", "Multi_summ", "Narrative_cite", "Transition", "Transition", "Transition", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 3, "token_end": 107, "char_start": 10, "char_end": 456, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Yang et al., 2015)": "1373518", "(Nguyen et al., 2016)": "1289517"}, "Reference": {}}}, {"token_start": 127, "token_end": 141, "char_start": 571, "char_end": 616, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"[Papineni et al., 2002": null}}}]}
{"id": "86611921_2", "paragraph": "[BOS] This contrasts with NQ, where individual questions often require reasoning over large bodies of text.\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "86611921_1", "paragraph": "[BOS] The QuAC (Choi et al., 2018) and CoQA (Reddy et al., 2018) data sets contain dialogues between a questioner, who is trying to learn about a text, and an answerer.\n[BOS] QuAC also prevents the questioner from seeing the evidence text.\n[BOS] Conversational QA is an exciting new area, but it is significantly different from the single turn QA task in NQ.\n[BOS] In both QuAC and CoQA, conversations tend to explore evidence texts incrementally, progressing from the start to the end of the text.\n\n", "discourse_tags": ["Multi_summ", "Multi_summ", "Multi_summ", "Multi_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 117, "char_start": 6, "char_end": 498, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Choi et al., 2018)": "52057510", "(Reddy et al., 2018)": "52055325"}, "Reference": {}}}]}
{"id": "86611921_0", "paragraph": "[BOS] The SQuAD (Rajpurkar et al., 2016) , SQuAD 2.0 (Rajpurkar et al., 2018) , NarrativeQA (Kocisky et al., 2018) , and HotpotQA (Yang et al., 2018) data sets contain questions and answers written by annotators who have first read a short text containing the answer.\n[BOS] The SQuAD data sets contain questions/paragraph/answer triples from Wikipedia.\n[BOS] In the original SQuAD data set, annotators often borrow part of the evidence paragraph to create a question.\n[BOS] Jia and Liang (2017) showed that systems trained on SQuAD could be easily fooled by the insertion of distractor sentences that should not change the answer, and SQuAD 2.0 introduces questions that are designed to be unanswerable.\n[BOS] However, we argue that questions written to be unanswerable can be identified as such with little reasoning, in contrast to NQ's task of deciding whether a paragraph contains all of the evidence required to answer a real question.\n[BOS] Both SQuAD tasks have driven significant advances in reading comprehension, but systems now outperform humans and harder challenges are needed.\n[BOS] NarrativeQA aims to elicit questions that are not close paraphrases of the evidence by separate summary texts.\n[BOS] No human performance upper bound is provided for the full task and, although an extractive system could theoretically perfectly recover all answers, current approaches only just outperform a random baseline.\n[BOS] NarrativeQA may just be too hard for the current state of NLU.\n[BOS] HotpotQA is designed to contain questions that require reasoning over text from separate Wikipedia pages.\n[BOS] As well as answering questions, systems must also identify passages that contain supporting facts.\n[BOS] This is similar in motivation to NQ's long answer task, where the selected passage must contain all of the information required to infer the answer.\n[BOS] Mirroring our identification of acceptable variability in the NQ task definition, HotpotQA's authors observe that the choice of supporting facts is somewhat subjective.\n[BOS] They set high human upper bounds by selecting, for each example, the score maximizing partition of four annotations into one prediction and three references.\n[BOS] The reference labels chosen by this maximization are not representative of the reference labels in HotpotQA's evaluation set, and it is not clear that the upper bounds are achievable.\n[BOS] A more robust approach is to keep the evaluation distribution fixed, and calculate an acheivable upper bound by approximating the expectation over annotations-as we have done for NQ in Section 5.\n\n", "discourse_tags": ["Multi_summ", "Multi_summ", "Multi_summ", "Single_summ", "Reflection", "Transition", "Transition", "Transition", "Reflection", "Reflection", "Transition", "Transition", "Transition", "Transition", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 117, "char_start": 6, "char_end": 467, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Rajpurkar et al., 2016)": "11816014", "(Rajpurkar et al., 2018)": "47018994", "(Kocisky et al., 2018)": null, "(Yang et al., 2018)": "52822214"}, "Reference": {}}}, {"token_start": 118, "token_end": 169, "char_start": 474, "char_end": 703, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Jia and Liang (2017)": "7228830"}, "Reference": {}}}]}
{"id": "203610243_0", "paragraph": "[BOS] Our discussion of exposure bias complements recent work that summarizes modern generative models, for example Caccia et al. (2018) and Lu et al. (2018) .\n[BOS] Shortcomings of maximum-likelihood training for sequence generation have often been discussed (Ding and Soricut, 2017; Leblond et al., 2018; Ranzato et al., 2016) , but without pointing to generalization as the key aspect.\n[BOS] An overview of recent deep reinforcement learning methods for conditional generation can be found in (Keneshloo et al., 2018) .\n[BOS] Our proposed approach follows work by Ding et al. (2017) and Tan et al. (2018) by employing both, policy and reward for exploration.\n[BOS] In contrast to them, we do not use n-gram based reward.\n[BOS] Compared to RAML (Norouzi et al., 2016) , we do not perturb the ground-truth context, but correct the policy predictions.\n[BOS] Scheduled sampling and word-dropout (Bowman et al., 2016) also apply a correction, yet one that only affects the probability of the ground-truth.\n[BOS] Chen et al. (2017) propose Bridge modules that similarly to Ding et al. (2017) can incorporate arbitrary ground-truth perturbations, yet in an objective motivated by an auxiliary KL-divergence.\n[BOS] Merity et al. (2017) have shown that generalization is crucial to language modeling, but their focus is regularizing parameters and activations.\n[BOS] Word-embeddings to measure deviations from the ground-truth have also been used by Inan et al. (2016) , yet under log-likelihood.\n[BOS] Concurrently to our work, Li et al. (2019) employ embeddings to design reward functions in abstractive summarization.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Narrative_cite", "Reflection", "Reflection", "Reflection", "Single_summ", "Multi_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 11, "token_end": 34, "char_start": 67, "char_end": 157, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Caccia et al. (2018)": "53208122", "Lu et al. (2018)": "4711067"}}}, {"token_start": 36, "token_end": 76, "char_start": 166, "char_end": 328, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ding and Soricut, 2017;": "4206469", "Leblond et al., 2018;": "34984289", "Ranzato et al., 2016)": "7147309"}}}, {"token_start": 97, "token_end": 113, "char_start": 457, "char_end": 520, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Keneshloo et al., 2018)": "44063077"}}}, {"token_start": 119, "token_end": 136, "char_start": 559, "char_end": 607, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Ding et al. (2017)": "4206469", "Tan et al. (2018)": "53773501"}}}, {"token_start": 165, "token_end": 177, "char_start": 742, "char_end": 769, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Norouzi et al., 2016)": "3631537"}}}, {"token_start": 195, "token_end": 228, "char_start": 858, "char_end": 1003, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 229, "token_end": 241, "char_start": 1010, "char_end": 1066, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Chen et al. (2017)": "195347778"}, "Reference": {}}}, {"token_start": 242, "token_end": 269, "char_start": 1070, "char_end": 1203, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ding et al. (2017)": "4206469"}, "Reference": {}}}, {"token_start": 270, "token_end": 298, "char_start": 1210, "char_end": 1354, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Merity et al. (2017)": "212756"}, "Reference": {}}}, {"token_start": 299, "token_end": 330, "char_start": 1361, "char_end": 1490, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Inan et al. (2016)": "7443908"}, "Reference": {}}}, {"token_start": 336, "token_end": 356, "char_start": 1523, "char_end": 1614, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Li et al. (2019)": "202541388"}, "Reference": {}}}]}
{"id": "53981714_1", "paragraph": "[BOS] Another line of research that relates to ours is the universal schema (Riedel et al., 2013) for relation extraction, KB completion, as well as its extensions (Toutanova et al., 2015; Verga et al., 2016) .\n[BOS] Wrong labeling problem still exists since their embedding is learned based on individual relation facts.\n[BOS] In contrast, we use the global cooccurrence statistics as explicit supervision signal.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 12, "token_end": 26, "char_start": 59, "char_end": 121, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Riedel et al., 2013)": "2687019"}}}, {"token_start": 27, "token_end": 54, "char_start": 123, "char_end": 208, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Toutanova et al., 2015;": "2127100", "Verga et al., 2016)": "9206785"}}}]}
{"id": "53981714_0", "paragraph": "[BOS] Distant supervision methods (Mintz et al., 2009) for relation extraction have been studied by a number of works (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015; Lin et al., 2016; Ji et al., 2017; Wu et al., 2017) .\n[BOS] (Su et al., 2018) use global co-occurrence statistics of 1 https://github.com/czyssrs/GloREPlus textual and KB relations to effectively combat the wrong labeling problem.\n[BOS] But the global statistics in their work is limited to NYT dataset, capturing domain-specific distributions.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 80, "char_start": 6, "char_end": 256, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mintz et al., 2009)": "10910955", "(Riedel et al., 2010;": "2386383", "Hoffmann et al., 2011;": "16483125", "Surdeanu et al., 2012;": "5869747", "Zeng et al., 2015;": "2778800", "Lin et al., 2016;": "397533", "Ji et al., 2017;": "29159360", "Wu et al., 2017)": "34190303"}}}, {"token_start": 82, "token_end": 149, "char_start": 265, "char_end": 549, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Su et al., 2018)": "182616"}, "Reference": {}}}]}
{"id": "90242101_5", "paragraph": "[BOS] Ours a man on a bicycle pedals in front of a archway .\n[BOS] four men , three are wearing these are wearing these are sitting on a blue and green patterned mat .\n[BOS] ing data.\n[BOS] Grnroos et al. (2018) trained their multimodal model with parallel corpora and achieved state-of-the-art performance in the WMT 2018.\n[BOS] However, the use of monolingual corpora has seldom been studied in multimodal machine translation.\n[BOS] Our study proposes using word embeddings that are pretrained on monolingual corpora.\n\n", "discourse_tags": ["Other", "Other", "Other", "Single_summ", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 44, "token_end": 77, "char_start": 190, "char_end": 323, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "90242101_4", "paragraph": "[BOS] IMAG+ a man on a bicycle pedals outside a monument .\n[BOS] four men , three of them are wearing alaska , are sitting on a blue patterned carpet and green green seating .\n\n", "discourse_tags": ["Other", "Other"], "span_citation_mapping": []}
{"id": "90242101_3", "paragraph": "[BOS] NMT a man on a bicycle pedal past an arch .\n[BOS] four men , three of whom are wearing aprons , are sitting on a blue and green speedo carpet .\n\n", "discourse_tags": ["Other", "Other"], "span_citation_mapping": []}
{"id": "90242101_2", "paragraph": "[BOS] Reference a man on a bicycle pedals through an archway .\n[BOS] four men , three of whom are wearing prayer caps , are sitting on a blue and olive green patterned mat .\n\n", "discourse_tags": ["Other", "Other"], "span_citation_mapping": []}
{"id": "90242101_1", "paragraph": "[BOS] Second, in data augmentation, parallel corpora without images are widely used as additional trainImage Source un homme en vlo pdale devant une vote .\n[BOS] quatre hommes , dont trois portent des kippas , sont assis sur un tapis motifs bleu et vert olive .\n\n", "discourse_tags": ["Other", "Other"], "span_citation_mapping": []}
{"id": "90242101_0", "paragraph": "[BOS] Most studies on multimodal machine translation are divided into two categories: visual feature adaptation and data augmentation.\n[BOS] First, in visual feature adaptation, visual features are extracted using image processing techniques and then integrated into a machine translation model.\n[BOS] In contrast, most multitask learning models use latent space learning as their auxiliary task.\n[BOS] Elliott and Kdr (2017) proposed the IMAGINATION model that learns to construct the corresponding visual feature from the textual hidden states of a source sentence.\n[BOS] The visual model shares its encoder with the machine translation model; this helps in improving the textual encoder.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 65, "token_end": 117, "char_start": 403, "char_end": 690, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Elliott and K\u00e1d\u00e1r (2017)": "20272964"}, "Reference": {}}}]}
{"id": "167217687_2", "paragraph": "[BOS] Moreover, building annotated semantic parsing datasets is highly labor-intensive and parsers built for one domain do not necessarily transfer across domains.\n[BOS] Fan et al. propose a multi-task setup and demonstrate that training using this setup can improve the accuracy in domains with smaller labeled datasets [8] .\n[BOS] This approach is aligned with one of the main contributions in our research.\n[BOS] Our proposed framework allows for pre-training the model in an unsupervied manner with data from multiple tasks that enables transfer learning.\n\n", "discourse_tags": ["Transition", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 29, "token_end": 59, "char_start": 170, "char_end": 324, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {"[8]": "5955929"}}}]}
{"id": "167217687_1", "paragraph": "[BOS] One disadvantage of this approach is that the decoder outputs are considered unstructured and can lead to invalid logical forms.\n[BOS] Krishnamurthy et al. propose to overcome this problem by imposing a grammar on the decoder that only generates well-typed logical forms [7] .\n[BOS] However, this approach increases the complexity of the system, which was unwarranted in our experiments and most of the results produced by the model in our experiments were valid logical forms (with sufficient training).\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 24, "token_end": 98, "char_start": 141, "char_end": 510, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {"[7]": "1675452"}}}]}
{"id": "167217687_0", "paragraph": "[BOS] The approach used in this work is a continuation of the work by Jia et al. [6] where the authors proposed a sequence-tosequence model with an attention-based copying mechanism.\n[BOS] This supervised approach leverages the flexibility of of the encoder-decoder architecture and the authors demonstrate that the model can learn very accurate parsers across three standard semantic parsing datasets.\n[BOS] The augmentation strategy used in this work allows for injecting prior knowledge which improve the generalization power of the model.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 15, "token_end": 97, "char_start": 70, "char_end": 542, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"[6]": "7218315"}, "Reference": {}}}]}
{"id": "201667895_2", "paragraph": "[BOS] The Gaussian Mixture Model (GMM) is a classic algorithm that assumes data is generated from a mixture of finite number of Gaussian distributions, whose parameters are typcially estimated with the Expectation-Maximization (EM) algorithm.\n[BOS] An extension to the EM algorithm is variational inference, which has the advantage of automatically choosing the number of components.\n[BOS] Bishop (2006) gives a comprehensive introduction to the topic.\n[BOS] We use the implementation of variational Bayesian estimation of Gaussian mixtures from scikit-learn (Pedregosa et al., 2011) .\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 70, "token_end": 83, "char_start": 390, "char_end": 452, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Bishop (2006)": null}, "Reference": {}}}, {"token_start": 96, "token_end": 110, "char_start": 546, "char_end": 583, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Pedregosa et al., 2011)": "10659969"}}}]}
{"id": "201667895_1", "paragraph": "[BOS] Unicode is the de facto standard for encoding characters from various languages, domains and sources (The Unicode Consortium, 2019).\n[BOS] It uses \"blocks\" to group characters with similar origins or functions.\n[BOS] The current version 12.0 defines 300 blocks, including Basic Latin, Latin-1 Supplement, CJK (Chinese, Japanese and Korean) Symbols and Punctuation, etc.\n[BOS] To identify the legalness of characters, the Unicode block information provides meaningful discriminative signals.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 20, "token_end": 30, "char_start": 107, "char_end": 138, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "201667895_0", "paragraph": "[BOS] Raw data in NLP is often noisy.\n[BOS] categorize five common noise sources in parallel corpora and count only about 23% of the sentences in the raw 2016 ParaCrawl corpus 2 to be \"Okay\".\n[BOS] Although illegal characters is not listed as a separate noise source, misuse of characters and shifting of character distributions may result in a sentence being classified into one of the five noise sources.\n[BOS] In previous work, a supervised model using bag-of-words translation features is developed to classify clean and noisy data (Xu and Koehn, 2017; .\n[BOS] In contrast, our model, which is trained in an unsupervised manner, tackles the illegal character problem explicitly.\n[BOS] describe a shared task on parallel corpus filtering.\n[BOS] While participating systems focus on addressing both monolingual fluency and bilingual adequacy, character-level filtering is common to all submissions.\n[BOS] JunczysDowmunt (2018) applies a language identification model to implicitly remove sentences with illegal characters.\n[BOS] Rossenbach et al. (2018) keep sentences with more than three words, with each word having at least one character from the predefined alphabet of the language.\n[BOS] Lu et al. (2018) remove characters outside of a predefined alphabet.\n[BOS] Ash et al. (2018) count most frequent characters, set a cutoff around eighty for each language, and remove sentences with illegal characters.\n[BOS] Erdmann and Gwinnup (2018) get rid of lines containing characters from the Unicode general category of \"other\".\n[BOS] Papavassiliou et al. (2018) simply consider Latin Unicode characters to be legal.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Narrative_cite", "Reflection", "Reflection", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 103, "token_end": 115, "char_start": 515, "char_end": 555, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 177, "token_end": 199, "char_start": 907, "char_end": 1024, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 200, "token_end": 233, "char_start": 1031, "char_end": 1189, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Rossenbach et al. (2018)": "53246405"}, "Reference": {}}}, {"token_start": 234, "token_end": 249, "char_start": 1196, "char_end": 1264, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Lu et al. (2018)": "53234776"}, "Reference": {}}}, {"token_start": 250, "token_end": 278, "char_start": 1271, "char_end": 1412, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ash et al. (2018)": "53249696"}, "Reference": {}}}, {"token_start": 279, "token_end": 304, "char_start": 1419, "char_end": 1528, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Erdmann and Gwinnup (2018)": "53222982"}, "Reference": {}}}, {"token_start": 307, "token_end": 327, "char_start": 1537, "char_end": 1618, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Papavassiliou et al. (2018)": "53224061"}, "Reference": {}}}]}
{"id": "209076846_0", "paragraph": "[BOS] Early abstractive summarization efforts were either template-based (Wang and Cardie, 2013; Genest and Lapalme, 2011) or employed ILPbased sentence compression (Filippova, 2010; Berg-Kirkpatrick et al., 2011; Banerjee et al., 2015) .\n[BOS] With the advent of deep sequenceto-sequence models (Sutskever et al., 2014) , attention-based neural models have been proposed for long text summarization (Rush et al., 2015; Chopra et al., 2016) .\n[BOS] Recent approaches (Nallapati et al., 2017; See et al., 2017) have focused on larger datasets such as the CNN/DailyMail corpus (Hermann et al., 2015; .\n[BOS] introduced the ability to copy out-of-vocabulary words from the article to incorporate rarely seen words like names in the generated text.\n[BOS] Tu et al. (2016) included the concept of coverage, to prevent the models from repeating the same phrases while generating a sentence.\n[BOS] See et al. (2017) proposed a pointergenerator framework which incorporates these improvements, and also learns to switch between generating new words and copying them from the source article.\n[BOS] We use this pointer-generator framework as the underlying architecture.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Narrative_cite", "Transition", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 10, "token_end": 30, "char_start": 58, "char_end": 122, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wang and Cardie, 2013;": "1030812", "Genest and Lapalme, 2011)": "4942873"}}}, {"token_start": 31, "token_end": 65, "char_start": 126, "char_end": 236, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Filippova, 2010;": "14750088", "Berg-Kirkpatrick et al., 2011;": "15467396", "Banerjee et al., 2015)": "15795297"}}}, {"token_start": 71, "token_end": 87, "char_start": 264, "char_end": 320, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sutskever et al., 2014)": "7961699"}}}, {"token_start": 88, "token_end": 118, "char_start": 323, "char_end": 440, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rush et al., 2015;": "1918428", "Chopra et al., 2016)": "133195"}}}, {"token_start": 120, "token_end": 139, "char_start": 449, "char_end": 509, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Nallapati et al., 2017;": "6405271", "See et al., 2017)": null}}}, {"token_start": 147, "token_end": 160, "char_start": 554, "char_end": 596, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 190, "token_end": 217, "char_start": 751, "char_end": 884, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Tu et al. (2016)": "146843"}, "Reference": {}}}, {"token_start": 218, "token_end": 253, "char_start": 891, "char_end": 1082, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"See et al. (2017)": null}, "Reference": {}}}]}
{"id": "202776758_2", "paragraph": "[BOS] Legal document analysis is frequently viewed as text representation and classification task.\n[BOS] Hu et al. (2018) introduces few-shot attributes to enrich the information of mapping from case descriptions to charges, and Sulea et al. (2017) used multiple SVM classifiers as an ensemble to perform law area classification.\n\n", "discourse_tags": ["Transition", "Multi_summ"], "span_citation_mapping": [{"token_start": 16, "token_end": 39, "char_start": 105, "char_end": 223, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hu et al. (2018)": "52011555"}, "Reference": {}}}, {"token_start": 41, "token_end": 62, "char_start": 229, "char_end": 329, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Sulea et al. (2017)": "21057530"}, "Reference": {}}}]}
{"id": "202776758_1", "paragraph": "[BOS] Law searching is an unneglectable demand of legal workers such as lawyers and procurators, as they need to support their views with sufficient articles.\n[BOS] In industry, article retrieval applications simply parse inputs into phrases and adopt common IR approaches.\n[BOS] Academically, Zhang et al. (2017) built a Chinese legal consultation sys-tem to improve the precision of retrieving articles and predicting sentences by exploiting legal precedents when performing logical reasoning.\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ"], "span_citation_mapping": [{"token_start": 53, "token_end": 91, "char_start": 280, "char_end": 495, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhang et al. (2017)": "33792517"}, "Reference": {}}}]}
{"id": "202776758_0", "paragraph": "[BOS] The current automated legal consultation applications usually rely on retrieving relevant text information from pre-constructed database containing legal question-answer pairs using text features such as TF-IDF and bag-of-words (BoW).\n[BOS] Do et al. (2017) proposed QA models for legal consultation, and Hang (2017) preformed legal question classification with deep convolutional neural network trained in multi-task manner.\n\n", "discourse_tags": ["Transition", "Multi_summ"], "span_citation_mapping": [{"token_start": 46, "token_end": 59, "char_start": 247, "char_end": 305, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Do et al. (2017)": "10892308"}, "Reference": {}}}, {"token_start": 61, "token_end": 83, "char_start": 311, "char_end": 431, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "202780757_2", "paragraph": "[BOS] Our work is also relevant to recent works on integrating external knowledge into neural models for other NLP tasks.\n[BOS] The motivations of incorporating external knowledge range from enriching the context information (Mihaylov and Frank, 2018) in reading comprehension, improving the inference ability of models (Chen et al., 2018) in natural language inference, to providing the model a knowledge source to copy from in language modelling (Ahn et al., 2016) .\n[BOS] Our model, KBAtt, is most relevant to Mihaylov and Frank (2018) , where they focus on similarity calculation but we focus on generation in this paper.\n[BOS] Moreover, in addition to demonstrating the positive effect of incorporating external knowledge as previous work, we also design a new metric to quantify the potential gains of external knowledge for a specific dataset which can explain when and why our model is effective.\n\n", "discourse_tags": ["Reflection", "Narrative_cite", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 32, "token_end": 50, "char_start": 191, "char_end": 276, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mihaylov and Frank, 2018)": "29151507"}}}, {"token_start": 51, "token_end": 69, "char_start": 278, "char_end": 369, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chen et al., 2018)": "85466114"}}}, {"token_start": 71, "token_end": 92, "char_start": 374, "char_end": 466, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ahn et al., 2016)": "2600027"}}}, {"token_start": 104, "token_end": 129, "char_start": 513, "char_end": 625, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Mihaylov and Frank (2018)": "29151507"}, "Reference": {}}}]}
{"id": "202780757_1", "paragraph": "[BOS] With the advent of neural text generation, the distinction between content selection and surface realization becomes blurred.\n[BOS] For example, Mei et al. (2016) proposed an end-to-end encoder-aligner-decoder model to learn both content selection and surface realization jointly which shows good results on WeatherGov and RoboCub datasets.\n[BOS] Wiseman et al. (2017) generate long descriptive game summaries from a database of basketball games where they show the current state-of-the-art neural models are quite good at generating fluent outputs, but perform poorly in content selection and capturing long-term structure.\n[BOS] Our work falls into the task of single sentence generation from Wikipedia infoboxes.\n[BOS] The model structure ranges from feed-forward networks work (Lebret et al., 2016) to encoderdecoder models Bao et al., 2018; Nema et al., 2018) .\n[BOS] Recently, Perez-Beltrachini and Lapata (2018) generalize this task to multi-sentence text generation, where they focus on bootstrapping generators from loosely aligned data.\n[BOS] However, most of the work mentioned above assume all the writing knowledge can be learned from massive parallel pairs of training data.\n[BOS] Different from the previous work, we exploit incorporating external knowledge into this task to improve the fidelity of generated text.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Reflection", "Narrative_cite", "Single_summ", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 26, "token_end": 70, "char_start": 151, "char_end": 346, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Mei et al. (2016)": "1354459"}, "Reference": {}}}, {"token_start": 71, "token_end": 127, "char_start": 353, "char_end": 630, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wiseman et al. (2017)": "23892230"}, "Reference": {}}}, {"token_start": 150, "token_end": 164, "char_start": 760, "char_end": 808, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lebret et al., 2016)": "1238927"}}}, {"token_start": 165, "token_end": 185, "char_start": 812, "char_end": 870, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Bao et al., 2018;": "19099243", "Nema et al., 2018)": "5072513"}}}, {"token_start": 189, "token_end": 224, "char_start": 889, "char_end": 1052, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "202780757_0", "paragraph": "[BOS] Data-to-text generation is an important task in natural language generation which has been studied for decades (Kukich, 1983; Holmes-Higgin, 1994; Reiter and Dale, 1997) .\n[BOS] This task is broadly divided into two subproblems: content selection (Kukich, 1983; Reiter and Dale, 1997; Duboue and McKeown, 2003; Barzilay and Lapata, 2005) and surface realization (Goldberg et al., 1994; Re-iter et al., 2005) .\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 13, "token_end": 44, "char_start": 54, "char_end": 175, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kukich, 1983;": "10056961", "Holmes-Higgin, 1994;": "44354897", "Reiter and Dale, 1997)": "8460470"}}}, {"token_start": 56, "token_end": 92, "char_start": 235, "char_end": 343, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kukich, 1983;": "10056961", "Reiter and Dale, 1997;": "8460470", "Duboue and McKeown, 2003;": "2120088", "Barzilay and Lapata, 2005)": "1589010"}}}, {"token_start": 93, "token_end": 112, "char_start": 348, "char_end": 413, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Goldberg et al., 1994;": null}}}]}
{"id": "195833714_2", "paragraph": "[BOS] The role of bilingual dictionary A common way to select a bilingual dictionary is by using either automatic translations of frequent words or word alignments.\n[BOS] For instance, (Faruqui and Dyer, 2014 ) select the target word to which the source word is most frequently aligned in parallel corpora.\n[BOS] use the 5,000 most frequent words from the source language with their translations.\n[BOS] To investigate the impact of the dictionary on the embedding quality, (Vuli and Korhonen, 2016) evaluate different factors and conclude that carefully selecting highly reliable symmetric translation pairs improves the performance of benchmark word-translation tasks.\n[BOS] The authors also demonstrate that increasing the lexicon size over 10,000 pairs show a slow and steady decrease in performance.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 30, "token_end": 77, "char_start": 175, "char_end": 396, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Faruqui and Dyer, 2014": "3792324"}, "Reference": {}}}, {"token_start": 78, "token_end": 147, "char_start": 403, "char_end": 803, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Vuli\u0107 and Korhonen, 2016)": "17515652"}, "Reference": {}}}]}
{"id": "195833714_1", "paragraph": "[BOS] An extensive survey of different approaches, including offline and online methods can be found in (Ruder, 2017) .\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 11, "token_end": 24, "char_start": 69, "char_end": 117, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ruder, 2017)": "195345851"}}}]}
{"id": "195833714_0", "paragraph": "[BOS] Offline linear map induction methods The earliest approach to induce a linear mapping from the monolingual embedding spaces into a shared space was introduced in .\n[BOS] They propose to learn the mapping by optimising the least squares objective on the monolingual embedding matrices corresponding to translational equivalent pairs.\n[BOS] Subsequent research aimed to improve the mapping quality by optimising different objectives such as max-margin (Lazaridou et al., 2015) and by introducing an orthogonality constraint to the bilingual map to enforce self-consistency (Xing et al., 2015; Smith et al., 2017) .\n[BOS] (Artetxe et al., 2016) provide a theoretical analysis to existing approaches and in a follow-up research (Artetxe et al., 2018 ) they propose to learn principled bilingual mappings via a series of linear transformations.\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite", "Multi_summ"], "span_citation_mapping": [{"token_start": 71, "token_end": 85, "char_start": 445, "char_end": 480, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lazaridou et al., 2015)": "12187767"}}}, {"token_start": 89, "token_end": 117, "char_start": 503, "char_end": 616, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Xing et al., 2015;": "3144258", "Smith et al., 2017)": "11591887"}}}, {"token_start": 119, "token_end": 168, "char_start": 625, "char_end": 845, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Artetxe et al., 2016)": "1040556", "(Artetxe et al., 2018": "4334731"}, "Reference": {}}}]}
{"id": "201070608_2", "paragraph": "[BOS] As for diverse text generation, existing methods can be divided into three categories: enriching conditions (Xing et al., 2017) , post-processing with beam search and rerank (Li et al., 2016) , and designing effective models (Xu et al., 2018) .\n[BOS] Some text-to-text generation models (Serban et al., 2017; Zhao et al., 2017) inject high-level variations with latent variables.\n[BOS] Variational Hierarchical Conversation RNN (VHCR) (Park et al., 2018) is a most similar model to ours, which also adopts a hierarchical latent structure.\n[BOS] Our method differs from VHCR in two aspects: (1) VHCR has no planning mechanism, and the global latent variable is mainly designed to address the KL collapse problem, while our global latent variable captures the diversity of reasonable planning; (2) VHCR injects distinct local latent variables without direct dependencies, while our method explicitly models the dependencies among local latent variables to better capture inter-sentence connections.\n[BOS] Shen et al. (2019) proposed ml-VAE-D with multi-level latent variables.\n[BOS] However, the latent structure of ml-VAE-D consists of two global latent variables: the top-level latent variable is introduced to learn a more flexible prior of the bottom-level latent variable which is then used to decode a whole paragraph.\n[BOS] By contrast, our hierarchical latent structure is tailored to our planning mechanism: the top level latent variable controls planning results and a sequence of local latent variables is introduced to obtain fine-grained control of sentence generation sub-tasks.\n[BOS] We evaluated our model on a new advertising text generation task which is to generate a long and diverse text that covers all given specifications about a product.\n[BOS] Different from our task, the advertising text generation task in is to generate personalized product description based on product title, product aspect (e.g., \"appearance\"), and user category.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Narrative_cite", "Reflection", "Single_summ", "Transition", "Reflection", "Reflection", "Transition"], "span_citation_mapping": [{"token_start": 17, "token_end": 29, "char_start": 93, "char_end": 133, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Xing et al., 2017)": "9514751"}}}, {"token_start": 30, "token_end": 47, "char_start": 136, "char_end": 197, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Li et al., 2016)": "7287895"}}}, {"token_start": 49, "token_end": 60, "char_start": 204, "char_end": 248, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Xu et al., 2018)": "53081554"}}}, {"token_start": 63, "token_end": 95, "char_start": 262, "char_end": 385, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Serban et al., 2017;": "14857825", "Zhao et al., 2017)": "14688760"}}}, {"token_start": 96, "token_end": 113, "char_start": 392, "char_end": 460, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Park et al., 2018)": "4829361"}}}, {"token_start": 212, "token_end": 234, "char_start": 1009, "char_end": 1080, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Shen et al. (2019)": "59553516"}, "Reference": {}}}]}
{"id": "201070608_1", "paragraph": "[BOS] As for long text generation, recent studies tackle the incoherence problem from different perspectives.\n[BOS] To keep the decoder aware of the crucial information in the already generated prefix, Shao et al. (2017) appended the generated prefix to the encoder, and leaked the extracted features of the generated prefix from the discriminator to the generator in a Generative Adversarial Nets (Goodfellow et al., 2014) .\n[BOS] To model dependencies among sentences, Li et al. (2015) utilized a hierarchical recurrent neural network (RNN) decoder.\n[BOS] Konstas and Lapata (2013) proposed to plan content organization with grammar rules while Puduppully et al. (2019) planned by reordering input data.\n[BOS] Most recently, Moryossef et al. (2019) proposed to select plans from all possible ones, which is infeasible for large inputs.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Single_summ", "Multi_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 22, "token_end": 87, "char_start": 116, "char_end": 423, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Shao et al. (2017)": "5586146"}, "Reference": {"(Goodfellow et al., 2014)": "1033682"}}}, {"token_start": 90, "token_end": 114, "char_start": 435, "char_end": 551, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Li et al. (2015)": "207468"}, "Reference": {}}}, {"token_start": 115, "token_end": 152, "char_start": 558, "char_end": 705, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Konstas and Lapata (2013)": "17769145", "Puduppully et al. (2019)": "52153976"}, "Reference": {}}}, {"token_start": 156, "token_end": 183, "char_start": 727, "char_end": 837, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Moryossef et al. (2019)": "102350767"}, "Reference": {}}}]}
{"id": "201070608_0", "paragraph": "[BOS] Traditional methods (Reiter and Dale, 1997; Stent et al., 2004) for data-to-text generation consist of three components: content planning, sentence planning, and surface realization.\n[BOS] Content planning and sentence planning are responsible for what to say and how to say respectively; they are typically based on hand-crafted (Kukich, 1983; Dalianis and Hovy, 1993; Hovy, 1993) or automatically-learnt rules (Duboue and McKeown, 2003) .\n[BOS] Surface realization generates natural language by carrying out the plan, which is template-based (McRoy et al., 2003; van Deemter et al., 2005) or grammar-based (Bateman, 1997; Espinosa et al., 2008) .\n[BOS] As these models are shallow and the two stages (planning and realization) often function separately, traditional methods are unable to capture rich variations of texts.\n[BOS] Recently, neural methods have become the mainstream models for data-to-text generation due to their strong ability of representation learning and scalability.\n[BOS] These methods perform well in generating weather forecasts (Mei et al., 2016) or very short biographies (Lebret et al., 2016 Nema et al., 2018) using well-designed data encoder and attention mechanisms.\n[BOS] However, as demonstrated in Wiseman et al. (2017) (a game report generation task), existing neural methods are still problematic for long text generation: they often generate incoherent texts.\n[BOS] In fact, these methods also lack the ability to model diversity of expressions.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Narrative_cite", "Transition", "Transition", "Narrative_cite", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 4, "token_end": 42, "char_start": 26, "char_end": 188, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Reiter and Dale, 1997;": "8460470", "Stent et al., 2004)": "1543141"}}}, {"token_start": 65, "token_end": 91, "char_start": 323, "char_end": 387, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kukich, 1983;": "10056961", "Dalianis and Hovy, 1993;": "2424421", "Hovy, 1993)": "45255104"}}}, {"token_start": 92, "token_end": 108, "char_start": 391, "char_end": 444, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Duboue and McKeown, 2003)": "2120088"}}}, {"token_start": 113, "token_end": 145, "char_start": 483, "char_end": 596, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(McRoy et al., 2003;": "16074781", "van Deemter et al., 2005)": "474726"}}}, {"token_start": 146, "token_end": 164, "char_start": 600, "char_end": 652, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bateman, 1997;": "15726221", "Espinosa et al., 2008)": "5943308"}}}, {"token_start": 229, "token_end": 241, "char_start": 1031, "char_end": 1078, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mei et al., 2016)": "1354459"}}}, {"token_start": 242, "token_end": 273, "char_start": 1082, "char_end": 1203, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lebret et al., 2016": "1238927", "Nema et al., 2018)": "5072513"}}}, {"token_start": 279, "token_end": 313, "char_start": 1238, "char_end": 1402, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wiseman et al. (2017)": "23892230"}, "Reference": {}}}]}
{"id": "202772099_0", "paragraph": "[BOS] The capsule networks have been recently applied to a number of NLP tasks (Xiao et al., 2018; Gong et al., 2018) .\n[BOS] In particular, Yang et al. (2018) represent text classification labels by a layer of capsules, and take the capsule actions as the classification probability.\n[BOS] Using a similar method, Xia et al. (2018) perform intent detection with the capsule networks.\n[BOS] and Li et al. (2019a) use capsule networks to capture rich features for machine translation.\n[BOS] More closely to our work, and Zhang et al. (2019) adopt the capsule networks for relation extraction.\n[BOS] The previous models apply the capsule networks to problems that have a fixed number of components in the output.\n[BOS] Their approach cannot be directly applied to our setting.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 13, "token_end": 32, "char_start": 69, "char_end": 117, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Xiao et al., 2018;": "53080947", "Gong et al., 2018)": "46938675"}}}, {"token_start": 37, "token_end": 64, "char_start": 141, "char_end": 284, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Yang et al. (2018)": "4588148"}, "Reference": {}}}, {"token_start": 70, "token_end": 85, "char_start": 315, "char_end": 384, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Xia et al. (2018)": "52156433"}, "Reference": {}}}, {"token_start": 87, "token_end": 107, "char_start": 395, "char_end": 483, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Li et al. (2019a)": "102352478"}, "Reference": {}}}, {"token_start": 115, "token_end": 131, "char_start": 520, "char_end": 591, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhang et al. (2019)": "53284606"}, "Reference": {}}}]}
{"id": "153312772_4", "paragraph": "[BOS] Platanios et al. (2018) augment a multilingual model with language-specific embeddings from which the encoder/decoder parameters are inferred with additional linear transformations.\n[BOS] They only mention its potential to transfer to an unseen language without any results on it.\n[BOS] Our work focuses on transferring a pre-trained model to a new language without any change in the model architecture but with an explicit guidance for crosslinguality on the word embedding level.\n[BOS] Wang et al. (2019) address the vocabulary mismatch in multilingual NMT by using shared embeddings of character n-grams and common semantic concepts.\n[BOS] Their method has a strict assumption that the languages should be related with shared alphabets, while our method is not limited to similar languages and directly benefits from advances in cross-lingual word embedding for distant languages.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Reflection", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 54, "char_start": 6, "char_end": 286, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 93, "token_end": 124, "char_start": 494, "char_end": 642, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wang et al. (2019)": "60440615"}, "Reference": {}}}]}
{"id": "153312772_3", "paragraph": "[BOS] Artificial noises for the source sentences are used to counteract word-by-word training data in unsupervised MT (Artetxe et al., 2018b; Lample et al., 2018; , but they are used to regularize the NMT in this work.\n[BOS] Neubig and Hu (2018) study adapting a multilingual NMT system to a new language.\n[BOS] They train for a child language pair with additional parallel data of its similar language pair.\n[BOS] Our synthetic data method does not rely on the relateness of languages but still shows a good performance.\n[BOS] They learn just a separate subword vocabulary for the child language without a further care, which we counteract with cross-lingual word embedding.\n[BOS] show ablation studies on parameter sharing/freezing in one-to-many multilingual setup with shared vocabularies.\n[BOS] Our work conduct the similar experiments in the transfer learning setting with separate vocabularies.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ", "Reflection", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 41, "char_start": 6, "char_end": 161, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Artetxe et al., 2018b;": "3515219"}}}, {"token_start": 58, "token_end": 96, "char_start": 225, "char_end": 408, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Neubig and Hu (2018)": "51976920"}, "Reference": {}}}]}
{"id": "153312772_2", "paragraph": "[BOS] Cross-lingual word embedding is studied for the usages in MT as follows.\n[BOS] In phrase-based SMT, Alkhouli et al. (2014) builds translation models with word/phrase embeddings.\n[BOS] uses cross-lingual word embedding as a basic translation model for unsupervised MT and attach other components on top of it.\n[BOS] Artetxe et al. (2018b) and Lample et al. (2018) initialize their unsupervised NMT models with pretrained crosslingual word embeddings.\n[BOS] Qi et al. (2018) do the same initialization for supervised cases, observing only improvements in multilingual setups.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Multi_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 21, "token_end": 70, "char_start": 88, "char_end": 314, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Alkhouli et al. (2014)": "55103"}, "Reference": {}}}, {"token_start": 71, "token_end": 105, "char_start": 321, "char_end": 455, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Artetxe et al. (2018b)": "3515219", "Lample et al. (2018)": "3470398"}, "Reference": {}}}, {"token_start": 106, "token_end": 130, "char_start": 462, "char_end": 579, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Qi et al. (2018)": "4929974"}, "Reference": {}}}]}
{"id": "153312772_1", "paragraph": "[BOS] Multilingual NMT trains a single model with parallel data of various translation directions jointly from scratch (Dong et al., 2015; Johnson et al., 2017; Firat et al., 2016) .\n[BOS] Their methods also rely on shared subword vocabularies so it is hard for their model to adapt to a new language.\n\n", "discourse_tags": ["Multi_summ", "Multi_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 70, "char_start": 6, "char_end": 301, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dong et al., 2015;": "3666937", "Johnson et al., 2017;": "6053988", "Firat et al., 2016)": "6359641"}}}]}
{"id": "153312772_0", "paragraph": "[BOS] Transfer learning is first introduced for NMT in Zoph et al. (2016) , yet with a small RNN architecture and on top frequent words instead of using subword units.\n[BOS] Nguyen and Chiang (2017) and Kocmi and Bojar (2018) use shared vocabularies of BPE tokens to improve the transfer learning, but this requires retraining of the parent model whenever we transfer to a new child language.\n\n", "discourse_tags": ["Single_summ", "Multi_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 39, "char_start": 6, "char_end": 167, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zoph et al. (2016)": "16631020"}, "Reference": {}}}, {"token_start": 40, "token_end": 90, "char_start": 174, "char_end": 392, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Nguyen and Chiang (2017)": "3526501", "Kocmi and Bojar (2018)": "52156032"}, "Reference": {}}}]}
{"id": "184482940_2", "paragraph": "[BOS] Previous editions of related workshops are TA-COS 1 , Abusive Language Online 2 , and TRAC 3 and related shared tasks such as GermEval (Wiegand et al., 2018) and TRAC (Kumar et al., 2018) .\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 29, "token_end": 42, "char_start": 132, "char_end": 163, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 43, "token_end": 52, "char_start": 168, "char_end": 193, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kumar et al., 2018)": "59336626"}}}]}
{"id": "184482940_1", "paragraph": "[BOS] A proposal of typology of abusive language sub-tasks is presented in (Waseem et al., 2017) .\n[BOS] For studies on languages other than English see work by Su et al. (2017) on Chinese and Fier et al. (2017) on Slovene.\n[BOS] Finally, for recent discussion on identifying profanity vs. hate speech see the work by .\n[BOS] This work highlighted the challenges of distinguishing between profanity, and threatening language which may not actually contain profane language.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 5, "token_end": 27, "char_start": 20, "char_end": 96, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Waseem et al., 2017)": "8821211"}}}, {"token_start": 39, "token_end": 48, "char_start": 161, "char_end": 188, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Su et al. (2017)": "32076269"}}}, {"token_start": 49, "token_end": 62, "char_start": 193, "char_end": 223, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "184482940_0", "paragraph": "[BOS] Papers published in the last two years include the surveys by Schmidt and Wiegand (2017) and Fortuna and Nunes (2018) .\n[BOS] The paper by presenting the Hate Speech Detection dataset were used in (Malmasi and Zampieri, 2017 ) and a few other recent papers such as (ElSherief et al., 2018; Gambck and Sikdar, 2017; Zhang et al., 2018) .\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 13, "token_end": 21, "char_start": 68, "char_end": 94, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Schmidt and Wiegand (2017)": "9626793"}}}, {"token_start": 22, "token_end": 30, "char_start": 99, "char_end": 123, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Fortuna and Nunes (2018)": "52184457"}}}, {"token_start": 37, "token_end": 55, "char_start": 160, "char_end": 230, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Malmasi and Zampieri, 2017": "19182892"}}}, {"token_start": 60, "token_end": 91, "char_start": 249, "char_end": 340, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(ElSherief et al., 2018;": "4809781", "Gamb\u00e4ck and Sikdar, 2017;": null, "Zhang et al., 2018)": "46939253"}}}]}
{"id": "208058458_4", "paragraph": "[BOS] Unlike the discriminators in (Wu et al., 2018; Yang et al., 2018; Kong et al., 2019) , our alignment-oriented discriminator learns a specific function to measure alignment score between source and target sentences, which is trained totally independently by the NMT generator.\n[BOS] The proposed discriminator assigns different weights to words and is sensitive to translation errors.\n[BOS] We also apply N -pair loss for training D to ensure that D will not punish the translations closed to the gold-standard overly.\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 30, "char_start": 6, "char_end": 90, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wu et al., 2018;": "12267036", "Yang et al., 2018;": "4702087", "Kong et al., 2019)": "53787112"}}}]}
{"id": "208058458_3", "paragraph": "[BOS] GAN (Goodfellow et al., 2014) is another promising framework to leverage sentence-level objectives in NMT.\n[BOS] Recently, there is some remarkable work in NMT (Wu et al., 2018; Yang et al., 2018) .\n[BOS] The framework comprises of two sub-models: i) an NMT model aims to produce sentences which are hard to be discriminated from the gold-standard sentences; and ii) a discriminator makes efforts to differentiate the model generated translations from the ground-truth ones.\n[BOS] A policy gradient method is leveraged to co-train the NMT model and the discriminator.\n[BOS] However, those approaches rarely take account of translation adequacy.\n[BOS] Furthermore, the discriminators of those work refer the target sentence in the corpus as the single gold-standard regardless the quality of model generated translations, which will punish too much to the good model generated translations.\n[BOS] Kong et al. (2019) propose an adequacyoriented discriminator which is trained to estimate the Coverage Difference Ratio (CDR) given the source and the generated translation.\n[BOS] However, CDR is unable to distinguish translation errors and it also neglects the importance of diversity between different words (as the examples shown in Fig.\n[BOS] 1 ).\n\n", "discourse_tags": ["Single_summ", "Narrative_cite", "Transition", "Transition", "Transition", "Transition", "Single_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 2, "token_end": 27, "char_start": 6, "char_end": 112, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 30, "token_end": 53, "char_start": 129, "char_end": 202, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wu et al., 2018;": "12267036", "Yang et al., 2018)": "4702087"}}}, {"token_start": 186, "token_end": 242, "char_start": 902, "char_end": 1211, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kong et al. (2019)": "53787112"}, "Reference": {}}}]}
{"id": "208058458_2", "paragraph": "[BOS] On the other hand, some recent efforts introduce additional source side constraints and explore duality properties of NMT Xia et al., 2017; Tu et al., 2017) .\n[BOS] present a semi-supervised approach to train bidirectional NMT models and reconstruct the monolingual corpora using an autoencoder (Socher et al., 2011) .\n[BOS] Tu et al. (2017) add a re-constructor to traditional NMT model, which introduces an auxiliary score to measure the adequacy of translation.\n[BOS] Dual learning and dual supervised learning (Xia et al., 2017) are also proposed to exploit the probabilistic correlation between dual tasks to regularize the training process.\n[BOS] These previous approaches apply a reconstruction reward by comparing the source input and the reconstructed sentence, while we use alignment score directly to model the discrepancy between the source and the translation.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 7, "token_end": 36, "char_start": 25, "char_end": 162, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Xia et al., 2017;": "3730033", "Tu et al., 2017)": "15830483"}}}, {"token_start": 46, "token_end": 70, "char_start": 215, "char_end": 322, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Socher et al., 2011)": "3116311"}}}, {"token_start": 72, "token_end": 103, "char_start": 331, "char_end": 470, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Tu et al. (2017)": "15830483"}, "Reference": {}}}, {"token_start": 104, "token_end": 136, "char_start": 477, "char_end": 652, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Xia et al., 2017)": "3730033"}, "Reference": {}}}]}
{"id": "208058458_1", "paragraph": "[BOS] One way to alleviate these problems is to apply coverage and fertility to NMT model.\n[BOS] Feng et al. (2016) aim at controlling the fertilities of source words by appending additional additive terms to train objectives.\n[BOS] Tu et al. (2016) employ coverage vector or coverage ratio as a lexical-level indicator to represent whether a source word is translated or not.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 20, "token_end": 46, "char_start": 97, "char_end": 226, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 47, "token_end": 77, "char_start": 233, "char_end": 376, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Tu et al. (2016)": "146843"}, "Reference": {}}}]}
{"id": "208058458_0", "paragraph": "[BOS] Most of the state-of-the-art NMT models are optimized by MLE-based objectives Gehring et al., 2017; Vaswani et al., 2017) , but likelihood fails to measure whether the source information is completely transformed to the target side.\n[BOS] Thus, it cannot handle translation adequacy problem (Tu et al., 2017) .\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 18, "token_end": 41, "char_start": 63, "char_end": 127, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Gehring et al., 2017;": "3648736", "Vaswani et al., 2017)": "13756489"}}}, {"token_start": 62, "token_end": 76, "char_start": 251, "char_end": 314, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Tu et al., 2017)": "15830483"}}}]}
{"id": "201070807_4", "paragraph": "[BOS] Our proposed algorithms depart from these approaches by capturing domain semantics through shallow layers for use with generic encoder architectures.\n[BOS] Since some of the most successful algorithms in text classification (Kim, 2014) and sentence embeddings (Conneau et al., 2017 ) make use of CNN and BiLSTM building blocks, we suggest a generic adaptation framework that can be interfaced with these standard neural network blocks to improve performance on downstream tasks, particularly on small sized data sets.\n\n", "discourse_tags": ["Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 32, "token_end": 61, "char_start": 210, "char_end": 332, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kim, 2014)": null, "(Conneau et al., 2017": "28971531"}}}]}
{"id": "201070807_3", "paragraph": "[BOS] On the other hand work such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018) propose deeply connected layers to learn sentence embeddings by exploiting bi-directional contexts.\n[BOS] While both methods have achieved tremendous success in producing word (ELMo) and sentence (BERT) level encodings that perform well in several disparate NLP tasks such as question-answer solving, paraphrasing, POS tagging, sentiment analysis etc, these models are computationally expensive and require large amounts of training data.\n[BOS] Particularly when used in a transfer learning setting, both algorithms assume that a large amount of data is present in the source as well as the target domains.\n[BOS] In contrast, our proposed adaptation layer is particularly well suited in applications with limited data in the target domain.\n\n", "discourse_tags": ["Multi_summ", "Multi_summ", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 9, "token_end": 108, "char_start": 37, "char_end": 533, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Peters et al., 2018)": "3626819", "(Devlin et al., 2018)": "52967399"}, "Reference": {}}}]}
{"id": "201070807_2", "paragraph": "[BOS] Typical downstream applications such as cross lingual and/or multi-domain sentiment classification, using algorithms proposed by (Hangya et al., 2018) , , make use of DNNs 1 https://github.com/naacl18sublong / Friedland with RNN blocks such as BiLSTMs to learn both generic and domain specific representations.\n[BOS] Particularly, work focused on multi-domain sentiment classification as in (Liu et al., 2016) , (Nam and Han, 2016) , proposes building neural architectures for each domain of interest, in addition to shared representation layers across all domains.\n[BOS] While these techniques are effective, they are not ideal in domains with limited data.\n\n", "discourse_tags": ["Single_summ", "Multi_summ", "Multi_summ"], "span_citation_mapping": [{"token_start": 23, "token_end": 76, "char_start": 135, "char_end": 316, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Hangya et al., 2018)": "51867382"}, "Reference": {}}}, {"token_start": 77, "token_end": 143, "char_start": 323, "char_end": 664, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Liu et al., 2016)": "16017905", "(Nam and Han, 2016)": "973101"}, "Reference": {}}}]}
{"id": "201070807_1", "paragraph": "[BOS] Recent work such as SEMAXIS ) investigates the use of word level domain semantics for applications beyond sentiment analysis.\n[BOS] The authors introduce the concept of a semantic axis based on word antonym pairs to capture semantic differences across corpora.\n[BOS] Similarly, work by (Hamilton et al., 2016) captures domain semantics in the form of sentiment lexicons via graph propagation.\n[BOS] While both these lexical based approaches are similar to the ideas of this paper, a major difference is that like (K Sarma et al., 2018) , we do not make use of any predefined domain specific lexicons to capture domain semantics.\n[BOS] Our idea is to use word occurrences and contexts to provide a raw estimate of the domain semantics.\n[BOS] Using generic embedding spaces as baselines, adaptation is performed by projecting generic embeddings into a learned 'adaptation' space.\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 51, "token_end": 73, "char_start": 292, "char_end": 398, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Hamilton et al., 2016)": "12245213"}, "Reference": {}}}, {"token_start": 95, "token_end": 123, "char_start": 519, "char_end": 634, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(K Sarma et al., 2018)": "44143625"}, "Reference": {}}}]}
{"id": "201070807_0", "paragraph": "[BOS] This paper discusses domain adaptation techniques and the applicability of existing algorithms to diverse downstream tasks.\n[BOS] Central themes of this paper tie into word level domain semantics, while being related to the overall objective of domain adaptation.\n\n", "discourse_tags": ["Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "202766615_2", "paragraph": "[BOS] In addition to the hard parameter sharing strategy that we discuss in Section 3.2, partial parameter sharing strategy is also a commonly studied approach in MTL and domain adaptation.\n[BOS] Kim et al. (2016) introduce simple neural extensions of feature argumentation by employing a global LSTM used across all domains and independent LSTMs used within individual domains.\n[BOS] Peng et al. (2017) explore a multitask learning approach which shares parameters across formalisms for semantic dependency parsing.\n[BOS] In addition, Peng et al. (2018) present a multi-task approach for frame-semantic parsing and semantic dependency parsing with latent structured variables.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 37, "token_end": 71, "char_start": 196, "char_end": 378, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kim et al. (2016)": "14816251"}, "Reference": {}}}, {"token_start": 72, "token_end": 96, "char_start": 385, "char_end": 516, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Peng et al. (2017)": "15939234"}, "Reference": {}}}, {"token_start": 100, "token_end": 127, "char_start": 536, "char_end": 677, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Peng et al. (2018)": "4956705"}, "Reference": {}}}]}
{"id": "202766615_1", "paragraph": "[BOS] Compared with the large amount of works on English SRL, Chinese SRL works are rare, mainly because of the limitation of datasize and lack of attention of Chinese researchers.\n[BOS] Sun et al. (2009) treat the Chinese SRL as a sequence labeling problem and build a SVM-based model by exploiting morphological and syntactic features.\n[BOS] Wang et al. (2015b) build a basic BiLSTM model and introduce a way to exploit heterogeneous data by sharing word embeddings.\n[BOS] Xia et al. (2017) propose a progressive model to learn and transfer knowledge from heterogeneous SRL data.\n[BOS] The above works are all focus on the span-based Chinese SRL, and we compare with their results in Table 2 .\n[BOS] Different from them, we propose a MTL framework to integrate implicit syntactic representations into a simple unified model on both span-based and wordbased SRL, achieving substantial improvements.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 38, "token_end": 69, "char_start": 187, "char_end": 337, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Sun et al. (2009)": "7786830"}, "Reference": {}}}, {"token_start": 70, "token_end": 97, "char_start": 344, "char_end": 468, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wang et al. (2015b)": "11264500"}, "Reference": {}}}, {"token_start": 98, "token_end": 120, "char_start": 475, "char_end": 581, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Xia et al. (2017)": "10303415"}, "Reference": {}}}]}
{"id": "202766615_0", "paragraph": "[BOS] Traditional discrete-feature-based SRL works (Swanson and Gordon, 2006; Zhao et al., 2009) mainly make heavy use of syntactic information.\n[BOS] Along with the impressive development of neuralnetwork-based approaches in the NLP community, much attention has been paid to build more powerful neural model without considering any syntactic information.\n[BOS] Zhou and Xu (2015) Apart from the above syntax-free works, researchers also pay much attention on improving the neural-based SRL approaches by introducing syntactic knowledge.\n[BOS] Roth and Lapata (2016) introduce the dependency path embeddings to the neural-based model and achieve substantial improvements.\n[BOS] employ the graph convolutional neural networks on top of the BiLSTM encoder to encode syntactic information.\n[BOS] He et al. (2018b) propose a k-th order argument pruning algorithm based on systematic dependency trees.\n[BOS] Strubell et al. (2018) propose a self-attention based neural MTL model which incorporate dependency parsing as a auxiliary task for SRL.\n[BOS] propose a MTL framework using hard parameter strategy to incorporate constituent parsing loss into semantic tasks, i.e. SRL and coreference resolution, which outperforms their baseline by +0.8 F1 score.\n[BOS] Xia et al. (2019) investigate and compare several syntax-aware methods on span-based SRL, showing the effectiveness of integrating syntactic information.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Single_summ", "Single_summ", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 3, "token_end": 26, "char_start": 18, "char_end": 96, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Swanson and Gordon, 2006;": "7893643", "Zhao et al., 2009)": "2193825"}}}, {"token_start": 70, "token_end": 104, "char_start": 363, "char_end": 538, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhou and Xu (2015)": "12688069"}, "Reference": {}}}, {"token_start": 105, "token_end": 128, "char_start": 545, "char_end": 672, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Roth and Lapata (2016)": "5779419"}, "Reference": {}}}, {"token_start": 148, "token_end": 171, "char_start": 794, "char_end": 897, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"He et al. (2018b)": "51879191"}, "Reference": {}}}, {"token_start": 172, "token_end": 203, "char_start": 904, "char_end": 1040, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Strubell et al. (2018)": "5068376"}, "Reference": {}}}, {"token_start": 247, "token_end": 278, "char_start": 1256, "char_end": 1409, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Xia et al. (2019)": "59561763"}, "Reference": {}}}]}
{"id": "184483257_1", "paragraph": "[BOS] Various approaches utilized deep learning models for text categorization.\n[BOS] proposed a character-level convolutional network for text classification on large-scale datasets.\n[BOS] Their network uses 1-dimensional convolutional filters to extract features from different character embed-dings.\n[BOS] Gambck and Sikdar (2017) further experimented with convolutional networks in the context of online hate speech classification.\n[BOS] Their research work compares different types of convolutional models, namely character-level, word vectors with a pretrained word2vec (w2v) model, randomly generated word vectors, and w2v in combination with character n-grams.\n[BOS] The results of their experiments suggest that w2v embeddings are the most suitable for this task.\n[BOS] Zhang et al. (2018) suggest an architecture similar to our network, where a convolutional filter extracts features from pretrained word embeddings.\n[BOS] After max pooling, the feature maps are processed using a unidirectional GRU.\n[BOS] Their model is compared to a bag-of-n-gram model on various multi-class hate speech datasets and shows promising results.\n[BOS] A detailed survey on different architectures, methods and features for offensive language detection is provided by Schmidt and Wiegand (2017) .\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 50, "token_end": 145, "char_start": 309, "char_end": 772, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Gamb\u00e4ck and Sikdar (2017)": null}, "Reference": {}}}, {"token_start": 146, "token_end": 218, "char_start": 779, "char_end": 1138, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhang et al. (2018)": "46939253"}, "Reference": {}}}, {"token_start": 223, "token_end": 245, "char_start": 1166, "char_end": 1286, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Schmidt and Wiegand (2017)": "9626793"}, "Reference": {}}}]}
{"id": "184483257_0", "paragraph": "[BOS] Several methods and models have been presented in literature over the last decade to address the predicament of identifying hate speech, offensive language, and online aggressiveness.\n[BOS] In the following section, we present the most notable contributions related to our work.\n[BOS] The tweets collected by Davidson et al. (2017) were divided into Hate, Offensive, and Neither.\n[BOS] Their proposed algorithm uses unigram, bigram, and trigram tokens as features, weighted by the respective TF-IDF, as well as Part-of-Speech (POS) tagging and different metrics to determine the readability and sentiment of a tweet.\n[BOS] Logisticregression and linear SVM result in the best performance for a wide range of assessed classifiers.\n[BOS] Nobata et al. (2016) collected comments from Yahoo!\n[BOS] Finance and News articles over a time period of one year and labeled them as either 'Abusive' or 'Clean'.\n[BOS] They experimented with various different features, including n-gram, linguistic, syntactic, and distributional semantics features.\n\n", "discourse_tags": ["Transition", "Reflection", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 54, "token_end": 150, "char_start": 295, "char_end": 735, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Davidson et al. (2017)": "1733167"}, "Reference": {}}}, {"token_start": 151, "token_end": 216, "char_start": 742, "char_end": 1042, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Nobata et al. (2016)": "11546523"}, "Reference": {}}}]}
{"id": "155100120_4", "paragraph": "[BOS] Another group of sequential models deals with multi-hop reasoning following Memory Networks (Sukhbaatar et al., 2015) .\n[BOS] Such models construct representations for queries and memory cells for contexts, then make interactions between them in a multi-hop manner.\n[BOS] Munkhdalai and Yu (2017) and Onishi et al. (2016) incorporate a hypothesis testing loop to update the query representation at each reasoning step and select the best answer among the candidate entities at the last step.\n[BOS] IR-Net (Zhou et al., 2018) generates a subject state and a relation state at each step, computing the similarity score between all the entities and relations given by the dataset KB.\n[BOS] The ones with highest score at each time step are linked together to form an interpretable reasoning chain.\n[BOS] However, these models perform reasoning on simple synthetic datasets with limited number of entities and relations, which are quite different with largescale QA dataset with complex question.\n[BOS] Also, the supervision of entity-level reasoning chains in synthetic datasets can be easily given following some patterns while they are not available in Hot-potQA.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Multi_summ", "Single_summ", "Single_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 9, "token_end": 27, "char_start": 52, "char_end": 123, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sukhbaatar et al., 2015)": null}}}, {"token_start": 54, "token_end": 100, "char_start": 278, "char_end": 497, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Munkhdalai and Yu (2017)": "11222874", "Onishi et al. (2016)": "5761781"}, "Reference": {}}}, {"token_start": 101, "token_end": 193, "char_start": 504, "char_end": 998, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Zhou et al., 2018)": "3806582"}, "Reference": {}}}]}
{"id": "155100120_3", "paragraph": "[BOS] Coref-GRN extracts and aggregates entity information in different references from scattered paragraphs (Dhingra et al., 2018) .\n[BOS] Coref-GRN utilizes co-reference resolution to detect different mentions of the same entity.\n[BOS] These mentions are combined with a graph recurrent neural network (GRN) (Song et al., 2018b) to produce aggregated entity representations.\n[BOS] MHQA-GRN (Song et al., 2018a) follows Coref-GRN, and refines the graph construction procedure with more connections: sliding-window, same entity, and co-reference, which shows further improvements.\n[BOS] Entity-GCN (De Cao et al., 2018) proposes to distinguish dif-ferent relations in the graphs through a relational graph convolutional neural network (GCN) (Kipf and Welling, 2017) .\n[BOS] Coref-GRN, MHQA-GRN and Entity-GCN explore the graph construction problem in answering real-world questions.\n[BOS] However, it is yet to investigate how to effectively reason about the constructed graphs, which is the main problem studied in this work.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Narrative_cite", "Single_summ", "Single_summ", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 2, "token_end": 50, "char_start": 6, "char_end": 231, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Dhingra et al., 2018)": "4957155"}, "Reference": {}}}, {"token_start": 58, "token_end": 75, "char_start": 273, "char_end": 330, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Song et al., 2018b)": null}}}, {"token_start": 82, "token_end": 131, "char_start": 383, "char_end": 580, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Song et al., 2018a)": "52172080"}, "Reference": {}}}, {"token_start": 132, "token_end": 176, "char_start": 587, "char_end": 765, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(De Cao et al., 2018)": "52116920"}, "Reference": {"(Kipf and Welling, 2017)": null}}}]}
{"id": "155100120_2", "paragraph": "[BOS] Multi-hop Reasoning for QA Popular GNN frameworks, e.g. graph convolution network (Kipf and Welling, 2017), graph attention network (Velikovi et al., 2018) , and graph recurrent network (Song et al., 2018b) , have been previously studied and show promising results in QA tasks requiring reasoning (Dhingra et al., 2018; De Cao et al., 2018; Song et al., 2018a) .\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 17, "token_end": 29, "char_start": 62, "char_end": 112, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 30, "token_end": 43, "char_start": 114, "char_end": 161, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 45, "token_end": 57, "char_start": 168, "char_end": 212, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 67, "token_end": 97, "char_start": 274, "char_end": 366, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dhingra et al., 2018;": "4957155", "De Cao et al., 2018;": "52116920"}}}]}
{"id": "155100120_1", "paragraph": "[BOS] Note that existing multi-hop QA datasets Wiki-Hop and ComplexWebQuestions , are constructed using existing KBs and constrained by the schema of the KBs they use.\n[BOS] For example the answers are limited in entities not free text in WikiHop.\n[BOS] In this work, we focus on multi-hop text-based QA, so we only evaluate on HotpotQA.\n\n", "discourse_tags": ["Transition", "Transition", "Reflection"], "span_citation_mapping": []}
{"id": "155100120_0", "paragraph": "[BOS] Text-based Question Answering Depending on whether the supporting information is structured or not, QA tasks can be categorized into knowledge-based (KBQA), text-based (TBQA), mixed, and others.\n[BOS] In KBQA, the supporting information is from structured knowledge bases (KBs), while the queries can be either structure or natural language utterances.\n[BOS] For example, SimpleQuestions is one large scale dataset of this kind (Bordes et al., 2015) .\n[BOS] In contrast, TBQA's supporting information is raw text, and hence the query is also text.\n[BOS] SQuAD (Rajpurkar et al., 2016) and HotpotQA are two such datasets.\n[BOS] There are also mixed QA tasks which combine both text and KBs, e.g. WikiHop (Welbl et al., 2018) and ComplexWebQuestions (Talmor and Berant, 2018) .\n[BOS] In this paper, we focus on TBQA, since TBQA tests a system's end-to-end capability of extracting relevant facts from raw language and reasoning about them.\n[BOS] Depending on the complexity in underlying reasoning, QA problems can be categorized into single-hop and multi-hop ones.\n[BOS] Single-hop QA only requires one fact extracted from the underlying information, no matter structured or unstructured, e.g. \"which city is the capital of California\".\n[BOS] The SQuAD dataset belongs to this type (Rajpurkar et al., 2016) .\n[BOS] On the contrary, multi-hop QA requires identifying multiple related facts and reasoning about them, e.g. \"what is the capital city of the largest state in U.S.\".\n[BOS] Example tasks and benchmarks of this kind include WikiHop, Com-plexWebQuestions, and HotpotQA.\n[BOS] Many IR techniques can be applied to answer single-hop questions (Rajpurkar et al., 2016) .\n[BOS] However, these IR techniques are hardly introduced in multi-hop QA, since a single fact can only partially match a question.\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite", "Transition", "Narrative_cite", "Narrative_cite", "Reflection", "Transition", "Transition", "Narrative_cite", "Transition", "Transition", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 79, "token_end": 99, "char_start": 378, "char_end": 455, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bordes et al., 2015)": "9605730"}}}, {"token_start": 123, "token_end": 135, "char_start": 560, "char_end": 590, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rajpurkar et al., 2016)": "11816014"}}}, {"token_start": 163, "token_end": 175, "char_start": 701, "char_end": 729, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Welbl et al., 2018)": null}}}, {"token_start": 176, "token_end": 189, "char_start": 734, "char_end": 779, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Talmor and Berant, 2018)": "3986974"}}}, {"token_start": 288, "token_end": 305, "char_start": 1252, "char_end": 1311, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rajpurkar et al., 2016)": "11816014"}}}, {"token_start": 380, "token_end": 394, "char_start": 1633, "char_end": 1678, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rajpurkar et al., 2016)": "11816014"}}}]}
{"id": "201670719_3", "paragraph": "[BOS] However, these modern pre-trained language models contain millions of parameters, which hinders their application in practice where computational resource is limited.\n[BOS] In this paper, we aim at addressing this critical and challenging problem, taking BERT as an example, i.e., how to compress a large BERT model into a shallower one without sacrificing performance.\n[BOS] Besides, the proposed approach can also be applied to other large-scale pre-trained language models, such as recently proposed XLNet (Yang et al., 2019) and RoBERTa (Liu et al., 2019b) .\n\n", "discourse_tags": ["Transition", "Reflection", "Narrative_cite"], "span_citation_mapping": [{"token_start": 97, "token_end": 108, "char_start": 509, "char_end": 534, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yang et al., 2019)": "195069387"}}}, {"token_start": 109, "token_end": 121, "char_start": 539, "char_end": 566, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Liu et al., 2019b)": "198953378"}}}]}
{"id": "201670719_2", "paragraph": "[BOS] On the other hand, fine-tuning approaches mainly pre-train a language model (e.g., GPT (Radford et al., 2018) , BERT (Devlin et al., 2018) ) on a large corpus with an unsupervised objective, and then fine-tune the model with in-domain labeled data for downstream applications (Dai and Le, 2015; Howard and Ruder, 2018) .\n[BOS] Specifically, BERT is a large-scale language model consisting of multiple layers of Transformer blocks (Vaswani et al., 2017) .\n[BOS] BERT-Base has 12 layers of Transformer and 110 million parameters, while BERT-Large has 24 layers of Transformer and 330 million parameters.\n[BOS] By pre-training via masked language modeling and next sentence prediction, BERT has achieved state-of-the-art performance on a wide-range of NLU tasks, such as the GLUE benchmark (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2016) .\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 24, "token_end": 35, "char_start": 89, "char_end": 115, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 36, "token_end": 46, "char_start": 118, "char_end": 144, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Devlin et al., 2018)": "52967399"}}}, {"token_start": 70, "token_end": 86, "char_start": 258, "char_end": 324, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dai and Le, 2015;": "7138078", "Howard and Ruder, 2018)": "40100965"}}}, {"token_start": 103, "token_end": 115, "char_start": 417, "char_end": 458, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vaswani et al., 2017)": "13756489"}}}, {"token_start": 182, "token_end": 192, "char_start": 778, "char_end": 812, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wang et al., 2018)": "5034059"}}}, {"token_start": 193, "token_end": 205, "char_start": 817, "char_end": 847, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rajpurkar et al., 2016)": "11816014"}}}]}
{"id": "201670719_1", "paragraph": "[BOS] Feature-based methods mainly focus on learning: (i) context-independent word representation (e.g., word2vec (Mikolov et al., 2013) , GloVe (Pennington et al., 2014 ), FastText (Bojanowski et al., 2017 ); (ii) sentence-level representation (e.g., Kiros et al. (2015) ; Conneau et al. (2017) ; Logeswaran and Lee (2018) ); and (iii) contextualized word representation (e.g., Cove (McCann et al., 2017) , ELMo (Peters et al., 2018) ).\n[BOS] Specifically, ELMo (Peters et al., 2018) learns high-quality, deep contextualized word representation using bidirectional language model, which can be directly plugged into standard NLU models for performance boosting.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ"], "span_citation_mapping": [{"token_start": 25, "token_end": 38, "char_start": 105, "char_end": 136, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mikolov et al., 2013)": "16447573"}}}, {"token_start": 39, "token_end": 49, "char_start": 139, "char_end": 169, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Pennington et al., 2014": "1957433"}}}, {"token_start": 51, "token_end": 62, "char_start": 173, "char_end": 206, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 67, "token_end": 105, "char_start": 215, "char_end": 323, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Kiros et al. (2015)": null, "Conneau et al. (2017)": "28971531", "Logeswaran and Lee (2018)": "3525802"}}}, {"token_start": 121, "token_end": 132, "char_start": 379, "char_end": 405, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(McCann et al., 2017)": "9447219"}}}, {"token_start": 133, "token_end": 143, "char_start": 408, "char_end": 434, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 148, "token_end": 188, "char_start": 458, "char_end": 662, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Peters et al., 2018)": "3626819"}, "Reference": {}}}]}
{"id": "201670719_0", "paragraph": "[BOS] Language Model Pre-training Pre-training has been widely applied to universal language representation learning.\n[BOS] Previous work can be divided into two main categories: (i) feature-based approach; (ii) fine-tuning approach.\n\n", "discourse_tags": ["Transition", "Transition"], "span_citation_mapping": []}
{"id": "201698266_1", "paragraph": "[BOS] One use of feature norms is to critically examine distributional semantic models on their ability to encode grounded, human-elicited semantic knowledge.\n[BOS] For example, Rubinstein et al. (2015) demonstrated that state-of-the-art distributional semantic models fail to predict attributive properties of concept words (e.g. the properties is-red and is-round for the word apple) as accurately as taxonomic properties (e.g. is-a-fruit).\n[BOS] Similarly, Sommerauer and Fokkens (2018) investigated the types of semantic knowledge encoded within pretrained word embeddings, concluding that some properties cannot be learned by supervised classifiers.\n[BOS] Collell and Moens (2016) compared linguistic and visual representations of object concepts on their ability to represent different types of property knowledge.\n[BOS] Research has shown that state-of-the-art distributional semantic models built from text corpora fail to capture important aspects of meaning related to grounded perceptual information, as this kind of information is not adequately represented in the statistical regularities of text data (Li and Gauthier, 2017; Kelly et al., 2014) .\n[BOS] Motivated by these issues, Silberer (2017) constructed multimodal semantic models from text and image data, with the goal of grounding word meaning using visual attributes.\n[BOS] More recently, Derby et al. (2018) built similar models with the added constraint of sparsity, demonstrating that sparse multimodal vectors provide a more faithful representation of human semantic representations.\n[BOS] Finally, the work that most resembles ours is that of Fagarasan et al. (2015) , who use Partial Least Squares Regression (PLSR) to learn a mapping from a word embedding model onto specific conceptual properties.\n[BOS] Concurrent work recently undertaken by Li and Summers-Stay (2019) replaces the PLSR model with a feedforward neural network.\n[BOS] In our work, we instead map property knowledge directly into vector space models of word meaning, rather than learning a supervised predictive function from concept embedding dimensions to feature terms.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Multi_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 31, "token_end": 96, "char_start": 178, "char_end": 442, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Rubinstein et al. (2015)": "17537663"}, "Reference": {}}}, {"token_start": 99, "token_end": 133, "char_start": 460, "char_end": 654, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Sommerauer and Fokkens (2018)": "52162885"}, "Reference": {}}}, {"token_start": 134, "token_end": 161, "char_start": 661, "char_end": 820, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Collell and Moens (2016)": "17308411"}, "Reference": {}}}, {"token_start": 204, "token_end": 226, "char_start": 1077, "char_end": 1158, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Li and Gauthier, 2017;": "718342", "Kelly et al., 2014)": null}}}, {"token_start": 233, "token_end": 261, "char_start": 1194, "char_end": 1339, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Silberer (2017)": "195972302"}, "Reference": {}}}, {"token_start": 265, "token_end": 299, "char_start": 1361, "char_end": 1559, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Derby et al. (2018)": "52177700"}, "Reference": {}}}, {"token_start": 300, "token_end": 346, "char_start": 1566, "char_end": 1777, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Fagarasan et al. (2015)": "16303338"}, "Reference": {}}}, {"token_start": 347, "token_end": 373, "char_start": 1784, "char_end": 1908, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Li and Summers-Stay (2019)": "181913055"}, "Reference": {}}}]}
{"id": "201698266_0", "paragraph": "[BOS] Several property-listing studies have been conducted with human participants in order to build property norms -datasets of normalized humanverbalizable feature listings for lexical concepts (McRae et al., 2005; .\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 30, "token_end": 40, "char_start": 179, "char_end": 215, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "118588094_1", "paragraph": "[BOS] In Aldarmaki and Diab (2016) , a modular training objective has been proposed for cross-lingual sentence embedding.\n[BOS] However, their application was limited to the specific matrix factorization model they discussed.\n[BOS] More recently, proposed a modular transfer learning objective and evaluated it on neural sentence encoders using cross-lingual natural language inference classification.\n[BOS] Our representation transfer framework is very similar to their approach, although we use a simpler loss function.\n[BOS] In addition, we evaluate the framework as a general-purpose sentence encoder and compare it to other frameworks.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Transition", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 3, "token_end": 43, "char_start": 9, "char_end": 225, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Aldarmaki and Diab (2016)": "57358780"}, "Reference": {}}}]}
{"id": "118588094_0", "paragraph": "[BOS] Learning bilingual compositional representations can be achieved by optimizing a bilingual objective on parallel corpora.\n[BOS] In Pham et al. (2015) , distributed representations for bilingual phrases and sentences are learned using an extended version of the paragraph vector model (Le and Mikolov, 2014) by forcing parallel sentences to share one vector.\n[BOS] In Soyer et al. (2014) , cross-lingual compositional embeddings are learned by optimizing a joint bilingual objective that aligns parallel source and target representations by minimizing the Euclidean distances between them, and a monolingual objective that maximizes the similarity between similar phrases.\n[BOS] The monolingual objective was implemented by maximizing the similarity between random phrases and subphrases within the same sentence.\n[BOS] Cross-lingual representations can also be induced implicitly within a machine learning framework that is trained jointly for multiple language pairs.\n[BOS] In Schwenk and Douze (2017) , encoders and decoders for the given languages are trained jointly using a neural sequence to sequence model (Sutskever et al., 2014) using parallel corpora that are partially aligned; that is, each language within a pair is also part of at least one other parallel corpus.\n[BOS] Neural machine translation can also be achieved with a single encoder and decoder that handles several input languages (Johnson et al., 2017) , but the latter has not been evaluated as a general-purpose sentence representation model.\n[BOS] According to Hill et al. (2016) , the quality of the representations induced using a machine translation objective is lower than other neural models trained with different compositional objectives, such as Denoising Auto-Encoders and Skip-Thought (Kiros et al., 2015) .\n[BOS] Mono-lingual evaluation of sentence representation models can be found in Hill et al. (2016) , , and Conneau and Kiela (2018) .\n\n", "discourse_tags": ["Transition", "Multi_summ", "Single_summ", "Single_summ", "Transition", "Multi_summ", "Narrative_cite", "Multi_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 20, "token_end": 65, "char_start": 137, "char_end": 363, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Pham et al. (2015)": "11174813"}, "Reference": {"(Le and Mikolov, 2014)": "2407601"}}}, {"token_start": 67, "token_end": 143, "char_start": 373, "char_end": 818, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Soyer et al. (2014)": "10316648"}, "Reference": {}}}, {"token_start": 170, "token_end": 236, "char_start": 984, "char_end": 1283, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Schwenk and Douze (2017)": "6660863"}, "Reference": {"(Sutskever et al., 2014)": "7961699"}}}, {"token_start": 237, "token_end": 263, "char_start": 1290, "char_end": 1431, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Johnson et al., 2017)": "6053988"}}}, {"token_start": 281, "token_end": 334, "char_start": 1530, "char_end": 1797, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hill et al. (2016)": "2937095"}, "Reference": {"(Kiros et al., 2015)": "9126867"}}}, {"token_start": 336, "token_end": 368, "char_start": 1806, "char_end": 1931, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Hill et al. (2016)": "2937095", "Conneau and Kiela (2018)": "3932228"}}}]}
{"id": "207901272_0", "paragraph": "[BOS] Text summarization has long been an active area of research and query-based summarization has gained momentum more recently.\n[BOS] Classical summarization models usually identify salient parts of a text by encapsulating manually crafted rules into linear functions (Lin and Bilmes, 2011) which are solved using integer linear programming (ILP) (Nayeem and Chali, 2017; Boudin et al., 2015) , conditional random fields (CRF) (Shen et al., 2007) , or graph algorithms (Parveen and Strube, 2015; Erkan and Radev, 2004) .\n[BOS] More recently, neural networks, mostly with an encoder-decoder framework (Bahdanau et al., 2014) , have been used to learn the underlying features (Jadhav and Rajan, 2018; Nallapati et al., 2016) trained by minimizing the cross-entropy loss (Nallapati et al., 2017) or reinforcement learning (Narayan et al., 2018; Paulus et al., 2017) .\n[BOS] Our baseline models for query-based summa-rization (Nema et al., 2017; Hasselqvist et al., 2017) are both implemented on the encoderdecoder framework with the former incorporating a diversity function in their model aimed at minimizing the problem of repetitive word generation inherent in encoder-decoder models.\n[BOS] However our approach is similar to neither, as our goal is not to train a query-based summarizer from scratch but rather to investigate the competitiveness of using pre-trained models for closely related tasks-i.e., MRC and MT-on query-based summarization.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 46, "token_end": 56, "char_start": 254, "char_end": 293, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lin and Bilmes, 2011)": "320371"}}}, {"token_start": 60, "token_end": 85, "char_start": 317, "char_end": 395, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Boudin et al., 2015)": "6171252"}}}, {"token_start": 86, "token_end": 100, "char_start": 398, "char_end": 449, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Shen et al., 2007)": null}}}, {"token_start": 102, "token_end": 121, "char_start": 455, "char_end": 521, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Erkan and Radev, 2004)": "506350"}}}, {"token_start": 132, "token_end": 146, "char_start": 577, "char_end": 626, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bahdanau et al., 2014)": "11212020"}}}, {"token_start": 151, "token_end": 174, "char_start": 647, "char_end": 725, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Jadhav and Rajan, 2018;": "51870490", "Nallapati et al., 2016)": "8928715"}}}, {"token_start": 176, "token_end": 192, "char_start": 737, "char_end": 795, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Nallapati et al., 2017)": "6405271"}}}, {"token_start": 193, "token_end": 212, "char_start": 799, "char_end": 865, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Narayan et al., 2018;": "3510042", "Paulus et al., 2017)": "21850704"}}}, {"token_start": 218, "token_end": 245, "char_start": 898, "char_end": 970, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Hasselqvist et al., 2017)": "21581059"}}}]}
{"id": "174799374_1", "paragraph": "[BOS] Since then, multiple works based on languagemodels have been proposed for the GEC task (Park and Levy, 2011; Dahlmeier and Ng, 2012a) , either relying entirely on LMs or using them for fine-tuning their systems.\n[BOS] Many of the topranked systems in the CoNLL-2013 GEC shared tasks (Ng et al., 2013 , were either based on language models or had them as integral parts of their systems (Kao et al., 2013; Yoshimoto et al., 2013; Xing et al., 2013; Lee and Lee, 2014; Junczys-Dowmunt and Grundkiewicz, 2014) .\n[BOS] LM-only approaches though took a backseat and were only sporadically used after the shared tasks, as Neural Machine Translationbased approaches took over, but LMs remained an integral part of the GEC systems (JunczysDowmunt and Grundkiewicz, 2016; Ji et al., 2017; Xie et al., 2016; Junczys-Dowmunt et al., 2018; Chollampatt and Ng, 2018) .\n[BOS] However, Bryant and Briscoe (2018) recently revived the idea, achieving competitive performance with the state-ofthe-art, demonstrating the effectiveness of the approaches to the task without using any annotated data for training.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Narrative_cite", "Single_summ"], "span_citation_mapping": [{"token_start": 16, "token_end": 34, "char_start": 84, "char_end": 139, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Levy, 2011;": "7784892"}}}, {"token_start": 60, "token_end": 75, "char_start": 261, "char_end": 305, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 80, "token_end": 137, "char_start": 329, "char_end": 512, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kao et al., 2013;": "589284", "Yoshimoto et al., 2013;": "10672840", "Xing et al., 2013;": "14785293", "Lee and Lee, 2014;": "18340693", "Junczys-Dowmunt and Grundkiewicz, 2014)": "18318073"}}}, {"token_start": 170, "token_end": 231, "char_start": 680, "char_end": 859, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Ji et al., 2017;": "235645", "Xie et al., 2016;": "8880428", "Junczys-Dowmunt et al., 2018;": "4953145"}}}, {"token_start": 235, "token_end": 278, "char_start": 877, "char_end": 1098, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Bryant and Briscoe (2018)": "46940766"}, "Reference": {}}}]}
{"id": "174799374_0", "paragraph": "[BOS] The idea of using language models is quite fundamental to the task of Grammatical Error Correction, which has fed a substantial body of work over the years.\n[BOS] More recently, with the availability of web-scale data powering the advances in language modeling, among most of the other advances in NLP, a plethora of language-modeling based approaches have been proposed for the GEC task.\n[BOS] Gamon et al. (2008); Matthieu Hermet and Szpakowicz (2008) and Yi et al. (2008) were some of the early works to successfully leverage language models trained on large amounts of web-scale data into a GEC system, reinforcing the idea that simple models and a lot of data trump more elaborate models based on annotated data (Halevy et al., 2009) .\n\n", "discourse_tags": ["Transition", "Transition", "Multi_summ"], "span_citation_mapping": [{"token_start": 80, "token_end": 166, "char_start": 401, "char_end": 744, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Gamon et al. (2008);": "5088255", "Yi et al. (2008)": "9389112"}, "Reference": {"(Halevy et al., 2009)": "14300215"}}}]}
{"id": "174801285_2", "paragraph": "[BOS] Machine Reading Comprehension: MRC models such as SQuAD-like models, aim to extract answer spans (starting and ending indices) Doc <title> Investigations </title> <p> \"Investigations\" is the 36th episode of the American science fiction television series from a given document for a given question (Seo et al., 2017; Liu et al., 2018b; Yu et al., 2018) .\n[BOS] These models differ in how they fuse information between questions and documents.\n[BOS] We chose SAN (Liu et al., 2018b ) because of its representative architecture and competitive performance on existing MRC tasks.\n[BOS] We note that other off-theshelf MRC models, such as BERT (Devlin et al., 2018) , can also be plugged in.\n[BOS] We leave the study of different MRC architectures for future work.\n[BOS] Questions are treated as entirely independent in these \"single-turn\" MRC models, so recent work (e.g., CoQA (Reddy et al., 2019) and QuAC (Choi et al., 2018) ) focuses on multi-turn MRC, modeling sequences of questions and answers in a conversation.\n[BOS] While multi-turn MRC aims to answer complex questions, that body of work is restricted to factual questions, whereas our work-like much of the prior work in end-to-end dialogue-models free-form dialogue, which also encompasses chitchat and non-factual responses.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Narrative_cite", "Narrative_cite", "Reflection", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 18, "token_end": 87, "char_start": 82, "char_end": 357, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Seo et al., 2017;": "8535316", "Liu et al., 2018b;": "24202507", "Yu et al., 2018)": "4842909"}}}, {"token_start": 105, "token_end": 114, "char_start": 463, "char_end": 485, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Liu et al., 2018b": "24202507"}}}, {"token_start": 143, "token_end": 153, "char_start": 640, "char_end": 666, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Devlin et al., 2018)": "52967399"}}}, {"token_start": 200, "token_end": 212, "char_start": 875, "char_end": 900, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Reddy et al., 2019)": "52055325"}}}, {"token_start": 213, "token_end": 223, "char_start": 905, "char_end": 929, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Choi et al., 2018)": "52057510"}}}]}
{"id": "174801285_1", "paragraph": "[BOS] Prior work, e.g, (Ghazvininejad et al., 2018; Zhang et al., 2018a; Huang et al., 2019) , uses grounding in the form of independent snippets of text: Foursquare tips and background information about a given speaker.\n[BOS] Our notion of grounding is different, as our inputs are much richer, encompassing the full text of a web page and its underlying structure.\n[BOS] Our setting also differs significantly from relatively recent work (Dinan et al., 2019; Moghe et al., 2018) exploiting crowdsourced conversations with detailed grounding labels: we use Reddit because of its very large scale and better characterization of real-world conversations.\n[BOS] We also require the system to learn grounding directly from conversation and document pairs, instead of relying on additional grounding labels.\n[BOS] Moghe et al. (2018) explored directly using a span-prediction QA model for conversation.\n[BOS] Our framework differs in that we combine MRC models with a sequence generator to produce free-form responses.\n\n", "discourse_tags": ["Multi_summ", "Reflection", "Multi_summ", "Reflection", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 9, "token_end": 64, "char_start": 23, "char_end": 220, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Ghazvininejad et al., 2018;": "15442925", "Zhang et al., 2018a;": "6869582", "Huang et al., 2019)": "153312680"}, "Reference": {}}}, {"token_start": 103, "token_end": 153, "char_start": 440, "char_end": 653, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Dinan et al., 2019;": "53218829", "Moghe et al., 2018)": "52333947"}, "Reference": {}}}, {"token_start": 180, "token_end": 200, "char_start": 810, "char_end": 898, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Moghe et al. (2018)": "52333947"}, "Reference": {}}}]}
{"id": "174801285_0", "paragraph": "[BOS] Dialogue: Traditional dialogue systems (see (Jurafsky and Martin, 2009 ) for an historical perspective) are typically grounded, enabling these systems to be reflective of the user's environment.\n[BOS] The lack of grounding has been a stumbling block for the earliest end-to-end dialogue systems, as various researchers have noted that their outputs tend to be bland (Li et al., 2016a; Gao et al., 2019b) , inconsistent (Zhang et al., 2018a; Li et al., 2016b; Zhang et al., 2019) , and lacking in factual content (Ghazvininejad et al., 2018; Agarwal et al., 2018) .\n[BOS] Recently there has been growing interest in exploring different forms of grounding, including images, knowledge bases, and plain texts (Das et al., 2017; Mostafazadeh et al., 2017; Agarwal et al., 2018; Yang et al., 2019) .\n[BOS] A recent survey is included in Gao et al. (2019a) .\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 4, "token_end": 17, "char_start": 16, "char_end": 76, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Jurafsky and Martin, 2009": null}}}, {"token_start": 70, "token_end": 95, "char_start": 341, "char_end": 409, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Li et al., 2016a;": "7287895", "Gao et al., 2019b)": "67855635"}}}, {"token_start": 96, "token_end": 122, "char_start": 412, "char_end": 484, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhang et al., 2018a;": "6869582", "Li et al.,": "7356547", "Zhang et al., 2019)": "76665205"}}}, {"token_start": 124, "token_end": 150, "char_start": 491, "char_end": 568, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ghazvininejad et al., 2018;": "15442925", "Agarwal et al., 2018)": "53105624"}}}, {"token_start": 167, "token_end": 210, "char_start": 671, "char_end": 798, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Das et al., 2017;": "47159154", "Mostafazadeh et al., 2017;": "9142609", "Agarwal et al., 2018;": "53105624", "Yang et al., 2019)": "126187120"}}}, {"token_start": 218, "token_end": 227, "char_start": 838, "char_end": 856, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Gao et al. (2019a)": null}}}]}
{"id": "202783277_0", "paragraph": "[BOS] Our work is related to several important research directions of NMT, and we describe the previous relevant works in this section.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "202541617_2", "paragraph": "[BOS] In a related task, Koncel-Kedziorski et al. (2019) propose an attention-based graph model that generates sentences from knowledge graphs.\n[BOS] Schlichtkrull et al. (2018) use Graph Convolutional Networks (GCNs) to tackle the tasks of link prediction and entity classification on knowledge graphs.\n[BOS] Damonte and Cohen (2019) show that off-theshelf GCNs cannot achieve good performance for AMR-to-text generation.\n[BOS] To tackle this issue, Guo et al. (2019) introduce dense connectivity to GNNs in order to integrate both local and global features, achieving good results on the task.\n[BOS] Our work is related to Damonte and Cohen (2019) , that use stacking of GCN and LSTM layers to improve the model capacity and employ anonymization.\n[BOS] However, our model is substantially different: (i) we learn dual representations capturing top-down and bottom-up adjuvant views of the graph, (ii) we employ more effective graph encoders (with different neighborhood aggregations) than GCNs and (iii) we employ copy and coverage mechanisms and do not resort to entity anonymization.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 7, "token_end": 35, "char_start": 25, "char_end": 143, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Koncel-Kedziorski et al. (2019)": "102354588"}, "Reference": {}}}, {"token_start": 36, "token_end": 68, "char_start": 150, "char_end": 303, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Schlichtkrull et al. (2018)": "207908316"}, "Reference": {}}}, {"token_start": 69, "token_end": 100, "char_start": 310, "char_end": 422, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Damonte and Cohen (2019)": "85528601"}, "Reference": {}}}, {"token_start": 106, "token_end": 137, "char_start": 451, "char_end": 595, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Guo et al. (2019)": "195064954"}, "Reference": {}}}, {"token_start": 139, "token_end": 173, "char_start": 606, "char_end": 748, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Damonte and Cohen (2019)": "85528601"}, "Reference": {}}}]}
{"id": "202541617_1", "paragraph": "[BOS] Several graph-to-sequence models have been proposed.\n[BOS] Marcheggiani and Perez Beltrachini (2018) show that explicitly encoding the structure of the graph is beneficial with respect to sequential encoding.\n[BOS] They evaluate their model on two tasks, WebNLG (Gardent et al., 2017) and SR11Deep (Belz et al., 2011) , but do not apply it to AMR benchmarks.\n[BOS] Song et al. (2018) and Beck et al. (2018) apply recurrent neural networks to directly encode AMR graphs.\n[BOS] Song et al. (2018) use a graph LSTM as the graph encoder, whereas Beck et al. (2018) develop a model based on GRUs.\n[BOS] We go a step further in that direction by developing parallel encodings of graphs which are able to highlight different graph properties.\n\n", "discourse_tags": ["Transition", "Single_summ", "Narrative_cite", "Multi_summ", "Multi_summ", "Reflection"], "span_citation_mapping": [{"token_start": 14, "token_end": 91, "char_start": 65, "char_end": 364, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Marcheggiani and Perez Beltrachini (2018)": "53034346"}, "Reference": {"(Gardent et al., 2017)": "28193461", "(Belz et al., 2011)": "12040771"}}}, {"token_start": 92, "token_end": 118, "char_start": 371, "char_end": 475, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Song et al. (2018)": "25111673", "Beck et al. (2018)": "49430686"}, "Reference": {}}}, {"token_start": 119, "token_end": 135, "char_start": 482, "char_end": 538, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Song et al. (2018)": "25111673"}, "Reference": {}}}, {"token_start": 137, "token_end": 152, "char_start": 548, "char_end": 597, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Beck et al. (2018)": "49430686"}, "Reference": {}}}]}
{"id": "202541617_0", "paragraph": "[BOS] Early works on AMR-to-text generation employ statistical methods (Flanigan et al., 2016b; Pourdamghani et al., 2016; Castro Ferreira et al., 2017) and apply linearization of the graph by means of a depth-first traversal.\n[BOS] Recent neural approaches have exhibited success by linearising the input graph and using a sequence-to-sequence architecture.\n[BOS] Konstas et al. (2017) achieve promising results on this task.\n[BOS] However, they strongly rely on named entities anonymisation.\n[BOS] Anonymisation requires an ad hoc procedure for each new corpus.\n[BOS] The matching procedure needs to match a rare input item correctly (e.g., \"United States of America\") with the corresponding part in the output text (e.g., \"USA\") which may be challenging and may result in incorrect or incomplete delexicalisations.\n[BOS] In contrast, our approach omits anonymisation.\n[BOS] Instead, we use a copy mechanism (See et al., 2017) , a generic technique which is easy to integrate in the encoder-decoder framework and can be used independently of the particular domain and application.\n[BOS] Our approach further differs from Konstas et al. (2017) in that we build a dual TD/BU graph representation and use graph encoders to represent nodes.\n[BOS] Cao and Clark (2019) factor the generation process leveraging syntactic information to improve the performance.\n[BOS] However, they linearize both AMR and constituency graphs, which implies that important parts of the graphs cannot well be represented (e.g., coreference).\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection", "Reflection", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 12, "token_end": 47, "char_start": 44, "char_end": 152, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Flanigan et al., 2016b;": "9135033", "Pourdamghani et al., 2016;": "18600354", "Castro Ferreira et al., 2017)": "28639009"}}}, {"token_start": 86, "token_end": 186, "char_start": 365, "char_end": 817, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Konstas et al. (2017)": "8066499"}, "Reference": {}}}, {"token_start": 203, "token_end": 228, "char_start": 895, "char_end": 1010, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(See et al., 2017)": null}}}, {"token_start": 243, "token_end": 275, "char_start": 1102, "char_end": 1238, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Konstas et al. (2017)": "8066499"}, "Reference": {}}}, {"token_start": 276, "token_end": 332, "char_start": 1245, "char_end": 1517, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Cao and Clark (2019)": "102483832"}, "Reference": {}}}]}
{"id": "202565945_3", "paragraph": "[BOS] Architecture Learning Our work also shares the spirit of recent research on Neural Architecture Search (NAS) (Zoph and Le, 2016; Pham et al., 2018; Liu et al., 2018) , since the architecture of the model is learned dynamically based on controllers instead of being manually-designed.\n[BOS] However, Neural Architecture Search aims to learn the structure of the individual CNN/RNN cell with fixed inter-connections between the cells, while Modular Networks have preset individual modules but learns the way to assemble them into a larger network.\n[BOS] Moreover, Modular Networks' architectures are predicted dynamically on each data point, while previous NAS methods learn a single cell structure independent of the example.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 14, "token_end": 43, "char_start": 82, "char_end": 171, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zoph and Le, 2016;": "12713052", "Pham et al., 2018;": "3638969", "Liu et al., 2018)": "49411844"}}}]}
{"id": "202565945_2", "paragraph": "[BOS] However, since the the modular layout is sampled from the controller, the controller itself is not end-to-end differentiable and has to be optimized using Reinforcement Learning Algorithms like Reinforce (Williams, 1992) .\n[BOS] Hu et al. (2018) used soft program execution where the output of each step is the average of outputs from all modules weighted by the module distribution, and showed its superiority over hard-layout NMNs.\n[BOS] All previous works in NMN, including Hu et al. (2018) targeted visual question answering or structured knowledge-based GeoQA, and hence the modules are designed to process image or KB inputs.\n[BOS] We are the first to apply modular networks to unstructured, text-based QA, where we redesigned the modules for language-based reasoning by using bi-attention (Seo et al., 2017; Xiong et al., 2017) to replace convolution and multiplication of the question vector with the image feature.\n[BOS] Our model also has access to the full-sized bi-attention vector before it is projected down to the 1-d distribution.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 36, "token_end": 42, "char_start": 200, "char_end": 226, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Williams, 1992)": "19115634"}}}, {"token_start": 44, "token_end": 86, "char_start": 235, "char_end": 439, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hu et al. (2018)": "49908459"}, "Reference": {}}}, {"token_start": 95, "token_end": 127, "char_start": 483, "char_end": 637, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hu et al. (2018)": "49908459"}, "Reference": {}}}, {"token_start": 157, "token_end": 177, "char_start": 789, "char_end": 840, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Seo et al., 2017;": "8535316", "Xiong et al., 2017)": "3714278"}}}]}
{"id": "202565945_1", "paragraph": "[BOS] Neural Modular Network Neural Modular Network (NMN) is a class of models that are composed of a number of sub-modules, where each sub-module is capable of performing a specific subtask.\n[BOS] In NMN (Andreas et al., 2016b) , N2NMN (Hu et al., 2017) , PG+EE (Johnson et al., 2017) and TbD (Mascharka et al., 2018) , the entire reasoning procedure starts by analyzing the question and decomposing the reasoning procedure into a sequence of sub-tasks, each with a corresponding module.\n[BOS] This is done by either a parser (Andreas et al., 2016b) or a layout policy (Hu et al., 2017; Johnson et al., 2017; Mascharka et al., 2018) that turns the question into a module layout.\n[BOS] Then the module layout is executed with a neural module network.\n[BOS] Overall, given an input question, the layout policy learns what sub-tasks to perform, and the neural modules learn how to perform each individual sub-task.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 44, "token_end": 56, "char_start": 201, "char_end": 228, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Andreas et al., 2016b)": "5276660"}}}, {"token_start": 57, "token_end": 69, "char_start": 231, "char_end": 254, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hu et al., 2017)": "18682"}}}, {"token_start": 70, "token_end": 81, "char_start": 257, "char_end": 285, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Johnson et al., 2017)": "31319559"}}}, {"token_start": 82, "token_end": 94, "char_start": 290, "char_end": 318, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mascharka et al., 2018)": "3863856"}}}, {"token_start": 131, "token_end": 143, "char_start": 520, "char_end": 553, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Andreas et al., 2016b)": "5276660"}}}, {"token_start": 144, "token_end": 170, "char_start": 556, "char_end": 633, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hu et al., 2017;": "18682", "Johnson et al., 2017;": "31319559", "Mascharka et al., 2018)": "3863856"}}}]}
{"id": "202565945_0", "paragraph": "[BOS] Multi-hop Reading Comprehension The last few years have witnessed significant progress on large-scale QA datasets including clozestyle tasks (Hermann et al., 2015) , open-domain QA (Yang et al., 2015) , and more (Rajpurkar et al., 2016 (Rajpurkar et al., , 2018 .\n[BOS] However, all of the above datasets are confined to a single-document context per question domain.\n[BOS] Joshi et al. (2017) introduced a multi-document QA dataset with some questions requiring cross-sentence inferences to answer.\n[BOS] The bAbI dataset (Weston et al., 2016) requires the model to combine multiple pieces of evidence in the synthetic text.\n[BOS] Welbl et al. (2017) uses Wikipedia articles as the context and a subject-relation pair as the query, and constructs the multi-hop QAngaroo dataset by traversing a directed bipartite graph so that the evidence required to answer a query could be spread across multiple documents.\n[BOS] HotpotQA ) is a more recent multi-hop QA dataset that has crowd-sourced questions with more diverse syntactic and semantic features compared to QAngaroo.\n[BOS] It includes four types of questions, each requiring a different reasoning paradigm.\n[BOS] Some examples require inferring the bridge entity from the question (Type I in ), while others demand fact-checking or comparing subjects' properties from two different documents (Type II and comparison question).\n[BOS] Concurrently to our work, Min et al. (2019b) also tackle HotpotQA by decomposing its multi-hop questions into singlehop sub-questions to achieve better performance and interpretability.\n[BOS] However, their system approaches the question decomposition by having a decomposer model trained via human labels, while our controller accomplishes this task automatically with the soft attention over the questionwords' representation and is only distantly supervised by the answer and bridge-entity supervision, with no extra human labels.\n[BOS] Moreover, they propose a pipeline system with the decomposers, an answer-prediction model, and a decomposition scorer trained separately on the previous stage's output.\n[BOS] Our modular network, on the other hand, is an end-to-end system that is optimized jointly.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Single_summ", "Single_summ", "Single_summ", "Transition", "Transition", "Transition", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 22, "token_end": 36, "char_start": 130, "char_end": 169, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hermann et al., 2015)": "6203757"}}}, {"token_start": 37, "token_end": 49, "char_start": 172, "char_end": 206, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yang et al., 2015)": "1373518"}}}, {"token_start": 51, "token_end": 71, "char_start": 213, "char_end": 267, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rajpurkar et al., 2016": "11816014", "(Rajpurkar et al., , 2018": "47018994"}}}, {"token_start": 93, "token_end": 119, "char_start": 380, "char_end": 505, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Joshi et al. (2017)": "26501419"}, "Reference": {}}}, {"token_start": 120, "token_end": 147, "char_start": 512, "char_end": 631, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Weston et al., 2016)": "3178759"}, "Reference": {}}}, {"token_start": 148, "token_end": 206, "char_start": 638, "char_end": 916, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Welbl et al. (2017)": "9192723"}, "Reference": {}}}, {"token_start": 302, "token_end": 431, "char_start": 1419, "char_end": 2101, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Min et al. (2019b)": "174801080"}, "Reference": {}}}]}
{"id": "199372933_3", "paragraph": "[BOS] Although there is a series of related work that can contribute to coreference resolution in Chinese dialogue, there are many common restrictions when transferring them into a practical product: 1).\n[BOS] the limited data source in a general domain; 2).\n[BOS] most work concentrates on general coreference.\n[BOS] Few of them focus on pronoun or zero pronoun resolution, which is the vital step for dialogue NLU; 3).\n[BOS] no work known to us compares traditional feature-based methods and neural network based models on an end-to-end system for coreference resolution in Chinese dialogue.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Transition", "Transition"], "span_citation_mapping": []}
{"id": "199372933_2", "paragraph": "[BOS] Similarly, there has been much previous work in mention candidate ranking using deep neural network.\n[BOS] In recent years, applying deep neural networks on the task has reached great success.\n[BOS] Clark and Manning (2016) applied reinforcement learning on mention-ranking coreference resolution.\n[BOS] Lee et al. (2017) presented an end-to-end coreference resolution model which reasons over all the anteceding spans.\n[BOS] Lee et al. (2018) presented a high-order coreference resolution.\n[BOS] These approaches do not generalize to dialogue for the reason that 1) these approaches require a rich amount of well-annotated contextual data, 2) dialogue is short and has ambiguous syntactic structures which are difficult to handcraft rules, and 3) the resolution module should distinguish wrong detection results so that the systems have a higher fault tolerance on the detection module.\n[BOS] However, most existed work simply assumes a golden detection label and perform lots of feature engineering based on that.\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ", "Single_summ", "Single_summ", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 37, "token_end": 55, "char_start": 205, "char_end": 303, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Clark and Manning (2016)": "2012188"}, "Reference": {}}}, {"token_start": 56, "token_end": 83, "char_start": 310, "char_end": 425, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Lee et al. (2017)": "1222212"}, "Reference": {}}}, {"token_start": 84, "token_end": 100, "char_start": 432, "char_end": 496, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Lee et al. (2018)": null}, "Reference": {}}}]}
{"id": "199372933_1", "paragraph": "[BOS] Referring expressions detection can be further divided into two subtasks: 1).\n[BOS] find all words that do not have real meaning and refer to other mentions (/he /she/it/this/that,...).\n[BOS] We use the term 'pronoun' to represent these words without losing preciseness of linguistic definition in this paper.\n[BOS] 2).\n[BOS] find all zero pronouns.\n[BOS] A close task to the first subtask of referring expressions detection is coreference detection, which is to identify noun phrases and pronouns that are referring to the same entities.\n[BOS] Haghighi and Klein (2010) proposed an unsupervised generative approach for text coreference detections.\n[BOS] Uryupina and Moschitti (2013) proposed a rule-based approach which employed parse trees and SVM.\n[BOS] Peng et al. (2015) improved the performance of mention detections by applying a binary classifier on the feature set.\n\n", "discourse_tags": ["Transition", "Transition", "Reflection", "Other", "Other", "Transition", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 120, "token_end": 139, "char_start": 551, "char_end": 654, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Haghighi and Klein (2010)": "9203411"}, "Reference": {}}}, {"token_start": 140, "token_end": 165, "char_start": 661, "char_end": 757, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Uryupina and Moschitti (2013)": "15723339"}, "Reference": {}}}, {"token_start": 166, "token_end": 189, "char_start": 764, "char_end": 881, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Peng et al. (2015)": "17010034"}, "Reference": {}}}]}
{"id": "199372933_0", "paragraph": "[BOS] There has been much classical or linguistic theoretical work on coreference resolution in texts.\n[BOS] Coreference resolution is mainly concerned with two tasks, referring expressions detection, and mention candidate ranking.\n\n", "discourse_tags": ["Transition", "Transition"], "span_citation_mapping": []}
{"id": "196187188_2", "paragraph": "[BOS] Another related works are on Visual QA, aiming to answer the compositional questions with regards to a given image, such as \"What color is the matte thing to the right of the sphere in front of the tiny blue block?\"\n[BOS] In particular, Santoro et al. (2017) proposed a relation net, yet the net was restricted to relational question, such as comparison.\n[BOS] Later, Hudson and Manning (2018) introduced an iterative network.\n[BOS] The network separated memory and control to improve interpretability.\n[BOS] Our work leverages such separated design.\n[BOS] Different from previous researches, we dedicate to inferential machine comprehension, where the question may not be compositional, such as why question, but requires reasoning on an unknown evidence chain with uncertain depth.\n[BOS] The chain has to be inferred from the text semantics.\n[BOS] To the best of our knowledge, no previous studies have investigated an end-to-end approach to address this problem.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Reflection", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 52, "token_end": 78, "char_start": 243, "char_end": 360, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Santoro et al. (2017)": "8528277"}, "Reference": {}}}, {"token_start": 81, "token_end": 106, "char_start": 374, "char_end": 508, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hudson and Manning (2018)": "3728944"}, "Reference": {}}}]}
{"id": "196187188_1", "paragraph": "[BOS] To support inference, existing models can be classified into three categories, including predicate based methods (Richardson and Domingos, 2006) , rule-based methods relied on external parser (Sun et al., 2018b) or pre-built tree (Yu et al., 2012) , and multi-layer memory networks (Hill et al., 2015) , such as gated attended net (Dhingra et al., 2016) , double-sided attended net (Cui et al., 2016) , etc.\n[BOS] These models either lack end-to-end design for global training, or no prior structure to subtly guide the reasoning direction.\n[BOS] On the topic of multi-hop reasoning, current models often have to rely on the predefined graph constructed by external tools, such as interpretable network (Zhou et al., 2018) on knowledge graph.\n[BOS] The graph plainly links the facts, from which the intermediate result in the next hop can be directly derived.\n[BOS] However, in this paper, the evidence graph is not explicitly given by embodied in the text semantics.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Narrative_cite", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 16, "token_end": 28, "char_start": 95, "char_end": 150, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Richardson and Domingos, 2006)": "12698795"}}}, {"token_start": 29, "token_end": 46, "char_start": 153, "char_end": 217, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sun et al., 2018b)": "52011953"}}}, {"token_start": 47, "token_end": 59, "char_start": 221, "char_end": 253, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yu et al., 2012)": "2812037"}}}, {"token_start": 61, "token_end": 74, "char_start": 260, "char_end": 307, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hill et al., 2015)": "14915449"}}}, {"token_start": 77, "token_end": 90, "char_start": 318, "char_end": 359, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dhingra et al., 2016)": "6529193"}}}, {"token_start": 91, "token_end": 105, "char_start": 362, "char_end": 406, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cui et al., 2016)": "9205021"}}}, {"token_start": 162, "token_end": 173, "char_start": 687, "char_end": 728, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhou et al., 2018)": "3806582"}}}]}
{"id": "196187188_0", "paragraph": "[BOS] Earlier studies on machine comprehension mainly focused on the text span selection question.\n[BOS] It is often transformed into a similarity matching problem and solved by feature engineeringbased methods (Smith et al., 2015) or deep neural networks.\n[BOS] The classical features include lexical features (e.g. overlapping of words, Ngram, POS tagging) (Richardson et al., 2013) , syntactic features (Wang et al., 2015) , discourse features (Narasimhan and Barzilay, 2015) , etc.\n[BOS] Besides, the typical networks involve Stanford AR , AS Reader (Kadlec et al., 2016) , BiDAF (Seo et al., 2016) , Match-LSTM (Wang and Jiang, 2017) , etc, which used distributed vectors rather than discrete features to better compute the contextual similarity.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 29, "token_end": 41, "char_start": 178, "char_end": 231, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Smith et al., 2015)": "2949011"}}}, {"token_start": 51, "token_end": 76, "char_start": 294, "char_end": 384, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Richardson et al., 2013)": "2100831"}}}, {"token_start": 77, "token_end": 87, "char_start": 387, "char_end": 425, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wang et al., 2015)": "8764466"}}}, {"token_start": 88, "token_end": 102, "char_start": 428, "char_end": 478, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Narasimhan and Barzilay, 2015)": "15425307"}}}, {"token_start": 112, "token_end": 126, "char_start": 530, "char_end": 575, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kadlec et al., 2016)": "11022639"}}}, {"token_start": 127, "token_end": 138, "char_start": 578, "char_end": 602, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Seo et al., 2016)": "8535316"}}}, {"token_start": 139, "token_end": 150, "char_start": 605, "char_end": 638, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wang and Jiang, 2017)": "5592690"}}}]}
{"id": "57761084_0", "paragraph": "[BOS] There are studies on the presence of biases in many NLP applications.\n[BOS] Word embeddings can learn biases learn from human generated corpora.\n[BOS] (Bolukbasi et al., 2016) showed that stereotypical analogies are present in word embeddings both for gender and race.\n[BOS] (Caliskan et al., 2017) found also a strong gender and racial bias presence is found in pre-trained embeddings and proposed a method for measuring bias in word embeddings.\n[BOS] (Zhao et al., 2018b) proposed GN-GloVe, an algorithm to generated gender neutral word embeddings.\n[BOS] The approach is to restrict gender information attributes in certain dimensions to keep the remaining free of this attributes.\n[BOS] (Zhao et al., 2018a) shows that sexism present in a coreference resolution system is due to the word embeddings components.\n[BOS] Applications that use these embeddings, such as curriculum filtering, may discriminate candidates because of their gender.\n[BOS] The amplification of biases in downstream applications is a concerning problem also that can enlarge the gap between genders, for example in search engines, for professions where the name of the candidates may be discriminated by the algorithm because of their bias towards a specific gender.\n[BOS] Thus, broadening even further gender inequality for a given field.\n[BOS] (Zhao et al., 2017) shows that gender bias is learned and amplified in models trained from data sets containing web images used in language modelling tasks.\n[BOS] As an example of, the word \"cooking\" is more probable to be re-lated to females than males and it can be further amplified.\n[BOS] (Park et al., 2018) studies the reduction of such biases in abusive language detection.\n[BOS] These models have a strong bias towards words that identify gender because of the data sets in which they are trained.\n[BOS] Sentences that do not necessarily show sexism are detected as false positives compromising the robustness of the models.\n[BOS] Debiased word embeddings combined with augmenting and swapping gender data is the most effective method for reducing gender bias for this task.\n[BOS] (Prates et al., 2018) performs a case study on gender bias in machine translation.\n[BOS] They build a test set consisting of a list of jobs and gender-specific sentences.\n[BOS] Using English as a target language and a variety of gender neutral languages as a source, i.e. languages that do not explicitly give gender information about the subject, they test these sentences on the translating service Google Translate.\n[BOS] They find that occupations related to science, engineering and mathematics present a strong stereotype toward male subjects.\n[BOS] However, late 2018, Google announced in their developers blog 1 that efforts are put on providing gender-specific translations in Google Translate.\n[BOS] Thus, gives both the translation for female and male when translating from gender-neutral languages.\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Transition", "Transition", "Transition", "Single_summ", "Single_summ", "Single_summ", "Transition", "Transition", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 29, "token_end": 58, "char_start": 157, "char_end": 274, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Bolukbasi et al., 2016)": "1704893"}, "Reference": {}}}, {"token_start": 59, "token_end": 96, "char_start": 281, "char_end": 452, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Caliskan et al., 2017)": "14199015"}, "Reference": {}}}, {"token_start": 97, "token_end": 142, "char_start": 459, "char_end": 689, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Zhao et al., 2018b)": null}, "Reference": {}}}, {"token_start": 143, "token_end": 171, "char_start": 696, "char_end": 819, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Zhao et al., 2018a)": "4952494"}, "Reference": {}}}, {"token_start": 259, "token_end": 321, "char_start": 1327, "char_end": 1613, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Zhao et al., 2017)": "1389483"}, "Reference": {}}}, {"token_start": 322, "token_end": 342, "char_start": 1620, "char_end": 1707, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Park et al., 2018)": "52070035"}, "Reference": {}}}, {"token_start": 415, "token_end": 453, "char_start": 2116, "char_end": 2286, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Prates et al., 2018)": "52179151"}, "Reference": {}}}]}
{"id": "203610250_1", "paragraph": "[BOS] Recent summarization research has put special emphasis on faithfulness to the original text.\n[BOS] Cao et al. (2018a) use seq-to-seq models to rewrite templates that are prone to including irrelevant entities.\n[BOS] Incorporating additional information into a seq-to-seq model, such as entailment and dependency structure, has proven successful (Li et al., 2018; Song et al., 2018) .\n[BOS] The closest work to our human evaluation seems to be from Falke et al. (2019) .\n[BOS] Similar to our work, they find that the PG model is more faithful than Fast-Abs-RL and Bottom-Up, even though it has lower ROUGE.\n[BOS] They show that 25% of outputs from these stateof-the-art summarization models are unfaithful to the original article.\n[BOS] Cao et al. (2018b) reveal a similar finding that 27% of the summaries generated by a neural sequence-to-sequence model have errors.\n[BOS] Our study, by contrast, finds 38% to be unfaithful, but we limit our study to only summary sentences created by fusion.\n[BOS] Our work examines a wide variety of state-of-the-art summarization systems, and perform in-depth analysis over other measures including grammaticality, coverage, and method of merging.\n\n", "discourse_tags": ["Transition", "Single_summ", "Narrative_cite", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 19, "token_end": 45, "char_start": 105, "char_end": 215, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Cao et al. (2018a)": "51878811"}, "Reference": {}}}, {"token_start": 60, "token_end": 84, "char_start": 292, "char_end": 387, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Li et al., 2018;": "52012819", "Song et al., 2018)": "46936631"}}}, {"token_start": 97, "token_end": 171, "char_start": 454, "char_end": 735, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Falke et al. (2019)": "196187162"}, "Reference": {}}}, {"token_start": 172, "token_end": 203, "char_start": 742, "char_end": 873, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Cao et al. (2018b)": "19198109"}, "Reference": {}}}]}
{"id": "203610250_0", "paragraph": "[BOS] Sentence fusion aims to produce a single summary sentence by fusing multiple source sentences.\n[BOS] Dependency graphs and discourse structure have proven useful for aligning and combining multiple sentences into a single sentence (Barzilay and McKeown, 2005; Marsi and Krahmer, 2005; Filippova and Strube, 2008; Cheung and Penn, 2014; Gerani et al., 2014) .\n[BOS] Mehdad et al. (2013) construct an entailment graph over sentences for sentence selection, then fuse sentences together using a word graph.\n[BOS] Abstract meaning representation and other graph-based representations have also shown success in sentence fusion (Liu et al., 2015; Nayeem et al., 2018) .\n[BOS] Geva et al. (2019) fuse pairs of sentences together using Transformer, focusing on discourse connectives between sentences.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Single_summ", "Narrative_cite", "Single_summ"], "span_citation_mapping": [{"token_start": 19, "token_end": 82, "char_start": 107, "char_end": 362, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Barzilay and McKeown, 2005;": "16188305", "Marsi and Krahmer, 2005;": "2293515", "Filippova and Strube, 2008;": "14909308", "Cheung and Penn, 2014;": "9375250", "Gerani et al., 2014)": "2767900"}}}, {"token_start": 84, "token_end": 113, "char_start": 371, "char_end": 509, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Mehdad et al. (2013)": "5843073"}, "Reference": {}}}, {"token_start": 128, "token_end": 147, "char_start": 613, "char_end": 668, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Liu et al., 2015;": "5001921", "Nayeem et al., 2018)": "52011473"}}}, {"token_start": 149, "token_end": 174, "char_start": 677, "char_end": 800, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Geva et al. (2019)": "67855928"}, "Reference": {}}}]}
{"id": "202572744_0", "paragraph": "[BOS] Here, we discuss further related work, beyond that discussed in 4.1 on (dis)similarities between patterns learned by humans and neural networks.\n[BOS] (Lei et al., 2016; Li et al., 2016; Yu et al., 2019) .\n[BOS] Others extract evidence to improve downstream QA efficiency over large amounts of text (Choi et al., 2017; Kratzwald and Feuerriegel, 2019; Wang et al., 2018b Generic Summarization In contrast, various papers focus primarily on summarization rather than QA, using downstream QA accuracy only as a reward to optimize generic (question-agnostic) summarization models Liu, 2018, 2019; Eyal et al., 2019) .\n\n", "discourse_tags": ["Reflection", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 32, "token_end": 55, "char_start": 157, "char_end": 209, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 59, "token_end": 97, "char_start": 233, "char_end": 376, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Choi et al., 2017;": null}}}, {"token_start": 115, "token_end": 150, "char_start": 482, "char_end": 618, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Eyal et al., 2019)": "173990261"}}}]}
{"id": "195317107_1", "paragraph": "[BOS] Monolingual lexical definitions have been used for weak supervision of monolingual word similarities (Tissier et al., 2017) .\n[BOS] Our work demonstrates that dictionary information can be extended to a cross-lingual scenario, for which we develop a simple yet effective induction method to populate fine-grain word alignment.\n\n", "discourse_tags": ["Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 13, "token_end": 26, "char_start": 77, "char_end": 129, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Tissier et al., 2017)": "34057571"}}}]}
{"id": "195317107_0", "paragraph": "[BOS] Prior approaches to learning bilingual word embeddings often rely on word or sentence alignment (Ruder et al., 2017) .\n[BOS] In particular, seed lexicon methods (Mikolov et al., 2013a; Faruqui and Dyer, 2014; Guo et al., 2015) learn transformations across different language-specific embedding spaces based on predefined word alignment.\n[BOS] The performance of these approaches is limited by the sufficiency of seed lexicons.\n[BOS] Besides, parallel corpora methods (Gouws et al., 2015; Coulmance et al., 2015) leverage the aligned sentences in different languages and force the representations of corresponding sentence components to be similar.\n[BOS] However, aligned sentences merely provide weak alignment of lexicons that do not accurately capture the one-to-one mapping of words, while such a mapping is well-desired by translation tasks (Upadhyay et al., 2016) .\n[BOS] In addition, a few unsupervised approaches alleviate the use of bilingual resources (Chen and Cardie, 2018; Conneau et al., 2018) .\n[BOS] These models require considerable effort to train and rely heavily on massive monolingual corpora.\n\n", "discourse_tags": ["Narrative_cite", "Multi_summ", "Transition", "Multi_summ", "Narrative_cite", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 12, "token_end": 25, "char_start": 75, "char_end": 122, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ruder et al., 2017)": null}}}, {"token_start": 30, "token_end": 76, "char_start": 146, "char_end": 342, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Mikolov et al., 2013a;": "1966640", "Faruqui and Dyer, 2014;": "3792324", "Guo et al., 2015)": "18634877"}, "Reference": {}}}, {"token_start": 96, "token_end": 137, "char_start": 448, "char_end": 653, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Gouws et al., 2015;": "7021865"}, "Reference": {}}}, {"token_start": 173, "token_end": 186, "char_start": 833, "char_end": 874, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Upadhyay et al., 2016)": "5357629"}}}, {"token_start": 199, "token_end": 218, "char_start": 947, "char_end": 1012, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chen and Cardie, 2018;": "52099904", "Conneau et al., 2018)": "3470398"}}}]}
{"id": "202544143_3", "paragraph": "[BOS] GCN has been successfully applied to several individual monolingual NLP tasks, including relation extraction (Zhang et al., 2018b) , event detection (Nguyen and Grishman, 2018), SRL (Marcheggiani and Titov, 2017) and sentence classification (Yao et al., 2019) .\n[BOS] We apply GCN to construct multi-lingual structural representations for cross-lingual transfer learning.\n\n", "discourse_tags": ["Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 18, "token_end": 29, "char_start": 95, "char_end": 136, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhang et al., 2018b)": "49544037"}}}, {"token_start": 30, "token_end": 41, "char_start": 139, "char_end": 182, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 42, "token_end": 55, "char_start": 184, "char_end": 218, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 56, "token_end": 67, "char_start": 223, "char_end": 265, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yao et al., 2019)": "52284222"}}}]}
{"id": "202544143_2", "paragraph": "[BOS] Event extraction shares with Semantic Role Labeling (SRL) the task of assigning to each event argument its event role label, in the process of completing other tasks to extract the full event structure (assigning event types to predicates and more specific roles to arguments).\n[BOS] Cross-lingual transfer has been very successful for SRL.\n[BOS] Early attempts relied on word alignment (Van der Plas et al., 2011) or bilingual dictionaries (Kozhevnikov and Titov, 2013) .\n[BOS] Recent work incorporates universal dependencies (Prazk and Konopk, 2017) To the best of our knowledge, our work is the first to construct a cross-lingual structure transfer framework that combines language-universal symbolic representations and distributional representations for relation and event extraction over texts written in a language without any training data.\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 72, "token_end": 84, "char_start": 378, "char_end": 420, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Van der Plas et al., 2011)": "8774533"}}}, {"token_start": 85, "token_end": 100, "char_start": 424, "char_end": 476, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 105, "token_end": 117, "char_start": 510, "char_end": 557, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "202544143_1", "paragraph": "[BOS] Regardless of the recent successes in applying cross-lingual transfer learning to sequence labeling tasks, such as name tagging (e.g., (Mayhew et al., 2017; Lin et al., 2018; Huang et al., 2019) ), only limited work has explored cross-lingual relation and event structure transfer.\n[BOS] Most previous efforts working with cross-lingual structure trans-fer rely on bilingual dictionaries (Hsi et al., 2016), parallel data (Chen and Ji, 2009; Kim et al., 2010; Qian et al., 2014) or machine translation (Faruqui and Kumar, 2015; Zou et al., 2018) .\n[BOS] Recent methods (Lin et al., 2017; Wang et al., 2018b) aggregate consistent patterns and complementary information across languages to enhance Relation Extraction, but they do so exploiting only distributional representations.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Multi_summ"], "span_citation_mapping": [{"token_start": 23, "token_end": 56, "char_start": 121, "char_end": 200, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 89, "token_end": 100, "char_start": 371, "char_end": 412, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 101, "token_end": 125, "char_start": 414, "char_end": 484, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Qian et al., 2014)": "9312981"}}}, {"token_start": 126, "token_end": 146, "char_start": 488, "char_end": 551, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Zou et al., 2018)": "52010972"}}}, {"token_start": 148, "token_end": 188, "char_start": 560, "char_end": 785, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wang et al., 2018b)": "52013643"}, "Reference": {}}}]}
{"id": "202544143_0", "paragraph": "[BOS] A large number of supervised machine learning techniques have been used for English event extraction, including traditional techniques based on symbolic features (Ji and Grishman, 2008; Liao and Grishman, 2011), joint inference models (Li et al., 2014; Yang and Mitchell, 2016) , and recently with neural networks (Nguyen and Grishman, 2015a; Nguyen et al., 2016; Chen et al., 2015; Nguyen and Grishman, 2018; Liu et al., 2018b; Lu and Nguyen, 2018; Liu et al., 2018a; Zhang et al., , 2019 .\n[BOS] English relation extraction in the early days also followed supervised paradigms (Li and Ji, 2014; Zeng et al., 2014; Nguyen and Grishman, 2015b; Miwa and Bansal, 2016; Pawar et al., 2017; Bekoulis et al., 2018; Wang et al., 2018b) .\n[BOS] Recent efforts have attempted to reduce annotation costs using distant supervision (Mintz et al., 2009; Surdeanu et al., 2012; Min et al., 2013; Angeli et al., 2014; Zeng et al., 2015; Quirk and Poon, 2017; Qin et al., 2018; Wang et al., 2018a) through knowledge bases (KBs), where entities and static relations are plentiful.\n[BOS] Distant supervision is less applicable for the task of event extraction because very few dynamic events are included in KBs.\n[BOS] These approaches, however, incorporate language-specific characteristics and thus are costly in requiring substantial amount of annotations to adapt to a new language (Chen and Vincent, 2012; Blessing and Schtze, 2012; Li et al., 2012; Danilova et al., 2014; Agerri et al., 2016; Hsi et al., 2016; Feng et al., 2016) .\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Narrative_cite", "Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 23, "token_end": 43, "char_start": 150, "char_end": 216, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 44, "token_end": 61, "char_start": 218, "char_end": 283, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Yang and Mitchell, 2016)": "2367456"}}}, {"token_start": 65, "token_end": 129, "char_start": 304, "char_end": 495, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Liu et al., 2018b;": "52013643", "Zhang et al., , 2019": "135473179"}}}, {"token_start": 140, "token_end": 198, "char_start": 564, "char_end": 735, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Zeng et al., 2014;": "12873739", "Pawar et al., 2017;": "1125700", "Wang et al., 2018b)": "52013643"}}}, {"token_start": 209, "token_end": 275, "char_start": 807, "char_end": 988, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Surdeanu et al., 2012;": "5869747", "Zeng et al., 2015;": "2778800", "Quirk and Poon, 2017;": "15359942", "Qin et al., 2018;": "44063972", "Wang et al., 2018a)": "53099740"}}}, {"token_start": 323, "token_end": 400, "char_start": 1235, "char_end": 1524, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Agerri et al., 2016;": "4419743"}}}]}
{"id": "202565600_4", "paragraph": "[BOS] Comparatively, this work focuses on the creation of an explicit word vector space model (EWVM), with an associated explanation, evaluating the performance of different types of lexicosemantic resources in the context of the task of identification of discriminative attributes (IDA).\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "202565600_3", "paragraph": "[BOS] In the area of definition-based word vectors, similar initiatives were concentrated in the areas of definition-based distributional models and interpretable distributional semantic models.\n[BOS] Baroni et al. (2010) describes an approach that automatically constructs a distributional semantic model from a text corpus.\n[BOS] The model represents concepts in terms of weighted typed properties and is capable of describing the similarities between concepts as well as the properties responsible for this similarity.\n[BOS] Murphy et al. (2012) apply a matrixfactorization technique (NNSE) to produce sparse embeddings.\n[BOS] In addition, their embeddings are interpretable in a way, that given a dimension in the vector space, vectors in that dimension have a nonlatent relatedness to a human-interpretable concept.\n[BOS] Their work was extended to be able to form composed representation of word phrases while still maintaining the desired interpretability of the original model (Fyshe et al., 2015) .\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 33, "token_end": 86, "char_start": 201, "char_end": 521, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Baroni et al. (2010)": null}, "Reference": {}}}, {"token_start": 87, "token_end": 184, "char_start": 528, "char_end": 1005, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Murphy et al. (2012)": null}, "Reference": {"(Fyshe et al., 2015)": "3095388"}}}]}
{"id": "202565600_2", "paragraph": "[BOS] With regard to interpretability and explainability we can classify IDA approaches into three categories.\n[BOS] Frequency-based models over textbased features, heavily relying on textual features and frequency-based methods (Gamallo, 2018; Gonzlez et al., 2018) ; ML over Textual features (Dumitru et al., 2018; Sommerauer et al., 2018; King et al., 2018; Mao et al., 2018) and ML over dense vectors and textual features (Brychcn et al., 2018; Attia et al., 2018; Dumitru et al., 2018; Arroyo-Fernndez et al., 2018; Speer and Lowry-Duda, 2018; Santus et al., 2018; Grishin, 2018; Zhou et al., 2018; Vinayan et al., 2018; Kulmizev et al., 2018; Zhang and Carpuat, 2018; Shiue et al., 2018) .\n[BOS] While the first category concentrates on models with higher interpretability, none of these models provide explanations.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 33, "token_end": 55, "char_start": 184, "char_end": 266, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Gamallo, 2018;": "44135667", "Gonz\u00e1lez et al., 2018)": "44166475"}}}, {"token_start": 56, "token_end": 93, "char_start": 269, "char_end": 378, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dumitru et al., 2018;": "44151848", "Sommerauer et al., 2018;": "44062509", "King et al., 2018;": "44121293", "Mao et al., 2018)": "44079382"}}}, {"token_start": 94, "token_end": 207, "char_start": 383, "char_end": 693, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Brychc\u00edn et al., 2018;": "44063098", "Attia et al., 2018;": "44076752", "Dumitru et al., 2018;": "44151848", "Arroyo-Fern\u00e1ndez et al., 2018;": "44112954", "Speer and Lowry-Duda, 2018;": "44155085", "Santus et al., 2018;": "13745308", "Grishin, 2018;": "44144456", "Zhou et al., 2018;": "44128182", "Vinayan et al., 2018;": "44153267", "Kulmizev et al., 2018;": "44167704", "Zhang and Carpuat, 2018;": "44159207"}}}]}
{"id": "202565600_1", "paragraph": "[BOS] Existing approaches have explored combinations of linguistic and data resources (WordNet, ConceptNet, Wikipedia), linguistic features (syntactic dependencies), sparse word vector models (JoBimText), dense word-vector (DWV) models (W2V, GloVe) and supervised/ unsupervised machine learning approaches (SVM, MLP, Ensemble Methods).\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "202565600_0", "paragraph": "[BOS] In this section we group related work into two major categories: (i) approaches for the identification of discriminative attributes (IDA) and (ii) definition-based word vector space models.\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "196197006_3", "paragraph": "[BOS] Some previous work attempts to leverage structured knowledge from KBs to deal with the tasks of MRC and QA.\n[BOS] Weissenborn et al. (2017) , Bauer et al. (2018) , Mihaylov and Frank (2018) , Pan et al. (2019) , , follow a retrieve-then-encode paradigm, i.e., they first retrieve relevant knowledge from KBs, and only the retrieved knowledge relevant locally to the reading text will be encoded and integrated.\n[BOS] By contrast, we leverage pre-trained KB embeddings which encode whole KBs.\n[BOS] Then we use attention mechanisms to select and integrate knowledge that is relevant locally to the reading text.\n[BOS] Zhong et al. (2018) try to leverage pre-trained KB embeddings to solve the multi-choice MRC task.\n[BOS] However, the knowledge and text modules are not integrated,but used independently to predict the answer.\n[BOS] And the model cannot be applied to extractive MRC.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Reflection", "Reflection", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 24, "token_end": 102, "char_start": 120, "char_end": 416, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Weissenborn et al. (2017)": null, "Bauer et al. (2018)": "52290656", "Mihaylov and Frank (2018)": "29151507", "Pan et al. (2019)": "59600051"}, "Reference": {}}}, {"token_start": 140, "token_end": 196, "char_start": 623, "char_end": 888, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhong et al. (2018)": "52188664"}, "Reference": {}}}]}
{"id": "196197006_2", "paragraph": "[BOS] Incorporating KBs Several MRC datasets that require external knowledge have been proposed, such as ReCoRD (Zhang et al., 2018) , ARC , MCScript (Ostermann et al., 2018) , OpenBookQA and CommonsenseQA (Talmor et al., 2018) .\n[BOS] ReCoRD can be viewed as an extractive MRC dataset, while the later four are multi-choice MRC datasets, with relatively smaller size than ReCoRD.\n[BOS] In this paper, we focus on the extractive MRC task.\n[BOS] Hence, we choose ReCoRD and SQuAD in the experiments.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 18, "token_end": 27, "char_start": 105, "char_end": 132, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhang et al., 2018)": "53116244"}}}, {"token_start": 28, "token_end": 42, "char_start": 135, "char_end": 174, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ostermann et al., 2018)": "44156126"}}}, {"token_start": 43, "token_end": 59, "char_start": 177, "char_end": 227, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Talmor et al., 2018)": null}}}]}
{"id": "196197006_1", "paragraph": "[BOS] More recently, LMs such as ELMo (Peters et al., 2018b) , GPT (Radford et al., 2018) , and BERT (Devlin et al., 2018) have been devised.\n[BOS] They pre-train deep LMs on large-scale unlabeled corpora to obtain contextual representations of text.\n[BOS] When used in downstream tasks including MRC, the pre-trained contextual representations greatly improve the performance in either a fine-tuning or feature-based way.\n[BOS] Built upon pre-trained LMs, our work further explores the potential of incorporating structured knowledge from KBs, combining the strengths of both text and knowledge representations.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 8, "token_end": 19, "char_start": 33, "char_end": 60, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Peters et al., 2018b)": "52098907"}}}, {"token_start": 20, "token_end": 31, "char_start": 63, "char_end": 89, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Radford et al., 2018)": "49313245"}}}, {"token_start": 33, "token_end": 43, "char_start": 96, "char_end": 122, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Devlin et al., 2018)": null}}}]}
{"id": "196197006_0", "paragraph": "[BOS] Machine Reading Comprehension In the last few years, a number of datasets have been created for MRC, e.g., CNN/DM (Hermann et al., 2015) , SQuAD (Rajpurkar et al., 2016 (Rajpurkar et al., , 2018 , SearchQA (Dunn et al., 2017) , TriviaQA (Joshi et al., 2017) , and MS-MARCO (Nguyen et al., 2016) .\n[BOS] These datasets have led to advances like Match-LSTM (Wang and Jiang, 2017) , BiDAF (Seo et al., 2017) , AoA Reader (Cui et al., 2017) , DCN (Xiong et al., 2017) , R-Net , and QANet (Yu et al., 2018) .\n[BOS] These end-to-end neural models have similar architectures, starting off with an encoding layer to encode every question/passage word as a vector, passing through various attention-based interaction layers and finally a prediction layer.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 26, "token_end": 38, "char_start": 113, "char_end": 142, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hermann et al., 2015)": "6203757"}}}, {"token_start": 39, "token_end": 60, "char_start": 145, "char_end": 200, "span_type": "Reference", "span_citation_mapping": {"Dominant": {"(Rajpurkar et al., 2016": "11816014"}, "Reference": {"(Rajpurkar et al., , 2018": "47018994"}}}, {"token_start": 61, "token_end": 71, "char_start": 203, "char_end": 231, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dunn et al., 2017)": null}}}, {"token_start": 72, "token_end": 84, "char_start": 234, "char_end": 263, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Joshi et al., 2017)": "26501419"}}}, {"token_start": 86, "token_end": 98, "char_start": 270, "char_end": 300, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Nguyen et al., 2016)": "1289517"}}}, {"token_start": 107, "token_end": 118, "char_start": 350, "char_end": 383, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wang and Jiang, 2017)": "5592690"}}}, {"token_start": 119, "token_end": 130, "char_start": 386, "char_end": 410, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Seo et al., 2017)": "8535316"}}}, {"token_start": 131, "token_end": 142, "char_start": 413, "char_end": 442, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cui et al., 2017)": null}}}, {"token_start": 143, "token_end": 154, "char_start": 445, "char_end": 469, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Xiong et al., 2017)": "3714278"}}}, {"token_start": 160, "token_end": 170, "char_start": 484, "char_end": 507, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yu et al., 2018)": "4842909"}}}]}
{"id": "201634818_1", "paragraph": "[BOS] There are two common approaches for using multiple GPUs in training.\n[BOS] One is data parallelism, involving sending different data to different GPUs with the replicas of the same model.\n[BOS] The other is model parallelism, involving sending the same data to different GPUs having different parts of the model.\n\n", "discourse_tags": ["Transition", "Transition", "Transition"], "span_citation_mapping": []}
{"id": "201634818_0", "paragraph": "[BOS] The accuracy of NN models improves as the model sizes and data increases.\n[BOS] Thus, it is necessary to use multiple GPUs when training NN models within a short turnaround time.\n\n", "discourse_tags": ["Transition", "Transition"], "span_citation_mapping": []}
{"id": "207977046_4", "paragraph": "[BOS] important to note that our framework is encoderagnostic; namely, any answer encoder that produces a fixed-length feature vector can be used as the base component.\n\n", "discourse_tags": ["Other"], "span_citation_mapping": []}
{"id": "207977046_3", "paragraph": "[BOS] where w is a parameter vector,  is a promptspecific scaling constant, and b is a bias term.\n[BOS] Note that the model does not require explicit annotation of key elements on the training answer samples because the model implicitly estimates which key elements are included in each student answer in the course of training.\n[BOS] It is also   Embeddings Bi-LSTM Pooling Figure 3 : The base component.\n\n", "discourse_tags": ["Other", "Other", "Other"], "span_citation_mapping": []}
{"id": "207977046_2", "paragraph": "[BOS] The rubric component first encodes each key element that consists of m words, k = (w 1 , w 2 ,    , w m ), into its feature vector k and the answer a into a.\n[BOS] Then, it computes the relevance between the given answer a and each key element k  {k 1 , k 2 ,    , k K } using a word-level attention mechanism, and generates attentional feature vectors f r 1 ,    , f r K , which represent the aggregated information of each key element.\n[BOS] A rubric feature f r is generated based on the obtained K attentional feature vectors.\n[BOS] Finally, f a and f r are merged into one vector f , which is used for scoring:\n\n", "discourse_tags": ["Other", "Other", "Other", "Other"], "span_citation_mapping": []}
{"id": "207977046_1", "paragraph": "[BOS] Another issue in SAG is on low-resource settings.\n[BOS] investigate the importance of the training data size on nonneural SAG models with discrete features.\n[BOS] Horbach and Palmer (2016) show that active learning is effective for increasing useful training instances.\n[BOS] This is orthogonal to our approach: combining active learning with our rubric-aware SAG model is an interesting future direction.\n[BOS] We assume the base component encodes an answer into a feature vector f a .\n[BOS] We also assume that a given rubric stipulates a set of key elements in natural language.\n[BOS] We build a rubric component to encode rubric information, based on the relevance between the answer a and each key element k  {k 1 , k 2 ,    , k K } provided in the rubric.\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ", "Reflection", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 33, "token_end": 52, "char_start": 169, "char_end": 275, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Horbach and Palmer (2016)": "15841279"}, "Reference": {}}}]}
{"id": "207977046_0", "paragraph": "[BOS] A lot of existing SAG studies have a main interest in exploring better representations of answers and similarity measures between student answers and reference answers.\n[BOS] A wide variety of methods have been explored so far, ranging from Latent Semantic Analysis (LSA) (Mohler et al., 2011) , edit distance-based similarity, and knowledgebased similarity using WordNet (Pedersen et al., 2004 ) (Magooda et al., 2016 to word embeddingbased similarity (Sultan et al., 2016) .\n[BOS] Recently, Riordan et al. (2017) report that neural networkbased feature representation learning (Taghipour and Ng, 2016) is effective for SAG.\n[BOS] In contrast to the popularity of learning answer representations, the use of rubric information for SAG has been gained little attention so far.\n[BOS] In Sakaguchi et al. (2015) , the authors compute similarities, such as BLEU (Papineni et al., 2002) , between an answer and each key element in a rubric, and use them as features in a support vector regression (SVR) model.\n[BOS] Ramachandran et al. (2015) .\n[BOS] Ramachandran et al. (2015) generates text patterns from top answers and rubrics, and reports the automatically generated pattern performances better than manually generated regex pattern.\n[BOS] Nevertheless, it still remains an open issue (i) whether a rubric is effective or not even in the context of a neural representation learning paradigm (Riordan et al., 2017) , and (ii) what kinds of neural architectures should be employed for the efficient use of rubrics.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Single_summ", "Transition", "Single_summ", "Narrative_cite", "Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 42, "token_end": 58, "char_start": 247, "char_end": 299, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mohler et al., 2011)": "10477450"}}}, {"token_start": 59, "token_end": 90, "char_start": 302, "char_end": 424, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Pedersen et al., 2004": "1499545", ") (Magooda et al., 2016": "10363245"}}}, {"token_start": 91, "token_end": 104, "char_start": 428, "char_end": 480, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sultan et al., 2016)": "5975447"}}}, {"token_start": 108, "token_end": 139, "char_start": 499, "char_end": 631, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Riordan et al. (2017)": "12229972"}, "Reference": {}}}, {"token_start": 168, "token_end": 224, "char_start": 792, "char_end": 1011, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Sakaguchi et al. (2015)": "2079149"}, "Reference": {"(Papineni et al., 2002)": "11080756"}}}, {"token_start": 225, "token_end": 235, "char_start": 1018, "char_end": 1044, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Ramachandran et al. (2015)": "17049231"}}}, {"token_start": 237, "token_end": 273, "char_start": 1053, "char_end": 1240, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ramachandran et al. (2015)": "17049231"}, "Reference": {}}}, {"token_start": 299, "token_end": 313, "char_start": 1358, "char_end": 1420, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Riordan et al., 2017)": "12229972"}}}]}
{"id": "208051720_6", "paragraph": "[BOS] 3 System Description\n\n", "discourse_tags": ["Other"], "span_citation_mapping": []}
{"id": "208051720_5", "paragraph": "[BOS] The first specialized parser for UCCA was presented by Hershcovich et al. (2017) .\n[BOS] It utilized novel transition set and features based on bidirectional LSTMs and was developed to deal with specific features of UCCA graphs, such as DAG structure of the graph, discontinuous structures, and non-terminal nodes corresponding to complex semantic units.\n[BOS] The work saw further development in (Hershcovich et al., 2018) , where authors presented a generalized solution for transition-based parsing of DAGs and explored multitask learning across several representations, showing that using other formalisms in joint learning significantly improved UCCA parsing.\n[BOS] Buys and Blunsom (2017) proposed a neural encoder-decoder transition-based parser for full MRS-based semantic graphs.\n[BOS] The decoder is extended with stack-based embedding features which allows the graphs to be predicted jointly with unlexicalized predicates and their token alignments.\n[BOS] The parser was evaluated on DMRS, EDS and AMR graphs.\n[BOS] Lexicon extraction partially relies on Propbank (Palmer et al., 2005) , which is not in the shared task whitelist.\n[BOS] Unfortunately, we were not able to replace it with an analogous white-listed resource, therefore we did not use it.\n[BOS] Flanigan et al. (2014) presented the first approach to AMR parsing, which is based around the idea of identifying concepts and relations in source sentences utilizing a novel training algorithm and additional linguistic knowledge.\n[BOS] The parser was further improved for the SemEval 2016 Shared Task 8 (Flanigan et al., 2016) .\n[BOS] JAMR parser utilizes a rule-based aligner to match word spans in a sentence to concepts they evoke, which is applied in a pipeline before training the parser.\n[BOS] Damonte et al. (2017) proposed a transitionbased parser for AMR not dissimilar to the ARC-EAGER transition system for dependency tree parsing, which parses sentences left-to-right in real time.\n[BOS] Lyu and Titov (2018) presented an AMR parser that jointly learns to align and parse treating alignments as latent variables in a joint probabilistic model.\n[BOS] The authors argue that simultaneous learning of alignment and parses benefits the parsing in the sense that alignment is directly informed by the parsing objective thus producing overall better alignments.\n[BOS] Zhang et al. (2019a) and (Zhang et al., 2019b ) recently reported results that outperform all previously reported SMATCH scores, on both AMR 2.0 and AMR 1.0.\n[BOS] The proposed attention-based model is aligner-free and deals with AMR parsing as sequence-to-graph task.\n[BOS] Additionally, the authors proposed an alternative view on reentrancy converting an AMR graph into a tree by duplicating nodes that have reentrant relations and then adding an extra layer of annotation by assigning an index to each node so that the duplicates of the same node would have the same id and could be merged to recover the original AMR graph.\n[BOS] This series of papers looks very promising, but unfortunately we were not able to test the parser due to them being published after the end of the shared task.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Narrative_cite", "Reflection", "Single_summ", "Narrative_cite", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Multi_summ", "Multi_summ", "Multi_summ", "Transition"], "span_citation_mapping": [{"token_start": 2, "token_end": 72, "char_start": 6, "char_end": 360, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hershcovich et al. (2017)": "8233374"}, "Reference": {}}}, {"token_start": 73, "token_end": 129, "char_start": 367, "char_end": 670, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Hershcovich et al., 2018)": "19488885"}, "Reference": {}}}, {"token_start": 130, "token_end": 202, "char_start": 677, "char_end": 1026, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Buys and Blunsom (2017)": "14852353"}, "Reference": {}}}, {"token_start": 208, "token_end": 218, "char_start": 1072, "char_end": 1102, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Palmer et al., 2005)": "2486369"}}}, {"token_start": 255, "token_end": 297, "char_start": 1276, "char_end": 1506, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Flanigan et al. (2014)": "5000956"}, "Reference": {}}}, {"token_start": 305, "token_end": 322, "char_start": 1553, "char_end": 1603, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Flanigan et al., 2016)": "6453515"}}}, {"token_start": 359, "token_end": 404, "char_start": 1777, "char_end": 1970, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Damonte et al. (2017)": null}, "Reference": {}}}, {"token_start": 405, "token_end": 469, "char_start": 1977, "char_end": 2344, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Lyu and Titov (2018)": "46889674"}, "Reference": {}}}, {"token_start": 470, "token_end": 617, "char_start": 2351, "char_end": 2979, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhang et al. (2019a)": null, "(Zhang et al., 2019b": "202233360"}, "Reference": {}}}]}
{"id": "208051720_4", "paragraph": "[BOS]  using only the resources from the shared task whitelist.\n[BOS] Peng et al. (2017) presented a neural parser that was designed to work with three semantic dependency graph frameworks, namely, DM, PAS and PSD.\n[BOS] The authors proposed a single-task and two multitask learning approaches and extended their work with a new approach (Peng et al., 2018) to learning semantic parsers from multiple datasets.\n\n", "discourse_tags": ["Transition", "Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 14, "token_end": 45, "char_start": 70, "char_end": 214, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Peng et al. (2017)": "15939234"}, "Reference": {}}}, {"token_start": 50, "token_end": 75, "char_start": 244, "char_end": 357, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Peng et al., 2018)": "4956705"}}}]}
{"id": "208051720_3", "paragraph": "[BOS]  with instructions sufficient to run the code;\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "208051720_2", "paragraph": "[BOS]  accompanied by open-source code available to use;\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "208051720_1", "paragraph": "[BOS]  reporting reasonably good results;\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "208051720_0", "paragraph": "[BOS] For the purposes of this work we considered previous work matching the following criteria:\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "196173424_0", "paragraph": "[BOS] We present related work in this section, divided into three sub-topics.\n[BOS] Jia and Liang (2017) showed that QA models can be confused by appending a distracting sentence to the end of a passage.\n[BOS] While this highlighted an important weakness of trained models, the adversarial examples created are unnatural and not expected to be present in naturally occurring passages.\n[BOS] In contrast, semantic preserving changes to an input question that lead to returning the wrong answers present more relevant failure cases that occur in practice.\n[BOS] Some previous work used question paraphrasing to create more natural adversarial examples.\n[BOS] Ribeiro et al. (2018) made use of back translation to obtain paraphrasing rules that were subsequently filtered by human annotators.\n[BOS] Examples of rules obtained include \"What VERB  So what VERB\" and \"What NOUN  Which NOUN\".\n[BOS] Rychalska et al. (2018) replaced the most important question word identified using the LIME framework with a synonym from WordNet and ELMo embeddings, which was verified by human annotators.\n[BOS] These replacements are expected to maintain the meaning of the questions but can sometimes change initially correct answers.\n\n", "discourse_tags": ["Reflection", "Single_summ", "Single_summ", "Transition", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 18, "token_end": 75, "char_start": 84, "char_end": 384, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Jia and Liang (2017)": "7228830"}, "Reference": {}}}, {"token_start": 121, "token_end": 171, "char_start": 657, "char_end": 885, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ribeiro et al. (2018)": "21740766"}, "Reference": {}}}, {"token_start": 172, "token_end": 235, "char_start": 892, "char_end": 1213, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Rychalska et al. (2018)": "53570638"}, "Reference": {}}}]}
{"id": "174800577_1", "paragraph": "[BOS] A number of automatic evaluation metrics have shown high correlation with human judges (Liu and Liu, 2008; Graham, 2015) , but these results are either restricted to extractive systems or were performed with respect to human-generated summaries.\n[BOS] Correlation values are significantly reduced when performed on abstractive summarization systems and datasets (Toutanova et al., 2016) .\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 13, "token_end": 26, "char_start": 80, "char_end": 126, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Liu and Liu, 2008;": "17994781", "Graham, 2015)": "8651371"}}}, {"token_start": 57, "token_end": 75, "char_start": 321, "char_end": 392, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Toutanova et al., 2016)": "8241566"}}}]}
{"id": "174800577_0", "paragraph": "[BOS] Most work in neural abstractive summarization has focused on optimizing ROUGE, whether implicitly by maximum likelihood training or explicitly by reinforcement learning.\n[BOS] While this could certainly capture aspects of the content selection problem, we believe that the focus should now shift towards semantic correctness and readability.\n[BOS] Cao et al. (2018) took a step in this direction through their fact-aware neural abstractive summarization system.\n[BOS] They use fact descriptions of the source as additional features for the summarizer, and showed improved faithfulness according to human judgments.\n[BOS] Multi-task learning is another approach used by Pasunuru et al. (2017) to reduce semantic errors in the generated summaries.\n[BOS] They jointly learn summarization and entailment generation tasks, using different encoders but a shared decoder.\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 58, "token_end": 109, "char_start": 354, "char_end": 620, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Cao et al. (2018)": "19198109"}, "Reference": {}}}, {"token_start": 110, "token_end": 159, "char_start": 627, "char_end": 870, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Pasunuru et al. (2017)": "11918447"}, "Reference": {}}}]}
{"id": "184482605_0", "paragraph": "[BOS] Leveraging parallels between UCCA and known approaches for syntactic parsing, Hershcovich et al. (2017) proposed TUPA, a customized transition-based parser with dense feature representation.\n[BOS] Based on this model, Hershcovich et al. (2018a) used multitask learning effectively by training a UCCA model along with similar parsing tasks where more training data is available, such as Abstract Meaning Representation (AMR) (Banarescu et al., 2013) and Universal Dependencies (UD) (Nivre et al., 2016) .\n[BOS] Due to the requirements of reentrancy, discontinuity, and non-terminals, other powerful parsers were shown to be less suitable for parsing with UCCA (Hershcovich et al., 2017) .\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 16, "token_end": 41, "char_start": 84, "char_end": 196, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hershcovich et al. (2017)": "8233374"}, "Reference": {}}}, {"token_start": 47, "token_end": 115, "char_start": 224, "char_end": 507, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hershcovich et al. (2018a)": "19488885"}, "Reference": {"(Banarescu et al., 2013)": "7771402", "(Nivre et al., 2016)": "17954486"}}}, {"token_start": 145, "token_end": 160, "char_start": 647, "char_end": 691, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hershcovich et al., 2017)": "8233374"}}}]}
{"id": "195218547_2", "paragraph": "[BOS] Another group of work related to this paper is data augmentation in machine translation.\n[BOS] Although data augmentation is very popular in general learning tasks, such as image processing, it is non-trivial to do so in machine translation because even slight modifications of sentences can make huge difference in semantics.\n[BOS] To our best knowledge, there are two categories of successful data augmentation approaches for machine translation.\n[BOS] The first one is based on backtranslation ( (Sennrich et al., 2015a) ) which augments monolingual data into training set.\n[BOS] The second one is based on word replacement, such as (Sennrich et al., 2016) and .\n[BOS] Zheng et al. (2018b) make the use of multiple references and generates even more pseudoreferences and achieve improvement in both machine translation and image captioning.\n\n", "discourse_tags": ["Reflection", "Transition", "Transition", "Narrative_cite", "Narrative_cite", "Single_summ"], "span_citation_mapping": [{"token_start": 84, "token_end": 110, "char_start": 487, "char_end": 582, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sennrich et al., 2015a)": "15600925"}}}, {"token_start": 117, "token_end": 132, "char_start": 616, "char_end": 665, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sennrich et al., 2016)": "14919987"}}}, {"token_start": 135, "token_end": 169, "char_start": 678, "char_end": 849, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zheng et al. (2018b)": "52124999"}, "Reference": {}}}]}
{"id": "195218547_1", "paragraph": "[BOS] A similar method was proposed in (Xie et al., 2018) in the context of grammar correction, where a model is trained to add noises on original sentences to produce noisy sentences.\n[BOS] However, instead of learn how to generate arbitrary \"noises\", our goal is to learn \"social-media-style\" translations.\n[BOS] Singh et al. (2019) injects artificial noise in the clean data according to the distribution of noisy data.\n[BOS] Liu et al. (2019a) propose to leverage phonetic information to reduce the noises in data.\n\n", "discourse_tags": ["Single_summ", "Reflection", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 39, "char_start": 6, "char_end": 184, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Xie et al., 2018)": "21730715"}, "Reference": {}}}, {"token_start": 68, "token_end": 92, "char_start": 315, "char_end": 422, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Singh et al. (2019)": "67856759"}, "Reference": {}}}, {"token_start": 93, "token_end": 114, "char_start": 429, "char_end": 518, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Liu et al. (2019a)": "53113716"}, "Reference": {}}}]}
{"id": "195218547_0", "paragraph": "[BOS] The method proposed in this paper is a kind of domain adaptation technique.\n[BOS] There are many previous work on domain adaptation for machine translation (Britz et al., 2017; Wang et al., 2017; Chu et al., 2017; Chu and Wang, 2018) , which leverages out-of-domain parallel corpora and indomain monolingual corpora to improve translation.\n[BOS] The difference between our method and previous work lies in that we use back-translation (Sennrich et al., 2015a) for domain adaptation.\n[BOS] Different from some previous work using adversarial training tion (Zheng et al., 2018a) to differentiate multiple tasks, we simply assign different starting symbol for multiple tasks (Lample et al., 2018) .\n\n", "discourse_tags": ["Reflection", "Narrative_cite", "Reflection", "Narrative_cite"], "span_citation_mapping": [{"token_start": 23, "token_end": 57, "char_start": 120, "char_end": 239, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Britz et al., 2017;": "30042437", "Wang et al., 2017;": "1054586", "Chu et al., 2017;": "15201884", "Chu and Wang, 2018)": "44157913"}}}, {"token_start": 93, "token_end": 107, "char_start": 424, "char_end": 465, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sennrich et al., 2015a)": "15600925"}}}, {"token_start": 118, "token_end": 131, "char_start": 535, "char_end": 582, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zheng et al., 2018a)": "5040582"}}}, {"token_start": 141, "token_end": 154, "char_start": 652, "char_end": 699, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lample et al., 2018)": "5033497"}}}]}
{"id": "202540640_2", "paragraph": "[BOS] Recent studies also extend the Transformer to encode structural information for other NLP applications.\n[BOS] Shaw et al. (2018) propose relationaware self-attention to capture relative positions of word pairs for neural machine translation.\n[BOS] Ge et al. (2019) extend the relation-aware selfattention to capture syntactic and semantic structures.\n[BOS] Our model is inspired by theirs but aims to encode structural label sequences of concept pairs.\n[BOS] Koncel-Kedziorski et al. (2019) propose graph Transformer to encode graph structure.\n[BOS] Similar to the GCN, it focuses on the relations between directly connected nodes.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Reflection", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 19, "token_end": 44, "char_start": 116, "char_end": 247, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Shaw et al. (2018)": "3725815"}, "Reference": {}}}, {"token_start": 45, "token_end": 68, "char_start": 254, "char_end": 356, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ge et al. (2019)": "199466381"}, "Reference": {}}}, {"token_start": 88, "token_end": 127, "char_start": 465, "char_end": 637, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Koncel-Kedziorski et al. (2019)": "102354588"}, "Reference": {}}}]}
{"id": "202540640_1", "paragraph": "[BOS] Moving to neural seq2seq approaches, Konstas et al. (2017) successfully apply seq2seq model together with large-scale unlabeled data for both text-to-AMR parsing and AMR-to-text generation.\n[BOS] With special interest in the target side syntax, Cao and Clark (2019) use seq2seq models to generate target syntactic structure, and then the surface form.\n[BOS] To prevent the information loss in linearizing AMR graphs into sequences, (Song et al., 2018; Beck et al., 2018) propose graphto-sequence models to encode graph structure directly.\n[BOS] Focusing on reentrancies, Damonte and Cohen (2019) propose stacking encoders which consist of BiLSTM (Graves et al., 2013) , TreeLSTMs (Tai et al., 2015) , and Graph Convolutional Network (GCN) (Duvenaud et al., 2015; Kipf and Welling, 2016) .\n[BOS] Guo et al. (2019) propose densely connected GCN to better capture both local and non-local features.\n[BOS] However, all the aforementioned graph-based models only consider the relations between nodes that are directly connected, thus lose the structural information between nodes that are indirectly connected via an edge path.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Multi_summ", "Single_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 10, "token_end": 50, "char_start": 43, "char_end": 195, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Konstas et al. (2017)": "8066499"}, "Reference": {}}}, {"token_start": 52, "token_end": 84, "char_start": 207, "char_end": 357, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Cao and Clark (2019)": "102483832"}, "Reference": {}}}, {"token_start": 86, "token_end": 126, "char_start": 367, "char_end": 544, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Song et al., 2018;": "25111673", "Beck et al., 2018)": "49430686"}, "Reference": {}}}, {"token_start": 129, "token_end": 202, "char_start": 563, "char_end": 792, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Damonte and Cohen (2019)": "85528601"}, "Reference": {"(Graves et al., 2013)": "206741496", "(Tai et al., 2015)": "3033526", "(Duvenaud et al., 2015;": "1690180", "Kipf and Welling, 2016)": null}}}, {"token_start": 204, "token_end": 228, "char_start": 801, "char_end": 901, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Guo et al. (2019)": "195064954"}, "Reference": {}}}]}
{"id": "202540640_0", "paragraph": "[BOS] Most studies in AMR-to-text generation regard it as a translation problem and are motivated by the recent advances in both statistical machine translation (SMT) and neural machine translation (NMT).\n[BOS] Flanigan et al. (2016) first transform an AMR graph into a tree, then specify a number of tree-to-string transduction rules based on alignments that are used to drive a tree-based SMT model (Graehl and Knight, 2004) .\n[BOS] Pourdamghani et al. (2016) develop a method that learns to linearize AMR graphs into AMR strings, and then feed them into a phrase-based SMT model (Koehn et al., 2003) .\n[BOS] Song et al. (2017) use synchronous node replacement grammar (SNRG) to generate text.\n[BOS] Different from synchronous context-free grammar in hierarchical phrase-based SMT (Chiang, 2007) , SNRG is a grammar over graphs.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 44, "token_end": 100, "char_start": 211, "char_end": 426, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Flanigan et al. (2016)": "9135033"}, "Reference": {"(Graehl and Knight, 2004)": "2369967"}}}, {"token_start": 102, "token_end": 150, "char_start": 435, "char_end": 602, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Pourdamghani et al. (2016)": "18600354"}, "Reference": {"(Koehn et al., 2003)": "8884845"}}}, {"token_start": 152, "token_end": 172, "char_start": 611, "char_end": 695, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Song et al. (2017)": "9573708"}, "Reference": {}}}, {"token_start": 176, "token_end": 193, "char_start": 729, "char_end": 797, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chiang, 2007)": "3505719"}}}]}
{"id": "196187162_1", "paragraph": "[BOS] Moreover, Guo et al. (2018) and proposed to use NLI-based loss functions or multi-task learning with NLI for document summarization.\n[BOS] But unfortunately, their experiments do not evaluate whether the techniques improve summarization correctness.\n[BOS] We are the first to use NLI in a reranking setup, which is beneficial for this study as it allows to us to clearly isolate the net impact of the NLI component.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 4, "token_end": 51, "char_start": 16, "char_end": 255, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Guo et al. (2018)": "44105751"}, "Reference": {}}}]}
{"id": "196187162_0", "paragraph": "[BOS] Previous work already proposed the use of explicit proposition structures (Cao et al., 2018) and multi-task learning with NLI Pasunuru et al., 2017) to successfully improve the correctness of abstractive sentence summaries.\n[BOS] In this work, we instead focus on the more challenging single-document summarization, where longer summaries allow for more errors.\n[BOS] Very recently, Fan et al. (2018) showed that with ideas similar to Cao et al. (2018) 's work, the correctness of document summaries can also be improved.\n\n", "discourse_tags": ["Narrative_cite", "Reflection", "Single_summ"], "span_citation_mapping": [{"token_start": 9, "token_end": 20, "char_start": 48, "char_end": 98, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cao et al., 2018)": "19198109"}}}, {"token_start": 21, "token_end": 38, "char_start": 103, "char_end": 154, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Pasunuru et al., 2017)": "11918447"}}}, {"token_start": 79, "token_end": 113, "char_start": 389, "char_end": 527, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Fan et al. (2018)": "53113851"}, "Reference": {"Cao et al. (2018)": "19198109"}}}]}
{"id": "201652627_2", "paragraph": "[BOS] Meta-learning algorithms have received lots of attention recently due to their effectiveness (Finn et al., 2017; Fan et al., 2018) .\n[BOS] However, the potential of applying meta-learning algorithms in NLU tasks have not been fully investigated yet.\n[BOS] Gu et al. (2018) have tried to apply first-order MAML in machine translation and Qian and Yu (2019) propose to address the domain adaptation problem in dialogue generation by using MAML.\n[BOS] To the best of our knowledge, the Reptile algorithm, which is simpler than MAML and potentially more useful, has been given less attention.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Multi_summ", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 32, "char_start": 6, "char_end": 136, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 56, "token_end": 75, "char_start": 262, "char_end": 338, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 76, "token_end": 99, "char_start": 343, "char_end": 448, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "201652627_1", "paragraph": "[BOS] Another line of research on learning representations focus on multi-task learning (Collobert et al., 2011; Liu et al., 2015) .\n[BOS] In particular, Liu et al. (2019b) propose to combine multi-task learning with language model pre-training and demonstrate the two methods are complementary to each other.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ"], "span_citation_mapping": [{"token_start": 11, "token_end": 31, "char_start": 68, "char_end": 130, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Collobert et al., 2011;": "351666"}}}, {"token_start": 33, "token_end": 69, "char_start": 139, "char_end": 309, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "201652627_0", "paragraph": "[BOS] There is a long history of learning general language representations.\n[BOS] Previous work on learning general language representations focus on learning word (Mikolov et al., 2013; Pennington et al., 2014) or sentence representations (Le and Mikolov, 2014; Kiros et al., 2015) that are helpful for downstream tasks.\n[BOS] Recently, there is a trend of learning contextualized word embeddings (Dai and Le, 2015; McCann et al., 2017; Peters et al., 2018; Howard and Ruder, 2018) .\n[BOS] One representative approach is the BERT model (Devlin et al., 2019) which learns contextualized word embeddings via bidirectional Transformer models.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite", "Single_summ"], "span_citation_mapping": [{"token_start": 23, "token_end": 43, "char_start": 150, "char_end": 211, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 44, "token_end": 63, "char_start": 215, "char_end": 282, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 79, "token_end": 112, "char_start": 367, "char_end": 482, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dai and Le, 2015;": "7138078"}}}, {"token_start": 114, "token_end": 142, "char_start": 491, "char_end": 640, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Devlin et al., 2019)": "52967399"}, "Reference": {}}}]}
{"id": "174801735_6", "paragraph": "[BOS] The previously proposed methods have shown the effectiveness of integrating prior word alignments into the attention mechanism (Mi et al., 2016; Cheng et al., 2016; , leading to more accurate and adequate translation results with the assistance of prior guidance.\n[BOS] We provide an alternative that integrates the prior alignments through the sharing of features, which can also leads to a reduction of model parameters.\n[BOS] Kuang et al. (2018) propose to shorten the path length between the related source and target embeddings to enhance the embedding layer.\n[BOS] We believe that the shared features can be seem as the zero distance between the paired word embeddings.\n[BOS] Our proposed method also uses several ideas from the three-way WT method (Press and Wolf, 2017) .\n[BOS] Both of these methods are easy to implement and transparent to different NMT architectures.\n[BOS] The main differences are: 1) we share a part of features instead of all features; 2) the words of different relationship categories are allowed to share with differently sized features; and (3) it is adaptable to any language pairs, making the WT methods more widely used.\n\n", "discourse_tags": ["Narrative_cite", "Reflection", "Single_summ", "Reflection", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 17, "token_end": 33, "char_start": 113, "char_end": 169, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mi et al., 2016;": "18193214"}}}, {"token_start": 78, "token_end": 105, "char_start": 435, "char_end": 570, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kuang et al. (2018)": "13728350"}, "Reference": {}}}, {"token_start": 135, "token_end": 147, "char_start": 741, "char_end": 783, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Press and Wolf, 2017)": "836219"}}}]}
{"id": "174801735_5", "paragraph": "[BOS] In our work, both the source and target embeddings can make use of the common representation unit, i.e. the source and target embedding help each other to learn a better representation.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "174801735_4", "paragraph": "[BOS] Recently, Gu et al. (2018) propose to use the pre-trained target (English) embeddings as a universal representation to improve the representation learning of the source (low-resource) languages.\n\n", "discourse_tags": ["Single_summ"], "span_citation_mapping": [{"token_start": 4, "token_end": 42, "char_start": 16, "char_end": 200, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Gu et al. (2018)": "3295641"}, "Reference": {}}}]}
{"id": "174801735_3", "paragraph": "[BOS] Many previous works focus on improving the word representations of NMT by capturing the fine-grained (character) or coarse-grained (sub-word) monolingual characteristics, such as character-based NMT (Costa-Juss and Fonollosa, 2016; Ling et al., 2015; Cho et al., 2014; Chen et al., 2016) , sub-word NMT (Sennrich et al., 2016b; Johnson et al., 2017; Ataman and Federico, 2018) , and hybrid NMT (Luong and Manning, 2016) .\n[BOS] They effectively consider and utilize the morphological information to enhance the word representations.\n[BOS] Our work aims to enhance word representations through the bilingual features that are cooperatively learned by the source and target words.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 40, "token_end": 78, "char_start": 185, "char_end": 293, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Costa-Juss\u00e0 and Fonollosa, 2016;": "1712853", "Ling et al., 2015;": null, "Cho et al., 2014;": "5590763", "Chen et al., 2016)": "6035643"}}}, {"token_start": 79, "token_end": 111, "char_start": 296, "char_end": 382, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sennrich et al., 2016b;": "1114678", "Johnson et al., 2017;": "6053988", "Ataman and Federico, 2018)": "25437538"}}}, {"token_start": 113, "token_end": 125, "char_start": 389, "char_end": 425, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Luong and Manning, 2016)": "13972671"}}}]}
{"id": "174801735_2", "paragraph": "[BOS] by the NMT systems .\n[BOS] Motivated by the frequency clustering methods proposed by Chen et al. (2016) where they cluster the words with similar frequency for training a hierarchical language model, in this work, we propose to use a small vector to model the possible features that might be shared between the source and target words which are unrelated but having similar word frequencies.\n[BOS] In addition, it can be regarded as a way to improve the robustness of learning the embeddings of low-frequency words because of the noisy dimensions .\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 9, "token_end": 103, "char_start": 33, "char_end": 552, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Chen et al. (2016)": "6035643"}, "Reference": {}}}]}
{"id": "174801735_1", "paragraph": "[BOS] The example of assembling the source word embedding matrix.\n[BOS] The words in parentheses denote the paired words sharing features with them.\n\n", "discourse_tags": ["Other", "Other"], "span_citation_mapping": []}
{"id": "174801735_0", "paragraph": "[BOS] We regard source and target words that cannot be paired with each other as unrelated words.\n[BOS] Figure 2(c) shows an example of a pair of unrelated words.\n[BOS] This category is mainly composed of lowfrequency words, such as misspelled words, special characters, and foreign words.\n[BOS] In standard NMT, the embeddings of low-frequency words are usually inadequately trained, resulting in a poor word representation.\n[BOS] These words are often treated as noises and they are generally ignored\n\n", "discourse_tags": ["Reflection", "Transition", "Transition", "Transition", "Transition"], "span_citation_mapping": []}
{"id": "208263110_1", "paragraph": "[BOS] It is worth noting that much work has introduced question reformulation models into machine comprehension tasks (Feldman and ElYaniv, 2019; Das et al., 2019) .\n[BOS] Many question reformulation models can integrate the conversational history explicitly by making coreference resolution and completion for the current question.\n[BOS] Rastogi et al. (Rastogi et al., 2019) prove that can get a better answer when inputting a reformulated question to the single-turn question answering models.\n[BOS] Nogueira et al. (Nogueira and Cho, 2017) introduce a query reformulation reinforcement learning system with relevant documents recall as a reward.\n[BOS] Buck et al. (Buck et al., 2017) propose an active question answering model with reinforcement learning, and learn to reformulate questions to elicit the best possible answers with an agent that sits between the user and a QA system.\n[BOS] However, the above work is still in the preliminary exploratory stage, and there is no work to reformulate questions with feedback from downstream tasks in conversational machine comprehension tasks.\n[BOS] How to train the reformulation models with feedback from subsequent functions is still a major challenge.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Single_summ", "Single_summ", "Single_summ", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 16, "token_end": 38, "char_start": 90, "char_end": 163, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Das et al., 2019)": "85449634"}}}, {"token_start": 65, "token_end": 105, "char_start": 339, "char_end": 496, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Rastogi et al., 2019)": "76663609"}, "Reference": {}}}, {"token_start": 106, "token_end": 137, "char_start": 503, "char_end": 649, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Nogueira and Cho, 2017)": null}, "Reference": {}}}, {"token_start": 138, "token_end": 186, "char_start": 656, "char_end": 888, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Buck et al., 2017)": "3700344"}, "Reference": {}}}]}
{"id": "208263110_0", "paragraph": "[BOS] Recently, several approaches have been proposed for conversational machine comprehension.\n[BOS] BiDAF++ w/ k-ctx integrates the conversation history by encoding turn number to the question embedding and previous N answer locations to the context embedding.\n[BOS] FlowQA provides a FLOW mechanism that encodes the intermediate representation of the previous questions to the context embedding when processing the current question.\n[BOS] SDnet (Zhu et al., 2018) prepends previous questions and answers to the current question and leverages the contextual embedding of BERT to obtain an understanding of conversation history.\n[BOS] The existing models always integrate the conversational history implicitly and can not understand the history effectively.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 75, "token_end": 111, "char_start": 442, "char_end": 629, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Zhu et al., 2018)": "54460803"}, "Reference": {}}}]}
{"id": "184483889_4", "paragraph": "[BOS] Other approaches are based on neural networks, like Zhang and Luo (2018) .\n[BOS] Their base convolutional neural network with a gap window (skipped CNN) had higher results than their SVM.\n\n", "discourse_tags": ["Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 7, "token_end": 40, "char_start": 36, "char_end": 193, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhang and Luo (2018)": "4737415"}, "Reference": {}}}]}
{"id": "184483889_3", "paragraph": "[BOS] Both the j48graft algorithm in Watanabe et al. (2018) , and the SVM and LSTM in Del Vigna et al. (2017) performed better on a binary classification rather than a multiclass classification.\n[BOS] Davidson et al. (2017) also tried different classifiers including Naive Bayes, decision trees, SVM and logistic regression.\n[BOS] Their logistic regression and SVM classifiers achieved the best results.\n[BOS] Waseem and Hovy (2016) tried different features for a logistic regression classifier, among which the character n-grams up to length of four in combination with the user's gender information performed the best.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 4, "token_end": 17, "char_start": 15, "char_end": 59, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Watanabe et al. (2018)": "4570064"}}}, {"token_start": 20, "token_end": 34, "char_start": 70, "char_end": 109, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Del Vigna et al. (2017)": "8293149"}}}, {"token_start": 48, "token_end": 83, "char_start": 201, "char_end": 403, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Davidson et al. (2017)": "1733167"}, "Reference": {}}}, {"token_start": 84, "token_end": 127, "char_start": 410, "char_end": 620, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Waseem and Hovy (2016)": "1721388"}, "Reference": {}}}]}
{"id": "184483889_2", "paragraph": "[BOS] Some papers divided the 'hate' class into two classes.\n[BOS] For example, Watanabe et al. (2018) and Davidson et al. (2017) used the classes 'offensive' and 'hate', and Del Vigna et al. (2017) classified comments as 'weak hate' and 'strong hate'.\n\n", "discourse_tags": ["Transition", "Multi_summ"], "span_citation_mapping": [{"token_start": 19, "token_end": 46, "char_start": 80, "char_end": 169, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Watanabe et al. (2018)": "4570064", "Davidson et al. (2017)": "1733167"}, "Reference": {}}}, {"token_start": 48, "token_end": 72, "char_start": 175, "char_end": 252, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Vigna et al. (2017)": "8293149"}, "Reference": {}}}]}
{"id": "184483889_1", "paragraph": "[BOS] Most papers tried one or multiple different classifiers, albeit with different features, but in general SVM classifiers usually achieve the best performance (Saleem et al., 2016; Davidson et al., 2017) .\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 19, "token_end": 42, "char_start": 110, "char_end": 207, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Saleem et al., 2016;": "39424091", "Davidson et al., 2017)": "1733167"}}}]}
{"id": "184483889_0", "paragraph": "[BOS] There has been done a lot of research regarding the automatic detection of hate speech on social media, in particular Twitter.\n[BOS] A great deal of different approaches to solve this task had been implemented in different works.\n[BOS] The majority of these studies was done on English texts.\n[BOS] It is clear that there is quite some overlap between each of these approaches.\n[BOS] However, direct comparison of previous approaches is not straightforward, as different datasets were used.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Transition", "Transition"], "span_citation_mapping": []}
{"id": "202541667_1", "paragraph": "[BOS] Discourse-Aware NMT Recent years, contextaware architecture has been well studied for NMT (Wang et al., 2017; Jean et al., 2017a; Tu et al., 2018) .\n[BOS] Wang et al. (2017) proposed hierarchical recurrent neural networks to summarize inter-sentential context from previous sentences and then integrate it into a standard NMT model with difference strategies.\n[BOS] Jean et al. (2017a) introduced an additional set of an encoder and attention to encode and select part of the previous source sentence for generating each target word.\n[BOS] Besides, Tu et al. (2018) proposed to augment NMT models with a cache-like memory network, which stores the translation history in terms of bilingual hidden representations at decoding steps of previous sentences.\n[BOS] They also evaluated the above three models on different domains of data, showing that the hierarchical encoder performs comparable with the multi-attention model.\n[BOS] More recently, some researchers began to investigate the effects of context-aware NMT on cross-lingual pronoun prediction (Jean et al., 2017b; Bawden et al., 2018; Voita et al., 2018) .\n[BOS] They mainly exploited general anaphora in non-pro-drop languages such as EnglishRussian.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 10, "token_end": 43, "char_start": 40, "char_end": 152, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wang et al., 2017;": "9768369", "Jean et al., 2017a;": null, "Tu et al., 2018)": "7421176"}}}, {"token_start": 45, "token_end": 81, "char_start": 161, "char_end": 365, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wang et al. (2017)": "9768369"}, "Reference": {}}}, {"token_start": 82, "token_end": 115, "char_start": 372, "char_end": 539, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Jean et al. (2017a)": null}, "Reference": {}}}, {"token_start": 118, "token_end": 185, "char_start": 555, "char_end": 928, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Tu et al. (2018)": "7421176"}, "Reference": {}}}, {"token_start": 197, "token_end": 235, "char_start": 1003, "char_end": 1118, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Jean et al., 2017b;": "34245575", "Bawden et al., 2018;": "5016370", "Voita et al., 2018)": "44062236"}}}]}
{"id": "202541667_0", "paragraph": "[BOS] ZP Prediction and Translation ZP resolution is a challenging task which needs lexical, syntactic, discourse knowledge.\n[BOS] Previous studies have been conducted to improves the performance of ZP resolution for different pro-drop languages (Kong and Zhou, 2010; Chen and Ng, 2013; Park et al., 2015; Yin et al., 2017) .\n[BOS] However, directly using results of external ZP resolution systems for translation task shows limited improvements (Chung and Gildea, 2010; Le Nagard and Koehn, 2010; Taira et al., 2012; Xiang et al., 2013) , since such external systems are trained on small-scale data that is non-homologous to MT.\n[BOS] To overcome the data-level gap, Wang et al. (2016) proposed an automatic approach of ZP annotation by utilizing an alignment matrix from a large parallel data.\n[BOS] By using the translation-oriented ZP corpus, they exploited different approaches to alleviate ZP problems for translation models (Wang et al., 2016 (Wang et al., , 2018a .\n[BOS] Note that Wang et al. (2018b) also explored to address the problem of error propagation by jointly predicting ZP words given ZP position information.\n[BOS] However, this method still relies an external model that predicting ZP positions at decoding time.\n[BOS] Instead, this work proposes a unified model without any additional ZP annotations in decoding, thus release reliance on external ZP prediction in practice.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 32, "token_end": 67, "char_start": 199, "char_end": 323, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kong and Zhou, 2010;": "18390364", "Chen and Ng, 2013;": "12269372", "Park et al., 2015;": "211893", "Yin et al., 2017)": "5222450"}}}, {"token_start": 73, "token_end": 120, "char_start": 356, "char_end": 537, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chung and Gildea, 2010;": "5573614", "Le Nagard and Koehn, 2010;": "8665514", "Taira et al., 2012;": "17703770", "Xiang et al., 2013)": "10632612"}}}, {"token_start": 141, "token_end": 212, "char_start": 636, "char_end": 973, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wang et al. (2016)": null}, "Reference": {"(Wang et al., 2016": null, "(Wang et al., , 2018a": "19220538"}}}, {"token_start": 214, "token_end": 287, "char_start": 985, "char_end": 1396, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wang et al. (2018b)": "53079875"}, "Reference": {}}}]}
{"id": "202743962_1", "paragraph": "[BOS] Unsupervised Transfer Learning There has been previous work aiming at solving an unsupervised learning task of a target domain with the help of knowledge learned from a source domain (Dai et al., 2008; Wang et al., 2008; Pan and Yang, 2010) .\n[BOS] There is no labeled data in both the source and the target domains during training.\n[BOS] Our transfer grammar induction setting can be seen as an instance of unsupervised transfer learning.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 14, "token_end": 52, "char_start": 87, "char_end": 246, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dai et al., 2008;": "6587929", "Wang et al., 2008;": "6388997", "Pan and Yang, 2010)": null}}}]}
{"id": "202743962_0", "paragraph": "[BOS] Our work is related to many previous work.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "102350797_2", "paragraph": "[BOS] A number of non-adversarial methods have also been proposed recently.\n[BOS] Artetxe et al. (2018b) learn an initial dictionary by exploiting the structural similarity of the embeddings and use a robust self-learning algorithm to improve it iteratively.\n[BOS] Hoshen and Wolf (2018) align the second moment of word distributions of the two languages using principal component analysis (PCA) and then refine the alignment iteratively using a variation of the Iterative Closest Point (ICP) method used in computer vision.\n[BOS] Alvarez-Melis and Jaakkola (2018) cast the problem as an optimal transport problem and exploit the Gromov-Wasserstein distance which measures how similarities between pairs of words relate across languages.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 17, "token_end": 52, "char_start": 82, "char_end": 258, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Artetxe et al. (2018b)": "21728524"}, "Reference": {}}}, {"token_start": 53, "token_end": 101, "char_start": 265, "char_end": 524, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hoshen and Wolf (2018)": "51978342"}, "Reference": {}}}, {"token_start": 102, "token_end": 145, "char_start": 531, "char_end": 737, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Alvarez-Melis and Jaakkola (2018)": "52156206"}, "Reference": {}}}]}
{"id": "102350797_1", "paragraph": "[BOS] A more recent line of research attempts to eliminate the seed dictionary totally and learn the map-ping in a purely unsupervised way.\n[BOS] This was first proposed by Miceli Barone (2016) , who initially used an adversarial network similar to Conneau et al. (2018) , and found that the mapper (which is also the encoder) translates everything to a single embedding, known commonly as the mode collapse issue (Goodfellow, 2017) .\n[BOS] To preserve diversity in mapping, he used a decoder to reconstruct the source embedding from the mapped embedding, extending the framework to an adversarial autoencoder.\n[BOS] His preliminary qualitative analysis shows encouraging results but not competitive with methods using bilingual seeds.\n[BOS] He suspected issues with training and with the isomorphic assumption.\n[BOS] In our work, we successfully address these issues with an improved model that also relaxes the isomorphic assumption.\n[BOS] Our model uses two separate autoencoders, one for each language, which allows us to put more constraints to guide the mapping.\n[BOS] We also distinguish the role of an encoder from the role of a mapper.\n[BOS] The encoder projects embeddings to latent code vectors, which are then translated by the mapper.\n[BOS] Zhang et al. (2017a) improved adversarial training with orthogonal parameterization and cycle consistency.\n[BOS] To aid training, they incorporate additional techniques like noise injection which works as a regularizer.\n[BOS] For selecting the best model, they rely on sharp drops of the discriminator accuracy.\n[BOS] In their follow-up work (Zhang et al., 2017b) , they minimize Earth-Mover's distance between the distribution of the transformed source embeddings and the distribution of the target embeddings.\n[BOS] Conneau et al. (2018) show impressive results with adversarial training and refinement with the Procrustes solution.\n[BOS] Instead of using the adversarial loss, Xu et al. (2018a) use Sinkhorn distance and adopt cycle consistency inspired by the CycleGAN (Zhu et al., 2017) .\n[BOS] We also incorporate cycle consistency along with the adversarial loss.\n[BOS] However, while all these methods learn the mapping in the original embedding space, our approach learns it in the latent code space considering both the mapper and the target encoder as adversary.\n[BOS] In addition, we use a postcycle reconstruction to guide the mapping.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Multi_summ", "Multi_summ", "Multi_summ", "Reflection", "Reflection", "Reflection", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Narrative_cite", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 8, "token_end": 156, "char_start": 37, "char_end": 811, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Barone (2016)": null, "(Goodfellow, 2017)": "5216145"}, "Reference": {"Conneau et al. (2018)": "3470398"}}}, {"token_start": 242, "token_end": 262, "char_start": 1254, "char_end": 1360, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhang et al. (2017a)": "26873455"}, "Reference": {}}}, {"token_start": 306, "token_end": 341, "char_start": 1596, "char_end": 1765, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Zhang et al., 2017b)": "32923127"}, "Reference": {}}}, {"token_start": 342, "token_end": 367, "char_start": 1772, "char_end": 1888, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Conneau et al. (2018)": "3470398"}, "Reference": {}}}, {"token_start": 376, "token_end": 388, "char_start": 1934, "char_end": 1973, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Xu et al. (2018a)": "52186890"}}}, {"token_start": 390, "token_end": 405, "char_start": 1984, "char_end": 2045, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhu et al., 2017)": "206770979"}}}]}
{"id": "102350797_0", "paragraph": "[BOS] In recent years a number of methods have been proposed to learn bilingual dictionary from monolingual word embeddings.\n[BOS] 1 Many of these methods use an initial seed dictionary.\n[BOS] Mikolov et al. (2013a) show that a linear transformation can be learned from a seed dictionary of 5000 pairs by minimizing the squared Euclidean distance.\n[BOS] In their view, the key reason behind the good performance of their model is the similarity of geometric arrangements in vector spaces of the embeddings of different languages.\n[BOS] For translating a new source word, they map the corresponding word embedding to the target space using the learned mapping and find the nearest target word.\n[BOS] In their approach, they found that simple linear mapping works better than non-linear mappings with multilayer neural networks.\n[BOS] Xing et al. (2015) enforce the word vectors to be of unit length during the learning of the embeddings and modify the objective function for learning the mapping to maximize the cosine similarity instead of using Euclidean distance.\n[BOS] To preserve length normalization after mapping, they enforce the orthogonality constraint on the mapper.\n[BOS] Instead of learning a mapping from the source to the target embedding space, Faruqui and Dyer (2014) use a technique based on Canonical Correlation Analysis (CCA) to project both source and target embeddings to a common low-dimensional space, where the correlation of the word pairs in the seed dictionary is maximized.\n[BOS] Artetxe et al. (2016) show that the above methods are variants of the same core optimization objective and propose a closed form solution for the mapper under orthogonality constraint.\n[BOS] Smith et al. (2017) find that this solution is closely related to the orthogonal Procrustes solution.\n[BOS] In their follow-up work, Artetxe et al. (2017) obtain competitive results using a seed dictionary of only 25 word pairs.\n[BOS] They propose a self-learning framework that performs two steps iteratively until convergence.\n[BOS] In the first step, they use the dictionary (starting with the seed) to learn a linear mapping, which is then used in the second step to induce a new dictionary.\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 35, "token_end": 150, "char_start": 193, "char_end": 826, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Mikolov et al. (2013a)": "1966640"}, "Reference": {}}}, {"token_start": 151, "token_end": 213, "char_start": 833, "char_end": 1176, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Xing et al. (2015)": "3144258"}, "Reference": {}}}, {"token_start": 228, "token_end": 278, "char_start": 1260, "char_end": 1502, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Faruqui and Dyer (2014)": "3792324"}, "Reference": {}}}, {"token_start": 279, "token_end": 316, "char_start": 1509, "char_end": 1693, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Artetxe et al. (2016)": "1040556"}, "Reference": {}}}, {"token_start": 317, "token_end": 339, "char_start": 1700, "char_end": 1801, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Smith et al. (2017)": "11591887"}, "Reference": {}}}, {"token_start": 347, "token_end": 421, "char_start": 1833, "char_end": 2195, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Artetxe et al. (2017)": "13335042"}, "Reference": {}}}]}
{"id": "182953258_1", "paragraph": "[BOS] Transfer learning Another line of related work is transfer learning, which has been the driver of recent successes in NLP.\n[BOS] Recently-proposed objectives for transfer learning include surrounding sentence prediction (Kiros et al., 2015) , paraphrasing (Wieting and Gimpel, 2017) , entailment (Conneau et al., 2017) , machine translation (McCann et al., 2017) , discourse (Jernite et al., 2017; Nie et al., 2017) , and language modeling (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018) .\n\n", "discourse_tags": ["Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 35, "token_end": 47, "char_start": 194, "char_end": 246, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kiros et al., 2015)": "9126867"}}}, {"token_start": 48, "token_end": 61, "char_start": 249, "char_end": 288, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wieting and Gimpel, 2017)": "10668422"}}}, {"token_start": 62, "token_end": 74, "char_start": 291, "char_end": 324, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Conneau et al., 2017)": "28971531"}}}, {"token_start": 75, "token_end": 86, "char_start": 327, "char_end": 368, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(McCann et al., 2017)": "9447219"}}}, {"token_start": 87, "token_end": 104, "char_start": 371, "char_end": 421, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Jernite et al., 2017;": "6694822", "Nie et al., 2017)": "19047694"}}}, {"token_start": 106, "token_end": 132, "char_start": 428, "char_end": 510, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Peters et al., 2018;": "3626819", "Radford et al., 2018;": "49313245", "Devlin et al., 2018)": "52967399"}}}]}
{"id": "182953258_0", "paragraph": "[BOS] Text embeddings and probe tasks A variety of methods exist for obtaining fixed-length dense vector representations of words (e.g., Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2018) , sentences (e.g., Kiros et al., 2015; Conneau et al., 2017; Subramanian et al., 2018; Cer et al., 2018) , and larger bodies of text (e.g., Le and Mikolov, 2014; Dai et al., 2015; Iyyer et al., 2015; Li et al., 2015; Chen, 2017; Zhang et al., 2017) To analyze word and sentence embeddings, recent work has studied classification tasks that probe them for various linguistic properties (Shi et al., 2016; Adi et al., 2017; Belinkov et al., 2017a,b; Conneau et al., 2018; Tenney et al., 2019) .\n[BOS] In this paper, we extend the notion of probe tasks to the paragraph level.\n\n", "discourse_tags": ["Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 13, "token_end": 52, "char_start": 69, "char_end": 204, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Mikolov et al., 2013;": "16447573", "Pennington et al., 2014;": "1957433", "Peters et al., 2018)": "3626819"}}}, {"token_start": 53, "token_end": 93, "char_start": 207, "char_end": 309, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Kiros et al., 2015;": "9126867", "Conneau et al., 2017;": "28971531", "Subramanian et al., 2018;": "4567927", "Cer et al., 2018)": null}}}, {"token_start": 95, "token_end": 146, "char_start": 316, "char_end": 453, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Le and Mikolov, 2014;": "2407601", "Dai et al., 2015;": "7364264", "Iyyer et al., 2015;": null, "Li et al., 2015;": "207468", "Chen, 2017;": "8328889", "Zhang et al., 2017)": "5258986"}}}, {"token_start": 157, "token_end": 212, "char_start": 519, "char_end": 695, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Shi et al., 2016;": "7197724", "Adi et al., 2017;": "6771196", "Conneau et al., 2018;": "24461982", "Tenney et al., 2019)": "108300988"}}}]}
{"id": "195584234_4", "paragraph": "[BOS] The saliency method we propose in this work draws its inspiration from visual saliency proposed by Simonyan et al. (2013); Springenberg et al. (2014) ; Smilkov et al. (2017) .\n[BOS] It should be noted that these methods were mostly applied to computer vision tasks.\n[BOS] To the best of our knowledge, Li et al. (2016) presented the only work that directly employs saliency methods to interpret NLP models.\n[BOS] Most similar to our work in spirit, Ding et al. (2017) used Layer-wise Relevance Propagation (LRP; Bach et al. 2015) , an interpretation method resembling saliency, to interpret the internal working mechanisms of RNN-based neural machine translation systems.\n[BOS] Although conceptually LRP is also a good fit for word alignment interpretation, we have some concerns with the mathematical soundness of LRP when applied to attention models.\n[BOS] Our proposed method is also considerably more flexible and easier to implement than LRP.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Single_summ", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 14, "token_end": 45, "char_start": 77, "char_end": 179, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Simonyan et al. (2013);": "1450294", "Springenberg et al. (2014)": "12998557", "Smilkov et al. (2017)": "11695878"}}}, {"token_start": 70, "token_end": 92, "char_start": 308, "char_end": 412, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Li et al. (2016)": "14099741"}, "Reference": {}}}, {"token_start": 101, "token_end": 147, "char_start": 455, "char_end": 677, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ding et al. (2017)": "27930067"}, "Reference": {"Bach et al. 2015)": "215192424"}}}]}
{"id": "195584234_3", "paragraph": "[BOS] Beyond interpretation, in order to improve the translation of rare words, Nguyen and Chiang (2018) introduced LexNet, a feed-forward neural network that directly predicts the target word from a weighted sum of the source embeddings, on top of an RNN-based Seq2Seq models.\n[BOS] Their goal was to improve translation output and hence they did not empirically show AER improvements on manually-aligned corpora.\n[BOS] There are also a few other studies that inject alignment supervision during NMT training (Mi et al., 2016; Liu et al., 2016) .\n[BOS] In terms of improvements in word alignment quality, Legrand et al. (2016) ; Wang et al. (2018) ; proposed neu-ral word alignment modules decoupled from NMT systems, while Zenkel et al. (2019) introduced a separate module to extract alignment from NMT decoder states, with which they achieved comparable AER with fast-align with Transformer models.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Narrative_cite", "Multi_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 83, "char_start": 6, "char_end": 414, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 95, "token_end": 114, "char_start": 490, "char_end": 545, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mi et al., 2016;": "18193214", "Liu et al., 2016)": "13292366"}}}, {"token_start": 117, "token_end": 154, "char_start": 557, "char_end": 717, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Legrand et al. (2016)": "8129116", "Wang et al. (2018)": "46939554"}, "Reference": {}}}, {"token_start": 156, "token_end": 192, "char_start": 725, "char_end": 901, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zenkel et al. (2019)": "59523729"}, "Reference": {}}}]}
{"id": "195584234_2", "paragraph": "[BOS] There is also a number of other studies that analyze the attention in Transformer models.\n[BOS] Tang et al. (2018a,b) conducted targeted evaluation of neural machine translation models in two different evaluation tasks, namely subject-verb agreement and word sense disambiguation.\n[BOS] During the analysis, they noted that the pattern in Transformer model (what they refer to as advanced attention mechanism) is very different from that of the attention in RNN-based architecture, in that a lot of the probability mass is focused on the last input token.\n[BOS] They did not dive deeper in this phenomenon in their analysis.\n[BOS] Raganato and Tiedemann (2018) performed a brief but more refined analysis on each attention head and each layer, where they noticed several different patterns inside the modules, and concluded that Transformer tends to focus on local dependencies in lower layers but finds long dependencies on higher ones.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 19, "token_end": 121, "char_start": 102, "char_end": 630, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 122, "token_end": 177, "char_start": 637, "char_end": 943, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Raganato and Tiedemann (2018)": "53596423"}, "Reference": {}}}]}
{"id": "195584234_1", "paragraph": "[BOS] For the attention in RNN-based sequence-tosequence model, the first comprehensive analysis is conducted by Ghader and Monz (2017) .\n[BOS] They argued that the attention in such systems agree with word alignment to a certain extent by showing that the RNN-based system achieves comparable alignment error rate comparable to that of bidirectional GIZA++ with symmetrization.\n[BOS] However, they also point out that they are not exactly the same, as training the attention with alignments would occasionally cause the model to forget important information.\n[BOS] Lee et al. (2017) presented a toolkit that facilitates study for the attention in RNN-based models.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 108, "char_start": 6, "char_end": 559, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ghader and Monz (2017)": "2389139"}, "Reference": {}}}, {"token_start": 109, "token_end": 132, "char_start": 566, "char_end": 665, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Lee et al. (2017)": "9549525"}, "Reference": {}}}]}
{"id": "195584234_0", "paragraph": "[BOS] We start with work that combines word alignments with NMT.\n[BOS] Research in this area generally falls into one of three themes: (1) employing the notion of word alignments to interpret the prediction of NMT; (2) making use of word alignments to improve NMT performance; (3) making use of NMT to improve word alignments.\n[BOS] We mainly focus on related work in the first theme as this is the problem we are addressing in this work.\n[BOS] Then we briefly introduce work in the other themes that is relevant to our study.\n[BOS] We conclude by briefly summarizing related work to our proposed interpretation method.\n\n", "discourse_tags": ["Reflection", "Transition", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "173990832_2", "paragraph": "[BOS] Extracting summary-worthy sentences from the source documents is important even if the ultimate goal is to generate abstracts.\n[BOS] Recent abstractive studies recognize the importance of separating \"salience estimation\" from \"text generation\" so as to reduce the amount of training data required by encoder-decoder models (Gehrmann et al., 2018; Lebanoff et al., 2018 Lebanoff et al., , 2019 ).\n[BOS] An extractive method is often leveraged to identify salient source sentences, then a neural text generator rewrites the selected sentences into an abstract.\n[BOS] Our pursuit of the DPP method is especially meaningful in this context.\n[BOS] As described in the next section, DPP has an extraordinary ability to distinguish redundant descriptions, thereby avoiding passing redundant content to the abstractor that can cause an encoderdecoder model to fail.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 30, "token_end": 85, "char_start": 180, "char_end": 398, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Gehrmann et al., 2018;": "52144157", "Lebanoff et al., 2018": "52053741", "Lebanoff et al., , 2019": "173990628"}}}]}
{"id": "173990832_1", "paragraph": "[BOS] Recent years have also seen considerable interest in neural approaches to summarization.\n[BOS] In particular, neural extractive approaches focus on learning vector representations of source sentences; then based on these representations they determine if a source sentence is to be included in the summary (Cheng and Lapata, 2016; Yasunaga et al., 2017; Nallapati et al., 2017; Narayan et al., 2018) .\n[BOS] Neural abstractive approaches usually include an encoder used to convert the entire source document to a continuous vector, and a decoder for generating an abstract word by word conditioned on the document vector (Paulus et al., 2017; Tan et al., 2017; Guo et al., 2018; Kedzie et al., 2018) .\n[BOS] These neural models, however, require large training data containing hundreds of thousands to millions of examples, which are still unavailable for the multi-document summarization task.\n[BOS] To date, most neural summarization studies are performed for single document summarization.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Multi_summ", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 20, "token_end": 85, "char_start": 116, "char_end": 405, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cheng and Lapata, 2016;": "1499080", "Yasunaga et al., 2017;": "6532096", "Nallapati et al., 2017;": "6405271", "Narayan et al., 2018)": "3510042"}}}, {"token_start": 87, "token_end": 154, "char_start": 414, "char_end": 705, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Paulus et al., 2017;": "21850704", "Tan et al., 2017;": "26698484", "Guo et al., 2018;": "44105751", "Kedzie et al., 2018)": "53083054"}}}]}
{"id": "173990832_0", "paragraph": "[BOS] Extractive summarization approaches are the most popular in real-world applications (Carbonell and Goldstein, 1998; Daum III and Marcu, 2006; Galanis and Androutsopoulos, 2010; Hong et al., 2014; Yogatama et al., 2015) .\n[BOS] These approaches focus on identifying representative sentences from a single document or set of documents to form a summary.\n[BOS] The summary sentences can be optionally compressed to remove unimportant constituents such as prepositional phrases to yield a succinct summary (Knight and Marcu, 2002; Zajic et al., 2007; Martins and Smith, 2009; BergKirkpatrick et al., 2011; Thadani and McKeown, 2013; Wang et al., 2013; Li et al., 2013 Li et al., , 2014 Filippova et al., 2015; Durrett et al., 2016) .\n[BOS] Extractive summarization methods are mostly unsupervised or lightly-supervised using thousands of training examples.\n[BOS] Given its practical importance, we explore an extractive method in this work for multi-document summarization.\n[BOS] It is not uncommon to cast summarization as a discrete optimization problem (Gillick and Favre, 2009; Takamura and Okumura, 2009; Lin and Bilmes, 2010; Hirao et al., 2013) .\n[BOS] In this formulation, a set of binary variables are used to indicate whether their corresponding source sentences are to be included in the summary.\n[BOS] The summary sentences are selected to maximize the coverage of important source content, while minimizing the summary redundancy and subject to a length constraint.\n[BOS] The optimization can be performed using an off-the-shelf tool such as Gurobi, IBM CPLEX, or via a greedy approximation algorithm.\n[BOS] Notable optimization frameworks include integer linear programming (Gillick and Favre, 2009 ), determinantal point processes (Kulesza and Taskar, 2012) , submodular functions (Lin and Bilmes, 2010) , and minimum dominating set (Shen and Li, 2010) .\n[BOS] In this paper we employ the DPP framework because of its remarkable performance on various summarization problems (Zhang et al., 2016) .\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Narrative_cite", "Transition", "Reflection", "Narrative_cite", "Transition", "Transition", "Reflection", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 12, "token_end": 59, "char_start": 66, "char_end": 224, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Carbonell and Goldstein, 1998;": "6334682", "Daum\u00e9 III and Marcu, 2006;": "6241932", "Hong et al., 2014;": null, "Yogatama et al., 2015)": "12194143"}}}, {"token_start": 87, "token_end": 190, "char_start": 393, "char_end": 733, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Knight and Marcu, 2002;": "7793213", "Zajic et al., 2007;": null, "Martins and Smith, 2009;": "16148301", "Thadani and McKeown, 2013;": "13458891", "Wang et al., 2013;": "1260503", "Li et al., 2013": "8928513", "Li et al., , 2014": "10112929", "Filippova et al., 2015;": "1992250", "Durrett et al., 2016)": "5125975"}}}, {"token_start": 244, "token_end": 280, "char_start": 1028, "char_end": 1153, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Gillick and Favre, 2009;": "167874", "Takamura and Okumura, 2009;": "60469477", "Lin and Bilmes, 2010;": "1803710", "Hirao et al., 2013)": "18505561"}}}, {"token_start": 370, "token_end": 381, "char_start": 1663, "char_end": 1714, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Gillick and Favre, 2009": "167874"}}}, {"token_start": 383, "token_end": 397, "char_start": 1718, "char_end": 1774, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kulesza and Taskar, 2012)": "51975610"}}}, {"token_start": 398, "token_end": 410, "char_start": 1777, "char_end": 1820, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lin and Bilmes, 2010)": "1803710"}}}, {"token_start": 412, "token_end": 422, "char_start": 1827, "char_end": 1869, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Shen and Li, 2010)": "1671916"}}}, {"token_start": 438, "token_end": 450, "char_start": 1961, "char_end": 2012, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhang et al., 2016)": null}}}]}
{"id": "202712654_1", "paragraph": "[BOS] Existing Interpretation Toolkits In computer vision, various open-source toolkits exist for explaining and attacking models (e.g., Papernot et al. (2016) ; Ozbulak (2019), inter alia); some toolkits also include interactive demos (Norton and Qi, 2017) .\n[BOS] Similar toolkits for NLP are significantly scarcer, and most toolkits focus on specific models or tasks.\n[BOS] For instance, , Strobelt et al. (2019) , and Vig (2019) visualize attention weights for specific NLP models, while apply adversarial attacks to reading comprehension systems.\n[BOS] Our toolkit differs because it is flexible and diverse; we can interpret and attack any AllenNLP model.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Multi_summ", "Reflection"], "span_citation_mapping": [{"token_start": 18, "token_end": 50, "char_start": 98, "char_end": 189, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Papernot et al. (2016)": "125627987"}}}, {"token_start": 56, "token_end": 67, "char_start": 218, "char_end": 257, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Norton and Qi, 2017)": "206840692"}}}, {"token_start": 96, "token_end": 132, "char_start": 393, "char_end": 551, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Strobelt et al. (2019)": "13754931"}, "Reference": {}}}]}
{"id": "202712654_0", "paragraph": "[BOS] Alternative Interpretation Methods We focus on gradient-based methods (saliency maps and adversarial attacks) but numerous other instancelevel model interpretation methods exist.\n[BOS] For example, a common practice in NLP is to visualize attention weights (Bahdanau et al., 2015) or to isolate the effect of individual neurons (Karpathy et al., 2016) .\n[BOS] We focus on gradient-based methods because they are applicable to many models.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 43, "token_end": 55, "char_start": 245, "char_end": 286, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bahdanau et al., 2015)": "11212020"}}}, {"token_start": 57, "token_end": 73, "char_start": 293, "char_end": 357, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Karpathy et al., 2016)": "988348"}}}]}
{"id": "166228313_4", "paragraph": "[BOS] Flexible neural mechanisms that learn to point and/or generate have been also popular across many NLP tasks.\n[BOS] Our model incorporates PointerGenerator networks (See et al., 2017 ) which learns to copy or generate new words within the context of neural summarization.\n[BOS] Prior to Pointer Generators, CopyNet (Gu et al., 2016) incorporates a copy mechanism for sequence to sequence learning.\n[BOS] Pointer generators have also been recently adopted for learning a universal multi-task architecture for NLP (McCann et al., 2018) .\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 27, "token_end": 55, "char_start": 144, "char_end": 276, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(See et al., 2017": null}, "Reference": {}}}, {"token_start": 61, "token_end": 81, "char_start": 312, "char_end": 402, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Gu et al., 2016)": "8174613"}, "Reference": {}}}, {"token_start": 82, "token_end": 109, "char_start": 409, "char_end": 538, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(McCann et al., 2018)": "49393754"}, "Reference": {}}}]}
{"id": "166228313_3", "paragraph": "[BOS] While most of the prior work in this area has mainly focused on span prediction models (Wang and Jiang, 2016b) and/or multiple choice QA models (Wang and Jiang, 2016a) , there have been recent interest in generation based QA (Tan et al., 2017) .\n[BOS] S-NET (Tan et al., 2017) proposed a twostage retrieve then generate framework.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ"], "span_citation_mapping": [{"token_start": 15, "token_end": 26, "char_start": 70, "char_end": 116, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wang and Jiang, 2016b)": "5592690"}}}, {"token_start": 29, "token_end": 41, "char_start": 124, "char_end": 173, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wang and Jiang, 2016a)": null}}}, {"token_start": 48, "token_end": 59, "char_start": 211, "char_end": 249, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Tan et al., 2017)": "33257417"}}}, {"token_start": 61, "token_end": 81, "char_start": 258, "char_end": 336, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Tan et al., 2017)": "33257417"}, "Reference": {}}}]}
{"id": "166228313_2", "paragraph": "[BOS] State-of-the-art neural question answering models are mainly based on cross-sentence attention (Seo et al., 2016; Wang and Jiang, 2016b; Xiong et al., 2016; Tay et al., 2018c) .\n[BOS] Self-attention (Vaswani et al., 2017; Wang et al., 2017b) has also been popular for reading comprehension (Wang et al., 2018; Clark and Gardner, 2017) .\n[BOS] However, its memory complexity makes it a challenge for reading long context.\n[BOS] Notably, the truncated/summary setting of the NarrativeQA benchmark have been attempted recently (Tay et al., 2018c,b; Hu et al., 2018; Tay et al., 2018a) .\n[BOS] However, this summary setting bypasses the difficulties of long context reading comprehension, reverting to the more familiar RC setup.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Transition", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 17, "token_end": 54, "char_start": 76, "char_end": 181, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Seo et al., 2016;": "8535316", "Wang and Jiang, 2016b;": "5592690", "Xiong et al., 2016;": "3714278", "Tay et al., 2018c)": "53284053"}}}, {"token_start": 56, "token_end": 77, "char_start": 190, "char_end": 247, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vaswani et al., 2017;": "13756489", "Wang et al., 2017b)": "12501880"}}}, {"token_start": 82, "token_end": 99, "char_start": 274, "char_end": 340, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wang et al., 2018;": "19178620", "Clark and Gardner, 2017)": "223637"}}}, {"token_start": 119, "token_end": 160, "char_start": 446, "char_end": 587, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Hu et al., 2018;": "52077283", "Tay et al., 2018a)": "53080615"}}}]}
{"id": "166228313_1", "paragraph": "[BOS] Our work draws inspiration from curriculum learning (CL) (Bengio et al., 2009) .\n[BOS] One key difficulty in CL is to determine which samples are easy or hard.\n[BOS] Self-paced learning (Jiang et al., 2015) is a recently popular form of curriculum learning that treats this issue as an optimization problem.\n[BOS] To this end, (Sachan and Xing, 2016 ) applies selfpaced learning for neural question answering.\n[BOS] Automatic curriculum learning (Graves et al., 2017) , similarly, extracts signals from the learning process to infer progress.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 7, "token_end": 21, "char_start": 38, "char_end": 84, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bengio et al., 2009)": "873046"}}}, {"token_start": 39, "token_end": 69, "char_start": 172, "char_end": 313, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Jiang et al., 2015)": "10891229"}, "Reference": {}}}, {"token_start": 74, "token_end": 93, "char_start": 333, "char_end": 415, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Sachan and Xing, 2016": "16503693"}, "Reference": {}}}, {"token_start": 94, "token_end": 119, "char_start": 422, "char_end": 548, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Graves et al., 2017)": "11137059"}, "Reference": {}}}]}
{"id": "166228313_0", "paragraph": "[BOS] The existing work on open domain QA (Chen et al., 2017) has distinct similarities with our problem, largely owing to the overwhelming large corpus that a machine reader has to reason over.\n[BOS] In recent years, a multitude of techniques have been developed.\n[BOS] (Wang et al., 2018) ment learning to select passages using the reader as the reward.\n[BOS] (Min et al., 2018) proposed ranking the minimal context required to answer the question.\n[BOS] (Clark and Gardner, 2017) proposed shared norm method for predicting spans in the multiparagraph reading comprehension setting.\n[BOS] (Lin et al., 2018) proposed ranking and de-noising techniques.\n[BOS] (Wang et al., 2017a) proposed evidence aggregation based answer re-ranking.\n[BOS] Most techniques focused on constructing a conducive and less noisy context for the neural reader.\n[BOS] Our work provides the first evidence of diverse sampling for training neural reading comprehension models.\n\n", "discourse_tags": ["Single_summ", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 6, "token_end": 40, "char_start": 27, "char_end": 194, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Chen et al., 2017)": "3618568"}, "Reference": {}}}, {"token_start": 54, "token_end": 74, "char_start": 271, "char_end": 355, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Wang et al., 2018)": "19178620"}, "Reference": {}}}, {"token_start": 75, "token_end": 94, "char_start": 362, "char_end": 450, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Min et al., 2018)": "29161506"}, "Reference": {}}}, {"token_start": 95, "token_end": 119, "char_start": 457, "char_end": 584, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Clark and Gardner, 2017)": "223637"}, "Reference": {}}}, {"token_start": 120, "token_end": 137, "char_start": 591, "char_end": 653, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Lin et al., 2018)": "51875405"}, "Reference": {}}}, {"token_start": 138, "token_end": 156, "char_start": 660, "char_end": 735, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Wang et al., 2017a)": "13764176"}, "Reference": {}}}]}
{"id": "202542540_1", "paragraph": "[BOS] Our work is closely related with learning discrete representations with variational infer-ence (Wen et al., 2017; van den Oord et al., 2017; Kaiser et al., 2018; Lawson et al., 2018) , where we treat content selection as the latent representation.\n[BOS] Limiting the KL-term is a common technique to deal with the \"posterior collapse\" problem (Kingma et al., 2016; Shen et al., 2018b) .\n[BOS] We adopt a similar approach and use it to further control the selecting strategy.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 9, "token_end": 50, "char_start": 48, "char_end": 188, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wen et al., 2017;": "9769916", "van den Oord et al., 2017;": "20282961", "Kaiser et al., 2018;": "4720016", "Lawson et al., 2018)": "30442732"}}}, {"token_start": 75, "token_end": 97, "char_start": 320, "char_end": 390, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kingma et al., 2016;": null, "Shen et al., 2018b)": "3620643"}}}]}
{"id": "202542540_0", "paragraph": "[BOS] Most content selection models train the selector with heuristic rules (Hsu et al., 2018; Li et al., 2018; Gehrmann et al., 2018; Yao et al., 2019; Moryossef et al., 2019) , which fail to fully capture the relation between selection and generation.\n[BOS] Mei et al. (2016) ; ; ; Li et al. (2018) \"soft-select\" word or sentence embeddings based on a gating function.\n[BOS] The output score from the gate is a deterministic vector without any probabilistic variations, so controlling the selection to generate diverse text is impossible.\n[BOS] Very few works explicitly define a bernoulli distribution for the selector, then train with the REINFORCE algorithm (Ling and Rush, 2017; Chen and Bansal, 2018) , but the selection targets at a high recall regardless of the low precision, so the controllability over generated text is weak.\n[BOS] Fan et al. (2018) control the generation by manually concatenating entity embeddings, while our model is much more flexible by explicitly defining the selection probability over all source tokens.\n\n", "discourse_tags": ["Narrative_cite", "Multi_summ", "Multi_summ", "Narrative_cite", "Single_summ"], "span_citation_mapping": [{"token_start": 8, "token_end": 56, "char_start": 46, "char_end": 176, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hsu et al., 2018;": "21723747", "Li et al., 2018;": "53082908", "Gehrmann et al., 2018;": "52144157", "Yao et al., 2019;": null, "Moryossef et al., 2019)": "102350767"}}}, {"token_start": 70, "token_end": 130, "char_start": 260, "char_end": 540, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Mei et al. (2016)": "1354459", "Li et al. (2018)": "53082908"}, "Reference": {}}}, {"token_start": 146, "token_end": 166, "char_start": 628, "char_end": 707, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ling and Rush, 2017;": "27970287", "Chen and Bansal, 2018)": "44129061"}}}, {"token_start": 192, "token_end": 227, "char_start": 844, "char_end": 1040, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Fan et al. (2018)": "22716243"}, "Reference": {}}}]}
{"id": "85499373_1", "paragraph": "[BOS] Recently, many methods have been proposed to use information from KBs to facilitate relation extraction.\n[BOS] Sorokin and Gurevych (2017) considered other relations in the sentential context while predicting the target relation.\n[BOS] Vashishth et al. (2018) utilized additional side information from KBs for improved RE.\n[BOS] However, these methods didn't leverage KBE method to unify RE and KBE in a principled way.\n[BOS] Han et al. (2018) used a mutual attention between KBs and text to perform better on both RE and KBE, but their method was still based on TransE which can not fully exploit the advantage of the information from KBs.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Transition", "Single_summ"], "span_citation_mapping": [{"token_start": 21, "token_end": 45, "char_start": 117, "char_end": 235, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Sorokin and Gurevych (2017)": "6263378"}, "Reference": {}}}, {"token_start": 46, "token_end": 67, "char_start": 242, "char_end": 328, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Vashishth et al. (2018)": "53064621"}, "Reference": {}}}, {"token_start": 93, "token_end": 142, "char_start": 432, "char_end": 646, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Han et al. (2018)": "13742825"}, "Reference": {}}}]}
{"id": "85499373_0", "paragraph": "[BOS] Recent neural models have been shown superior to approaches using hand-crafted features for the RE task.\n[BOS] Among the pioneers, Zeng et al. (2015) proposed a piecewise convolutional network with multi-instance learning to handle weakly labeled text mentions.\n[BOS] Recurrent neural networks (RNN) are another popular architecture (Wu et al., 2017) .\n[BOS] Similar fast progress has been seen for the KBE task for representing entities and relations in KBs with vectors or matrices.\n[BOS] introduced the influential translation-based embeddings (TransE), while Yang et al. (2014) leveraged latent matrix factorization in their DistMult method.\n[BOS] We build on ComplEx (Trouillon et al., 2016) , which extends DistMult into the complex space and has been shown significantly better on several benchmarks.\n[BOS] were the first to connect RE and KBE models for the RE task.\n[BOS] Their simple idea was to train the two models independently and only combine them at inference time.\n[BOS] While they showed that combining the two models is better than using the RE model alone, newer and better models since then have obviated the net gains of such a simple strategy (Xu and Barbosa, 2018) .\n[BOS] We propose a much tighter integration of RE and KBE models: we not only use them for prediction, but also train them together, thus mutually reinforcing one another.\n\n", "discourse_tags": ["Transition", "Single_summ", "Narrative_cite", "Transition", "Single_summ", "Single_summ", "Transition", "Transition", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 29, "token_end": 54, "char_start": 137, "char_end": 267, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zeng et al. (2015)": "2778800"}, "Reference": {}}}, {"token_start": 55, "token_end": 74, "char_start": 274, "char_end": 356, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wu et al., 2017)": "34190303"}}}, {"token_start": 114, "token_end": 133, "char_start": 569, "char_end": 651, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Yang et al. (2014)": "2768038"}, "Reference": {}}}, {"token_start": 137, "token_end": 169, "char_start": 670, "char_end": 813, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Trouillon et al., 2016)": "15150247"}, "Reference": {}}}, {"token_start": 209, "token_end": 248, "char_start": 1017, "char_end": 1194, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Xu and Barbosa, 2018)": "3605881"}}}]}
{"id": "182952423_4", "paragraph": "[BOS] Bridging source and target languages through a pivot language was originally proposed for phrasebased MT (De Gispert and Marino, 2006; Cohn and Lapata, 2007) .\n[BOS] It was later adapted for Neural MT (Levinboim and Chiang, 2015) , and proposed joint training for pivot-based NMT.\n[BOS] proposed to use an existing pivottarget NMT model to guide the training of sourcetarget model.\n[BOS] Lakew et al. (2018) proposed an iterative procedure to realize zero-shot translation by pivoting on a third language.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 16, "token_end": 37, "char_start": 96, "char_end": 163, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(De Gispert and Marino, 2006;": null, "Cohn and Lapata, 2007)": "9334744"}}}, {"token_start": 44, "token_end": 56, "char_start": 197, "char_end": 235, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Levinboim and Chiang, 2015)": "3519240"}}}, {"token_start": 91, "token_end": 117, "char_start": 394, "char_end": 511, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Lakew et al. (2018)": "53220272"}, "Reference": {}}}]}
{"id": "182952423_3", "paragraph": "[BOS] Bilingual dictionaries learned in both supervised and unsupervised ways have been used in lowresource settings for tasks such as named entity recognition (Xie et al., 2018) or information retrieval (Litschko et al., 2018) .\n[BOS] Hassan et al. (2017) synthesized data with word embeddings for spoken dialect translation, with a process that requires a LRL-ENG as well as a HRL-LRL dictionary, while our work only uses a HRL-LRL dictionary.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ"], "span_citation_mapping": [{"token_start": 23, "token_end": 34, "char_start": 135, "char_end": 178, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Xie et al., 2018)": "52117484"}}}, {"token_start": 35, "token_end": 47, "char_start": 182, "char_end": 227, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Litschko et al., 2018)": null}}}, {"token_start": 49, "token_end": 102, "char_start": 236, "char_end": 445, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hassan et al. (2017)": "7141401"}, "Reference": {}}}]}
{"id": "182952423_2", "paragraph": "[BOS] Unsupervised machine translation enables training NMT models without parallel data (Artetxe et al., 2018; Lample et al., 2018a,c) .\n[BOS] Recently, multiple methods have been proposed to further improve the framework.\n[BOS] By incorporating a statistical MT system as posterior regularization, Ren et al. (2019) achieved state-of-the-art for en-fr and en-de MT.\n[BOS] Besides MT, the framework has also been applied to other unsupervised tasks like nonparallel style transfer (Subramanian et al., 2019; Zhang et al., 2018) .\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 6, "token_end": 34, "char_start": 47, "char_end": 135, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Artetxe et al., 2018;": "3515219"}}}, {"token_start": 51, "token_end": 86, "char_start": 233, "char_end": 367, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 101, "token_end": 123, "char_start": 455, "char_end": 528, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Zhang et al., 2018)": "52091536"}}}]}
{"id": "182952423_1", "paragraph": "[BOS] In a low-resource MT scenario, multilingual training that aims at sharing parameters by leveraging parallel datasets of multiple languages is a common practice.\n[BOS] Some works target learning a universal representation for all languages either by leveraging semantic sharing between mapped word embeddings (Gu et al., 2018) or by using character n-gram embeddings (Wang et al., 2019) optimizing subword sharing.\n[BOS] More related with data augmentation, Nishimura et al. (2018) fill in missing data with a multi-source setting to boost multilingual translation.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Single_summ"], "span_citation_mapping": [{"token_start": 44, "token_end": 60, "char_start": 255, "char_end": 331, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Gu et al., 2018)": "3295641"}}}, {"token_start": 62, "token_end": 77, "char_start": 338, "char_end": 391, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wang et al., 2019)": "60440615"}}}, {"token_start": 89, "token_end": 113, "char_start": 463, "char_end": 570, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Nishimura et al. (2018)": "53116366"}, "Reference": {}}}]}
{"id": "182952423_0", "paragraph": "[BOS] Our work is related to multilingual and unsupervised translation, bilingual dictionary induction, as well as approaches for triangulation (pivoting).\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "173990628_5", "paragraph": "[BOS] Pakistan denies its spy agency helped plan bombing that (B) Wajid Shamsul Hasan, Pakistan's high commissioner to Britain, and Hamid Gul, killed 58. former head of the ISI, firmly denied the agency's involvement in the attack.\n\n", "discourse_tags": ["Other"], "span_citation_mapping": []}
{"id": "173990628_4", "paragraph": "[BOS] Merged Sentence: (A) The bombing killed 58 people.\n\n", "discourse_tags": ["Other"], "span_citation_mapping": []}
{"id": "173990628_3", "paragraph": "[BOS] A sentence can also be generated through fusing multiple source sentences.\n[BOS] However, many aspects of this approach are largely underinvestigated, such as determining the set of source sentences to be fused, handling its large cardinality, and identifying the sentence relationships for per-\n\n", "discourse_tags": ["Transition", "Transition"], "span_citation_mapping": []}
{"id": "173990628_2", "paragraph": "[BOS] A succinct sentence can be generated by shortening or rewriting a lengthy source text.\n[BOS] Recent studies have leveraged neural encoder-decoder models to rewrite the first sentence of an article to a title-like summary (Nallapati et al., 2016; Zhou et al., 2017; Li et al., 2017; Guo et al., 2018; Cao et al., 2018a) .\n[BOS] Compressive summaries can be generated in a similar vein by selecting important source sentences and then dropping inessential sentence elements such as prepositional phrases.\n[BOS] Before the era of deep neural networks it has been an active area of research, where sentence selection and compression can be accomplished using a pipeline or a joint model (Daum III and Marcu, 2002; Zajic et al., 2007; Gillick and Favre, 2009; Wang et al., 2013; Li et al., 2013 Li et al., , 2014 Filippova et al., 2015) .\n[BOS] A majority of these studies focus on selecting and compressing sentence singletons only.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 25, "token_end": 83, "char_start": 129, "char_end": 324, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Nallapati et al., 2016;": "8928715", "Zhou et al., 2017;": "1770102", "Li et al., 2017;": "1508909", "Guo et al., 2018;": "44105751", "Cao et al., 2018a)": "51878811"}}}, {"token_start": 132, "token_end": 202, "char_start": 600, "char_end": 837, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Daum\u00e9 III and Marcu, 2002;": null, "Zajic et al., 2007;": null, "Gillick and Favre, 2009;": "167874", "Wang et al., 2013;": "1260503", "Li et al., 2013": "8928513", "Li et al., , 2014": "10112929", "Filippova et al., 2015)": "1992250"}}}]}
{"id": "173990628_1", "paragraph": "[BOS] Recent approaches emphasize the importance of separating content selection from summary generation for abstractive summarization.\n[BOS] Studies exploit extractive methods to identify content words and sentences that should be part of the summary and use them to guide the generation of abstracts (Chen and Bansal, 2018; Gehrmann et al., 2018; .\n[BOS] On the other hand, surface lexical features have been shown to be effective in identifying pertinent content (Carenini et al., 2006; Wong et al., 2008; Galanis et al., 2012) .\n[BOS] Examples include sentence length, position, centrality, word frequency, whether a sentence contains topic words, and others.\n[BOS] The surface cues can also be customized for new domains relatively easily.\n[BOS] This paper represents a step forward in this direction, where we focus on developing lightweight models to select summary-worthy sentence singletons and pairs and use them as the basis for summary generation.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite", "Transition", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 23, "token_end": 64, "char_start": 158, "char_end": 347, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chen and Bansal, 2018;": "44129061"}}}, {"token_start": 72, "token_end": 111, "char_start": 376, "char_end": 530, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Carenini et al., 2006;": "884798", "Wong et al., 2008;": "18517541", "Galanis et al., 2012)": "12873285"}}}]}
{"id": "173990628_0", "paragraph": "[BOS] Content selection is integral to any summarization system.\n[BOS] Neural approaches to abstractive summarization often perform content selection jointly with surface realization using an encoder-decoder architecture (Rush et al., 2015; Nallapati et al., 2016; Chen et al., 2016b; Tan et al., 2017; See 1 We make our code and models publicly available at https: Paulus et al., 2017; Celikyilmaz et al., 2018; Narayan et al., 2018) .\n[BOS] Training these models end-to-end means learning to perform both tasks simultaneously and can require a massive amount of data that is unavailable and unaffordable for many summarization tasks.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 30, "token_end": 66, "char_start": 192, "char_end": 301, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rush et al., 2015;": "15761498", "Nallapati et al., 2016;": "8928715", "Chen et al., 2016b;": "12755643"}}}, {"token_start": 80, "token_end": 87, "char_start": 366, "char_end": 385, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 88, "token_end": 98, "char_start": 387, "char_end": 411, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 99, "token_end": 107, "char_start": 413, "char_end": 434, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Narayan et al., 2018)": "52096531"}}}]}
{"id": "202661136_0", "paragraph": "[BOS] Embeddings Previous works have extensively explored different representations such as word (Mikolov et al., 2013; Pennington et al., 2014; Grave et al., 2018; Xu et al., 2018) , subword (Sennrich et al., 2016; Heinzerling and Strube, 2018) , and character (dos Santos and Zadrozny, 2014; Wieting et al., 2016) .\n[BOS] Lample et al. (2016) has successfully concatenated character and word embeddings to their model, showing the potential of combining multiple representations.\n[BOS] Liu et al. (2019) proposed to leverage word and subword embeddings into the application of unsupervised machine translation.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 12, "token_end": 46, "char_start": 92, "char_end": 181, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mikolov et al., 2013;": "16447573", "Pennington et al., 2014;": "1957433", "Grave et al., 2018;": "3411445", "Xu et al., 2018)": "52197809"}}}, {"token_start": 47, "token_end": 68, "char_start": 184, "char_end": 245, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sennrich et al., 2016;": "1114678", "Heinzerling and Strube, 2018)": "21697629"}}}, {"token_start": 70, "token_end": 90, "char_start": 252, "char_end": 315, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Santos and Zadrozny, 2014;": "2834402", "Wieting et al., 2016)": "3202289"}}}, {"token_start": 92, "token_end": 119, "char_start": 324, "char_end": 481, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Lample et al. (2016)": "6042994"}, "Reference": {}}}, {"token_start": 120, "token_end": 144, "char_start": 488, "char_end": 612, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Liu et al. (2019)": "201058668"}, "Reference": {}}}]}
{"id": "162183964_3", "paragraph": "[BOS] Recently Bau et al. (2019) proposed a method for identifying important individual neurons in NMT models.\n[BOS] They show that similar important neurons emerge in different models.\n[BOS] Rather than verifying the importance of individual neurons, we identify the importance of entire attention heads using layer-wise relevance propagation and verify our findings by observing which heads are retained when pruning the model.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 3, "token_end": 37, "char_start": 15, "char_end": 185, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Bau et al. (2019)": "53215110"}, "Reference": {}}}]}
{"id": "162183964_2", "paragraph": "[BOS] There are several works analyzing attention weights of different NMT models (Ghader and Monz, 2017; Voita et al., 2018; Raganato and Tiedemann, 2018) .\n[BOS] Raganato and Tiedemann (2018) use the self-attention weights of the Transformer's encoder to induce a tree structure for each sentence and compute the unlabeled attachment score of these trees.\n[BOS] However they do not evaluate specific syntactic relations (i.e. labeled attachment scores) or consider how different heads specialize to specific dependency relations.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 6, "token_end": 40, "char_start": 30, "char_end": 155, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Voita et al., 2018;": "44062236", "Raganato and Tiedemann, 2018)": "53596423"}}}, {"token_start": 42, "token_end": 111, "char_start": 164, "char_end": 531, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Raganato and Tiedemann (2018)": "53596423"}, "Reference": {}}}]}
{"id": "162183964_1", "paragraph": "[BOS] An alternative method to study the ability of language models and machine translation models to capture hierarchical information is to test their sensitivity to specific grammatical errors (Linzen et al., 2016; Gulordava et al., 2018; Tran et al., 2018; Sennrich, 2017; .\n[BOS] While this line of work has shown that NMT models, including the Transformer, do learn some syntactic structures, our work provides further insight into the role of multi-head attention.\n\n", "discourse_tags": ["Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 10, "token_end": 60, "char_start": 52, "char_end": 274, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Linzen et al., 2016;": "14091946", "Tran et al., 2018;": "3785155"}}}]}
{"id": "162183964_0", "paragraph": "[BOS] One popular approach to the analysis of NMT representations is to evaluate how informative they are for various linguistic tasks.\n[BOS] Different levels of linguistic analysis have been considered including morphology (Belinkov et al., 2017a; Dalvi et al., 2017; Bisazza and Tump, 2018) , syntax (Shi et al., 2016) and semantics (Hill et al., 2017; Belinkov et al., 2017b; Raganato and Tiedemann, 2018) .\n[BOS] Bisazza and Tump (2018) showed that the target language determines which information gets encoded.\n[BOS] This agrees with our results for different domains on the English-Russian translation task in Section 5.2.2.\n[BOS] There we observed that attention heads are more likely to track syntactic relations requiring more complex agreement in the target language (in this case the subject-verb relation).\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Single_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 28, "token_end": 102, "char_start": 162, "char_end": 408, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Shi et al., 2016)": "7197724", "Raganato and Tiedemann, 2018)": "53596423"}}}, {"token_start": 104, "token_end": 148, "char_start": 417, "char_end": 630, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "201306636_2", "paragraph": "[BOS] Text Generation covers a wide spectrum of NLP tasks, including machine translation (Wu et al., 2016) , summarization (See et al., 2017) , response generation (Vinyals and Le, 2015) , paraphrase generation, grammatical error correction etc.\n[BOS] Early studies on text generation mainly adopt template-based (Reiter and Dale, 2000) or example-based (Watanabe and Takeda, 1998) methods.\n[BOS] With the emergence of deep learning for NLP, seq2seq models (Sutskever et al., 2014) become a popular choice for text generation tasks and show better performance in terms of both automatic evaluation metrics and human evaluations (Wu et al., 2016) .\n[BOS] There are also studies focusing on text generation from structured data such as SQL-to-text .\n[BOS] Previous pre-training for text generation is usually done by independently pre-training encoder-side or decoder-side language models (Ramachandran et al., 2016) .\n[BOS] Concurrent to our work, Edunov et al. augment encoder representation with ELMostyle models, MASS (Song et al., 2019) masks continuous text fragments for pre-training, and UNILM (Dong et al., 2019) proposes to pre-train for both language understanding and generation tasks.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Narrative_cite", "Transition", "Narrative_cite", "Multi_summ"], "span_citation_mapping": [{"token_start": 14, "token_end": 24, "char_start": 69, "char_end": 106, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wu et al., 2016)": null}}}, {"token_start": 25, "token_end": 35, "char_start": 109, "char_end": 141, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(See et al., 2017)": null}}}, {"token_start": 36, "token_end": 47, "char_start": 144, "char_end": 186, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vinyals and Le, 2015)": "12300158"}}}, {"token_start": 66, "token_end": 78, "char_start": 298, "char_end": 336, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Reiter and Dale, 2000)": null}}}, {"token_start": 79, "token_end": 90, "char_start": 340, "char_end": 381, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Watanabe and Takeda, 1998)": "3695341"}}}, {"token_start": 103, "token_end": 117, "char_start": 442, "char_end": 481, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sutskever et al., 2014)": "7961699"}}}, {"token_start": 137, "token_end": 147, "char_start": 610, "char_end": 645, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wu et al., 2016)": null}}}, {"token_start": 181, "token_end": 204, "char_start": 829, "char_end": 914, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ramachandran et al., 2016)": "3488076"}}}, {"token_start": 211, "token_end": 224, "char_start": 947, "char_end": 1013, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 225, "token_end": 243, "char_start": 1015, "char_end": 1088, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Song et al., 2019)": "146808476"}, "Reference": {}}}, {"token_start": 245, "token_end": 269, "char_start": 1094, "char_end": 1195, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "201306636_1", "paragraph": "[BOS] Autoencoders have long been used for representation learning of images (Vincent et al., 2010) and text (Li et al., 2015) .\n[BOS] However, precisely reconstructing the clean input is probably too easy for high-capacity models.\n[BOS] Sparse autoencoders (Deng et al., 2013) , contractive autoencoders (Rifai et al., 2011) , and denoising autoencoders (Vincent et al., 2010) are several popular variants.\n[BOS] Denoising autoencoders (DA) are shown to be able to learn better representations for downstream tasks (Vincent et al., 2010 (Vincent et al., , 2008 Hill et al., 2016) .\n[BOS] Freitag and Roy (2018) use seq2seq DAs for unsupervised natural language generation in dialogue, and (Kim et al., 2018) propose to improve the quality of machine translation with DAs.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Narrative_cite", "Narrative_cite", "Multi_summ"], "span_citation_mapping": [{"token_start": 10, "token_end": 22, "char_start": 43, "char_end": 99, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vincent et al., 2010)": "17804904"}}}, {"token_start": 23, "token_end": 32, "char_start": 104, "char_end": 126, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Li et al., 2015)": "207468"}}}, {"token_start": 53, "token_end": 65, "char_start": 238, "char_end": 277, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 66, "token_end": 80, "char_start": 280, "char_end": 325, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rifai et al., 2011)": "8141422"}}}, {"token_start": 82, "token_end": 94, "char_start": 332, "char_end": 377, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vincent et al., 2010)": "17804904"}}}, {"token_start": 113, "token_end": 141, "char_start": 466, "char_end": 580, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vincent et al., 2010": "17804904", "(Vincent et al., , 2008": "207168299", "Hill et al., 2016)": "2937095"}}}, {"token_start": 143, "token_end": 163, "char_start": 589, "char_end": 684, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 165, "token_end": 184, "char_start": 690, "char_end": 772, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Kim et al., 2018)": "52237660"}, "Reference": {}}}]}
{"id": "201306636_0", "paragraph": "[BOS] Network Pre-training The idea of pre-training neural networks dates back to the early days of deep learning.\n[BOS] Bengio et al. (2007) proposed layer-wise pre-training for deep belief networks (DBN) to tackle the difficulty of training deep neural networks based on a reconstruction objective.\n[BOS] (Erhan et al., 2010; Dahl et al., 2012) showed the effectiveness of pre-training for tasks such as speech recognition.\n[BOS] In the area of computer vision, using ImageNet pre-trained models have become a standard practice.\n[BOS] In NLP community, using pre-trained word embeddings is the most popular way to transfer knowledge from the unlabeled corpus.\n[BOS] There are also work on semi-supervised sequence learning (Dai and Le, 2015; Peters et al., 2017) attempting to incorporate language modeling as an auxiliary task.\n[BOS] Recently, several pre-training methods based on language models are presented, such as ELMo (Peters et al., 2018) , OpenAI GPT (Radford et al., 2018) , BERT (Devlin et al., 2018) , XLM (Lample and Conneau, 2019) etc.\n[BOS] The combination of more compute, larger model capacity and large-scale text corpora lead to significant improvements on NLP benchmarks .\n\n", "discourse_tags": ["Transition", "Single_summ", "Multi_summ", "Transition", "Transition", "Narrative_cite", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 25, "token_end": 63, "char_start": 121, "char_end": 300, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Bengio et al. (2007)": "14201947"}, "Reference": {}}}, {"token_start": 64, "token_end": 94, "char_start": 307, "char_end": 425, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 146, "token_end": 175, "char_start": 691, "char_end": 830, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Peters et al., 2017)": "7197241"}}}, {"token_start": 192, "token_end": 202, "char_start": 924, "char_end": 950, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Peters et al., 2018)": "3626819"}}}, {"token_start": 203, "token_end": 216, "char_start": 953, "char_end": 986, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Radford et al., 2018)": null}}}, {"token_start": 217, "token_end": 227, "char_start": 989, "char_end": 1015, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 228, "token_end": 241, "char_start": 1018, "char_end": 1048, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lample and Conneau, 2019)": null}}}]}
{"id": "201698174_4", "paragraph": "[BOS] Other approaches to tackle teacher-forcing have been proposed in an adversarial setting (Goyal et al., 2016) , in search based optimization (Leblond et al., 2018) and in a reinforcement learning setting (Rennie et al., 2016) .\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 14, "token_end": 26, "char_start": 74, "char_end": 114, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Goyal et al., 2016)": null}}}, {"token_start": 28, "token_end": 41, "char_start": 120, "char_end": 168, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Leblond et al., 2018)": "34984289"}}}, {"token_start": 44, "token_end": 56, "char_start": 178, "char_end": 230, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rennie et al., 2016)": "206594923"}}}]}
{"id": "201698174_3", "paragraph": "[BOS] Local correlations such as our pairwise potentials have been used by (Noraset et al., 2018 ), yet as an auxiliary loss and not for model design.\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 6, "token_end": 22, "char_start": 33, "char_end": 96, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Noraset et al., 2018": "13746544"}}}]}
{"id": "201698174_2", "paragraph": "[BOS] Word-embeddings have been reported as excellent dense representations of sparse co-occurrence statistics within several learning frameworks (Mikolov et al., 2013; Pennington et al., 2014) .\n[BOS] Using embeddings in pairwise potentials has been proposed by Goldman and Goldberger (2017) , but they do not compute the true log-likelihood during training as we do.\n[BOS] Similar techniques have been applied for various message passing schemata (Kim et al., 2017; Domke, 2013) .\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 19, "token_end": 40, "char_start": 118, "char_end": 193, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mikolov et al., 2013;": "16447573", "Pennington et al., 2014)": "1957433"}}}, {"token_start": 42, "token_end": 76, "char_start": 202, "char_end": 368, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Goldman and Goldberger (2017)": "195347284"}, "Reference": {}}}, {"token_start": 83, "token_end": 101, "char_start": 416, "char_end": 480, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kim et al., 2017;": "6961760", "Domke, 2013)": "10880491"}}}]}
{"id": "201698174_1", "paragraph": "[BOS] The importance of global normalization for sequence generation has only lately been emphasized, most notably by Wiseman and Rush (2016) for conditional generation in a learning-as-searchoptimization framework and by (Andor et al., 2016) for parsing.\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 20, "token_end": 41, "char_start": 118, "char_end": 214, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Wiseman and Rush (2016)": "2783746"}}}, {"token_start": 43, "token_end": 55, "char_start": 222, "char_end": 255, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Andor et al., 2016)": "2952144"}}}]}
{"id": "201698174_0", "paragraph": "[BOS] Conditional Random Fields (CRF) were originally introduced by Sha and Pereira (2003) to overcome label bias, a shortcoming of locally normalized observation models.\n[BOS] They have been applied and integrated into neural-network architectures (Ma and Hovy, 2016; Huang et al., 2017) in various sequence labeling tasks (Goldman and Goldberger, 2017) where the observation space exhibits small cardinality (typically tens to hundreds).\n\n", "discourse_tags": ["Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 2, "token_end": 34, "char_start": 6, "char_end": 170, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Sha and Pereira (2003)": "13936575"}, "Reference": {}}}, {"token_start": 42, "token_end": 61, "char_start": 220, "char_end": 288, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ma and Hovy, 2016;": "10489017", "Huang et al., 2017)": "12740621"}}}, {"token_start": 63, "token_end": 75, "char_start": 300, "char_end": 354, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Goldman and Goldberger, 2017)": "195347284"}}}]}
{"id": "204950087_1", "paragraph": "[BOS] Our work is also inspired by researches on multi-party chatbots.\n[BOS] Dielmann and Renals (2008) automatically recognize dialogue acts in multiparty speech conversations.\n[BOS] Recently, some studies focus on the three elements (speaker, addressee, response) on multi-party chatbots.\n[BOS] Meng et al. (2017) introduce speaker classification as a surrogate task.\n[BOS] Addressee selection is researched by (Akhtiamov et al., 2017b) .\n[BOS] Some researches strive to the response selection (Ouchi and Tsuboi, 2016; Zhang et al., 2018) .\n[BOS] However, the response selection heavily relies on the candidates, and it can not generate new responses in new dialogue contexts.\n[BOS] Response generation could solve this problem.\n[BOS] Li et al. (2016b) learn fixed person vector for response generation.\n[BOS] Unfortunately, it needs to be obtained from large-scale dialogue turns, which has a sparsity issue: some interlocutors have very little dialog data.\n[BOS] Differently, our model has no such restrictions.\n\n", "discourse_tags": ["Reflection", "Single_summ", "Transition", "Single_summ", "Single_summ", "Narrative_cite", "Transition", "Transition", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 18, "token_end": 37, "char_start": 77, "char_end": 177, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Dielmann and Renals (2008)": "7996233"}, "Reference": {}}}, {"token_start": 64, "token_end": 79, "char_start": 297, "char_end": 369, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Meng et al. (2017)": "14308824"}, "Reference": {}}}, {"token_start": 80, "token_end": 99, "char_start": 376, "char_end": 438, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Akhtiamov et al., 2017b)": "37777321"}, "Reference": {}}}, {"token_start": 107, "token_end": 126, "char_start": 477, "char_end": 540, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ouchi and Tsuboi, 2016;": "16537814", "Zhang et al., 2018)": "19107193"}}}, {"token_start": 160, "token_end": 207, "char_start": 737, "char_end": 960, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "204950087_0", "paragraph": "[BOS] Our work is inspired by a large number of applications utilizing recurrent encoder-decoder frameworks (Cho et al., 2014) on NLP tasks such as machine translation (Bahdanau et al., 2015) and text summarization (Chopra et al., 2016) .\n[BOS] Recently, many researches extend the encoder-decoder framework on response generation.\n[BOS] HRED (Serban et al., 2016) utilizes hierarchical encoder to capture the context.\n[BOS] VHRED (Serban et al., 2017) extends HRED by adding a high-dimensional latent variable for utterances.\n[BOS] These researches demonstrate the importance of contexts on response generation.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Single_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 13, "token_end": 26, "char_start": 71, "char_end": 126, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cho et al., 2014)": "5590763"}}}, {"token_start": 32, "token_end": 44, "char_start": 148, "char_end": 191, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bahdanau et al., 2015)": "11212020"}}}, {"token_start": 45, "token_end": 57, "char_start": 196, "char_end": 236, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chopra et al., 2016)": "133195"}}}, {"token_start": 74, "token_end": 93, "char_start": 338, "char_end": 418, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 94, "token_end": 119, "char_start": 425, "char_end": 526, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Serban et al., 2017)": "14857825"}, "Reference": {}}}]}
{"id": "196184169_3", "paragraph": "[BOS] Apply the GAT to a directed complete graph similar to the Transformer encoder (Vaswani et al., 2017) .\n[BOS] But the transformer framework focuses only on head-dep-like dependency, we further explore it to capture high-order information on dependency parsing.\n[BOS] Several works have investigated high-order features in neural parsing.\n[BOS] Kiperwasser and Goldberg (2016b) uses a bottom-up tree-encoding to extract hard high-order features from an intermediate predicted tree.\n[BOS] Zheng (2017) uses an incremental refinement framework to extract hard high-order features from a whole predicted tree.\n[BOS] Ma et al. (2018) uses greedy decoding to replace the MST decoding and extract local 2-order features at the current decoding time.\n[BOS] Comparing with the previous work, GNNs can efficiently capture global and soft high-order features.\n\n", "discourse_tags": ["Narrative_cite", "Reflection", "Transition", "Single_summ", "Single_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 13, "token_end": 25, "char_start": 64, "char_end": 106, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vaswani et al., 2017)": "13756489"}}}, {"token_start": 69, "token_end": 100, "char_start": 349, "char_end": 485, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 101, "token_end": 123, "char_start": 492, "char_end": 610, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zheng (2017)": "9950135"}, "Reference": {}}}, {"token_start": 124, "token_end": 152, "char_start": 617, "char_end": 747, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ma et al. (2018)": null}, "Reference": {}}}]}
{"id": "196184169_2", "paragraph": "[BOS] Given a graph, a GNN can embed the node by recursively aggregating the node representations of its neighbors (Battaglia et al., 2018) .\n[BOS] Based on a biaffine mapping, GNNs can enhance the node representation by recursively integrating neighbors' information.\n[BOS] The message passing neural network (MPNN) (Gilmer et al., 2017) and the non-local neural network (NLNN) are two popular GNN methods.\n[BOS] Due to the convenience of self-attention in handling variable sentence length, we use a GAT-like network (Velikovi et al., 2018) belonging to NLNN.\n[BOS] Then, we further explore its aggregating functions and update methods on special task.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Narrative_cite", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 7, "token_end": 33, "char_start": 23, "char_end": 139, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 59, "token_end": 76, "char_start": 279, "char_end": 338, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Gilmer et al., 2017)": "9665943"}}}, {"token_start": 112, "token_end": 126, "char_start": 502, "char_end": 542, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Velikovi et al., 2018)": "3292002"}}}]}
{"id": "196184169_1", "paragraph": "[BOS] The design of the node representation network is a key problem in neural graph-based parsers.\n[BOS] Kiperwasser and Goldberg (2016b) use BiRNNs to obtain node representation with sentence-level information.\n[BOS] To better characterize the direction of edge, Dozat and Manning (2017) feed BiRNNs outputs to two MLPs to distinguish word as head or dependent, and then construct a biaffine mapping for prediction.\n[BOS] It also performs well on multilingual UD datasets (Che et al., 2018) .\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 22, "token_end": 46, "char_start": 106, "char_end": 212, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kiperwasser and Goldberg (2016b)": "1642392"}, "Reference": {}}}, {"token_start": 55, "token_end": 92, "char_start": 265, "char_end": 417, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Dozat and Manning (2017)": "7942973"}, "Reference": {}}}, {"token_start": 98, "token_end": 110, "char_start": 449, "char_end": 492, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Che et al., 2018)": "49656421"}}}]}
{"id": "196184169_0", "paragraph": "[BOS] Graph structures have been extended to model text representation, giving competitive results for a number of NLP tasks.\n[BOS] By introducing context neighbors, the graph structure is added to the sequence modeling tool LSTMs, which improves performance on text classification, POS tagging and NER tasks (Zhang et al., 2018a) .\n[BOS] Based on syntactic dependency trees, DAG LSTMs (Peng et al., 2017) and GCNs (Zhang et al., 2018b) are used to improve the performance of relation extraction task.\n[BOS] Based on the AMR semantic graph representation, graph state LSTMs , GCNs (Bastings et al., 2017) and gated GNNs (Beck et al., 2018) are used as encoder to construct graph-to-sequence learning.\n[BOS] To our knowledge, we are the first to investigate GNNs for dependency parsing task.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 47, "token_end": 64, "char_start": 262, "char_end": 330, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhang et al., 2018a)": "19175261"}}}, {"token_start": 72, "token_end": 84, "char_start": 376, "char_end": 405, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Peng et al., 2017)": "2797612"}}}, {"token_start": 85, "token_end": 96, "char_start": 410, "char_end": 436, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhang et al., 2018b)": "49544037"}}}, {"token_start": 123, "token_end": 135, "char_start": 576, "char_end": 604, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bastings et al., 2017)": "6206777"}}}, {"token_start": 136, "token_end": 147, "char_start": 609, "char_end": 639, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Beck et al., 2018)": "49430686"}}}]}
{"id": "196175184_3", "paragraph": "[BOS] The third way to enhance the model is building heavy post-processing layers for the alignment results.\n[BOS] CAFE (Tay et al., 2018b ) extracts additional indicators from the alignment process using alignment factorization layers.\n[BOS] DIIN (Gong et al., 2018) adopts DenseNet as a deep convolutional feature extractor to distill information from the alignment results.\n[BOS] More effective models can be built if intersequence matching is allowed to be performed more than once.\n[BOS] CSRAN (Tay et al., 2018a) performs multi-level attention refinement with dense connections among multiple levels.\n[BOS] DRCN (Kim et al., 2018) stacks encoding and alignment layers.\n[BOS] It concatenates all previously aligned results and has to use an autoencoder to deal with exploding feature spaces.\n[BOS] SAN (Liu et al., 2018) utilizes recurrent networks to combine multiple alignment results.\n[BOS] This paper also proposes a deep architecture based on a new way to connect consecutive blocks named augmented residual connections, to distill previous aligned information which serves as an important feature for text matching.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 22, "token_end": 46, "char_start": 115, "char_end": 236, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Tay et al., 2018b": null}, "Reference": {}}}, {"token_start": 47, "token_end": 77, "char_start": 243, "char_end": 376, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Gong et al., 2018)": "20472740"}, "Reference": {}}}, {"token_start": 99, "token_end": 124, "char_start": 493, "char_end": 606, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Tay et al., 2018a)": "52943910"}, "Reference": {}}}, {"token_start": 125, "token_end": 165, "char_start": 613, "char_end": 796, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Kim et al., 2018)": "44132329"}, "Reference": {}}}, {"token_start": 166, "token_end": 222, "char_start": 803, "char_end": 1126, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Liu et al., 2018)": "5058361"}, "Reference": {}}}]}
{"id": "196175184_2", "paragraph": "[BOS] Three major paradigms are adopted to further improve performance.\n[BOS] First is to use richer syntactic or hand-designed features.\n[BOS] HIM (Chen et al., 2017) uses syntactic parse trees.\n[BOS] POS tags are found in many previous works including Tay et al. (2018b) and Gong et al. (2018) .\n[BOS] The exact match of lemmatized tokens is reported as a powerful binary feature in Gong et al. (2018) and Kim et al. (2018) .\n[BOS] The second way is adding complexity to the alignment computation.\n[BOS] BiMPM utilizes an advanced multiperspective matching operation, and MwAN (Tan et al., 2018) applies multiple heterogeneous attention functions to compute the alignment results.\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ", "Narrative_cite", "Narrative_cite", "Transition", "Single_summ"], "span_citation_mapping": [{"token_start": 26, "token_end": 40, "char_start": 144, "char_end": 195, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Chen et al., 2017)": null}, "Reference": {}}}, {"token_start": 41, "token_end": 68, "char_start": 202, "char_end": 295, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Tay et al. (2018b)": null, "Gong et al. (2018)": "20472740"}}}, {"token_start": 71, "token_end": 101, "char_start": 308, "char_end": 425, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Gong et al. (2018)": "20472740", "Kim et al. (2018)": "44132329"}}}, {"token_start": 127, "token_end": 148, "char_start": 574, "char_end": 682, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Tan et al., 2018)": "51607048"}, "Reference": {}}}]}
{"id": "196175184_1", "paragraph": "[BOS] Later works, therefore, adopt the matching aggregation framework to match two sequences at lower levels and aggregate the results based on the attention mechanism.\n[BOS] DecompAtt (Parikh et al., 2016) uses a simple form of attention for alignment and aggregate aligned representations with feed-forward networks.\n[BOS] ESIM (Chen et al., 2017) uses a similar attention mechanism but employs bidirectional LSTMs as encoders and aggregators.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 30, "token_end": 60, "char_start": 176, "char_end": 319, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Parikh et al., 2016)": "8495258"}, "Reference": {}}}, {"token_start": 61, "token_end": 89, "char_start": 326, "char_end": 446, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Chen et al., 2017)": null}, "Reference": {}}}]}
{"id": "196175184_0", "paragraph": "[BOS] Deep neural networks are dominant in the text matching area.\n[BOS] Semantic alignment and comparison between two text sequences lie in the core of text matching.\n[BOS] Early works explore encoding each sequence individually into a vector and then building a neural network classifier upon the two vectors.\n[BOS] In this paradigm, recurrent (Bowman et al., 2015) , recursive (Tai et al., 2015) and convolutional (Yu et al., 2014; Tan et al., 2016) networks are used as the sequence encoder.\n[BOS] The encoding of one sequence is independent of the other in these models, making the final classifier hard to model complex relations.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 58, "token_end": 69, "char_start": 336, "char_end": 369, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bowman et al., 2015)": "14604520"}}}, {"token_start": 69, "token_end": 78, "char_start": 370, "char_end": 398, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Tai et al., 2015)": "3033526"}}}, {"token_start": 79, "token_end": 99, "char_start": 403, "char_end": 473, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yu et al., 2014;": "12211448", "Tan et al., 2016)": "12320170"}}}]}
{"id": "204960839_1", "paragraph": "[BOS] 3 Interrogative-Word-Aware Question Generation\n\n", "discourse_tags": ["Other"], "span_citation_mapping": []}
{"id": "204960839_0", "paragraph": "[BOS] Question Generation (QG) problem has been approached in two ways.\n[BOS] One is based on heuristics, templates and syntactic rules (Heilman and Smith, 2010; Mazidi and Nielsen, 2014; Labutov et al., 2015) .\n[BOS] This type of approach requires a heavy human effort, so they do not scale well.\n[BOS] The other approach is based on neural networks and it is becoming popular due to the recent progress of deep learning in NLP (Pan et al., 2019) .\n[BOS] Du et al. (2017) is the first one to propose an sequence-to-sequence model to tackle the QG problem and outperformed the previous state-of-the-art model using human and automatic evaluations.\n[BOS] Sun et al. (2018) proposed a similar approach to us, an answer-aware sequence-to-sequence model with a special decoding mode in charge of only the interrogative word.\n[BOS] However, we propose to predict the interrogative word before the encoding stage, so that the decoder can focus more on the rest of the question rather than on the interrogative word.\n[BOS] Besides, they cannot train the interrogativeword classifier using golden labels because it is learned implicitly inside the decoder.\n[BOS] Duan et al. (2017) proposed, in a similar way to us, a pipeline approach.\n[BOS] First, the authors create a long list of question templates like \"who is author of\", and \"who is wife of\".\n[BOS] Then, when generating the question, they select first the question template and next, they fill it in.\n[BOS] To select the question template, they proposed two approaches.\n[BOS] One is a retrievalbased question pattern prediction, and the second one is a generation-based question pattern prediction.\n[BOS] The first one has the problem that is computationally expensive when the question pattern size is large, and the second one, although it yields to better results, it is a generative approach and we argue that just modeling the interrogative word prediction as a classification task is easier and can lead to better results.\n[BOS] As far as we know, we are the first one to propose an explicit interrogativeword classifier that provides the interrogative word to the question generator.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Narrative_cite", "Single_summ", "Single_summ", "Reflection", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 21, "token_end": 51, "char_start": 94, "char_end": 209, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Heilman and Smith, 2010;": "1809816", "Mazidi and Nielsen, 2014;": "14792899", "Labutov et al., 2015)": "418759"}}}, {"token_start": 90, "token_end": 104, "char_start": 408, "char_end": 447, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Pan et al., 2019)": "162168545"}}}, {"token_start": 106, "token_end": 151, "char_start": 456, "char_end": 647, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Du et al. (2017)": "2172129"}, "Reference": {}}}, {"token_start": 152, "token_end": 190, "char_start": 654, "char_end": 820, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Sun et al. (2018)": "53083677"}, "Reference": {}}}, {"token_start": 251, "token_end": 357, "char_start": 1155, "char_end": 1648, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Duan et al. (2017)": "427742"}, "Reference": {}}}]}
{"id": "184487889_1", "paragraph": "[BOS] GCNN was firstly proposed by Kipf and Welling (2017) and applied on citation networks and knowledge graph datasets.\n[BOS] It was later used for semantic role labelling (Marcheggiani and Titov, 2017), multi-document summarization (Yasunaga et al., 2017) and temporal relation extraction (Vashishth et al., 2018) .\n[BOS] Zhang et al. (2018) used a GCNN on a dependency tree for intrasentence RE.\n[BOS] Unlike previous work, we introduced a GCNN on a document-level graph, with both intra-and inter-sentence dependencies for intersentence RE.\n\n", "discourse_tags": ["Single_summ", "Narrative_cite", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 26, "char_start": 6, "char_end": 121, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 32, "token_end": 46, "char_start": 150, "char_end": 204, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 47, "token_end": 62, "char_start": 206, "char_end": 258, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yasunaga et al., 2017)": "6532096"}}}, {"token_start": 63, "token_end": 77, "char_start": 263, "char_end": 316, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vashishth et al., 2018)": "51876439"}}}, {"token_start": 79, "token_end": 101, "char_start": 325, "char_end": 399, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhang et al. (2018)": "49544037"}, "Reference": {}}}]}
{"id": "184487889_0", "paragraph": "[BOS] Inter-sentence RE is a recently introduced task.\n[BOS] Peng et al. (2017) and Song et al. (2018) used graph-based LSTM networks for n-ary RE in multiple sentences for protein-drug-disease associations.\n[BOS] They restricted the relation candidates in up to two-span sentences.\n[BOS] Verga et al. (2018) considered multi-instance learning for document-level RE.\n[BOS] Our work is different from Verga et al. (2018) in that we replace Transformer with a GCNN model for full-abstract encoding using non-local dependencies such as entity coreference.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Multi_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 13, "token_end": 66, "char_start": 61, "char_end": 282, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 67, "token_end": 86, "char_start": 289, "char_end": 366, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Verga et al. (2018)": "3576631"}, "Reference": {}}}, {"token_start": 92, "token_end": 100, "char_start": 400, "char_end": 419, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Verga et al. (2018)": "3576631"}}}]}
{"id": "85543290_2", "paragraph": "[BOS] For the analysis of our models, we draw inspiration from critical work on Natural Language Inference datasets (Dasgupta et al., 2018; .\n[BOS] Gururangan et al. (2018) ; Poliak et al. (2018) show that baseline models that disregard the hypothesis yield good results on SNLI, which suggests that the model does not perform the high level reasoning we would expect in order to predict the correct label.\n[BOS] They attribute this effect to bias in human annotations.\n[BOS] In this work, we show that this issue is not inherent to human labeled data, and propose the shuffle perturbation in order to measure to what extent the relationship between sentences is used.\n\n", "discourse_tags": ["Narrative_cite", "Multi_summ", "Multi_summ", "Reflection"], "span_citation_mapping": [{"token_start": 16, "token_end": 30, "char_start": 80, "char_end": 138, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 33, "token_end": 101, "char_start": 148, "char_end": 469, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Gururangan et al. (2018)": "4537113", "Poliak et al. (2018)": "21382535"}, "Reference": {}}}]}
{"id": "85543290_1", "paragraph": "[BOS] by QuickThought (Logeswaran and Lee, 2018) , which uses a much simpler training task.\n[BOS] Both of these rely on pre-established lists of discourse markers provided by the PDTB, and both perform a manual annotation for each marker-Nie et al. (2017) (Felbo et al., 2017) have been sucessfully exploited in order to learn sentiment analysis from unlabelled tweets, but their availability is mainly limited to the microblogging domain.\n[BOS] Language modeling provides a general training signal for representation learning, even though there is no obvious way to derive sentence representations from language models.\n[BOS] BERT (Devlin et al., 2018) currently holds the best results in transfer learning based on language modeling, but it relies on sentence pair classification in order to compute sentence embeddings, and it makes use of a simple sentence contiguity detection task (like QuickThought); this task does not seem challenging enough since BERT reportedly achieves 98% detection accuracy.\n[BOS] Phang et al. (2018) showed that the use of SNLI datasets yields significant gains for the sentence embeddings from Radford (2018) , which are based on language modeling.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Transition", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 3, "token_end": 25, "char_start": 9, "char_end": 91, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Logeswaran and Lee, 2018)": "3525802"}}}, {"token_start": 48, "token_end": 70, "char_start": 204, "char_end": 276, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Felbo et al., 2017)": "2493033"}}}, {"token_start": 128, "token_end": 201, "char_start": 627, "char_end": 1005, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Devlin et al., 2018)": "52967399"}, "Reference": {}}}, {"token_start": 202, "token_end": 239, "char_start": 1012, "char_end": 1181, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Phang et al. (2018)": "53221289"}, "Reference": {"Radford (2018)": "49313245"}}}]}
{"id": "85543290_0", "paragraph": "[BOS] Though discourse marker prediction in itself is an interesting and useful task (Malmi et al., 2017) , discourse markers have often been used as a training cue in order to improve implicit relation prediction (Marcu and Echihabi, 2001; Sporleder and Lascarides, 2005; Zhou et al., 2010; Braud and Table 9 : Accuracy of various models on linguistic probing tasks using logistic regression on SentEval.\n[BOS] BShift is detection of token inversion.\n[BOS] CoordInv is detection of clause inversion.\n[BOS] ObjNum/SubjNum is prediction of the number of object resp.\n[BOS] subject.\n[BOS] Tense is prediction of the main verb tense.\n[BOS] Depth is prediction of parse tree depth.\n[BOS] TC is detection of common sequences of constituents.\n[BOS] WC is prediction of words contained in the sentence.\n[BOS] OddM is detection of random replacement of verbs/nouns by other verbs/nouns.\n[BOS] AVG is the average score of those tasks for each model.\n[BOS] For more details see Conneau et al. (2018) .\n[BOS] SkipThought and Infersent results come from Perone et al. (2018) , QuickThought results come from Brahma (2018) .\n\n", "discourse_tags": ["Narrative_cite", "Other", "Other", "Other", "Other", "Other", "Other", "Other", "Other", "Other", "Other", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 3, "token_end": 23, "char_start": 13, "char_end": 105, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Malmi et al., 2017)": "2967131"}}}, {"token_start": 24, "token_end": 67, "char_start": 108, "char_end": 290, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Marcu and Echihabi, 2001;": "210363", "Sporleder and Lascarides, 2005;": "16991652"}}}, {"token_start": 204, "token_end": 213, "char_start": 968, "char_end": 989, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Conneau et al. (2018)": "24461982"}}}, {"token_start": 215, "token_end": 233, "char_start": 998, "char_end": 1062, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Perone et al. (2018)": "49306018"}}}, {"token_start": 234, "token_end": 245, "char_start": 1065, "char_end": 1109, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Brahma (2018)": "51969623"}}}]}
{"id": "186206745_0", "paragraph": "[BOS] The last few years have witnessed significant progress on text-based machine reading comprehension and question answering (MRC-QA) including cloze-style blank-filling tasks (Hermann et al., 2015) , open-domain QA (Yang et al., 2015) , answer span prediction (Rajpurkar et al., 2016 (Rajpurkar et al., , 2018 , and generative QA (Nguyen et al., 2016) .\n[BOS] However, all of the above datasets are confined to a single-document context per question setup.\n[BOS] Joshi et al. (2017) extended the task to the multidocument regime, with some examples requiring cross-sentence inference.\n[BOS] Earlier attempts in multi-hop MRC focused on reasoning about the relations in a knowledge base (Jain, 2016; Lin et al., 2018) or tables (Yin et al., 2015) .\n[BOS] QAngaroo WikiHop and MedHop (Welbl et al., 2017) , on the other hand, are created as natural language MRC tasks.\n[BOS] They are designed in a way such that the evidence required to answer a query could be spread across multiple documents.\n[BOS] Thus, finding some evidence requires building a reasoning chain from the query with intermediate inference steps, which poses extra difficulty for MRC-QA systems.\n[BOS] HotpotQA is another recent multi-hop dataset which focuses on four different reasoning paradigms.\n[BOS] The emergence of large-scale MRC datasets has led to innovative neural models such as coattention (Xiong et al., 2017) , bi-directional attention flow (Seo et al., 2017) , and gated attention (Dhingra et al., 2017) , all of which are metic-\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Single_summ", "Narrative_cite", "Multi_summ", "Transition", "Transition", "Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 26, "token_end": 43, "char_start": 147, "char_end": 201, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hermann et al., 2015)": "6203757"}}}, {"token_start": 44, "token_end": 56, "char_start": 204, "char_end": 238, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yang et al., 2015)": "1373518"}}}, {"token_start": 57, "token_end": 79, "char_start": 241, "char_end": 313, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rajpurkar et al., 2016": "11816014", "(Rajpurkar et al., , 2018": "47018994"}}}, {"token_start": 81, "token_end": 91, "char_start": 320, "char_end": 355, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Nguyen et al., 2016)": "1289517"}}}, {"token_start": 113, "token_end": 139, "char_start": 467, "char_end": 588, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Joshi et al. (2017)": "26501419"}, "Reference": {}}}, {"token_start": 155, "token_end": 169, "char_start": 675, "char_end": 720, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Jain, 2016;": "18780529", "Lin et al., 2018)": "52143467"}}}, {"token_start": 170, "token_end": 179, "char_start": 724, "char_end": 749, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yin et al., 2015)": "6715526"}}}, {"token_start": 189, "token_end": 200, "char_start": 779, "char_end": 806, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Welbl et al., 2017)": "9192723"}}}, {"token_start": 302, "token_end": 313, "char_start": 1362, "char_end": 1394, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Xiong et al., 2017)": "3714278"}}}, {"token_start": 314, "token_end": 328, "char_start": 1397, "char_end": 1445, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Seo et al., 2017)": "8535316"}}}, {"token_start": 330, "token_end": 342, "char_start": 1452, "char_end": 1490, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dhingra et al., 2017)": null}}}]}
{"id": "202787248_2", "paragraph": "[BOS] A number of studies about generation-based chatbots have considered multi-turn response generation.\n[BOS] Sordoni et al. (2015) is the pioneer of this type of research, it encodes history information into a vector and feeds to the decoder.\n[BOS] Shang et al. (2015) propose three types of attention to utilize the context information.\n[BOS] In addition, propose a Hierarchical Recurrent Encoder-Decoder model (HRED), which employs a hierarchical structure to represent the context.\n[BOS] After that, latent variables (Serban et al., 2017b) and hierarchical attention mechanism (Xing et al., 2018) have been introduced to modify the architecture of HRED.\n[BOS] Compared to previous work, the originality of this study is that it proposes a principle way instead of heuristic rules for context rewriting, and it does not depend on parallel data.\n[BOS] (Su et al., 2019) propose an utterance rewrite approach but need human annotation.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Narrative_cite", "Transition", "Single_summ"], "span_citation_mapping": [{"token_start": 22, "token_end": 54, "char_start": 112, "char_end": 245, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Sordoni et al. (2015)": "94285"}, "Reference": {}}}, {"token_start": 55, "token_end": 74, "char_start": 252, "char_end": 340, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Shang et al. (2015)": "7356547"}, "Reference": {}}}, {"token_start": 105, "token_end": 117, "char_start": 506, "char_end": 545, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 118, "token_end": 130, "char_start": 550, "char_end": 602, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Xing et al., 2018)": "14247119"}}}, {"token_start": 178, "token_end": 197, "char_start": 856, "char_end": 938, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Su et al., 2019)": "189928008"}, "Reference": {}}}]}
{"id": "202787248_1", "paragraph": "[BOS] Early research into retrieval-based chatbots (Wang et al., 2013; Hu et al., 2014; Wang et al., 2015) only considers the last utterances and ignores previous ones, which is also called Short Text Conversation (STC).\n[BOS] Recently, several studies (Lowe et al., 2015; Wu et al., , 2018b have investigated multi-turn response selection, and obtained better results in a comparison with STC.\n[BOS] A common practice for multi-turn retrieval-based chatbots first retrieve candidates from a large index with a heuristic context rewriting method.\n[BOS] For example, and refine the last utterance by appending keywords in history, and retrieve candidates with the refined utterance.\n[BOS] Then, response selection methods are applied to measure the relevance between history and candidates.\n\n", "discourse_tags": ["Narrative_cite", "Multi_summ", "Transition", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 5, "token_end": 33, "char_start": 26, "char_end": 106, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wang et al., 2013;": "12588798", "Hu et al., 2014;": "4497054", "Wang et al., 2015)": "14029406"}}}, {"token_start": 59, "token_end": 97, "char_start": 237, "char_end": 394, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Lowe et al., 2015;": "8379583", "Wu et al., , 2018b": "49312395"}, "Reference": {}}}]}
{"id": "202787248_0", "paragraph": "[BOS] Recently, data-driven approaches for chatbots (Ritter et al., 2011; Ji et al., 2014) has achieved promising results.\n[BOS] Existing work along this line includes retrieval-based methods (Hu et al., 2014; Ji et al., 2014; Wang et al., 2015; and generation-based methods (Shang et al., 2015; Vinyals and Le, 2015; Li et al., 2016a,b; Serban et al., 2017a) .\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 4, "token_end": 28, "char_start": 16, "char_end": 90, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ritter et al., 2011;": "780171", "Ji et al., 2014)": "18380963"}}}, {"token_start": 40, "token_end": 65, "char_start": 168, "char_end": 244, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hu et al., 2014;": "4497054", "Ji et al., 2014;": "18380963"}}}, {"token_start": 67, "token_end": 107, "char_start": 250, "char_end": 359, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Shang et al., 2015;": "7356547", "Vinyals and Le, 2015;": "12300158", "Serban et al., 2017a)": "7562717"}}}]}
{"id": "184482926_2", "paragraph": "[BOS] As emotion detection is a part of sentiment analysis, and the data from the task organizers contains emoticons and emoji for emotion expressions, we can utilize a neural network method with pretrained embedding to solve this task.\n[BOS] We also need to address the lack of representations of emotionrelated symbols.\n\n", "discourse_tags": ["Transition", "Reflection"], "span_citation_mapping": []}
{"id": "184482926_1", "paragraph": "[BOS] To achieve generalization and robustness in social media sentiment analysis, pre-trained embeddings should contain the representations of not only words from natural language but also emotion-related symbols, such as emoticons and emoji (Eisner et al., 2016) .\n[BOS] Both pre-trained embeddings GloVe (Pennington et al., 2014) and word2vec (Mikolov et al., 2013) do not contain representations for emotion-related symbols, which restricts the performance of sentiment analysis in social media.\n[BOS] Although pre-trained emoji2vec embedding contains Unicode emoji representation, not all emotion-related symbols are included, such as emoticons.\n\n", "discourse_tags": ["Narrative_cite", "Multi_summ", "Transition"], "span_citation_mapping": [{"token_start": 37, "token_end": 52, "char_start": 223, "char_end": 264, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Eisner et al., 2016)": "7819714"}}}, {"token_start": 59, "token_end": 105, "char_start": 301, "char_end": 499, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Pennington et al., 2014)": "1957433", "(Mikolov et al., 2013)": "16447573"}, "Reference": {}}}]}
{"id": "184482926_0", "paragraph": "[BOS] Natural language processing in social media as an emergent area has attracted a lot of attention (Poria et al., 2017) , especially from the recent advances in applying neural network methods with pre-trained embeddings (Eisner et al., 2016) .\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 2, "token_end": 27, "char_start": 6, "char_end": 123, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Poria et al., 2017)": "23583643"}}}, {"token_start": 39, "token_end": 52, "char_start": 202, "char_end": 246, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Eisner et al., 2016)": "7819714"}}}]}
{"id": "202540793_4", "paragraph": "[BOS] Our work is also related to recently proposed approaches to code generation by editing (Hayati et al., 2018; Yin et al., 2019; Hashimoto et al., 2018) .\n[BOS] While they follow the framework of generating code by editing the relevant examples retrieved from training data, we focus on a contextdependent setting where we generate queries from the previous query predicted by the system itself.\n\n", "discourse_tags": ["Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 12, "token_end": 41, "char_start": 66, "char_end": 156, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hayati et al., 2018;": "52136345", "Yin et al., 2019;": "53109277", "Hashimoto et al., 2018)": "54446010"}}}]}
{"id": "202540793_3", "paragraph": "[BOS] Concurrent with our work, Yu et al. (2019a) introduced CoSQL, a large-scale cross-domain conversational text-to-SQL corpus collected under the Wizard-of-Oz setting.\n[BOS] Each dialogue in CoSQL simulates a DB querying scenario with a crowd worker as a user and a college computer science student who is familiar with SQL as an expert.\n[BOS] Question-SQL pairs in CoSQL reflect greater diversity in user backgrounds compared to other corpora and involve frequent changes in user intent between pairs or ambiguous questions that require user clarification.\n[BOS] These features pose new chal-lenges for text-to-SQL systems.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 7, "token_end": 133, "char_start": 32, "char_end": 627, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Yu et al. (2019a)": "202565697"}, "Reference": {}}}]}
{"id": "202540793_2", "paragraph": "[BOS] Relatively less effort has been devoted to context-dependent semantic parsing on datasets including ATIS (Hemphill et al., 1990; Dahl et al., 1994b) , SpaceBook (Vlachos and Clark, 2014) , SCONE (Long et al., 2016; Guu et al., 2017; Fried et al., 2018; Huang et al., 2019) , SequentialQA (Iyyer et al., 2017) , SParC (Yu et al., 2019b) and CoSQL (Yu et al., 2019a) .\n[BOS] On ATIS, Miller et al. (1996) maps utterances to semantic frames which are then mapped to SQL queries; Zettlemoyer and Collins (2009) starts with context-independent Combinatory Categorial Grammar (CCG) parsing and then resolves references to generate lambda-calculus logical forms for sequences of sentences.\n[BOS] The most relevant to our work is , who generate ATIS SQL queries from interactions by incorporating history with an interaction-level encoder and copying segments of previously generated queries.\n[BOS] Furthermore, SCONE contains three domains using stack-or list-like elements and most queries include a single binary predicate.\n[BOS] Se-quentialQA is created by decomposing some complicated questions in WikiTableQuestions (Pasupat and Liang, 2015) .\n[BOS] Since both SCONE and Se-quentialQA are annotated with only denotations but not query labels, they don't include many questions with rich semantic and contextual types.\n[BOS] For example, SequentialQA (Iyyer et al., 2017) requires that the answer to follow-up questions must be a subset of previous answers, and most of the questions can be answered by simple SQL queries with SELECT and WHERE clauses.\n\n", "discourse_tags": ["Narrative_cite", "Multi_summ", "Reflection", "Transition", "Narrative_cite", "Transition", "Single_summ"], "span_citation_mapping": [{"token_start": 17, "token_end": 37, "char_start": 106, "char_end": 154, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hemphill et al., 1990;": "1094063", "Dahl et al., 1994b)": "8180378"}}}, {"token_start": 38, "token_end": 49, "char_start": 157, "char_end": 192, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vlachos and Clark, 2014)": null}}}, {"token_start": 50, "token_end": 83, "char_start": 195, "char_end": 278, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Long et al., 2016;": "8202418", "Guu et al., 2017;": "9268430", "Fried et al., 2018;": "21015570", "Huang et al., 2019)": "53113561"}}}, {"token_start": 84, "token_end": 95, "char_start": 281, "char_end": 314, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Iyyer et al., 2017)": "2623009"}}}, {"token_start": 96, "token_end": 108, "char_start": 317, "char_end": 341, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yu et al., 2019b)": null}}}, {"token_start": 109, "token_end": 121, "char_start": 346, "char_end": 370, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yu et al., 2019a)": "202565697"}}}, {"token_start": 124, "token_end": 146, "char_start": 382, "char_end": 480, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Miller et al. (1996)": "10983275"}, "Reference": {}}}, {"token_start": 147, "token_end": 189, "char_start": 482, "char_end": 688, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zettlemoyer and Collins (2009)": "1950452"}, "Reference": {}}}, {"token_start": 264, "token_end": 277, "char_start": 1101, "char_end": 1145, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Pasupat and Liang, 2015)": "9027681"}}}, {"token_start": 319, "token_end": 365, "char_start": 1341, "char_end": 1555, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Iyyer et al., 2017)": "2623009"}, "Reference": {}}}]}
{"id": "202540793_1", "paragraph": "[BOS] Recently, large and cross-domain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) have received an increasing amount of attention as many data-driven neural approaches achieve promising results (Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Guo et al., 2018; Shi et al., 2018) .\n[BOS] Most of them still focus on context-independent semantic parsing by converting single-turn questions into executable queries.\n\n", "discourse_tags": ["Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 17, "token_end": 28, "char_start": 68, "char_end": 96, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhong et al., 2017)": "25156106"}}}, {"token_start": 29, "token_end": 39, "char_start": 101, "char_end": 126, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yu et al., 2018c)": null}}}, {"token_start": 48, "token_end": 132, "char_start": 183, "char_end": 442, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dong and Lapata, 2016;": "15412473", "Su and Yan, 2017;": "2439226", "Iyer et al., 2017;": "497108", "Xu et al., 2017;": "10746949", "Finegan-Dollak et al., 2018;": "49417344", "Yu et al., 2018a;": "13748720", "Huang et al., 2018;": "3740285", "Dong and Lapata, 2018;": "44167998", "Guo et al., 2018;": "52054725", "Shi et al., 2018)": "52271710"}}}]}
{"id": "202540793_0", "paragraph": "[BOS] Semantic parsing is the task of mapping natural language sentences into formal representations.\n[BOS] It has been studied for decades including using linguistically-motivated compositional representations, such as logical forms (Zelle and Mooney, 1996; Clarke et al., 2010) and lambda calculus (Zettlemoyer and Collins, 2005; Artzi and Zettlemoyer, 2011) , and using executable programs, such as SQL queries (Miller et al., 1996; Zhong et al., 2017) and other general-purpose programming languages (Yin and Neubig, 2017; .\n[BOS] Most of the early studies worked on a few domains and small datasets such as GeoQuery (Zelle and Mooney, 1996) and Overnight (Wang et al., 2015) .\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 34, "token_end": 52, "char_start": 220, "char_end": 279, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zelle and Mooney, 1996;": "263135", "Clarke et al., 2010)": "5667590"}}}, {"token_start": 53, "token_end": 77, "char_start": 284, "char_end": 360, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zettlemoyer and Collins, 2005;": "449252", "Artzi and Zettlemoyer, 2011)": "1140108"}}}, {"token_start": 85, "token_end": 102, "char_start": 402, "char_end": 455, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Miller et al., 1996;": "10983275", "Zhong et al., 2017)": "25156106"}}}, {"token_start": 104, "token_end": 116, "char_start": 466, "char_end": 525, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 134, "token_end": 145, "char_start": 612, "char_end": 645, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zelle and Mooney, 1996)": "263135"}}}, {"token_start": 146, "token_end": 155, "char_start": 650, "char_end": 679, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wang et al., 2015)": "14472576"}}}]}
{"id": "201624879_1", "paragraph": "[BOS] We also extend the Transformer model with a simple document representation which we assume provides for a domain signal.\n[BOS] This could be useful for domain disambiguation and improved coherence and cohesion.\n[BOS] This model is similar to previous work on domain adaptation for NMT (Kobus et al., 2017; Tars and Fishel, 2018) where special domain tokens are either added to the beginning of the sentence or concatenated as additional features to the token-level embeddings.\n[BOS] However, they assume a set of known domains in advance which is not the case in our work.\n[BOS] We model the domain implicitly.\n\n", "discourse_tags": ["Reflection", "Reflection", "Multi_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 38, "token_end": 93, "char_start": 223, "char_end": 482, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Kobus et al., 2017;": "7497218", "Tars and Fishel, 2018)": null}, "Reference": {}}}]}
{"id": "201624879_0", "paragraph": "[BOS] There are large number of works in NMT focusing on integrating document-level information into otherwise sentence-level models Wang et al., 2017; Tiedemann and Scherrer, 2017; Bawden et al., 2018; Voita et al., 2018; Stojanovski and Fraser, 2018; Miculicich et al., 2018; Tu et al., 2018; Maruf and Haffari, 2018) .\n[BOS] These works have shown that improvements in pronoun translation are achieved by better handling coreference resolution.\n[BOS] Smaller improvements are observed for coherence and cohesion.\n[BOS] The main intuition behind the models in these works is that they employ an additional encoder for contextual sentences and integrate the information in the encoder or decoder using a gating mechanism.\n[BOS] Our model is similar to the context-aware Transformer models proposed in these works with some specifics which we discuss in Section 3.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 13, "token_end": 93, "char_start": 57, "char_end": 319, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Wang et al., 2017;": "9768369", "Tiedemann and Scherrer, 2017;": "2496355", "Bawden et al., 2018;": "5016370", "Voita et al., 2018;": "44062236", "Stojanovski and Fraser, 2018;": "53222571", "Miculicich et al., 2018;": "52044834", "Tu et al., 2018;": "7421176", "Maruf and Haffari, 2018)": "21686013"}}}]}
{"id": "118683718_7", "paragraph": "[BOS] In this paper, we focus on Task A exclusively, for both English and Spanish.\n[BOS] We participated in the competition using the team name UTFPR.\n\n", "discourse_tags": ["Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "118683718_6", "paragraph": "[BOS]  Sub-task B: Correctly predicting all three of the aforementioned labels.\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "118683718_5", "paragraph": "[BOS]  Sub-task A: Judging whether or not a tweet is hateful.\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "118683718_4", "paragraph": "[BOS] The training, development, trial, and test sets provided for English are composed of 9,000, 1,000, 100 and 3,000 instances, respectively.\n[BOS] The training, development, trial and test sets provided for Spanish are composed of 4,500, 500, 100 and 1,600 instances, respectively.\n[BOS] Each instance is composed of a tweet and three binary labels: One that indicates whether or not hate speech is featured in the tweet, one indicating whether the hate speech targets a group or an individual, and another indicating whether or not the author of the tweet is aggressive.\n[BOS] HatEval has 2 sub-tasks:\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Transition"], "span_citation_mapping": []}
{"id": "118683718_3", "paragraph": "[BOS] ten in both English and Spanish.\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "118683718_2", "paragraph": "[BOS] The two SemEval-2019 shared tasks, HatEval and OffensEval, both include a sub-task on target identification as discussed in Waseem et al. (2017) .\n[BOS] HatEval includes the target annotation in its sub-task B with two classes (individual or group) whereas OffensEval includes it in its subtask C with three classes (individual, group or others).\n[BOS] Another important similarity between these two tasks is that both include a more basic binary classification task in sub-task A.\n[BOS] In HatEval, posts are labeled as as to whether they contain hate speech or not and in OffensEval, posts are labeled as being offensive or not.\n[BOS] As OffensEval considers multiple types of offensive contents, the hierarchical annotation model used to annotate OLID (Zampieri et al., 2019a) , the dataset used in OffensEval, includes an annotation level distinguishing between the type of offensive content that posts include with two classes: insults and threats, and general profanity.\n[BOS] This type annotation is used in OffensEval's sub-task B.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Transition", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 22, "token_end": 42, "char_start": 70, "char_end": 150, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Waseem et al. (2017)": "8821211"}}}, {"token_start": 168, "token_end": 221, "char_start": 756, "char_end": 982, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Zampieri et al., 2019a)": "67856299"}, "Reference": {}}}]}
{"id": "118683718_1", "paragraph": "[BOS] Along with the recent studies published, there have been a few related shared tasks organized on the topic.\n[BOS] These include GermEval (Wiegand et al., 2018) for German, TRAC (Kumar et al., 2018) for English and Hindi, and OffensEval 1 (Zampieri et al., 2019b) for English.\n[BOS] The latter is also organized within the scope of SemEval-2019.\n[BOS] OffensEval considers offensive language in general whereas HatEval focuses on hate speech.\n[BOS] Waseem et al. (2017) proposes a typology of abusive language detection sub-tasks taking two factors into account: the target of the message and whether the content is explicit or implicit.\n[BOS] Considering that hate speech is commonly understood as speech attacking a group based on ethnicity, religion, etc, and that cyber bulling, for example, is understood as an attack towards an individual, the target factor plays an important role in the identification and the definition of hate speech when compared to other forms of abusive content.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Transition", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 25, "token_end": 40, "char_start": 134, "char_end": 176, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 41, "token_end": 55, "char_start": 178, "char_end": 225, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kumar et al., 2018)": "59336626"}}}, {"token_start": 57, "token_end": 76, "char_start": 231, "char_end": 281, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zampieri et al., 2019b)": "84843035"}}}, {"token_start": 114, "token_end": 155, "char_start": 454, "char_end": 642, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Waseem et al. (2017)": "8821211"}, "Reference": {}}}]}
{"id": "118683718_0", "paragraph": "[BOS] As evidenced in the introduction of this paper, there have been a number of studies on automatic hate speech identification published in the last few years.\n[BOS] One of the most influential recent papers on hate speech identification is the one by .\n[BOS] In this paper, the authors presented the Hate Speech Detection dataset which contains posts retrieved from social media labeled with three categories: OK (posts not containing profanity or hate speech), Offensive (posts containing swear words and general profanity), and Hate (posts containing hate speech).\n[BOS] It has been noted in , and in other works , that training models to discriminate between general profanity and hate speech is far from trivial due to, for example, the fact that a significant percentage of hate speech posts contain swear words.\n[BOS] It has been ar-gued that annotating texts with respect to the presence of hate speech has an intrinsic degree of subjectivity .\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Transition", "Transition"], "span_citation_mapping": []}
{"id": "208010268_4", "paragraph": "[BOS] In the present paper, we further investigate the problem of abstractive dialogue summarization.\n[BOS] With the growing popularity of online conversations via applications like Messenger, What-sApp and WeChat, summarization of chats between a few participants is a new interesting direction of summarization research.\n[BOS] For this purpose we have created the SAMSum Corpus 1 which contains over 16k chat dialogues with manually annotated summaries.\n[BOS] The dataset is freely available for the research community 2 .\n\n", "discourse_tags": ["Reflection", "Transition", "Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "208010268_3", "paragraph": "[BOS] To benefit from large news corpora, Ganesh and Dingliwal (2019) built a dialogue summarization model that first converts a conversation into a structured text document and later applies an attention-based pointer network to create an abstractive summary.\n[BOS] Their model, trained on structured text documents of CNN/Daily Mail dataset, was evaluated on the Argumentative Dialogue Summary Corpus (Misra et al., 2015) , which, however, contains only 45 dialogues.\n\n", "discourse_tags": ["Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 9, "token_end": 96, "char_start": 42, "char_end": 469, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ganesh and Dingliwal (2019)": "59604445"}, "Reference": {"(Misra et al., 2015)": "5973442"}}}]}
{"id": "208010268_2", "paragraph": "[BOS] The challenges posed by the abstractive dialogue summarization task have been discussed in the literature with regard to AMI meeting corpus (McCowan et al., 2005) , e.g. Banerjee et al. (2015) , Mehdad et al. (2014) , Goo and Chen (2018) .\n[BOS] Since the corpus has a low number of summaries (for 141 dialogues), Goo and Chen (2018) proposed to use assigned topic descriptions as gold references.\n[BOS] These are short, label-like goals of the meeting, e.g., costing evaluation of project process; components, materials and energy sources; chitchat.\n[BOS] Such descriptions, however, are very general, lacking the messenger-like structure and any information about the speakers.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 22, "token_end": 67, "char_start": 127, "char_end": 243, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(McCowan et al., 2005)": "8156476", "Mehdad et al. (2014)": "15009101", "Goo and Chen (2018)": "52290192"}}}, {"token_start": 85, "token_end": 138, "char_start": 320, "char_end": 556, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Goo and Chen (2018)": "52290192"}, "Reference": {}}}]}
{"id": "208010268_1", "paragraph": "[BOS] Major research efforts have focused so far on summarization of single-speaker documents like news (e.g., Nallapati et al. (2016) ) or scientific publications (e.g., Nikolov et al. (2018) ).\n[BOS] One of the reasons is the availability of large, high-quality news datasets with annotated summaries, e.g., CNN/Daily Mail (Hermann et al., 2015; Nallapati et al., 2016) .\n[BOS] Such a comprehensive dataset for dialogues is lacking.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 18, "token_end": 34, "char_start": 99, "char_end": 134, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Nallapati et al. (2016)": "8928715"}}}, {"token_start": 36, "token_end": 53, "char_start": 140, "char_end": 192, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Nikolov et al. (2018)": "13746087"}}}, {"token_start": 80, "token_end": 102, "char_start": 310, "char_end": 371, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hermann et al., 2015;": "6203757", "Nallapati et al., 2016)": "8928715"}}}]}
{"id": "208010268_0", "paragraph": "[BOS] The goal of the summarization task is condensing a piece of text into a shorter version that covers the main points succinctly.\n[BOS] In the abstractive approach important pieces of information are presented using words and phrases not necessarily appearing in the source text.\n[BOS] This requires natural language generation techniques with high level of semantic understanding (Chopra et al., 2016; Rush et al., 2015; Khandelwal et al., 2019; Zhang et al., 2019; See et al., 2017; Chen and Bansal, 2018; Gehrmann et al., 2018) .\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 61, "token_end": 124, "char_start": 348, "char_end": 534, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chopra et al., 2016;": "133195", "Rush et al., 2015;": "1918428", "Khandelwal et al., 2019;": "162169061", "Zhang et al., 2019;": "67855893", "See et al., 2017;": null, "Chen and Bansal, 2018;": "44129061", "Gehrmann et al., 2018)": "52144157"}}}]}
{"id": "201070518_2", "paragraph": "[BOS] SyntaxSQLNet (Yu et al., 2018b) is the first and state-of-the-art model for the Spider (Yu et al., 2018c) , a complex and cross-domain text-to-SQL task.\n[BOS] They proposed an SQL specific syntax tree-based decoder with SQL generation history.\n[BOS] Our approach differs from their model in the following aspects.\n[BOS] First, taking into account that SQL corresponds to non-procedural language, we develop a clause-specific decoder for each SQL clause, where SyntaxSQLNet predicts SQL tokens sequentially.\n[BOS] For example, in SyntaxSQL-Net, a single column prediction module works both in the SELECT and WHERE clauses, depending on the SQL decoding history.\n[BOS] In contrast, we define and train decoding modules separately for each SQL clause to fully utilize clausedependent context.\n[BOS] Second, we apply sequenceto-sequence architecture to predict columns instead of using the sequence-to-set framework from SyntaxSQLNet, because correct ordering is essential for the GROUP BY and ORDER BY clauses.\n[BOS] Finally, we introduce a self-attention mechanism (Lin et al., 2017) to efficiently encode database schema, which includes multiple tables.\n\n", "discourse_tags": ["Multi_summ", "Multi_summ", "Reflection", "Reflection", "Reflection", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 69, "char_start": 6, "char_end": 249, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Yu et al., 2018b)": "52979524", "(Yu et al., 2018c)": "52815560"}, "Reference": {}}}, {"token_start": 221, "token_end": 233, "char_start": 1044, "char_end": 1087, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lin et al., 2017)": "15280949"}}}]}
{"id": "201070518_1", "paragraph": "[BOS] For text-to-SQL generation, several SQLspecific approaches have been proposed (Zhong et al., 2017; Xu et al., 2017; Huang et al., 2018; Yu et al., 2018a; Dong and Lapata, 2018; Yavuz et al., 2018) based on WikiSQL dataset (Zhong et al., 2017) .\n[BOS] However, all of them are limited to the specific WikiSQL SQL sketch, which only supports very simple queries.\n[BOS] It includes only the SELECT and WHERE clauses, only a single expression in the SELECT clause, and works only for a single table.\n[BOS] To predict more complex SQL queries, sequence-to-sequence (Iyer et al., 2017; Finegan-Dollak et al., 2018) and template-based (Finegan-Dollak et al., 2018; Lee et al., 2019) approaches have been proposed.\n[BOS] However, they focused only on specific databases such as ATIS (Price, 1990) and GeoQuery (Zelle and Mooney, 1996) .\n[BOS] Because they only considered question and SQL pairs without requiring an understanding of database schema, their approaches cannot generalize to unseen databases.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Narrative_cite", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 10, "token_end": 63, "char_start": 34, "char_end": 202, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhong et al., 2017;": "25156106", "Xu et al., 2017;": "10746949", "Huang et al., 2018;": "3740285", "Yu et al., 2018a;": "13748720", "Dong and Lapata, 2018;": "44167998", "Yavuz et al., 2018)": "53604363"}}}, {"token_start": 65, "token_end": 77, "char_start": 212, "char_end": 248, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhong et al., 2017)": "25156106"}}}, {"token_start": 137, "token_end": 162, "char_start": 545, "char_end": 614, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Iyer et al., 2017;": "497108", "Finegan-Dollak et al., 2018)": "49417344"}}}, {"token_start": 163, "token_end": 186, "char_start": 619, "char_end": 681, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Finegan-Dollak et al., 2018;": "49417344", "Lee et al., 2019)": "167217555"}}}, {"token_start": 202, "token_end": 209, "char_start": 776, "char_end": 794, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Price, 1990)": "3047811"}}}, {"token_start": 210, "token_end": 221, "char_start": 799, "char_end": 832, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zelle and Mooney, 1996)": "263135"}}}]}
{"id": "201070518_0", "paragraph": "[BOS] Our work is related to the grammar-based constrained decoding approaches for semantic parsing (Yin and Neubig, 2017; Rabinovich et al., 2017; Iyer et al., 2018) .\n[BOS] While their approaches are focused on general purpose code generation, we instead focus on SQL-specific grammar to address the text-to-SQL task.\n[BOS] Our task differs from code generation in two aspects.\n[BOS] First, it takes a database schema as an input in addition to natural language.\n[BOS] To predict SQL correctly, a model should fully understand the relationship between the question and the schema.\n[BOS] Second, as SQL is a non-procedural language, predictions of SQL clauses do not need to be done sequentially.\n\n", "discourse_tags": ["Narrative_cite", "Reflection", "Reflection", "Reflection", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 8, "token_end": 43, "char_start": 33, "char_end": 166, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yin and Neubig, 2017;": "12718048", "Rabinovich et al., 2017;": "13529592", "Iyer et al., 2018)": "52125417"}}}]}
{"id": "201670757_0", "paragraph": "[BOS] Text Style Transfer.\n[BOS] Text style transfer using neural networks has been widely studied in the past few years.\n[BOS] A common paradigm is to first disentangle latent space as content and style features, and then generate stylistic sentences by tweaking the style-relevant features and passing through a decoder.\n[BOS] Hu et al. (2017); Fu et al. (2018) ; ; ; Gong et al. (2019) ; Lin et al. (2017) explored this direction by assuming the disentanglement can be achieved in an auto-encoding procedure with a suitable style regularization, implemented by either adversarial discriminators or style classifiers.\n[BOS] ; Xu et al. (2018) ; Zhang et al. (2018c) achieved disentanglement by filtering the stylistic words of input sentences.\n[BOS] Recently, Prabhumoye et al. (2018) has proposed to use back-translation for text style transfer with a de-noising auto-encoding objective (Logeswaran et al., 2018; Subramanian et al., 2018) .\n[BOS] Our work differs from the above in that we leverage domain adaptation to deal with limited target domain data, whereas previous methods require massive target domain style-labelled samples.\n[BOS] Domain Adaptation.\n[BOS] Domain adaptation has been studied in various natural language processing tasks, such as sentiment classification (Qu et al., 2019) , dialogue system (Wen et al., 2016) , abstractive summarization (Hua and Wang, 2017; Zhang et al., 2018b) , machine translation (Koehn and Schroeder, 2007; Axelrod et al., 2011; Sennrich et al., 2016b; Michel and Neubig, 2018) , etc.\n[BOS] However, no work has been done for exploring domain adaptation on text style transfer.\n[BOS] To our best knowledge, we are the first to explore the adaptation of text style transfer models for a new domain with limited non-parallel data available.\n[BOS] The task requires both style transfer and domain-specific generation on the target domain.\n[BOS] To differentiate different domains, Sennrich et al. (2016a) ; Chu et al. (2017) appended domain tokens to the input sentences.\n[BOS] Our model uses learnable domain vectors combining domain-specific style classifiers, which force the model to learn distinct stylized information in each domain.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Multi_summ", "Multi_summ", "Single_summ", "Reflection", "Transition", "Narrative_cite", "Transition", "Reflection", "Reflection", "Multi_summ", "Reflection"], "span_citation_mapping": [{"token_start": 62, "token_end": 133, "char_start": 329, "char_end": 619, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hu et al. (2017);": "20981275", "Fu et al. (2018)": "6484065", "Gong et al. (2019)": "85517799", "Lin et al. (2017)": "4857922"}, "Reference": {}}}, {"token_start": 135, "token_end": 166, "char_start": 628, "char_end": 745, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Xu et al. (2018)": "46889991", "Zhang et al. (2018c)": "52068673"}, "Reference": {}}}, {"token_start": 169, "token_end": 221, "char_start": 762, "char_end": 941, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(2018)": null}, "Reference": {"(Logeswaran et al., 2018;": "53217784"}}}, {"token_start": 274, "token_end": 285, "char_start": 1260, "char_end": 1302, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Qu et al., 2019)": "174799742"}}}, {"token_start": 286, "token_end": 296, "char_start": 1305, "char_end": 1339, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wen et al., 2016)": "6508854"}}}, {"token_start": 297, "token_end": 317, "char_start": 1342, "char_end": 1409, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hua and Wang, 2017;": "9508540", "Zhang et al., 2018b)": "4709462"}}}, {"token_start": 318, "token_end": 357, "char_start": 1412, "char_end": 1530, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Koehn and Schroeder, 2007;": "9536363", "Axelrod et al., 2011;": "10766958", "Sennrich et al., 2016b;": "15600925", "Michel and Neubig, 2018)": "19247366"}}}, {"token_start": 431, "token_end": 458, "char_start": 1931, "char_end": 2021, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Sennrich et al. (2016a)": "845121", "Chu et al. (2017)": "15201884"}, "Reference": {}}}]}
{"id": "202776138_5", "paragraph": "[BOS] (1) Tan et al. (2019) mainly focused on constructing a unified NMT model for multi-lingual translation task, while we aim at how to effectively transfer out-of-domain translation knowledge to indomain NMT model; (2) Our translation knowledge transfer is bidirectional, while the procedure of knowledge distillation in (Tan et al., 2019 ) is unidirectional; (3) When using knowledge distil-lation under our framework, we iteratively update teacher models for better domain adaptation.\n[BOS] In contrast, all language-specific teacher NMT models in (Tan et al., 2019) remain fixed.\n\n", "discourse_tags": ["Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 5, "token_end": 134, "char_start": 10, "char_end": 585, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Tan et al. (2019)": "67856276"}, "Reference": {"(Tan et al., 2019": "67856276", "(Tan et al., 2019)": "67856276"}}}]}
{"id": "202776138_4", "paragraph": "[BOS] Finally, note that similar to our work, Tan et al. (2019) introduced knowledge distillation into multilingual NMT.\n[BOS] However, our work is still different from (Tan et al., 2019) in the following aspects:\n\n", "discourse_tags": ["Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 11, "token_end": 30, "char_start": 46, "char_end": 120, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Tan et al. (2019)": "67856276"}, "Reference": {}}}, {"token_start": 33, "token_end": 52, "char_start": 136, "char_end": 212, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Tan et al., 2019)": "67856276"}}}]}
{"id": "202776138_3", "paragraph": "[BOS] end if 14: end for of multi-domain NMT, which focus on building a unified NMT model trained on the mixed-domain training corpus for translation tasks in all domains (Kobus et al., 2016; Tars and Fishel, 2018; Farajian et al., 2017; Pryzant et al., 2017; Sajjad et al., 2017; Bapna and Firat, 2019) .\n[BOS] Although our framework is also able to refine outof-domain NMT model, it is still significantly different from multi-domain NMT, since only the performance of in-domain NMT model is considered.\n\n", "discourse_tags": ["Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 20, "token_end": 91, "char_start": 72, "char_end": 303, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kobus et al., 2016;": "7497218", "Tars and Fishel, 2018;": "19173844", "Farajian et al., 2017;": "22006749", "Pryzant et al., 2017;": "30042437", "Sajjad et al., 2017;": "35167387", "Bapna and Firat, 2019)": "67855706"}}}]}
{"id": "202776138_2", "paragraph": "[BOS] Besides, our work is also related to the studies Algorithm 1 Iterative Dual Domain Adaptation for NMT\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "202776138_1", "paragraph": "[BOS] Significantly different from the above methods, along with the studies of dual learning for NMT (He et al., 2016; Zhang et al., 2019b) , we iteratively perform bidirectional translation knowledge transfer between in-domain and out-of-domain training corpora.\n[BOS] To the best of our knowledge, our work is the first attempt to explore such a dual learning based framework for NMT domain adaptation.\n[BOS] Furthermore, we extend our framework to the scenario of multiple out-of-domain corpora.\n[BOS] Particularly, we introduce knowledge distillation into the domain adaptation for NMT and experimental results demonstrate its effectiveness, echoing its successful applications on many tasks, such as speech recognition (Hinton et al., 2015) and natural language processing (Kim and Rush, 2016; Tan et al., 2019) .\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 17, "token_end": 36, "char_start": 98, "char_end": 140, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(He et al., 2016;": "5758868", "Zhang et al., 2019b)": "51971914"}}}, {"token_start": 138, "token_end": 149, "char_start": 706, "char_end": 746, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hinton et al., 2015)": "7200347"}}}, {"token_start": 150, "token_end": 169, "char_start": 751, "char_end": 817, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kim and Rush, 2016;": null, "Tan et al., 2019)": "67856276"}}}]}
{"id": "202776138_0", "paragraph": "[BOS] Our work is obviously related to the research on transferring the out-of-domain translation knowledge into the in-domain NMT model.\n[BOS] In this aspect, fine-tuning (Luong and Manning, 2015; Zoph et al., 2016; Servan et al., 2016) is the most popular approach, where the NMT model is first trained using the out-of-domain training corpus, and then fine-tuned on the in-domain training corpus.\n[BOS] To avoid overfitting, Chu et al. (2017) blended in-domain with out-of-domain corpora to fine-tune the pre-trained model, and Freitag and Al-Onaizan (2016) combined the fine-tuned model with the baseline via ensemble method.\n[BOS] Meanwhile, applying data weighting into NMT domain adaptation has attracted much attention.\n[BOS] Wang et al. (2017a) and Wang et al. (2017b) proposed several sentence and domain weighting methods with a dynamic weight learning strategy.\n[BOS] Zhang et al. (2019a) ranked unlabeled domain training samples based on their similarity to in-domain data, and then adopts a probabilistic curriculum learning strategy during training.\n[BOS] applied the sentence-level cost weighting to refine the training of NMT model.\n[BOS] Recently, Vilar (2018) introduced a weight to each hidden unit of out-of-domain model.\n[BOS] Chu and Wang (2018) gave a comprehensive survey of the dominant domain adaptation techniques for NMT.\n[BOS] Gu et al. (2019) not only maintained a private encoder and a private decoder for each domain, but also introduced a common encoder and a common decoder shared by all domains.\n\n", "discourse_tags": ["Reflection", "Multi_summ", "Multi_summ", "Transition", "Multi_summ", "Single_summ", "Transition", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 34, "token_end": 62, "char_start": 160, "char_end": 237, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Luong and Manning, 2015;": null, "Zoph et al., 2016;": "16631020", "Servan et al., 2016)": "7395053"}}}, {"token_start": 105, "token_end": 132, "char_start": 428, "char_end": 525, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Chu et al. (2017)": "35273027"}, "Reference": {}}}, {"token_start": 134, "token_end": 160, "char_start": 531, "char_end": 629, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Freitag and Al-Onaizan (2016)": "1236010"}, "Reference": {}}}, {"token_start": 177, "token_end": 208, "char_start": 734, "char_end": 873, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wang et al. (2017a)": "9990193", "Wang et al. (2017b)": "1054586"}, "Reference": {}}}, {"token_start": 209, "token_end": 244, "char_start": 880, "char_end": 1064, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhang et al. (2019a)": "155089817"}, "Reference": {}}}, {"token_start": 264, "token_end": 284, "char_start": 1166, "char_end": 1242, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 285, "token_end": 305, "char_start": 1249, "char_end": 1350, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Chu and Wang (2018)": "44157913"}, "Reference": {}}}, {"token_start": 306, "token_end": 343, "char_start": 1357, "char_end": 1531, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Gu et al. (2019)": "102350939"}, "Reference": {}}}]}
{"id": "174798390_4", "paragraph": "[BOS] Our work is inspired by two recent models: one is proposed to progressively mine discriminative object regions using classification networks to address the weakly-supervised semantic segmentation problems, and the other one is (Xu et al., 2018) where a dropout method integrating with global information is presented to encourage the model to mine inapparent features or patterns for text classification.\n[BOS] To the best of our knowledge, our work is the first one to explore automatic mining of attention supervision information for ASC.\n\n", "discourse_tags": ["Multi_summ", "Reflection"], "span_citation_mapping": [{"token_start": 38, "token_end": 72, "char_start": 233, "char_end": 410, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Xu et al., 2018)": "52093134"}, "Reference": {}}}]}
{"id": "174798390_3", "paragraph": "[BOS] Different from these work, our work is in line with the studies of introducing attention supervision to refine the attention mechanism, which have become hot research topics in several NNbased NLP tasks, such as event detection , machine translation (Liu et al., 2016) , and police killing detection (Nguyen and Nguyen, 2018) .\n[BOS] However, such supervised attention acquisition is labor-intense.\n[BOS] Therefore, we mainly commits to automatic mining supervision information for attention mechanisms of neural ASC models.\n[BOS] Theoretically, our approach is orthogonal to these models, and we leave the adaptation of our approach into these models as future work.\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 44, "token_end": 54, "char_start": 236, "char_end": 274, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Liu et al., 2016)": "13292366"}}}, {"token_start": 56, "token_end": 66, "char_start": 281, "char_end": 331, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Nguyen and Nguyen, 2018)": "49670531"}}}]}
{"id": "174798390_2", "paragraph": "[BOS] Following this trend, researchers have resorted to more sophisticated attention mechanisms to refine neural ASC models.\n[BOS] proposed a multiple-attention mechanism to capture sentiment features separated by a long distance, so that it is more robust against irrelevant information.\n[BOS] An interactive attention network has been designed by Ma et al., (2017) for ASC, where two attention networks were introduced to model the target and context interactively.\n[BOS] proposed to leverage multiple attentions for ASC: one obtained from the left context and the other one acquired from the right context of a given aspect.\n[BOS] Very recently, transformation-based model has also been explored for ASC , and the attention mechanism is replaced by CNN.\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 49, "token_end": 83, "char_start": 296, "char_end": 468, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ma et al., (2017)": "20407824"}, "Reference": {}}}]}
{"id": "174798390_1", "paragraph": "[BOS] Another prevailing neural model is LSTM that also involves an attention mechanism to explicitly capture the importance of each context word (Wang et al., 2016) .\n[BOS] Overall, attention mechanisms play crucial roles in all these models.\n\n", "discourse_tags": ["Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 2, "token_end": 32, "char_start": 6, "char_end": 165, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Wang et al., 2016)": null}, "Reference": {}}}]}
{"id": "174798390_0", "paragraph": "[BOS] Recently, neural models have been shown to be successful on ASC.\n[BOS] For example, due to its multiple advantages, such as being simpler and faster, MNs with attention mechanisms (Tang et al., 2016b; Wang et al., 2018) have been widely used.\n\n", "discourse_tags": ["Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 27, "token_end": 53, "char_start": 130, "char_end": 225, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Tang et al., 2016b;": "359042", "Wang et al., 2018)": "51879362"}}}]}
{"id": "202771124_2", "paragraph": "[BOS] The reading comprehension setup of CQA provides a controlled environment where the main source of difficulty is interpreting a question in its context.\n[BOS] The interactive component of CQA also provides a natural mechanism for improving rewriting.\n[BOS] When the computer cannot understand (rewrite) a question because of complicated context, missing world knowledge, or upstream errors (Peskov et al., 2019) in the course of a conversation, it should be able to ask its interlocutor, \"can you unpack that?\"\n[BOS] This dataset helps start that conversation; the next steps are developing and evaluating models that efficiently decide when to ask for human assistance, and how to best use this assistance.\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 45, "token_end": 80, "char_start": 267, "char_end": 433, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Peskov et al., 2019)": "199501690"}}}]}
{"id": "202771124_1", "paragraph": "[BOS] More broadly, we hope CANARD can drive human-computer collaboration in QA .\n[BOS] While questions typically vary in difficulty (Sugawara et al., 2018) , existing research either introduces new benchmarks of difficult (adversarial) stand-alone questions (Dua et al., 2019; Wallace et al., 2019, inter alia) , or models that simplify hard questions through paraphrasing (Dong et al., 2017) or decomposition (Talmor and Berant, 2018) .\n[BOS] We aim at studying QA models that can ask for human assistance (feedback) when they struggle to answer a question.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 22, "token_end": 35, "char_start": 114, "char_end": 156, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sugawara et al., 2018)": "52113519"}}}, {"token_start": 42, "token_end": 74, "char_start": 210, "char_end": 311, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dua et al., 2019;": "67855846"}}}, {"token_start": 78, "token_end": 93, "char_start": 329, "char_end": 393, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dong et al., 2017)": "1282002"}}}, {"token_start": 94, "token_end": 104, "char_start": 397, "char_end": 436, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Talmor and Berant, 2018)": "3986974"}}}]}
{"id": "202771124_0", "paragraph": "[BOS] Recent work in CQA has used simple concatenation (Elgohary et al., 2018) , sequential neural models (Huang et al., 2019) , and transformers (Qu et al., 2019a) for modeling the interaction between the conversation history, the question and reference documents.\n[BOS] Some of the components in those models, such as relevant history turn selection (Qu et al., 2019b) , can be adopted in question rewriting models for our task.\n[BOS] An interesting avenue for future work is to incorporate deeper context, either from other modalities (Das et al., 2017) or from other dialog comprehension tasks .\n[BOS] Parallel to our work, Rastogi et al. (2019) and Su et al. (2019) introduce utterance rewriting datasets for dialog state tracking.\n[BOS] Rastogi et al. (2019) covers a narrow set of domains and the rewrites of Su et al. (2019) are based on Chinese dialog with two-turn fixed histories.\n[BOS] In contrast, CANARD has histories of variable turn lengths, covers wider topics, and is based on CQA.\n[BOS] Training question rewriting using reinforcement learning with the task accuracy as reward signal is explored in retrieval-based QA (Liu et al., 2019) and in MRC (Buck et al., 2018) .\n[BOS] A natural question is whether reinforcement learning could learn to retain the necessary context to rewrite questions in CQA.\n[BOS] However, our dataset could be used to pretrain a question rewriter that can further be refined using reinforcement learning.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Narrative_cite", "Multi_summ", "Single_summ", "Transition", "Narrative_cite", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 9, "token_end": 22, "char_start": 34, "char_end": 78, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Elgohary et al., 2018)": null}}}, {"token_start": 23, "token_end": 35, "char_start": 81, "char_end": 126, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Huang et al., 2019)": "53113561"}}}, {"token_start": 37, "token_end": 48, "char_start": 133, "char_end": 164, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 74, "token_end": 88, "char_start": 320, "char_end": 370, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Qu et al., 2019b)": "201669002"}}}, {"token_start": 116, "token_end": 125, "char_start": 527, "char_end": 556, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Das et al., 2017)": "64711781"}}}, {"token_start": 138, "token_end": 166, "char_start": 628, "char_end": 736, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Rastogi et al. (2019)": "76663609", "Su et al. (2019)": null}, "Reference": {}}}, {"token_start": 167, "token_end": 208, "char_start": 743, "char_end": 891, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Rastogi et al. (2019)": "76663609", "Su et al. (2019)": null}, "Reference": {}}}, {"token_start": 249, "token_end": 264, "char_start": 1118, "char_end": 1162, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Liu et al., 2019)": "199668978"}}}, {"token_start": 264, "token_end": 273, "char_start": 1163, "char_end": 1186, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Buck et al., 2018)": "3700344"}}}]}
{"id": "67855815_0", "paragraph": "[BOS] While the mentioned studies provide valuable contributions to improving multilingual models, they apply their models on only up to 7 languages (Johnson et al., 2017) and 20 trained directions (Cettolo et al., 2017 ) in a single model, whereas we focus on scaling NMT to much larger numbers of languages and trained directions.\n[BOS] Regarding massively multilingual models, Neubig and Hu (2018) explored methods for rapid adaptation of NMT to new languages by training multilingual models on the 59-language TED Talks corpus and fine-tuning them using data from the new languages.\n[BOS] While modeling significantly more languages than previous studies, they only train many-to-one models, which we show are inferior in comparison to our proposed massively multilingual many-to-many models when evaluated into English on this dataset.\n[BOS] Tiedemann (2018) trained an English-centric many-to-many model on translations of the bible including 927 languages.\n[BOS] While this work pointed to an interesting phenomena in the latent space learned by the model where it clusters representations of typologically-similar languages together, it did not include any evaluation of the produced translations.\n[BOS] Similarly, Malaviya et al. (2017) trained a many-to-English system including 1017 languages from bible translations, and used it to infer typological features for the different languages (without evaluating the translation quality).\n[BOS] In another relevant work, Artetxe and Schwenk (2018) trained an NMT model on 93 languages and used the learned representations to perform cross-lingual transfer learning.\n[BOS] Again, they did not report the performance of the translation model learned in that massively multilingual setting.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Reflection", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 23, "token_end": 33, "char_start": 137, "char_end": 171, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Johnson et al., 2017)": "6053988"}}}, {"token_start": 34, "token_end": 46, "char_start": 176, "char_end": 219, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cettolo et al., 2017": null}}}, {"token_start": 77, "token_end": 121, "char_start": 380, "char_end": 586, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Neubig and Hu (2018)": "51976920"}, "Reference": {}}}, {"token_start": 170, "token_end": 238, "char_start": 847, "char_end": 1205, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Tiedemann (2018)": "9098487"}, "Reference": {}}}, {"token_start": 241, "token_end": 288, "char_start": 1223, "char_end": 1444, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Malaviya et al. (2017)": "23943173"}, "Reference": {}}}, {"token_start": 294, "token_end": 347, "char_start": 1477, "char_end": 1743, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Artetxe and Schwenk (2018)": "56895585"}, "Reference": {}}}]}
{"id": "202718810_0", "paragraph": "[BOS] Temporal Relation Data.\n[BOS] Temporal relation corpora such as TimeBank (Pustejovsky et al., 2003) and RED (O'Gorman et al., 2016) facilitate the research in temporal relation extraction.\n[BOS] The common issue in these corpora is missing annotation.\n[BOS] Collecting densely annotated temporal relation corpora with all event pairs fully annotated has been reported to be a challenging task as annotators could easily overlook some pairs Bethard et al., 2007; .\n[BOS] TB-Dense dataset mitigates this issue by forcing annotators to examine all pairs of events within the same or neighboring sentences.\n[BOS] Recent data construction efforts such as MATRES (Ning et al., 2018a) and TCR (Ning et al., 2018b) further enhance the data quality by using a multiaxis annotation scheme and adopting start-point of events to improve inter-annotator agreements.\n[BOS] However, densely annotated datasets are relatively small both in terms of number of documents and event pairs, which restricts the complexity of machine learning models used in previous research.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Narrative_cite", "Transition", "Multi_summ", "Transition"], "span_citation_mapping": [{"token_start": 12, "token_end": 26, "char_start": 70, "char_end": 105, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Pustejovsky et al., 2003)": null}}}, {"token_start": 27, "token_end": 39, "char_start": 110, "char_end": 137, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(O'Gorman et al., 2016)": "15139323"}}}, {"token_start": 80, "token_end": 95, "char_start": 402, "char_end": 466, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 130, "token_end": 142, "char_start": 656, "char_end": 687, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ning et al., 2018a)": "51878335"}}}, {"token_start": 142, "token_end": 179, "char_start": 688, "char_end": 858, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Ning et al., 2018b)": null}, "Reference": {}}}]}
{"id": "207892291_2", "paragraph": "[BOS] More recently, work on this topic has focused on end-to-end generation models.\n[BOS] Konstas and Lapata (2012) described an end-to-end generation model which jointly models content selection and surface realization.\n[BOS] Mei et al. (2015) proposed a neural encoder-aligner-decoder model which first encodes the entire input record dataset then the aligner module performs the content selection for the decoder to generate output summary.\n[BOS] Some other work extends the encoder-decoder model to be able to copy words directly from the input (Yang et al., 2016; Gu et al., 2016; Gulcehre et al., 2016) .\n[BOS] Wiseman et al. (2017) investigates different data-to-text generation approaches and introduces a new corpus (ROTOWIRE, see Table 1) for the data-to-text generation task along with a series of automatic measures for the contentoriented evaluation.\n[BOS] Based on (Wiseman et al., 2017) , Puduppully et al. (2019) incorporates content selection and planing mechanisms into the encoder-decoder system and improves the stateof-the-art on the ROTOWIRE dataset.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Narrative_cite", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 21, "token_end": 48, "char_start": 91, "char_end": 221, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Konstas and Lapata (2012)": "15747255"}, "Reference": {}}}, {"token_start": 49, "token_end": 92, "char_start": 228, "char_end": 444, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Mei et al. (2015)": "1354459"}, "Reference": {}}}, {"token_start": 98, "token_end": 137, "char_start": 479, "char_end": 609, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yang et al., 2016;": "1899153", "Gu et al., 2016;": "8174613", "Gulcehre et al., 2016)": "969555"}}}, {"token_start": 139, "token_end": 192, "char_start": 618, "char_end": 864, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 193, "token_end": 246, "char_start": 871, "char_end": 1073, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Puduppully et al. (2019)": "52153976"}, "Reference": {}}}]}
{"id": "207892291_1", "paragraph": "[BOS] Traditional data-to-text generation approaches perform the summary generation in two separate steps: content selection and surface realization.\n[BOS] For content selection, a number of approaches were proposed to automatically select the elements of content and extract ordering constraints from an aligned corpus of input data and output summaries McKeown, 2001, 2003) .\n[BOS] In (Barzilay and Lapata, 2005) , the content selection is treated as a collective classification problem which allows the system to capture contextual dependencies between input data items.\n[BOS] For surface realization, Stent et al. (2004) proposed to transform the input data into an intermediary structure and then to generate natural language text from it; Reiter et al. (2005) presented a method to generate text using consistent data-to-word rules.\n[BOS] Angeli et al. (2010) broke up the two steps into a sequence of local decisions where they used two classifiers to select content form database and another classifier to choose a suitable template to render the content.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Single_summ", "Multi_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 37, "token_end": 65, "char_start": 219, "char_end": 375, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 67, "token_end": 102, "char_start": 384, "char_end": 573, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Barzilay and Lapata, 2005)": "1589010"}, "Reference": {}}}, {"token_start": 104, "token_end": 133, "char_start": 584, "char_end": 743, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Stent et al. (2004)": "1543141"}, "Reference": {}}}, {"token_start": 134, "token_end": 157, "char_start": 745, "char_end": 838, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Reiter et al. (2005)": "13461687"}, "Reference": {}}}, {"token_start": 158, "token_end": 201, "char_start": 845, "char_end": 1063, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Angeli et al. (2010)": "13402912"}, "Reference": {}}}]}
{"id": "207892291_0", "paragraph": "[BOS] Automatic summary generation has been a topic of interest for a long time (Reiter and Dale, 1997; Tanaka-Ishii et al., 1998) .\n[BOS] It has interesting applications in many different domains, such as sport game summary generation (Barzilay and Lapata, 2005; Liang et al., 2009) , weather-forecast generation (Reiter et al., 2005) and recipe generation (Yang et al., 2016) .\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 2, "token_end": 34, "char_start": 6, "char_end": 130, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Reiter and Dale, 1997;": "8460470", "Tanaka-Ishii et al., 1998)": "2488223"}}}, {"token_start": 47, "token_end": 68, "char_start": 206, "char_end": 283, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Barzilay and Lapata, 2005;": "1589010", "Liang et al., 2009)": "238873"}}}, {"token_start": 69, "token_end": 82, "char_start": 286, "char_end": 335, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Reiter et al., 2005)": "13461687"}}}, {"token_start": 83, "token_end": 94, "char_start": 340, "char_end": 377, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yang et al., 2016)": "1899153"}}}]}
{"id": "208246040_5", "paragraph": "[BOS] Similar to Rush et al. (2015) ; Chen and Bansal (2018) we abstract by simplifying our extracted sentences.\n[BOS] We jointly learn to paraphrase and compress, but different from existing models purely based on RNN, we implement a blend of two proven efficient models -transformer encoder and GRU-RNN.\n[BOS] paraphrased with a transformer-decoder, we find that using the GRU-RNN decoder but with a two-level stack of hybrid encoders (transformer and GRU-RNN) gives better performance.\n[BOS] To the best of our knowledge, this architectural blend is novel.\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 28, "char_start": 6, "char_end": 112, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Rush et al. (2015)": "1918428", "Chen and Bansal (2018)": "44129061"}}}]}
{"id": "208246040_4", "paragraph": "[BOS] For generation of abstractive summaries, before the ubiquitous use of neural nets, manually crafted rules and graph techniques were utilized with considerable success.\n[BOS] Barzilay and McKeown (2005) ; Cheung and Penn (2014) fused two sentences into one using their dependency parsed trees.\n[BOS] Re-cently, sequence-to-sequence models (Sutskever et al., 2014) with attention Chopra et al., 2016) , copy mechanism Gu et al., 2016) , pointer-generator (See et al., 2017) , graph-based attention (Tan et al., 2017) have been explored.\n[BOS] Since the system generated summaries are usually evaluated on ROUGE, its been beneficial to directly optimize this metric during training via a suitable policy using reinforcement learning (Paulus et al., 2017; Celikyilmaz et al., 2018) .\n\n", "discourse_tags": ["Transition", "Multi_summ", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 32, "token_end": 62, "char_start": 180, "char_end": 298, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Barzilay and McKeown (2005)": "16188305"}, "Reference": {}}}, {"token_start": 68, "token_end": 84, "char_start": 316, "char_end": 368, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sutskever et al., 2014)": "7961699"}}}, {"token_start": 85, "token_end": 94, "char_start": 374, "char_end": 404, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Chopra et al., 2016)": "133195"}}}, {"token_start": 95, "token_end": 104, "char_start": 407, "char_end": 438, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Gu et al., 2016)": "8174613"}}}, {"token_start": 105, "token_end": 116, "char_start": 441, "char_end": 477, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(See et al., 2017)": null}}}, {"token_start": 117, "token_end": 129, "char_start": 480, "char_end": 520, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Tan et al., 2017)": "26698484"}}}, {"token_start": 161, "token_end": 183, "char_start": 713, "char_end": 783, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Paulus et al., 2017;": "21850704", "Celikyilmaz et al., 2018)": "4406182"}}}]}
{"id": "208246040_3", "paragraph": "[BOS] Identifying the likely most salient part of the text as summary-worthy is very crucial.\n[BOS] Some authors have employed integer linear programming (Martins and Smith, 2009; Gillick and Favre, 2009; Boudin et al., 2015) , graph concepts (Erkan and Radev, 2004; , ranking with reinforcement learning (Narayan et al., 2018) and mostly related to our work -binary classification (Shen et al., 2007; Nallapati et al., 2017; Chen and Bansal, 2018) Our binary classification architecture differs significantly from existing models because it uses a transformer as the building block instead of a bidirectional GRU-RNN (Nallapati et al., 2017) , or bidirectional LSTM-RNN (Chen and Bansal, 2018) .\n[BOS] To the best of our knowledge, our utilization of the transformer encoder model as a building block for binary classification is novel, although the transformer has been successfully used for language understanding (Devlin et al., 2018) , machine translation (MT) (Vaswani et al., 2017) and paraphrase generation .\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 24, "token_end": 51, "char_start": 127, "char_end": 225, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Martins and Smith, 2009;": "16148301", "Gillick and Favre, 2009;": "167874", "Boudin et al., 2015)": "6171252"}}}, {"token_start": 52, "token_end": 62, "char_start": 228, "char_end": 265, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 66, "token_end": 77, "char_start": 282, "char_end": 327, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Narayan et al., 2018)": "3510042"}}}, {"token_start": 82, "token_end": 110, "char_start": 354, "char_end": 448, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Shen et al., 2007;": null, "Nallapati et al., 2017;": "6405271", "Chen and Bansal, 2018)": "44129061"}}}, {"token_start": 131, "token_end": 147, "char_start": 596, "char_end": 642, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Nallapati et al., 2017)": "6405271"}}}, {"token_start": 149, "token_end": 163, "char_start": 648, "char_end": 694, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chen and Bansal, 2018)": "44129061"}}}, {"token_start": 197, "token_end": 208, "char_start": 894, "char_end": 938, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Devlin et al., 2018)": "52967399"}}}, {"token_start": 209, "token_end": 224, "char_start": 941, "char_end": 988, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vaswani et al., 2017)": "13756489"}}}]}
{"id": "208246040_2", "paragraph": "[BOS] Extraction has been handled on different levels of granularity -word (Cheng and Lapata, 2016) , phrases (Bui et al., 2016; Gehrmann et al., 2018) , sentence (Cheng and Lapata, 2016; Nallapati et al., 2016 Nallapati et al., , 2017 each with its challenges.\n[BOS] Word and phrase level extraction although more concise usually suffers from grammatical incorrectness, while sentence-level extraction are too lengthy and sometimes contain redundant information.\n[BOS] Hence Berg-Kirkpatrick et al. (2011); Filippova et al. (2015) ; Durrett et al. (2016) learn to extract and compress at sentence-level.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 10, "token_end": 21, "char_start": 57, "char_end": 99, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cheng and Lapata, 2016)": "1499080"}}}, {"token_start": 22, "token_end": 41, "char_start": 102, "char_end": 151, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bui et al., 2016;": "205715054", "Gehrmann et al., 2018)": "52144157"}}}, {"token_start": 42, "token_end": 68, "char_start": 154, "char_end": 235, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cheng and Lapata, 2016;": "1499080", "Nallapati et al., 2016": "8928715", "Nallapati et al., , 2017": "6405271"}}}, {"token_start": 106, "token_end": 147, "char_start": 476, "char_end": 604, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Berg-Kirkpatrick et al. (2011);": "15467396", "Filippova et al. (2015)": "1992250", "Durrett et al. (2016)": "5125975"}}}]}
{"id": "208246040_1", "paragraph": "[BOS] Early summarization models were mostly extractive and manual-feature engineered (Knight and Marcu, 2000; Jing and McKeown, 2000; Dorr et al., 2003; Berg-Kirkpatrick et al., 2011) .\n[BOS] With the introduction of neural networks (Sutskever et al., 2014) and availability of large training data, deep learning became a viable approach (Rush et al., 2015; Chopra et al., 2016) .\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 2, "token_end": 52, "char_start": 6, "char_end": 184, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Knight and Marcu, 2000;": null, "Jing and McKeown, 2000;": "800331", "Dorr et al., 2003;": "1729177", "Berg-Kirkpatrick et al., 2011)": "15467396"}}}, {"token_start": 58, "token_end": 70, "char_start": 218, "char_end": 258, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sutskever et al., 2014)": "7961699"}}}, {"token_start": 73, "token_end": 100, "char_start": 279, "char_end": 379, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rush et al., 2015;": "1918428", "Chopra et al., 2016)": "133195"}}}]}
{"id": "208246040_0", "paragraph": "[BOS] Summarization has remained an interesting and important NLP task for years due to its diverse applications -news headline generation, weather forecasting, emails filtering, medical cases, recommendation systems, machine reading compre-hension MRC and so forth (Khargharia et al., 2018) .\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 18, "token_end": 61, "char_start": 92, "char_end": 291, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Khargharia et al., 2018)": "196157150"}}}]}
{"id": "203593998_4", "paragraph": "[BOS] 3 Learning Methods and Datasets\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "203593998_3", "paragraph": "[BOS] Understanding the Generalization Ability of Neural Networks While neural networks have shown superior generalization ability, yet it remains largely unexplained.\n[BOS] Recently, some researchers begin to take a step towards understanding the generalization behaviour of neural networks from the perspective of network architectures or optimization procedure (Schmidt et al., 2018; Baluja and Fischer, 2017; Zhang et al., 2016; Arpit et al., 2017) .\n[BOS] Different from these work, in this paper, we claim that interpreting the generalization ability of neural networks is built on a good understanding of the characteristic of the data.\n\n", "discourse_tags": ["Transition", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 46, "token_end": 82, "char_start": 316, "char_end": 452, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Schmidt et al., 2018;": "13753923", "Baluja and Fischer, 2017;": "16716974", "Arpit et al., 2017)": null}}}]}
{"id": "203593998_2", "paragraph": "[BOS] Concurrent with our work, (Jung et al., 2019) conducts a quite similar analysis on datasets biases and proposes three factors which matter for the text summarization task.\n[BOS] One major difference between these two works is that we additionally focus on how dataset biases influence the designs of models.\n\n", "discourse_tags": ["Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 7, "token_end": 38, "char_start": 32, "char_end": 177, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Jung et al., 2019)": "201698380"}, "Reference": {}}}]}
{"id": "203593998_1", "paragraph": "[BOS] Neural Extractive Summarization Recently, neural network-based models have achieved great success in extractive summarization.\n[BOS] (Celikyilmaz et al., 2018; Jadhav and Rajan, 2018; Liu, 2019) .\n[BOS] Existing works on text summarization can roughly fall into one of three classes: exploring networks' structures with suitable bias (Cheng and Lapata, 2016; Nallapati et al., 2017; Zhou et al., 2018) ; introducing new training schemas (Narayan et al., 2018; Wu and Hu, 2018; Chen and Bansal, 2018) and incorporating large pre-trained knowledge (Peters et al., 2018; Devlin et al., 2018; Liu, 2019; Dong et al., 2019) .\n[BOS] Instead of exploring the possibility for a new state-of-the-art along one of above three lines, in this paper, we aim to bridge the gap between the lack of understanding of the characteristics for the datasets and the increasing development of above three learning methods.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 19, "token_end": 51, "char_start": 107, "char_end": 200, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Celikyilmaz et al., 2018;": "4406182", "Jadhav and Rajan, 2018;": "51870490", "Liu, 2019)": null}}}, {"token_start": 68, "token_end": 99, "char_start": 290, "char_end": 407, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cheng and Lapata, 2016;": "1499080", "Nallapati et al., 2017;": "6405271", "Zhou et al., 2018)": "49656757"}}}, {"token_start": 100, "token_end": 126, "char_start": 410, "char_end": 505, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Narayan et al., 2018;": "3510042", "Wu and Hu, 2018;": "4999752", "Chen and Bansal, 2018)": "44129061"}}}, {"token_start": 127, "token_end": 162, "char_start": 510, "char_end": 624, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Peters et al., 2018;": "3626819", "Devlin et al., 2018;": "52967399", "Liu, 2019;": null, "Dong et al., 2019)": "147704286"}}}]}
{"id": "203593998_0", "paragraph": "[BOS] We briefly outline connections and differences to following related lines of research.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "209330599_1", "paragraph": "[BOS] -Shuttle will deliver final U.S. portion of the international space station -NASA has been looking for places to house the shuttles once they are retired\n\n", "discourse_tags": ["Other"], "span_citation_mapping": []}
{"id": "209330599_0", "paragraph": "[BOS] Shared tasks on automatic summarization were first introduced at the Document Understanding Conferences (DUC).\n[BOS] 3 In addition to new summarization technology, equal emphasis was given to formulating strong evaluation measures.\n[BOS] Methods such as basic elements (Hovy et al., 2006) , pyramid (Nenkova and Passonneau, 2004) , and ROUGE (Lin, 2004) were introduced for automatically evaluating the content selection capabilities of the participating systems.\n[BOS] Furthermore, Dang (2005) presented the first guideline for manually judging summary quality.\n[BOS] In 2008, DUC became a summarization track at the Text Analysis Conference Example -CNN/DailyMail Corpus Article NASA will launch Space Shuttle Endeavour on February 7, which will be the first of five launches this year before the shuttle fleet is retired.\n[BOS] Endeavour will blast off from the Kennedy Space Center in Florida on a 13-day mission to the international space station.\n[BOS] The mission will include three spacewalks, NASA said.\n[BOS] The shuttle will also deliver the final U.S. portion of the space station.\n[BOS] This portion will provide more room for crew members.\n[BOS] NASA plans to retire its space shuttles Discovery, Endeavour and Atlantis later this year.\n[BOS] The space agency has been looking for places, such as museums, to house the shuttles after they are retired.\n[BOS] Space Shuttle Discovery will be transferred to the Smithsonian National Air and Space Museum in Washington.\n[BOS] The privilege of showing off a shuttle won't be cheap -about $29 million, NASA said.\n[BOS] Highlights -This will be first of five launches this year before the shuttle fleet is retired -NASA is scheduled to launch Space Shuttle Endeavour on February 7.\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite", "Single_summ", "Other", "Other", "Other", "Other", "Other", "Other", "Other", "Other", "Other", "Other"], "span_citation_mapping": [{"token_start": 46, "token_end": 56, "char_start": 266, "char_end": 294, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hovy et al., 2006)": "3050993"}}}, {"token_start": 57, "token_end": 70, "char_start": 297, "char_end": 335, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Nenkova and Passonneau, 2004)": "1046281"}}}, {"token_start": 72, "token_end": 79, "char_start": 342, "char_end": 359, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lin, 2004)": null}}}, {"token_start": 96, "token_end": 112, "char_start": 489, "char_end": 568, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Dang (2005)": null}, "Reference": {}}}]}
{"id": "202770038_0", "paragraph": "[BOS] Neural attention models (Bahdanau et al., 2015) with the seq2seq architecture (Sutskever et al., 2014) have achieved impressive results in text summarization tasks.\n[BOS] However, the attention vector comes from a weighted sum of source information and does not model the source-target alignment in a probabilistic sense.\n[BOS] This makes it difficult to interpret or control model generations through the attention mechanism.\n[BOS] In practice, people do find the attention vector is often blurred and suffers from poor alignment (Koehn and Knowles, 2017; Kiyono et al., 2018; Jain and Wallace, 2019) .\n[BOS] Hard alignment models, on the other hand, explicitly models the alignment relation between each source-target pair.\n[BOS] Though theoretically sound, hard alignment models are hard to train.\n[BOS] Exact marginalization is only feasible for data with limited length (Yu et al., 2016; Aharoni and Goldberg, 2017; Backes et al., 2018) , or by assuming a simple copy generation process (Vinyals et al., 2015; Gu et al., 2016; See et al., 2017) .\n[BOS] Our model can be viewed as a combination of soft attention and hard alignment, where a simple top-k approximation is used to train the alignment part (Shankar et al., 2018; Shankar and Sarawagi, 2019) .\n[BOS] The hard alignment generation probability is designed as a relation summation operation to better fit the sum-marization task.\n[BOS] In this way, the generalized copy mode acts as a hard alignment component to capture the direct word-to-word transitions.\n[BOS] On the contrary, the generation mode is a standard softattention structure to only model words that are purely functional, or need fusion, high-level inference and can be hardly aligned to any specific source context (Daum III and Marcu, 2005) .\n\n", "discourse_tags": ["Multi_summ", "Transition", "Transition", "Narrative_cite", "Transition", "Transition", "Narrative_cite", "Narrative_cite", "Transition", "Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 2, "token_end": 41, "char_start": 6, "char_end": 170, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Bahdanau et al., 2015)": "11212020", "(Sutskever et al., 2014)": "7961699"}, "Reference": {}}}, {"token_start": 93, "token_end": 130, "char_start": 471, "char_end": 607, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Koehn and Knowles, 2017;": "8822680", "Kiyono et al., 2018;": "53629348", "Jain and Wallace, 2019)": "67855860"}}}, {"token_start": 167, "token_end": 202, "char_start": 813, "char_end": 947, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yu et al., 2016;": "371926", "Aharoni and Goldberg, 2017;": "122829", "Backes et al., 2018)": "51719120"}}}, {"token_start": 205, "token_end": 235, "char_start": 956, "char_end": 1055, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vinyals et al., 2015;": "5692837", "Gu et al., 2016;": "8174613", "See et al., 2017)": null}}}, {"token_start": 254, "token_end": 285, "char_start": 1151, "char_end": 1264, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Shankar et al., 2018;": "52201669", "Shankar and Sarawagi, 2019)": "59242633"}}}, {"token_start": 374, "token_end": 387, "char_start": 1727, "char_end": 1777, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Daum\u00e9 III and Marcu, 2005)": "11468390"}}}]}
{"id": "202540962_2", "paragraph": "[BOS] Our work is also motivated by the work of training with two agents, including dual learning (He et al., 2016; Xia et al., 2017 Xia et al., , 2018 , and bidirectional decoding Zhang et al., 2018 Zhang et al., , 2019b .\n[BOS] Our method can be viewed as a general learning framework to train multiple agents, which explores the relationship among all agents to enhance the performance of each agent efficiently.\n\n", "discourse_tags": ["Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 17, "token_end": 40, "char_start": 84, "char_end": 151, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(He et al., 2016;": "5758868", "Xia et al., 2017": "3730033", "Xia et al., , 2018": "51868177"}}}, {"token_start": 42, "token_end": 59, "char_start": 158, "char_end": 221, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Zhang et al., 2018": "19169084", "Zhang et al., , 2019b": "51971914"}}}]}
{"id": "202540962_1", "paragraph": "[BOS] Another related work is ensemble knowledge distillation (Fukuda et al., 2017; Freitag et al., 2017; Zhu et al., 2018) , in which the ensemble model of all agents is leveraged as the Teacher network, to distill the knowledge for the corresponding Student network.\n[BOS] However, as described in the previous section, the knowledge distillation is one particular case of our model, as the performance of the Teacher in their model is fixed, and cannot be further improved by the learning process.\n\n", "discourse_tags": ["Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 6, "token_end": 36, "char_start": 30, "char_end": 123, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Fukuda et al., 2017;": "30258763", "Freitag et al., 2017;": "9474415", "Zhu et al., 2018)": "48352434"}}}]}
{"id": "202540962_0", "paragraph": "[BOS] The most related work is recently proposed by Wang et al. (2019) , who introduced a multi-agent algorithm for dual learning.\n[BOS] However, the major differences are 1) the training in their work is conducted on the two agents while fixing the parameters of other agents.\n[BOS] 2) they use identical agent with different initialization seeds.\n[BOS] 3) our model is simple yet effective.\n[BOS] Actually, it is easy to incorporate additional agent trained with their dual learning strategy in our model, to further improve the performance.\n[BOS] More importantly, both of our work and their work indicate that using more agents can improve the translation quality significantly.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 10, "token_end": 70, "char_start": 52, "char_end": 348, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wang et al. (2019)": null}, "Reference": {}}}]}
{"id": "196185937_4", "paragraph": "[BOS] We target Organization and Argument Strength dimension of essays which are related to coherence and cohesion.\n[BOS] Persing et al. (2010) proposed heuristic rules utilizing various DIs, words and phrases to capture the organizational structure of texts.\n[BOS] Persing and Ng (2015) used several features such as part-of-speech, n-grams, semantic frames, coreference, and argument components for calculating Argument Strength in essays.\n[BOS] Wachsmuth et al. (2016) achieved stateof-the-art performance on Organization and Argument Strength scoring of essays by utilizing argumentative features such as sequence of argumentative discourse units (e.g., (conclusion, premise, conclusion)).\n[BOS] However, Wachsmuth et al. (2016) used an expensive argument parser to obtain such units.\n\n", "discourse_tags": ["Reflection", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 21, "token_end": 47, "char_start": 122, "char_end": 259, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Persing et al. (2010)": "3131971"}, "Reference": {}}}, {"token_start": 48, "token_end": 87, "char_start": 266, "char_end": 441, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Persing and Ng (2015)": "2743089"}, "Reference": {}}}, {"token_start": 88, "token_end": 143, "char_start": 448, "char_end": 693, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wachsmuth et al. (2016)": "6464371"}, "Reference": {}}}, {"token_start": 146, "token_end": 166, "char_start": 709, "char_end": 788, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wachsmuth et al. (2016)": "6464371"}, "Reference": {}}}]}
{"id": "196185937_3", "paragraph": "[BOS] Previous studies have modeled text coherence (Li and Jurafsky, 2016; Joty et al., 2018; Mesgar and Strube, 2018) .\n[BOS] Farag et al. (2018) demonstrated that state-of-the-art neural automated essay scoring (AES) is not well-suited for capturing adversarial input of grammatically correct but incoherent sequences of sentences.\n[BOS] Therefore, they developed a neural local coherence model and jointly trained it with a state-of-the-art AES model to build an adversarially robust AES system.\n[BOS] Mesgar and Strube (2018) used a local coherence model to assess essay scoring performance on a dataset of holistic scores where it is unclear which criteria of the essay the score considers.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 5, "token_end": 33, "char_start": 28, "char_end": 118, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Li and Jurafsky, 2016;": "18706304", "Joty et al., 2018;": "24129906", "Mesgar and Strube, 2018)": "53082956"}}}, {"token_start": 35, "token_end": 115, "char_start": 127, "char_end": 498, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Farag et al. (2018)": "4932015"}, "Reference": {}}}, {"token_start": 116, "token_end": 153, "char_start": 505, "char_end": 695, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Mesgar and Strube (2018)": "53082956"}, "Reference": {}}}]}
{"id": "196185937_2", "paragraph": "[BOS] One exception is the study by Ji and Smith (2017) who illustrated the role of discourse structure for document representation by implementing a discourse structure (defined by RST) aware model and showed that their model improves text categorization performance (e.g., sentiment classification of movies and Yelp reviews, and prediction of news article frames).\n[BOS] The authors utilized an RST-parser to obtain the discourse dependency tree of a document and then built a recursive neural network on top of it.\n[BOS] The issue with their approach is that texts need to be parsed by an RST parser which is computationally expensive.\n[BOS] Furthermore, the performance of RST parsing is dependent on the genre of documents (Ji and Smith, 2017) .\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 8, "token_end": 120, "char_start": 36, "char_end": 639, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ji and Smith (2017)": "5914002"}, "Reference": {}}}, {"token_start": 121, "token_end": 142, "char_start": 646, "char_end": 749, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ji and Smith, 2017)": "5914002"}}}]}
{"id": "196185937_1", "paragraph": "[BOS] Several methods for document representation learning have been introduced in recent years.\n[BOS] One popular unsupervised method is doc2vec (Le and Mikolov, 2014) , where a document is mapped to a unique vector and every word in the document is also mapped to a unique vector.\n[BOS] Then, the document vector and and word vectors are either concatenated or averaged to predict the next word in a context.\n[BOS] Liu et al. (2017) used a convolutional neural network (CNN) to capture longer range semantic structure within a document where the learning objective predicted the next word.\n[BOS] Wu et al. (2018) proposed Word Mover's Embedding (WME) utilizing Word Mover's Distance (WMD) that considers both word alignments and pre-trained word vectors to learn feature representation of documents.\n[BOS] Tang et al. (2015) proposed a semi-supervised method called Predictive Text Embedding (PTE) where both labeled information and different levels of word co-occurrence were encoded in a large-scale heterogeneous text network, which was then embedded into a low dimensional space.\n[BOS] Although these approaches have been proven useful for several document classification and regression tasks, their focus is not on capturing the discourse structure of documents.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 16, "token_end": 81, "char_start": 103, "char_end": 410, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Le and Mikolov, 2014)": "2407601"}, "Reference": {}}}, {"token_start": 82, "token_end": 115, "char_start": 417, "char_end": 591, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 116, "token_end": 163, "char_start": 598, "char_end": 801, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wu et al. (2018)": "53081529"}, "Reference": {}}}, {"token_start": 164, "token_end": 218, "char_start": 808, "char_end": 1085, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Tang et al. (2015)": "35493"}, "Reference": {}}}]}
{"id": "196185937_0", "paragraph": "[BOS] The focus of this study is the unsupervised encapsulation of discourse structure (coherence and cohesion) into document representation for essay scoring.\n[BOS] A popular approach for document representation is the use of fixed-length features such as bag-of-words (BOW) and bag-of-ngrams due to their simplicity and highly competitive results (Wang and Manning, 2012) .\n[BOS] However, such approaches fail to capture the semantic similarity of words and phrases since they treat each word or 1 Our implementation is publicly available at https://github.com/FarjanaSultanaMim/ DiscoShuffle phrase as a discrete token.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 52, "token_end": 75, "char_start": 280, "char_end": 373, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wang and Manning, 2012)": "217537"}}}]}
{"id": "202541473_2", "paragraph": "[BOS] In this paper, we focus on English-to-Chinese and Chinese-to-English CLS and try to automatically construct two large-scale corpora respectively.\n[BOS] In addition, based on the two corpora, we perform several end-to-end training methods noted as Neural Cross-Lingual Summarization.\n\n", "discourse_tags": ["Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "202541473_1", "paragraph": "[BOS] However, all these researches belong to the pipeline paradigm which not only relies heavily on hand-crafted features but also causes error propagation.\n[BOS] End-to-end deep learning has proven to be able to alleviate these two problems, while it has been absent due to the lack of largescale training data.\n[BOS] Recently, Ayana et al. (2018) present zero-shot cross-lingual headline generation based on existing parallel corpora of translation and monolingual headline generation.\n[BOS] Similarly, Duan et al. (2019) propose to use monolingual abstractive sentence summarization system to teach zero-shot cross-lingual abstractive sentence summarization on both summary word generation and attention.\n[BOS] Although great efforts have been made in cross-lingual summarization, how to automatically build a high-quality large-scale cross-lingual summarization dataset remains unexplored.\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 67, "token_end": 100, "char_start": 330, "char_end": 488, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ayana et al. (2018)": null}, "Reference": {}}}, {"token_start": 103, "token_end": 145, "char_start": 506, "char_end": 708, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Duan et al. (2019)": "196180279"}, "Reference": {}}}]}
{"id": "202541473_0", "paragraph": "[BOS] Cross-lingual summarization has been proposed to present the most salient information of a source document in a different language, which is very important in the field of multilingual information processing.\n[BOS] Most of the existing methods handle the task of CLS via simply applying two typical translation schemes, i.e., early translation (Leuski et al., 2003; Ouyang et al., 2019) and late translation (Orasan and Chiorean, 2008; Wan et al., 2010) .\n[BOS] The early translation scheme first translates the original document into target language and then generates the summary of the translated document.\n[BOS] The late translation scheme first summarizes the original document into a summary in the source language and then translates it into target language.\n[BOS] Leuski et al. (2003) translate the Hindi document to English and then generate the English headline for it.\n[BOS] Ouyang et al. (2019) present a robust abstractive summarization system for low resource languages where no summarization corpora are currently available.\n[BOS] They train a neural abstractive summarization model on noisy English documents and clean English reference summaries.\n[BOS] Then the model can learn to produce fluent summaries from disfluent inputs, which allows generating summaries for translated documents.\n[BOS] Orasan and Chiorean (2008) summarize the Romanian news with the maximal marginal relevance method (Goldstein et al., 2000) and produce the English summaries for English speakers.\n[BOS] Wan et al. (2010) adopt the late translation scheme for the task of English-to-Chinese CLS.\n[BOS] They extract English sentences considering both the informativeness and translation quality of sentences and automatically translate the English summary into the final Chinese summary.\n[BOS] The above researches only make use of the information from only one language side.\n[BOS] Some methods have been proposed to improve CLS with bilingual information.\n[BOS] Wan (2011) proposes two graph-based summarization methods to leverage both the English-side and Chineseside information in the task of English-to-Chinese CLS.\n[BOS] Inspired by the phrase-based translation models, Yao et al. (2015) introduce a compressive CLS, which simultaneously performs sentence selection and compression.\n[BOS] They calculate the sentence scores based on the aligned bilingual phrases obtained by MT service and perform compression via deleting redundant or poorly translated phrases.\n[BOS] Zhang et al. (2016) propose an abstractive CLS which constructs a pool of bilingual concepts represented by the bilingual elements of the source-side predicate-argument structures (PAS) and the target-side counterparts.\n[BOS] The final summary is generated by maximizing both the salience and translation quality of the PAS elements.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Transition", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 64, "token_end": 84, "char_start": 332, "char_end": 392, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Leuski et al., 2003;": "2897948", "Ouyang et al., 2019)": "92991693"}}}, {"token_start": 85, "token_end": 105, "char_start": 397, "char_end": 459, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Orasan and Chiorean, 2008;": null, "Wan et al., 2010)": "11116896"}}}, {"token_start": 155, "token_end": 180, "char_start": 778, "char_end": 885, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Leuski et al. (2003)": "2897948"}, "Reference": {}}}, {"token_start": 181, "token_end": 256, "char_start": 892, "char_end": 1311, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ouyang et al. (2019)": "92991693"}, "Reference": {}}}, {"token_start": 257, "token_end": 295, "char_start": 1318, "char_end": 1496, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Orasan and Chiorean (2008)": null}, "Reference": {"(Goldstein et al., 2000)": "8294822"}}}, {"token_start": 296, "token_end": 347, "char_start": 1503, "char_end": 1785, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wan et al. (2010)": "11116896"}, "Reference": {}}}, {"token_start": 378, "token_end": 413, "char_start": 1962, "char_end": 2120, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wan (2011)": "7836946"}, "Reference": {}}}, {"token_start": 414, "token_end": 471, "char_start": 2127, "char_end": 2468, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Yao et al. (2015)": "7489682"}, "Reference": {}}}, {"token_start": 472, "token_end": 535, "char_start": 2475, "char_end": 2808, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhang et al. (2016)": "3712220"}, "Reference": {}}}]}
{"id": "174797840_4", "paragraph": "[BOS] In the future work we would like to consider setups where human-annotated data is combined with naturally occurring one (i.e. distantly-supervised one).\n[BOS] It would also be interesting to see if mistakes made by fully-supervised systems differ from the ones made by our system and other Wikipediabased linkers.\n\n", "discourse_tags": ["Reflection", "Transition"], "span_citation_mapping": []}
{"id": "174797840_3", "paragraph": "[BOS] In this paper we proposed a weakly-supervised model for entity linking.\n[BOS] The model was trained on unlabeled documents which were automatically annotated using Wikipedia.\n[BOS] Our model substantially outperforms previous methods, which used the same form of supervision, and rivals fullysupervised models trained on data specifically annotated for the entity-linking problem.\n[BOS] This result may be interpreted as suggesting that humanannotated data is not beneficial for entity linking, given that we have Wikipedia and web links.\n[BOS] However, we believe that the two sources of information are likely to be complementary.\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "174797840_2", "paragraph": "[BOS] Our model (but not the estimation method) bears similarities to the approaches of Le and Titov (2018) and Globerson at al. (2016) .\n[BOS] Both these supervised approaches are global and use attention.\n\n", "discourse_tags": ["Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 17, "token_end": 24, "char_start": 88, "char_end": 107, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Le and Titov (2018)": "13747961"}}}, {"token_start": 25, "token_end": 34, "char_start": 112, "char_end": 135, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Globerson at al. (2016)": "17507793"}}}]}
{"id": "174797840_1", "paragraph": "[BOS] The weakly-or semi-supervised set-up, which we use, is not common for entity linking.\n[BOS] The only other approach which uses a combination of Wikipedia and unlabeled data, as far as we are aware of, is by Lazic et al. (2015) .\n[BOS] We discussed it and compared to in previous sections.\n[BOS] Our setup is inspired by distantly-supervised learning in relation extraction (Mintz et al., 2009 ).\n[BOS] In distant learning, the annotation is automatically (and noisily) induced relying on a knowledge base instead of annotating the data by hand.\n[BOS] Fan, Zhou, and Zheng (2015) learned a Freebase linker using distance supervision.\n[BOS] Their evaluation is nonstandard.\n[BOS] They also do not attempt to learn a disambiguation model but directly train their system to replicate noisy projected annotations.\n[BOS] Wang et al. (2015) refer to their approach as unsupervised, as they do not use unlabeled data.\n[BOS] However, their method does not involve any learning and relies on matching heuristics.\n[BOS] Some aspects of their approach (e.g., using Wikipedia link statitics) resemble our candidate generation stage.\n[BOS] So, in principle, their approach could be compared to the 'no-disambiguation' baselines (s c ) in Table 3 .\n[BOS] Their evaluation set-up is not standard.\n\n", "discourse_tags": ["Reflection", "Narrative_cite", "Reflection", "Narrative_cite", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 30, "token_end": 57, "char_start": 128, "char_end": 232, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Lazic et al. (2015)": "2061341"}}}, {"token_start": 75, "token_end": 91, "char_start": 326, "char_end": 398, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mintz et al., 2009": "10910955"}}}, {"token_start": 124, "token_end": 174, "char_start": 557, "char_end": 814, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 175, "token_end": 277, "char_start": 821, "char_end": 1286, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "174797840_0", "paragraph": "[BOS] Using Wikipedia pages to learn linkers ('wikifiers') has been a popular line of research both for named entity linking (Cheng and Roth, 2013; Milne and Witten, 2008) and generally entity disambiguation tasks (Ratinov et al., 2011b) .\n[BOS] How-ever, since introduction of the AIDA CoNLL dataset, fully-supervised learning on this dataset became standard for named entity linking, with supervised systems (Globerson et al., 2016; Guo and Barbosa, 2016; Yamada et al., 2016) outperforming alternatives even on out-of-domain datasets such as MSNBC and ACE2004.\n[BOS] Note though that supervised systems also rely on Wikipedia-derived features.\n[BOS] As an alternative to using Wikipedia pages, links to Wikipedia pages from the general Web were used as supervision (Singh et al., 2012) .\n[BOS] As far as we are aware, the system of Chisholm and Hachey (2015) is the only such system evaluated on standard named-entity linking benchmarks, and we compare to them in our experiments.\n[BOS] This line of work is potentially complementary to what we propose, as we could use the Web links to construct weak supervision.\n\n", "discourse_tags": ["Narrative_cite", "Multi_summ", "Transition", "Narrative_cite", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 3, "token_end": 42, "char_start": 12, "char_end": 171, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cheng and Roth, 2013;": "17784265", "Milne and Witten, 2008)": "207170378"}}}, {"token_start": 44, "token_end": 58, "char_start": 186, "char_end": 237, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ratinov et al., 2011b)": "6430811"}}}, {"token_start": 89, "token_end": 116, "char_start": 391, "char_end": 478, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Globerson et al., 2016;": "17507793", "Guo and Barbosa, 2016;": null}}}, {"token_start": 160, "token_end": 180, "char_start": 697, "char_end": 788, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Singh et al., 2012)": "11619517"}}}, {"token_start": 192, "token_end": 216, "char_start": 835, "char_end": 939, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Chisholm and Hachey (2015)": "10523208"}}}]}
{"id": "83458715_4", "paragraph": "[BOS] Finally, BERT as well as Radford et al. (2018) consider only a single data source to pretrain their models, either BooksCorpus (Radford et al., 2018) , or BooksCorpus and additional Wikipedia data (Devlin et al., 2018) , whereas our study ablates the effect of various amounts of training data as well as different data sources.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": [{"token_start": 8, "token_end": 59, "char_start": 31, "char_end": 224, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Radford et al. (2018)": null}, "Reference": {"(Radford et al., 2018)": null, "(Devlin et al., 2018)": "52967399"}}}]}
{"id": "83458715_3", "paragraph": "[BOS] BERT tailors pretraining to capture dependencies between sentences via a next sentence prediction task as well as by constructing training examples of sentence-pairs with input markers that distinguish between tokens of the two sentences.\n[BOS] Our model is trained similarly to a classical language model since we do not adapt the training examples to resemble the end task data and we do not solve a denoising task during training.\n\n", "discourse_tags": ["Transition", "Reflection"], "span_citation_mapping": []}
{"id": "83458715_2", "paragraph": "[BOS] The concurrently introduced BERT model (Devlin et al., 2018) is a transformer encoder model that captures left and right context.\n[BOS] There is significant overlap between their work and ours but there are also significant differences: our model is a bi-directional transformer language model that predicts every single token in a sequence.\n[BOS] BERT is also a transformer encoder that has access to the entire input which makes it bi-directional but this choice requires a special training regime.\n[BOS] In particular, they multi-task between predicting a subset of masked input tokens, similar to a denoising autoencoder, and a next sentence prediction task.\n[BOS] In comparison, we optimize a single loss function that requires the model to predict each token of an input sentence given all surrounding tokens.\n[BOS] We use all tokens as training targets and therefore extract learning signal from every single token in the sentence and not just a subset.\n\n", "discourse_tags": ["Single_summ", "Reflection", "Reflection", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 28, "char_start": 6, "char_end": 135, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Devlin et al., 2018)": "52967399"}, "Reference": {}}}]}
{"id": "83458715_1", "paragraph": "[BOS] Our work was inspired by ELMo (Peters et al., 2018) and the generative pretraining (GPT) approach of Radford et al. (2018) .\n[BOS] ELMo introduces language models to pretrain word representations for downstream tasks including a novel mechanism to learn a combination of different layers in the language model that is most beneficial to the current task.\n[BOS] GPT relies on a left to right language model and an added projection layer for each downstream task without a task-specific model.\n[BOS] Our approach mostly follows GPT, though we show that our model also works well with an ELMo module on NER and constituency parsing.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 7, "token_end": 17, "char_start": 31, "char_end": 57, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Peters et al., 2018)": "3626819"}}}, {"token_start": 19, "token_end": 36, "char_start": 66, "char_end": 128, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Radford et al. (2018)": null}}}]}
{"id": "83458715_0", "paragraph": "[BOS] There has been much recent work on learning sentence-specific representations for language understanding tasks.\n[BOS] McCann et al. (2017) learn contextualized word representations from a sequence to sequence translation task and uses the representations from the encoder network to improve a variety of language understanding tasks.\n[BOS] Subsequent work focused on language modeling pretraining which has been shown to be more effective and which does not require bilingual data (Zhang and Bowman, 2018) .\n\n", "discourse_tags": ["Transition", "Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 20, "token_end": 57, "char_start": 124, "char_end": 339, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"McCann et al. (2017)": "9447219"}, "Reference": {}}}, {"token_start": 62, "token_end": 89, "char_start": 373, "char_end": 511, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhang and Bowman, 2018)": "52845092"}}}]}
{"id": "174800890_0", "paragraph": "[BOS] Factoid QA data sets involving question-answer pairs as well as their corresponding Freebase matches have been created in the past.\n[BOS] In (Berant et al., 2013) , factoid QA over knowledge graphs are formulated as semantic parsing problems, where each natural language question is first converted into a logic form to retrieve the answer with traditional symbolic approaches.\n[BOS] In (Berant et al., 2013) , a small-scale data set of several thousands of question-answer pairs, called WebQuestions, is created by human annotators.\n[BOS] In (Yih et al., 2016) , the WebQuestions set is further refined by providing human-annotated semantic parses for some questions that are answerable using Freebase, which is called WebQuestionsSP (WebQSP).\n[BOS] Recently, deep learning approaches have become popular in the field of NLP.\n[BOS] Neural networks require far more training data than a small data set of several thousands of samples.\n[BOS] In (Bordes et al., 2015) , a much larger QA data set of about 100K question/answer pairs, called SimpleQuestions, is created.\n[BOS] In this work, some randomly chosen Freebase triples are shown to human annotators.\n[BOS] For each given triple, an annotator is asked to manually compose a question to reflect the relation in the triple.\n[BOS] The issues with SimpleQuestions lie in that most constructed questions are quite simple in linguistic structure and many questions even directly use the keywords in the Freebase predicates since human annotators may be greatly limited in composition when a particular triple is shown.\n[BOS] According to (Petrochuk and Zettlemoyer, 2018) , SimpleQuestions is nearly solved with only standard neural network methods if its linguistic ambiguity is taken into account.\n[BOS] In (Vlad Serban et al., 2016) , a large QA data set is automatically generated by neural networks but it obviously lacks rich linguistic variations.\n[BOS] Additionally, many similar factoid QA data sets are also released for other non-English languages, e.g. WebQA in .\n[BOS] Meanwhile, another direction of data collection efforts involve QA in various reading comprehension tasks, e.g. SQuAD in (Rajpurkar et al., 2016) , MS-MARCO in (Nguyen et al., 2016) , TriviaQA in (Joshi et al., 2017) .\n[BOS] However, we believe question answering over structured knowledge graphs remains a viable NLP task for the promising research direction to combine neural computing methods with the traditional symbolic processing approaches.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Transition", "Transition", "Single_summ", "Single_summ", "Single_summ", "Transition", "Single_summ", "Single_summ", "Transition", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 29, "token_end": 73, "char_start": 147, "char_end": 383, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Berant et al., 2013)": "6401679"}, "Reference": {}}}, {"token_start": 75, "token_end": 112, "char_start": 393, "char_end": 539, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Berant et al., 2013)": "6401679"}, "Reference": {}}}, {"token_start": 114, "token_end": 164, "char_start": 549, "char_end": 750, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Yih et al., 2016)": null}, "Reference": {}}}, {"token_start": 201, "token_end": 276, "char_start": 950, "char_end": 1282, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Bordes et al., 2015)": "9605730"}, "Reference": {}}}, {"token_start": 326, "token_end": 363, "char_start": 1580, "char_end": 1754, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Petrochuk and Zettlemoyer, 2018)": "13745156"}, "Reference": {}}}, {"token_start": 365, "token_end": 396, "char_start": 1764, "char_end": 1909, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Vlad Serban et al., 2016)": "12241221"}, "Reference": {}}}, {"token_start": 445, "token_end": 458, "char_start": 2149, "char_end": 2182, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rajpurkar et al., 2016)": "11816014"}}}, {"token_start": 459, "token_end": 472, "char_start": 2185, "char_end": 2218, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Nguyen et al., 2016)": "1289517"}}}, {"token_start": 473, "token_end": 486, "char_start": 2221, "char_end": 2253, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Joshi et al., 2017)": "26501419"}}}]}
{"id": "208325681_2", "paragraph": "[BOS] Recent work has explored some approaches to consider inter-sentence relations, such as Graph LSTMs (Peng et al., 2017) , self-attention (Verga et al., 2018 ), Graph CNNs (Sahu et al., 2019 .\n[BOS] However, none of these work investigated lexical chains for inter-sentence relation extraction.\n[BOS] In the future, we will evaluate our approach on some large-scale datasets for intra-and inter-sentence relation extraction (Yao et al., 2019) .\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 17, "token_end": 29, "char_start": 93, "char_end": 124, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Peng et al., 2017)": null}}}, {"token_start": 30, "token_end": 41, "char_start": 127, "char_end": 161, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Verga et al., 2018": "3576631"}}}, {"token_start": 43, "token_end": 55, "char_start": 165, "char_end": 194, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 90, "token_end": 107, "char_start": 383, "char_end": 446, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yao et al., 2019)": "189898081"}}}]}
{"id": "208325681_1", "paragraph": "[BOS] In the NLP community, it has proven to be effective to combine linguistic features with neural networks for relation extraction Miwa and Bansal, 2016) .\n[BOS] Bunescu et al. (2005) demonstrated that the relationship of an entity pair can be captured along their shortest dependency path in the dependency graph because the words on the shortest dependency path concentrate the most relevant information and diminish redundant information.\n[BOS] Following this observation, several studies (Xu et al., 2015; Liu et al., 2015) achieved outstanding performance by combining shortest dependency paths with various neural networks.\n[BOS] As deep learning develops, some attention-based neural architectures (Zhou et al., 2016; Lin et al., 2016) have been proposed for relation classification and show the state-of-the-art performance.\n[BOS] But with a few exceptions, almost all related work only focused on intra-sentence relation extraction, without considering the inter-sentence relations.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Multi_summ", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 16, "token_end": 32, "char_start": 69, "char_end": 156, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Miwa and Bansal, 2016)": "2476229"}}}, {"token_start": 34, "token_end": 81, "char_start": 165, "char_end": 444, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Bunescu et al. (2005)": "5165854"}, "Reference": {}}}, {"token_start": 86, "token_end": 116, "char_start": 479, "char_end": 632, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Xu et al., 2015;": "5403702", "Liu et al., 2015)": "7475429"}, "Reference": {}}}, {"token_start": 123, "token_end": 143, "char_start": 671, "char_end": 745, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhou et al., 2016;": "9870160", "Lin et al., 2016)": "397533"}}}]}
{"id": "208325681_0", "paragraph": "[BOS] In the natural language processing community, there are a number of related competitions and tasks Delger et al., 2016) .\n[BOS] Most prior work focused on extracting the relations within one sentence, and ignored the relations beyond one sentence.\n\n", "discourse_tags": ["Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 2, "token_end": 27, "char_start": 6, "char_end": 125, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Del\u00e9ger et al., 2016)": "17909922"}}}]}
{"id": "195218693_4", "paragraph": "[BOS] While our method enables model builders to inject priors to aid a model, it has several limitations.\n[BOS] In solving the fairness problem in question, it causes the classifier to not focus on the identity terms even for the cases where an identity term itself is being used as an insult.\n[BOS] Moreover, our approach requires prior terms to be manually provided, which bears resemblance to blacklist approaches and suffers from the same drawbacks.\n[BOS] Lastly, the evaluation methodology that we and previous papers (Dixon et al., 2018; Park et al., 2018) rely on are based on a syntheticallygenerated dataset, which may contain biases of the individuals creating it.\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection", "Multi_summ"], "span_citation_mapping": [{"token_start": 90, "token_end": 114, "char_start": 469, "char_end": 563, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dixon et al., 2018;": "54997157", "Park et al., 2018)": "52070035"}}}]}
{"id": "195218693_3", "paragraph": "[BOS] One of the main reasons our approach improves the model in the original task is that the model is now more robust thanks to the reinforcement provided to the model builder through attributions.\n[BOS] From a fairness angle, our technique shares similarities with adversarial training (Zhang et al., 2018a; Madras et al., 2018) in asking the model to optimize for an additional objective that transitively unbiases the classifier.\n[BOS] However, those approaches work to remove protected attributes from the representation layer, which is unstable.\n[BOS] Our approach, on the other hand, works with basic human-interpretable units of information -tokens.\n[BOS] Also, those approaches propose to sacrifice main task performance for fairness as well.\n\n", "discourse_tags": ["Reflection", "Narrative_cite", "Transition", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 49, "token_end": 69, "char_start": 268, "char_end": 331, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhang et al., 2018a;": "9424845", "Madras et al., 2018)": "3419504"}}}]}
{"id": "195218693_2", "paragraph": "[BOS] Addressing and mitigating bias in NLP models are paramount tasks as the effects on these models adversely affect protected subpopulations (Schmidt and Wiegand, 2017) .\n[BOS] One of the earliest works is Calders and Verwer (2010) .\n[BOS] Later, Bolukbasi et al. (2016) proposed to unbias word vectors from gender stereotypes.\n[BOS] Park et al. (2018) also try to address gender bias for abusive language detection models by debiasing word vectors, augmenting more data and changing model architecture.\n[BOS] While their results seem to show promise for removing gender bias, their method doesn't scale for other identity dimensions such as race and religion.\n[BOS] The authors of Dixon et al. (2018) highlight the bias in toxic comment classifier models originating from the dataset.\n[BOS] They also supplement the training dataset from Wikipedia articles to shift positive class imbalance for sentences containing identity terms to dataset average.\n[BOS] Similarly, their approach alleviates the issue to a certain extent, but does not scale to similar problems as their augmentation technique is too data-specific.\n[BOS] Also, both methods trade original task accuracy for fairness, while our method does not.\n[BOS] Lastly, there are several works (Davidson et al., 2017; Zhang et al., 2018b) offering methodologies or datasets to evaluate models for unintended bias, but they fail to offer a general framework.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection", "Narrative_cite"], "span_citation_mapping": [{"token_start": 19, "token_end": 33, "char_start": 95, "char_end": 171, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Schmidt and Wiegand, 2017)": "9626793"}}}, {"token_start": 39, "token_end": 49, "char_start": 200, "char_end": 234, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Calders and Verwer (2010)": "12856537"}}}, {"token_start": 53, "token_end": 74, "char_start": 250, "char_end": 330, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Bolukbasi et al. (2016)": "1704893"}, "Reference": {}}}, {"token_start": 75, "token_end": 139, "char_start": 337, "char_end": 663, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Park et al. (2018)": "52070035"}, "Reference": {}}}, {"token_start": 143, "token_end": 219, "char_start": 685, "char_end": 1121, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Dixon et al. (2018)": "54997157"}, "Reference": {}}}, {"token_start": 241, "token_end": 272, "char_start": 1237, "char_end": 1373, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Davidson et al., 2017;": "1733167", "Zhang et al., 2018b)": "46939253"}}}]}
{"id": "195218693_1", "paragraph": "[BOS] On the other hand, there are many papers criticizing the aforementioned methods by questioning their faithfulness, correctness (Adebayo et al., 2018; Kindermans et al., 2017) and usefulness.\n[BOS] Smilkov et al. (2017) show that gradient based methods are susceptible to saturation and can be fooled by adversarial techniques.\n[BOS] Other sets of papers (Miller, 2019; Gilpin et al., 2018) attack model explanation papers from a philosophical perspective.\n[BOS] However, the lack of actionability angle is often overlooked.\n[BOS] Lipton (2018) briefly questions the practical benefit of having model explanations from a practitioners perspective.\n[BOS] There are several works taking advantage of model explanations.\n[BOS] Namely, using model explanations to aid doctors in diagnosing retinopathy patients , and removing minimal features, called pathologies, from neural networks by tuning the model to have high entropy on pathologies (Feng et al., 2018) .\n[BOS] The authors of Ross et al. (2017) propose a similar idea to our approach in that they regularize input gradients to alter the decision boundary of the model to make it more consistent with domain knowledge.\n[BOS] However, the input gradients technique has been shown to be an inaccurate explanation technique (Adebayo et al., 2018) .\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Multi_summ", "Transition", "Single_summ", "Transition", "Narrative_cite", "Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 17, "token_end": 43, "char_start": 89, "char_end": 180, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Adebayo et al., 2018;": "52938797", "Kindermans et al., 2017)": "28562869"}}}, {"token_start": 47, "token_end": 76, "char_start": 203, "char_end": 332, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Smilkov et al. (2017)": "11695878"}, "Reference": {}}}, {"token_start": 77, "token_end": 104, "char_start": 339, "char_end": 461, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Miller, 2019;": "36024272", "Gilpin et al., 2018)": "59600034"}, "Reference": {}}}, {"token_start": 118, "token_end": 137, "char_start": 536, "char_end": 652, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Lipton (2018)": null}, "Reference": {}}}, {"token_start": 174, "token_end": 191, "char_start": 889, "char_end": 961, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Feng et al., 2018)": "52003282"}}}, {"token_start": 194, "token_end": 234, "char_start": 974, "char_end": 1176, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ross et al. (2017)": "7053611"}, "Reference": {}}}, {"token_start": 238, "token_end": 261, "char_start": 1196, "char_end": 1301, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Adebayo et al., 2018)": "52938797"}}}]}
{"id": "195218693_0", "paragraph": "[BOS] For explaining ML models, recent research attempts offer techniques ranging from building inherently interpretable models to building a proxy model for explaining a more complex model (Ribeiro et al., 2016; Frosst and Hinton, 2017) to explaining inner mechanics of mostly uninterpretable neural networks (Sundararajan et al., 2017; Bach et al., 2015) .\n[BOS] One family of interpretability methods uses sensitivity of the network with respect to data points (Koh and Liang, 2017) or features (Ribeiro et al., 2016) as a form of explanation.\n[BOS] These methods rely on small, local perturbations and check how a network's response changes.\n[BOS] Explaining text models has another layer of complexity due to a lock of proper technique to generate counterfactuals in the form of small perturbations.\n[BOS] Hence, interpretability methods tailored for text are quite sparse (Mudrakarta et al., 2018; Jia and Liang, 2017; Murdoch et al., 2018) .\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Transition", "Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 20, "token_end": 49, "char_start": 131, "char_end": 237, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ribeiro et al., 2016;": "13029170", "Frosst and Hinton, 2017)": "3976789"}}}, {"token_start": 54, "token_end": 79, "char_start": 271, "char_end": 356, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sundararajan et al., 2017;": "16747630", "Bach et al., 2015)": "215192424"}}}, {"token_start": 88, "token_end": 104, "char_start": 409, "char_end": 485, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Koh and Liang, 2017)": "13193974"}}}, {"token_start": 105, "token_end": 116, "char_start": 489, "char_end": 520, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ribeiro et al., 2016)": "13029170"}}}, {"token_start": 172, "token_end": 207, "char_start": 818, "char_end": 946, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mudrakarta et al., 2018;": "21673814", "Jia and Liang, 2017;": "7228830", "Murdoch et al., 2018)": "25717172"}}}]}
{"id": "202542397_2", "paragraph": "[BOS] While most datasets are from single sources, Kennedy III et al. (2017) (Waseem and Hovy, 2016; Gao et al., 2017; Burnap and Williams, 2016; Badjatiya et al., 2017; Davidson et al., 2017) .\n[BOS] While there are several studies on the other sources, such as Instagram (Zhong et al., 2016) , Yahoo!\n[BOS] (Warner and Hirschberg, 2012; Nobata et al., 2016) , and Ask.fm (Van Hee et al., 2015) , the hate speech on Reddit and Gab is not widely studied.\n[BOS] What's more, all the previous hate speech datasets are built for the classification or detection of hate speech from a single post or user on social media, ignoring the context of the post and intervention methods needed to effectively calm down the users and diffuse negative online conversations.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 4, "token_end": 59, "char_start": 17, "char_end": 192, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Kennedy III et al. (2017)": "117121394", "(Waseem and Hovy, 2016;": "1721388", "Gao et al., 2017;": "25407580", "Burnap and Williams, 2016;": "1062269", "Badjatiya et al., 2017;": "2880908", "Davidson et al., 2017)": "1733167"}}}, {"token_start": 73, "token_end": 84, "char_start": 263, "char_end": 293, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhong et al., 2016)": "17529286"}}}, {"token_start": 85, "token_end": 108, "char_start": 296, "char_end": 359, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Warner and Hirschberg, 2012;": "12477446", "Nobata et al., 2016)": "11546523"}}}, {"token_start": 110, "token_end": 123, "char_start": 366, "char_end": 395, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "202542397_1", "paragraph": "[BOS] The dataset introduced by Chatzakou et al. (2017) is different from the other datasets as it investigates the behavior of hate-related users on Twitter, instead of evaluating hate-related tweets.\n[BOS] The large majority of the 1.5k users are labeled as spammers (31.8%) or normal (60.3%).\n[BOS] Only a small fraction of the users are labeled as bullies (4.5%) or aggressors (3.4%).\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 106, "char_start": 6, "char_end": 388, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Chatzakou et al. (2017)": "3347689"}, "Reference": {}}}]}
{"id": "202542397_0", "paragraph": "[BOS] In recent years, a few datasets for hate speech detection have been built and released by re-searchers.\n[BOS] Most are collected from Twitter and are labeled using a combination of expert and nonexpert hand labeling, or through machine learning assistance using a list of common negative words.\n[BOS] It is widely accepted that labels can vary in their accuracy overall, though this can be mitigated by relying on a consensus rule to rectify disagreements in labels.\n[BOS] A synopsis of these datasets can be found in Table 1 .\n[BOS] Waseem and Hovy (2016) collect 17k tweets based on hate-related slurs and users.\n[BOS] The tweets are manually annotated with three categories: sexist (20.0%), racist (11.7%), and normal (68.3%).\n[BOS] Because the authors identified a number of prolific users during the initial manual search, the resulting dataset has a small number of users (1,236 users) involved, causing a potential selection bias.\n[BOS] This problem is most prevalent on the 1,972 racist tweets, which are sent by only 9 Twitter users.\n[BOS] To avoid this problem, we did not identify suspicious user accounts or utilize user information when collecting our data.\n[BOS] Davidson et al. (2017) use a similar strategy, which combines the utilization of hate keywords and suspicious user accounts to build a dataset from Twitter.\n[BOS] But different from Waseem and Hovy (2016) , this dataset consists of 25k tweets randomly sampled from the 85.4 million posts of a large number of users (33,458 users) .\n[BOS] This dataset is proposed mainly to distinguish hateful and offensive language, which tend to be conflated by many studies.\n[BOS] Golbeck et al. (2017) focus on online harassment on Twitter and propose a fine-grained labeled dataset with 6 categories.\n[BOS] Founta et al. (2018) introduce a large Twitter dataset with 100k tweets.\n[BOS] Despite the large size of this dataset, the ratio of the hateful tweets are relatively low (5%).\n[BOS] Thus the size of the hateful tweets is around 5k in this dataset, which is not significantly larger than that of the previous datasets.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 107, "token_end": 233, "char_start": 540, "char_end": 1048, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Waseem and Hovy (2016)": "1721388"}, "Reference": {}}}, {"token_start": 256, "token_end": 357, "char_start": 1183, "char_end": 1643, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Davidson et al. (2017)": "1733167"}, "Reference": {"Waseem and Hovy (2016)": "1721388"}}}, {"token_start": 358, "token_end": 387, "char_start": 1650, "char_end": 1771, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Golbeck et al. (2017)": "30103179"}, "Reference": {}}}, {"token_start": 388, "token_end": 462, "char_start": 1778, "char_end": 2095, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Founta et al. (2018)": "3640499"}, "Reference": {}}}]}
{"id": "209079290_7", "paragraph": "[BOS] 3 Cross-lingual In-Domain Sentiment Analysis\n\n", "discourse_tags": ["Other"], "span_citation_mapping": []}
{"id": "209079290_6", "paragraph": "[BOS] Wada and Iwata (2018) proposed a similar cross-lingual language modeling architecture for unsupervised word translation.\n[BOS] They show that it outperforms mapping based approaches (Artetxe et al., 2018; , but only when a small amount of monolingual data is used.\n[BOS] The difference between their model and ours is that we adopt different parameter sharing strategies and consider the correlation between multiple domains.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 58, "char_start": 6, "char_end": 270, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Artetxe et al., 2018;": "21728524"}}}]}
{"id": "209079290_5", "paragraph": "[BOS] Cross-lingual Language Modeling Our work is also related to cross-lingual language modeling, which is a topic that has been explored by researchers very recently.\n[BOS] Lample and Conneau (2019) , pretrain a language model with a joint vocabulary on the concatenation of multiple largescale monolingual corpora and finetune it on labeled data.\n[BOS] However, this approach exploits crosslingual supervision provided by shared sub-word units, which has been shown to improve performance (Lample et al., 2018) , and it remains a challenge to efficiently perform cross-lingual transfer without exploiting shared identical strings.\n[BOS] In this work, we treat identical words from different languages as different words and thus eliminate any form of cross-lingual supervision.\n\n", "discourse_tags": ["Transition", "Single_summ", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 35, "token_end": 74, "char_start": 175, "char_end": 349, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Lample and Conneau (2019)": null}, "Reference": {}}}, {"token_start": 92, "token_end": 108, "char_start": 448, "char_end": 513, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lample et al., 2018)": "5033497"}}}]}
{"id": "209079290_4", "paragraph": "[BOS] While traditional CLSA methods assume that data in both languages is within the same domain (e.g. English hotel reviews for training and Chinese hotel review for testing, we refer to this setting as \"cross-lingual in-domain sentiment analysis\"), the more challenging cross-lingual crossdomain setting has also been explored.\n[BOS] Ziser and Reichart (2018) extend pivot-based monolingual domain adaption methods to the cross-lingual setting.\n[BOS] However, their method is not unsupervised and requires expensive cross-lingual resources.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 70, "token_end": 114, "char_start": 337, "char_end": 543, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ziser and Reichart (2018)": "52239522"}, "Reference": {}}}]}
{"id": "209079290_3", "paragraph": "[BOS] It is worth pointing out that the languageadversarial training model of (Chen et al., 2018b) is able to perform unsupervised CLSA without CLWE.\n[BOS] The proposed model consists of a feature extractor, a sentiment classifier and a language discriminator.\n[BOS] The feature extractor is trained to fool the discriminator so that the extracted features are language invariant.\n[BOS] However, its performance is significantly lower than the variant that uses pretrained CLWE.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 96, "char_start": 6, "char_end": 478, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Chen et al., 2018b)": "9387600"}, "Reference": {}}}]}
{"id": "209079290_2", "paragraph": "[BOS] Very recently, unsupervised CLSA methods that do not require either cross-lingual supervision or target language supervision have been proposed (Chen et al., 2018b,a) .\n[BOS] Chen et al. (2018a) transfer sentiment information from multiple source languages by jointly learning language invariant and language specific features.\n[BOS] Yet, these unsupervised CLSA methods rely on unsupervised CLWE which builds on the assumption that pretrained monolingual embeddings can be properly aligned.\n[BOS] This assumption, however, is not true in low-resource scenarios (Sgaard et al., 2018) .\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 14, "token_end": 37, "char_start": 74, "char_end": 172, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 39, "token_end": 64, "char_start": 181, "char_end": 333, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Chen et al. (2018a)": "52943384"}, "Reference": {}}}, {"token_start": 94, "token_end": 117, "char_start": 504, "char_end": 589, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(S\u00f8gaard et al., 2018)": null}}}]}
{"id": "209079290_1", "paragraph": "[BOS] Another line of CLSA research bridges the language gap using CLWE, which saves the efforts of training a machine translation system thus requires less cross-lingual resources.\n[BOS] Some work has proposed to map pretrained monolingual embeddings to a shared space (Barnes et al., 2018) to obtain CLWE while others proposed jointly learning CLWE and a sentiment classifier, allowing the embeddings to encode sentiment information (Zhou et al., 2016b; Xu and Wan, 2017) .\n\n", "discourse_tags": ["Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 42, "token_end": 60, "char_start": 214, "char_end": 291, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Barnes et al., 2018)": "43943649"}}}, {"token_start": 67, "token_end": 98, "char_start": 329, "char_end": 473, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhou et al., 2016b;": "14043561", "Xu and Wan, 2017)": "593447"}}}]}
{"id": "209079290_0", "paragraph": "[BOS] Cross-lingual Sentiment Analysis The most related topic to our work is cross-lingual sentiment analysis.\n[BOS] Some CLSA methods rely on machine translation systems (Wan, 2009; Demirtas and Pechenizkiy, 2013; Xiao and Guo, 2012; Zhou et al., 2016a) to provide cross-lingual supervision, making themselves implicitly dependant on largescale parallel corpus which may not be available for low-resource languages.\n[BOS] Wan (2009) apply the co-training algorithm to translated data while other researchers have proposed multi-view learning (Xiao and Guo, 2012) .\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Single_summ"], "span_citation_mapping": [{"token_start": 30, "token_end": 64, "char_start": 143, "char_end": 254, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wan, 2009;": "3135458", "Demirtas and Pechenizkiy, 2013;": null, "Xiao and Guo, 2012;": null, "Zhou et al., 2016a)": "12939637"}}}, {"token_start": 94, "token_end": 123, "char_start": 423, "char_end": 563, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wan (2009)": "3135458"}, "Reference": {"(Xiao and Guo, 2012)": null}}}]}
{"id": "202781416_1", "paragraph": "[BOS] End-to-end neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015) learns to generate an output sequence given an input sequence, using an encoder-decoder model.\n[BOS] The encoder captures the contextualized representation of the words in the input sentence for the decoder to generate the output sentence.\n[BOS] Following this intuition, McCann et al. (2017) trained an encoder-decoder model on parallel texts and obtained pre-trained contextualized word representations from the encoder.\n\n", "discourse_tags": ["Multi_summ", "Multi_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 75, "char_start": 6, "char_end": 337, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Sutskever et al., 2014;": "7961699", "Bahdanau et al., 2015)": "11212020"}, "Reference": {}}}, {"token_start": 80, "token_end": 110, "char_start": 370, "char_end": 520, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"McCann et al. (2017)": "9447219"}, "Reference": {}}}]}
{"id": "202781416_0", "paragraph": "[BOS] Continuous word representations in real-valued vectors, or commonly known as word embeddings, have been shown to help improve NLP performance.\n[BOS] Initially, exploiting continuous representations was achieved by adding real-valued vectors as classification features (Turian et al., 2010) .\n[BOS] Taghipour and Ng (2015) fine-tuned non-contextualized word embeddings by a feedforward neural network such that those word embeddings were more suited for WSD.\n[BOS] The finetuned embeddings were incorporated into an SVM classifier.\n[BOS] explored different strategies of incorporating word embeddings and found that their best strategy involved exponential decay that decreased the contribution of surrounding word features as their distances to the target word increased.\n[BOS] The neural sequence tagging approach has also been explored for WSD.\n[BOS] Kgebck and Salomonsson (2016) proposed bidirectional long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) for WSD.\n[BOS] They concatenated the hidden states of the forward and backward LSTMs and fed the concatenation into an affine transformation followed by softmax normalization, similar to the approach to incorporate a bidirectional LSTM adopted in sequence labeling tasks such as part-ofspeech tagging and named entity recognition (Ma and Hovy, 2016) .\n[BOS] proposed a self-attention layer on top of the concatenated bidirectional LSTM hidden states for WSD and introduced multi-task learning with part-ofspeech tagging and semantic labeling as auxiliary tasks.\n[BOS] However, on average across the test sets, their approach did not outperform SVM with word embedding features.\n[BOS] Subsequently, proposed the incorporation of glosses from WordNet in a bidirectional LSTM for WSD, and reported better results than both SVM and prior bidirectional LSTM models.\n[BOS] A neural language model (LM) is aimed at predicting a word given its surrounding context.\n[BOS] As such, the resulting hidden representation vector captures the context of a word in a sentence.\n[BOS] designed context2vec, which is a one-layer bidirectional LSTM trained to maximize the similarity between the hidden state representation of the LSTM and the target word embedding.\n[BOS] designed ELMo, which is a two-layer bidirectional LSTM language model trained to predict the next word in the forward LSTM and the previous word in the backward LSTM.\n[BOS] In both models, WSD was evaluated by nearest neighbor matching between the test and training instance representations.\n[BOS] However, despite training on a huge amount of raw texts, the resulting accuracies were still lower than those achieved by WSD approaches with pre-trained non-contextualized word representations.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Single_summ", "Single_summ", "Single_summ", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Transition", "Transition", "Single_summ", "Single_summ", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 43, "token_end": 54, "char_start": 250, "char_end": 295, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Turian et al., 2010)": "629094"}}}, {"token_start": 56, "token_end": 137, "char_start": 304, "char_end": 777, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Taghipour and Ng (2015)": "6443523"}, "Reference": {}}}, {"token_start": 152, "token_end": 251, "char_start": 859, "char_end": 1326, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"K\u00e5geb\u00e4ck and Salomonsson (2016)": "14748840"}, "Reference": {"(Hochreiter and Schmidhuber, 1997)": "1915014"}}}]}
{"id": "195504787_3", "paragraph": "[BOS] 4 System overview 4.1 Transformer models Our neural GEC systems are based on Transformer models (Vaswani et al., 2017) very good results (Junczys-Dowmunt et al., 2018b; Lichtarge et al., 2018) .\n[BOS] We apply GEC-specific adaptations proposed by Junczys-Dowmunt et al. (2018b) with some modifications.\n[BOS] Following the paper, we use extensive regularization to avoid overfitting to the limited labelled data, including dropping out entire source embeddings (Sennrich et al., 2016) , and additional dropout on attention and feed-forward network transformer layers.\n[BOS] For the sake of simplicity, we replace averaging the best four model checkpoints with exponential smoothing (Gardner, 1985) .\n[BOS] We increase the size of mini-batches as this improved the performance in early experiments.\n[BOS] Parameters of the full model are pre-trained on synthetic parallel data, instead of pre-training only the decoder parameters (Ramachandran et al., 2017) .\n[BOS] We also experiment with larger Transformer models as described in Section 5.3.\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection", "Reflection", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 18, "token_end": 57, "char_start": 83, "char_end": 198, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vaswani et al., 2017)": "13756489", "(Junczys-Dowmunt et al., 2018b;": "4953145", "Lichtarge et al., 2018)": "53220585"}}}, {"token_start": 61, "token_end": 86, "char_start": 216, "char_end": 308, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Junczys-Dowmunt et al. (2018b)": "4953145"}}}, {"token_start": 108, "token_end": 121, "char_start": 442, "char_end": 490, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sennrich et al., 2016)": null}}}, {"token_start": 152, "token_end": 160, "char_start": 666, "char_end": 703, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 201, "token_end": 214, "char_start": 916, "char_end": 962, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ramachandran et al., 2017)": "3488076"}}}]}
{"id": "195504787_2", "paragraph": "[BOS] Other recent work focuses on improving model inference.\n[BOS] Ge et al. (2018a) proposed correcting a sentence more than once through multi-round model inference.\n[BOS] Lichtarge et al. (2018) introduced iterative decoding to incrementally correct a sentence with a high-precision system.\n[BOS] The multiround correction approach has been further extended (Ge et al., 2018b) by interchanging decoding of a standard left-to-right model with a right-toleft model.\n[BOS] The authors claim that the two models display unique advantages for specific error types as they decode with different contexts.\n[BOS] Inspired by this finding, we adapt a common technique from NMT (Sennrich et al., 2016 ) that reranks with a right-to-left model, but without multiple rounds.\n[BOS] We contend that multiple rounds are only necessary if the system has low recall.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 12, "token_end": 34, "char_start": 68, "char_end": 168, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 35, "token_end": 60, "char_start": 175, "char_end": 294, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Lichtarge et al. (2018)": "53220585"}, "Reference": {}}}, {"token_start": 61, "token_end": 124, "char_start": 301, "char_end": 602, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 136, "token_end": 148, "char_start": 668, "char_end": 696, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sennrich et al., 2016": null}}}]}
{"id": "195504787_1", "paragraph": "[BOS] Although our unsupervised method for synthesising parallel data by means of an (inverted) spellchecker is novel, the idea of generating artificial errors has been explored in the literature before, as summarized by Felice (2016) .\n[BOS] Previously proposed methods usually require a errorannotated corpus as a seed to generate artificial errors reflecting linguistic properties and error distributions observed in natural-error corpora (Foster and Andersen, 2009; Felice and Yuan, 2014) .\n[BOS] Artificial error generation methods spanned conditional probabilistic models for specific error types only (Rozovskaya and Roth, 2010; Rozovskaya et al., 2014; Felice and Yuan, 2014) , statistical or neural MT systems trained on reversed source and target sides (Rei et al., 2017; Kasewa et al., 2018) or neural sequence transduction models (Xie et al., 2018) .\n[BOS] None of these methods is unsupervised.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 26, "token_end": 47, "char_start": 123, "char_end": 234, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 75, "token_end": 93, "char_start": 420, "char_end": 492, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 100, "token_end": 137, "char_start": 545, "char_end": 683, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rozovskaya and Roth, 2010;": "15175549", "Rozovskaya et al., 2014;": "5223238"}}}, {"token_start": 140, "token_end": 168, "char_start": 701, "char_end": 802, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rei et al., 2017;": "6751341", "Kasewa et al., 2018)": "52896498"}}}, {"token_start": 169, "token_end": 181, "char_start": 806, "char_end": 860, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Xie et al., 2018)": "21730715"}}}]}
{"id": "195504787_0", "paragraph": "[BOS] Many recent advances in neural GEC aim at overcoming the mentioned data sparsity problem.\n[BOS] Ge et al. (2018a) proposed fluency-boost learning that generates additional training examples during training from an independent backward model or the forward model being trained.\n[BOS] Xie et al. (2018) supplied their model with noisy examples synthesized from clean sentences.\n[BOS] Junczys-Dowmunt et al. (2018b) utilized a large amount of monolingual data by pre-training decoder parameters with a language model, and Lichtarge et al. (2018 Lichtarge et al. ( , 2019 , on the other hand, used a large-scale out-of-domain parallel corpus extracted from Wikipedia revisions to pre-train their models.\n[BOS] We also pre-train a neural sequence-to-sequence model, but we do so solely on synthetic data.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Multi_summ", "Reflection"], "span_citation_mapping": [{"token_start": 19, "token_end": 51, "char_start": 102, "char_end": 282, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 52, "token_end": 70, "char_start": 289, "char_end": 381, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Xie et al. (2018)": "21730715"}, "Reference": {}}}, {"token_start": 71, "token_end": 103, "char_start": 388, "char_end": 519, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Junczys-Dowmunt et al. (2018b)": "4953145"}, "Reference": {}}}, {"token_start": 105, "token_end": 152, "char_start": 525, "char_end": 705, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Lichtarge et al. (2018": "53220585", "Lichtarge et al. ( , 2019": "118680003"}, "Reference": {}}}]}
{"id": "196212349_2", "paragraph": "[BOS] The literature relating distributional semantics to neural data started with Mitchell et al. (2008) , who predicted fMRI brain activity patterns from distributional representations for 60 hand-picked nouns from 12 different semantic categories (e.g. 'animals', 'vegetables', etc.).\n[BOS] Many later studies built on top of this; for example, Sudre et al. (2012) was a similar experiment using MEG, another neuroimaging technique.\n[BOS] Other studies (e.g., Xu et al. 2016) reused Mitchell et al. (2008) 's original dataset but experimented with different word embedding models, including distributional models such as word2vec (Mikolov et al., 2013) or GloVe, perceptual models (Anderson et al., 2013; Abnar et al., 2018) and dependency-based models (Abnar et al., 2018) .\n[BOS] Similarly, Gauthier and Ivanova (2018) reused Pereira et al. (2018) 's data and regression model but tested it on alternative sentence embedding models.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Multi_summ", "Multi_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 57, "char_start": 6, "char_end": 287, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Mitchell et al. (2008)": "6105164"}, "Reference": {}}}, {"token_start": 59, "token_end": 89, "char_start": 299, "char_end": 435, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Sudre et al. (2012)": "4926897"}, "Reference": {}}}, {"token_start": 90, "token_end": 116, "char_start": 442, "char_end": 528, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Xu et al. 2016)": "2401071", "Mitchell et al. (2008)": "6105164"}}}, {"token_start": 130, "token_end": 143, "char_start": 624, "char_end": 655, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mikolov et al., 2013)": "5959482"}}}, {"token_start": 144, "token_end": 165, "char_start": 659, "char_end": 727, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Anderson et al., 2013;": "6467224", "Abnar et al., 2018)": "3915390"}}}, {"token_start": 166, "token_end": 179, "char_start": 732, "char_end": 776, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Abnar et al., 2018)": "3915390"}}}, {"token_start": 181, "token_end": 217, "char_start": 785, "char_end": 937, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Gauthier and Ivanova (2018)": "46937111", "Pereira et al. (2018)": "3664748"}, "Reference": {}}}]}
{"id": "196212349_1", "paragraph": "[BOS] The vocabulary was selected by clustering a pre-trained GloVe space (Pennington et al., 2014) 2 consisting of 30,000 words into regions, and then manually selecting a word from each region, yielding a set of 180 content words that include nouns (both concrete and abstract), verbs, and adjectives.\n[BOS] Next, for every participant, a vector space was created whose dimensions are voxel activation values in that participant's brain scan.\n[BOS] 3 This (approximately) 200,000-dimensional space can be optionally reduced to 5,000 dimensions using a complex feature selection process.\n[BOS] Finally, for every participant, a ridge regression model was trained for mapping this brain space to the GloVe space.\n[BOS] Crucially, this model predicts each of the 300 GloVe dimensions separately, the authors' hypothesis being that variation in each dimension of semantic space corresponds to specific brain activation patterns.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 12, "token_end": 24, "char_start": 62, "char_end": 99, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Pennington et al., 2014)": "1957433"}}}]}
{"id": "196212349_0", "paragraph": "[BOS] Our work is largely built on top of Pereira et al. (2018) , which to date is the most extensive attempt at decoding meaning representations from brain imaging data.\n[BOS] In this study (Experiment 1), fMRI images of 180 different content words were collected for 16 participants.\n[BOS] The stimulus words were presented in three different ways: the written word plus an image representing the word, the word in a word cloud, and the word in a sentence.\n[BOS] Thus, the dataset consists of 1803 = 540 images per participant.\n[BOS] Additionally, a combined representation was created for each word by averaging the images from the three stimulus presentation paradigms.\n[BOS] Note that data for different participants cannot be directly combined due to differences in brain organization; 1 decoders are always trained for each participant individually.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 10, "token_end": 160, "char_start": 42, "char_end": 856, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Pereira et al. (2018)": "3664748"}, "Reference": {}}}]}
{"id": "199379345_1", "paragraph": "[BOS] The current work is also inspired by the fluency boost learning proposed in Ge et al. (2018) .\n[BOS] In their study, sentence fluency is defined as the inverse of the sentence's cross entropy.\n[BOS] During fluency boost training, the fluency of candidate sentences generated by their GEC seq2seq model is monitored.\n[BOS] Candidate sentences with less than perfect fluency compared to the correct ones are appended as additional error-contained data for subsequent training.\n[BOS] Fluency is also used during multi-round GEC inference, in that inference continues as long as the fluency of the output sentences keeps improving.\n[BOS] The present study uses fluency measure in an opposite way.\n[BOS] We examine how the decrease of fluency in artificial error sentences influences the performance of grammatical error correction.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 10, "token_end": 121, "char_start": 47, "char_end": 633, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ge et al. (2018)": "49664123"}, "Reference": {}}}]}
{"id": "199379345_0", "paragraph": "[BOS] Our work mainly builds on the context window approach to artificial error generation.\n[BOS] In this approach, all the possible error fragments (errors and their surrounding context) and their corresponding correct fragments are first extracted from GEC annotated corpora.\n[BOS] For example, I follows his and I follow his are the fragments extracted from the example sentences in the first paragraph.\n[BOS] With these correct-incorrect fragments, for each errorfree sentence, if we find the same correct fragment in the sentence, we can inject errors by replacing that fragment with the incorrect one.\n[BOS] Felice (2016) has shown that a context window size of one, that is, one token before and after the error words or phrases, is able to generate a decent amount of error sentences while maintaining the plausibility of the errors.\n[BOS] Thus, the current study also adopts this context window size in extracting fragments.\n\n", "discourse_tags": ["Reflection", "Transition", "Transition", "Transition", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 110, "token_end": 159, "char_start": 614, "char_end": 841, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "196171642_2", "paragraph": "[BOS] Conversational Question Answering (CQA).\n[BOS] CQA aims to automatically answer a sequence of questions.\n[BOS] It has been studied in the knowledge base setting (Saha et al., 2018; Iyyer et al., 2017) and is often framed as a semantic parsing problem.\n[BOS] Recently released large-scale datasets (Reddy et al., 2018; enabled studying it in the textual setting where the information source used to answer questions is a given passage, and they inspired many significant work (Zhu et al., 2018; Yatskar, 2018) .\n[BOS] However, collecting such datasets has heavily relied on human efforts and can be very costly.\n[BOS] Based on one of the most popular datasets CoQA (Reddy et al., 2018) , we examine the possibility of automatically generating conversational questions, which can potentially reduce the data collection cost for CQA.\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite", "Narrative_cite", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 29, "token_end": 50, "char_start": 140, "char_end": 206, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Saha et al., 2018;": "19240019", "Iyyer et al., 2017)": "2623009"}}}, {"token_start": 64, "token_end": 117, "char_start": 282, "char_end": 514, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Reddy et al., 2018;": "52055325"}, "Reference": {"(Zhu et al., 2018;": "54460803", "Yatskar, 2018)": "52895001"}}}, {"token_start": 145, "token_end": 156, "char_start": 665, "char_end": 690, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Reddy et al., 2018)": "52055325"}}}]}
{"id": "196171642_1", "paragraph": "[BOS] Conversation Generation.\n[BOS] Building chatbots and conversational agents has been pursued by many previous work (Ritter et al., 2011; Vinyals and Le, 2015; Sordoni et al., 2015; Serban et al., 2016a; Li et al., 2016a,b) .\n[BOS] Vinyals and Le (2015) used a Sequence-to-Sequence neural network for generating a response given the dialog history.\n[BOS] Li et al. (2016a) further optimized the response diversity by maximizing the mutual information between inputs and output responses.\n[BOS] Different from these work where the response can be in any form (usually a declarative statement) and is generated solely based on the dialog history, our task is potentially more challenging as it additionally restricts the generated response to be a follow-up question about a given passage.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Single_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 6, "token_end": 66, "char_start": 37, "char_end": 227, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ritter et al., 2011;": "780171", "Vinyals and Le, 2015;": "12300158", "Sordoni et al., 2015;": "94285", "Serban et al., 2016a;": "6126582"}}}, {"token_start": 68, "token_end": 94, "char_start": 236, "char_end": 352, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Vinyals and Le (2015)": "12300158"}, "Reference": {}}}, {"token_start": 95, "token_end": 119, "char_start": 359, "char_end": 491, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Li et al. (2016a)": "7287895"}, "Reference": {}}}]}
{"id": "196171642_0", "paragraph": "[BOS] Question Generation.\n[BOS] Generating questions from various kinds of sources, such as texts (Rus et al., 2010; Heilman and Smith, 2010; Mitkov and Ha, 2003; Du et al., 2017) , search queries (Zhao et al., 2011) , knowledge bases (Serban et al., 2016b) and images (Mostafazadeh et al., 2016) , has attracted much attention recently.\n[BOS] Our work is most related to previous work on generating questions from sentences or paragraphs.\n[BOS] Most early approaches are based on rules and templates (Heilman and Smith, 2010; Mitkov and Ha, 2003) , while Du et al. (2017) recently proposed to generate a question by a Sequence-to-Sequence neural network model with attention (Luong et al., 2015) .\n[BOS] Other approaches such as Subramanian et al., 2017) take into account the answer information in addition to the given sentence or paragraph.\n[BOS] (Du and Cardie, 2018; Song et al., 2018) further modeled the surrounding paragraph-level information of the given sentence.\n[BOS] However, most of the work focused on generating standalone questions solely based on a sentence or a paragraph.\n[BOS] In contrast, this work explores conversational question generation and has to additionally consider the conversation history in order to generate a coherent question, making the task much more challenging.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Reflection", "Multi_summ", "Single_summ", "Multi_summ", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 6, "token_end": 46, "char_start": 33, "char_end": 180, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rus et al., 2010;": "266197", "Heilman and Smith, 2010;": "1809816", "Mitkov and Ha, 2003;": "173900", "Du et al., 2017)": "2172129"}}}, {"token_start": 47, "token_end": 57, "char_start": 183, "char_end": 217, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhao et al., 2011)": "1969946"}}}, {"token_start": 58, "token_end": 70, "char_start": 220, "char_end": 258, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 71, "token_end": 84, "char_start": 263, "char_end": 297, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mostafazadeh et al., 2016)": "16227864"}}}, {"token_start": 115, "token_end": 133, "char_start": 482, "char_end": 548, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Heilman and Smith, 2010;": "1809816", "Mitkov and Ha, 2003)": "173900"}}}, {"token_start": 135, "token_end": 169, "char_start": 557, "char_end": 697, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Du et al. (2017)": "2172129"}, "Reference": {"(Luong et al., 2015)": "1998416"}}}, {"token_start": 175, "token_end": 199, "char_start": 731, "char_end": 845, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Subramanian et al., 2017)": "7618554"}, "Reference": {}}}, {"token_start": 200, "token_end": 228, "char_start": 852, "char_end": 975, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Du and Cardie, 2018;": "21702856", "Song et al., 2018)": "44178763"}, "Reference": {}}}]}
{"id": "184482894_0", "paragraph": "[BOS] Offensive language on social media hardly remains unnoticed.\n[BOS] Contents involving hateful messages vary from hate speech to group-based racism and could target anyone irrespective of their status, identity, location and so forth.\n[BOS] Even when it is not materialized into a hate-motivated crime, the damage is done victims are being labeled, marginalised and exposed to negative stereotyping.\n[BOS] The overall consequences of online hate can be the dehumanisation of individuals or groups of individuals.\n[BOS] The need for proper strategies to tackle hate speech on social media is unquestionable.\n[BOS] The core focus of the thesis is not to find a solution to the challenge, but rather to identify central problems that have contributed to the formation of the existing reality.\n[BOS] To unrave the contributing factors, a holistic analysis of both international human rights principles regarding hate speech and the practical application of those standards is necessary.\n[BOS] (Schofield and Davidson, 2017) There have been many studies and publication on the topic of offensive language and hate speech over the last few years.\n[BOS] Examples on such studies include , (Malmasi and Zampieri, 2017) , (ElSherief et al., 2018) , (Gambck and Sikdar, 2017) , (Zhang et al., 2018) .\n[BOS] Also there have been challenges on how to distinguish profanity from hate-speech presented by (Malmasi and Zampieri, 2018) .\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Transition", "Transition", "Other", "Transition", "Narrative_cite", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 191, "token_end": 199, "char_start": 994, "char_end": 1024, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Schofield and Davidson, 2017)": "28481466"}}}, {"token_start": 230, "token_end": 241, "char_start": 1187, "char_end": 1215, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Malmasi and Zampieri, 2017)": "19182892"}}}, {"token_start": 242, "token_end": 252, "char_start": 1218, "char_end": 1242, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(ElSherief et al., 2018)": "4809781"}}}, {"token_start": 253, "token_end": 264, "char_start": 1245, "char_end": 1270, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Gamb\u00e4ck and Sikdar, 2017)": null}}}, {"token_start": 265, "token_end": 273, "char_start": 1273, "char_end": 1293, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhang et al., 2018)": "46939253"}}}, {"token_start": 288, "token_end": 305, "char_start": 1371, "char_end": 1424, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Malmasi and Zampieri, 2018)": "3936688"}}}]}
{"id": "202540345_5", "paragraph": "[BOS] Unlike the above work that attempts to learn a summary evaluation metric, the target of our work is to learn a good reward, which is not necessarily a good evaluation metric.\n[BOS] A good evaluation metric should be able to correctly rank summaries of different quality levels, while a good reward function focuses more on distinguishing the best summaries from the mediocre and bad summaries.\n[BOS] Also, an evaluation metric should be able to evaluate summaries of different types (e.g. extractive and abstractive) and from different genres, while a reward function can be specifically designed for a single task.\n[BOS] We leave the learning of a generic summarisation evaluation metric for future work.\n\n", "discourse_tags": ["Reflection", "Transition", "Transition", "Reflection"], "span_citation_mapping": []}
{"id": "202540345_4", "paragraph": "[BOS] Rewards learned with extra data.\n[BOS] Pasunuru and Bansal (2018) propose two novel rewards for training RL-based abstractive summarisers: RougeSal, which up-weights the salient phrases and words detected via a keyphrase classifier, and Entail reward, which gives high scores to logically-entailed summaries using an entailment classifier.\n[BOS] RougeSal is trained with the SQuAD reading comprehension dataset (Rajpurkar et al., 2016) , and Entail is trained with the SNLI (Bowman et al., 2015) and Multi-NLI (Williams et al., 2018) datasets.\n[BOS] Although their system achieves new state-of-the-art results in terms of ROUGE, it remains unclear whether their system generates more human-appealing summaries as they do not perform human evaluation experiments.\n[BOS] Additionally, both rewards require reference summaries.\n[BOS] Louis and Nenkova (2013) , Peyrard et al. (2017) and build featurerich regression models to learn a summary evaluation metric directly from the human judgement scores (Pyramid and Responsiveness) provided in the TAC'08 and '09 datasets 1 .\n[BOS] Some features they use require reference summaries (e.g. ROUGE metrics); the others are heuristic-based and do not use reference summaries (e.g. the JensenShannon divergence between the word distributions in the summary and the documents).\n[BOS] Their experiments suggest that with only non-referencesummary-based features, the correlation of their learned metric with human judgements is lower than ROUGE; with reference-summary-based features, the learned metric marginally outperforms ROUGE.\n[BOS] In 6, we show that our reward model does not use reference summaries but outperforms the feature-based baseline by as well as ROUGE.\n\n", "discourse_tags": ["Transition", "Single_summ", "Narrative_cite", "Single_summ", "Transition", "Multi_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 9, "token_end": 76, "char_start": 45, "char_end": 345, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Pasunuru and Bansal (2018)": "4940548"}, "Reference": {}}}, {"token_start": 84, "token_end": 99, "char_start": 381, "char_end": 441, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rajpurkar et al., 2016)": "11816014"}}}, {"token_start": 106, "token_end": 117, "char_start": 475, "char_end": 501, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bowman et al., 2015)": "14604520"}}}, {"token_start": 118, "token_end": 130, "char_start": 506, "char_end": 539, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Williams et al., 2018)": "3432876"}}}, {"token_start": 183, "token_end": 338, "char_start": 837, "char_end": 1577, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Louis and Nenkova (2013)": "17829732", "Peyrard et al. (2017)": "38926436"}, "Reference": {}}}]}
{"id": "202540345_3", "paragraph": "[BOS] Heuristic-based rewards.\n[BOS] Prior work proposed heuristic-based reward functions to train crossinput RL summarisers, in order to strengthen certain properties of the generated summaries.\n[BOS] Arumae and Liu (2019) propose four reward functions to train an RL-based extractive summariser, including the question-answering competency rewards, which encourage the RL agent to generate summaries that can answer cloze-style questions.\n[BOS] Such questions are automatically created by removing some words in the reference summaries.\n[BOS] Experiments suggest that human subjects can answer the questions with high accuracy by reading their generated summaries; but the human judgement scores of their summaries are not higher than the summaries generated by the stateof-the-art supervised system.\n[BOS] Kryscinski et al. (2018) propose a simple heuristic that encourages the RL-based abstractive summariser to generate summaries with more novel tokens, i.e. tokens that do not appear in the input document.\n[BOS] However, both ROUGE and human evaluation scores of their system are lower than the state-of-the-art summarisation systems (e.g. See et al., 2017) .\n[BOS] In addition, the above rewards require reference summaries, unlike our reward that only takes a document and a generated summary as input.\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ", "Single_summ", "Transition", "Single_summ", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 37, "token_end": 101, "char_start": 202, "char_end": 538, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 147, "token_end": 193, "char_start": 809, "char_end": 1012, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kryscinski et al. (2018)": "52091366"}, "Reference": {}}}, {"token_start": 210, "token_end": 232, "char_start": 1102, "char_end": 1164, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"See et al., 2017)": null}}}]}
{"id": "202540345_2", "paragraph": "[BOS] The reward learning idea is also related to inverse RL (IRL) (Ng and Russell, 2000) .\n[BOS] By observing some (near-)optimal sequences of actions, IRL algorithms learn a reward function that is consistent with the observed sequences.\n[BOS] In the case of summarisation, human-written reference summaries are the (near-)optimal sequences, which are expensive to provide.\n[BOS] Our method only needs human ratings on some generated summaries, also known as the bandit feedback (Kreutzer et al., 2017) , to learn the reward function.\n[BOS] Furthermore, when employing certain loss functions (see 4 and Eq.\n[BOS] (2)), our method can even learn the reward function from preferences over generated summaries, an even cheaper feedback to elicit (Kreutzer et al., 2018; Gao et al., 2018) .\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 10, "token_end": 23, "char_start": 50, "char_end": 89, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ng and Russell, 2000)": "34196655"}}}, {"token_start": 96, "token_end": 109, "char_start": 465, "char_end": 504, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kreutzer et al., 2017)": "17355453"}}}, {"token_start": 147, "token_end": 173, "char_start": 689, "char_end": 786, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kreutzer et al., 2018;": "44062452", "Gao et al., 2018)": "52119858"}}}]}
{"id": "202540345_1", "paragraph": "[BOS] In 7 we use our learned reward to train both cross-input and input-specific RL systems.\n[BOS] A similar idea has been explored by , but unlike their work that learns the reward from ROUGE scores, we learn our reward directly from human ratings.\n[BOS] Human evaluation experiments suggest that our reward can guide both kinds of RL-based systems to generate human-appealing summaries without using reference summaries.\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "202540345_0", "paragraph": "[BOS] RL-based summarisation.\n[BOS] Most existing RLbased summarisers fall into two categories: crossinput systems and input-specific systems .\n[BOS] Cross-input systems learn a summarisation policy at training time by letting the RL agent interact with a ROUGE-based reward function.\n[BOS] At test time, the learned policy is used to generate a summary for each input document.\n[BOS] Most RL-based summarisers fall into this category (Chen and Bansal, 2018; Narayan et al., 2018b; Dong et al., 2018; Kryscinski et al., 2018; Pasunuru and Bansal, 2018; .\n[BOS] As an alternative, input-specific RL (Rioux et al., 2014; Ryang and Abekawa, 2012) does not require reference summaries: for each input document at test time, a summarisation policy is trained specifically for the input, by letting the RL summariser interact with a heuristic-based reward function, e.g. ROUGE between the generated summary and the input document (without using any reference summaries).\n[BOS] However, the performance of inputspecific RL falls far behind the cross-input counterparts.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Transition", "Narrative_cite", "Multi_summ", "Multi_summ"], "span_citation_mapping": [{"token_start": 78, "token_end": 132, "char_start": 385, "char_end": 551, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chen and Bansal, 2018;": "44129061", "Narayan et al., 2018b;": "3510042", "Dong et al., 2018;": "52843977", "Kryscinski et al., 2018;": "52091366"}}}, {"token_start": 139, "token_end": 241, "char_start": 580, "char_end": 1062, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Rioux et al., 2014;": "10221032", "Ryang and Abekawa, 2012)": "14666654"}, "Reference": {}}}]}
{"id": "174800714_3", "paragraph": "[BOS] There have been several studies reporting case studies on headline generation for different real services: (a) question headlines on question answering service (Higurashi et al., 2018) , (b) product headlines on e-commerce service (Wang et al., 2018) , and (c) headlines for product curation pages Camargo de Souza et al., 2018) .\n[BOS] The first two (a) and (b) are extractive approaches, and the last one (c) is an abstractive approach, where the input is a set of slot/value pairs, such as \"color/white.\"\n[BOS] That is, our task is more difficult to use in the real-world.\n[BOS] In addition, application to news services tends to be sensitive since news articles contain serious contents such as incidents, accidents, and disasters.\n[BOS] Thus, our work should be valuable as a rare case study applying a neural model to such a news service.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Reflection", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 22, "token_end": 39, "char_start": 117, "char_end": 190, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Higurashi et al., 2018)": "52009825"}}}, {"token_start": 43, "token_end": 59, "char_start": 197, "char_end": 256, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wang et al., 2018)": "19208846"}}}, {"token_start": 64, "token_end": 82, "char_start": 267, "char_end": 334, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Camargo de Souza et al., 2018)": "53245252"}}}]}
{"id": "174800714_2", "paragraph": "[BOS] Considering technologies used for editing support, there have been many studies for various purposes, such as spelling error correction (Farra et al., 2014; Hasan et al., 2015; Etoori et al., 2018) , grammatical error correction (Dahlmeier and Ng, 2012; Susanto et al., 2014; Choshen and Abend, 2018) , fact checking (Baly et al., 2018; Thorne and Vlachos, 2018; Lee et al., 2018) , fluency evaluation (Vadlapudi and Katragadda, 2010; Heilman et al., 2014; Kann et al., 2018) , and so on.\n[BOS] However, when we consider their studies on our task, they are only used after editing (writing a draft).\n[BOS] On the other hand, the purpose of our tool is different from theirs since our tool can support editors before or during editing.\n[BOS] The usage of (interactive) machine translation systems (Denkowski et al., 2014; Gonzlez-Rubio et al., 2016; Wuebker et al., 2016; Ye et al., 2016; Takeno et al., 2017) for supporting manual post-editing are similar to our purpose, but their task is completely different from ours.\n[BOS] In other words, their task is a translation without information loss, whereas our task is a summarization that requires information compression.\n[BOS] We believe that a case study on summarization is still important for the summarization community.\n\n", "discourse_tags": ["Narrative_cite", "Reflection", "Reflection", "Narrative_cite", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 20, "token_end": 51, "char_start": 116, "char_end": 203, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Farra et al., 2014;": "8332619", "Hasan et al., 2015;": "18201257", "Etoori et al., 2018)": "51871135"}}}, {"token_start": 52, "token_end": 80, "char_start": 206, "char_end": 306, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dahlmeier and Ng, 2012;": "9613043", "Susanto et al., 2014;": "884251", "Choshen and Abend, 2018)": "13747175"}}}, {"token_start": 81, "token_end": 108, "char_start": 309, "char_end": 386, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Baly et al., 2018;": "5061667", "Thorne and Vlachos, 2018;": "49320819", "Lee et al., 2018)": "53079238"}}}, {"token_start": 109, "token_end": 139, "char_start": 389, "char_end": 481, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vadlapudi and Katragadda, 2010;": "1130599", "Heilman et al., 2014;": "8719319", "Kann et al., 2018)": null}}}, {"token_start": 202, "token_end": 249, "char_start": 774, "char_end": 914, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Denkowski et al., 2014;": "8550980", "Gonz\u00e1lez-Rubio et al., 2016;": "11523646", "Wuebker et al., 2016;": "8014052", "Ye et al., 2016;": "2584684", "Takeno et al., 2017)": "27834461"}}}]}
{"id": "174800714_1", "paragraph": "[BOS] News-headline-generation tasks have been extensively studied since early times (Wang et al., 2005; Soricut and Marcu, 2006; Woodsend et al., 2010; Alfonseca et al., 2013; Sun et al., 2015; Colmenares et al., 2015) .\n[BOS] In this line of research, Rush et al. (2015) proposed a neural model to generate news headlines and released a benchmark dataset for their task, and consequently this task has recently received increasing attention (Chopra et al., 2016; Takase et al., 2016; Kiyono et al., 2017; Zhou et al., 2017; Ayana et al., 2017; Raffel et al., 2017; Cao et al., 2018; Kobayashi, 2018) .\n[BOS] However, their approaches were basically based on the encoderdecoder model, which is trained with a lot of (article, headline) pairs.\n[BOS] This means that there are few situations for putting their models into the real world because news articles typically already have corresponding headlines, and most editors create a headline before its content (according to a senior journalist).\n[BOS] Therefore, our work can strongly support their approaches from a practical perspective.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 66, "char_start": 6, "char_end": 219, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wang et al., 2005;": "18805726", "Soricut and Marcu, 2006;": "934325", "Woodsend et al., 2010;": "2292300", "Alfonseca et al., 2013;": "16997286", "Sun et al., 2015;": "7213770", "Colmenares et al., 2015)": "16992492"}}}, {"token_start": 74, "token_end": 200, "char_start": 254, "char_end": 743, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Rush et al. (2015)": "1918428"}, "Reference": {"(Chopra et al., 2016;": "133195", "Takase et al., 2016;": "5450302", "Kiyono et al., 2017;": "23971423", "Zhou et al., 2017;": "1770102", "Ayana et al., 2017;": "22324032", "Raffel et al., 2017;": "14345813", "Cao et al., 2018;": "19198109", "Kobayashi, 2018)": "53080554"}}}]}
{"id": "174800714_0", "paragraph": "[BOS] We briefly review related studies from three aspects: news headline generation, editing support, and application of headline generation.\n[BOS] In summary, our work is the first attempt to deploy a neural news-headline-generation model to a realworld application, i.e., news editing support tool.\n\n", "discourse_tags": ["Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "208332276_1", "paragraph": "[BOS] MT has been greatly used to generate paraphrases (Quirk et al., 2004; Zhao et al., 2008) due to the availability of large corpora.\n[BOS] While much earlier works have explored the use of manually drafted rules (Hassan et al., 2007; Kozlowski et al., 2003) .\n[BOS] Similar to our model architecture, combined transformers and RNN-based encoders for MT.\n[BOS] Zhao et al. (2018) recently used the transformer model for paraphrasing on different datasets.\n[BOS] We experimented using solely a transformer but got better results with TRANSEQ.\n[BOS] To the best of our knowledge, our work is the first to cross-breed the transformer and seq2seq for the task of paraphrase generation.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Reflection", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 8, "token_end": 28, "char_start": 34, "char_end": 94, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Quirk et al., 2004;": "13043395", "Zhao et al., 2008)": "12003953"}}}, {"token_start": 37, "token_end": 68, "char_start": 143, "char_end": 261, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Kozlowski et al., 2003)": "3099330"}}}, {"token_start": 89, "token_end": 109, "char_start": 364, "char_end": 458, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhao et al. (2018)": "53080846"}, "Reference": {}}}]}
{"id": "208332276_0", "paragraph": "[BOS] Our baseline models -VAE-SVG-EQ (Gupta et al., 2018) and RbM-SL (Li et al., 2018) are both deep learning models.\n[BOS] While the former uses a variational-autoencoder and is capable of generating multiple paraphrases of a given sentence, the later uses deep reinforcement learning.\n[BOS] In tune, with part of our approach, ie, seq2seq, there exists ample models with interesting variants -residual LSTM (Prakash et al., 2016) , bi-directional GRU with attention and special decoding tweaks (Cao et al., 2017) , attention from the perspective of semantic parsing (Su and Yan, 2017) .\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 6, "token_end": 21, "char_start": 27, "char_end": 58, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 22, "token_end": 34, "char_start": 63, "char_end": 87, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Li et al., 2018)": "21646317"}}}, {"token_start": 96, "token_end": 108, "char_start": 396, "char_end": 432, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Prakash et al., 2016)": "9385494"}}}, {"token_start": 109, "token_end": 129, "char_start": 435, "char_end": 515, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cao et al., 2017)": "13531903"}}}, {"token_start": 130, "token_end": 144, "char_start": 518, "char_end": 587, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Su and Yan, 2017)": "2439226"}}}]}
{"id": "201646551_1", "paragraph": "[BOS] More recently, advances in coreference resolution and other NLP tasks have been driven by unsupervised contextualized representations (Peters et al., 2018; Devlin et al., 2019; McCann et al., 2017; Joshi et al., 2019; Liu et al., 2019b) .\n[BOS] Of these, BERT (Devlin et al., 2019) notably uses pretraining on passage-level sequences (in conjunction with a bidirectional masked language modeling objective) to more effectively model long-range dependencies.\n[BOS] SpanBERT (Joshi et al., 2019) focuses on pretraining span representations achieving current state of the art results on OntoNotes with the independent variant.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 19, "token_end": 66, "char_start": 96, "char_end": 242, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Peters et al., 2018;": "3626819", "Devlin et al., 2019;": "52967399", "McCann et al., 2017;": "9447219", "Joshi et al., 2019;": "198229624"}}}, {"token_start": 71, "token_end": 111, "char_start": 261, "char_end": 463, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Devlin et al., 2019)": "52967399"}, "Reference": {}}}, {"token_start": 112, "token_end": 146, "char_start": 470, "char_end": 629, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Joshi et al., 2019)": "198229624"}, "Reference": {}}}]}
{"id": "201646551_0", "paragraph": "[BOS] Scoring span or mention pairs has perhaps been one of the most dominant paradigms in coreference resolution.\n[BOS] The base coreference model used in this paper from belongs to this family of models (Ng and Cardie, 2002; Bengtson and Roth, 2008; Denis and Baldridge, 2008; Fernandes et al., 2012; Durrett and Klein, 2013; Wiseman et al., 2015; Clark and Manning, 2016; Lee et al., 2017) .\n\n", "discourse_tags": ["Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 35, "token_end": 100, "char_start": 188, "char_end": 392, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ng and Cardie, 2002;": null, "Bengtson and Roth, 2008;": "8179642", "Denis and Baldridge, 2008;": "535939", "Fernandes et al., 2012;": "1884019", "Durrett and Klein, 2013;": "16039645", "Wiseman et al., 2015;": "15842085", "Clark and Manning, 2016;": "2012188", "Lee et al., 2017)": "1222212"}}}]}
{"id": "201646223_1", "paragraph": "[BOS] To the best of our knowledge, this is the first work that reports the exact number of search errors in NMT as prior work often relied on approximations, e.g. via n-best lists (Niehues et al., 2017) or constraints (Stahlberg et al., 2018b) .\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 37, "token_end": 52, "char_start": 164, "char_end": 203, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Niehues et al., 2017)": "6834924"}}}, {"token_start": 53, "token_end": 65, "char_start": 207, "char_end": 244, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Stahlberg et al., 2018b)": "4049159"}}}]}
{"id": "201646223_0", "paragraph": "[BOS] Other researchers have also noted that large beam sizes yield shorter translations (Koehn and Knowles, 2017) .\n[BOS] Sountsov and Sarawagi (2016) argue that this model error is due to the locally normalized maximum likelihood training objective in NMT that underestimates the margin between the correct translation and shorter ones if trained with regularization and finite data.\n[BOS] A similar argument was made by Murray and Chiang (2018) who pointed out the difficulty for a locally normalized model to estimate the \"budget\" for all remaining (longer) translations.\n[BOS] Kumar and Sarawagi (2019) demonstrated that NMT models are often poorly calibrated, and that that can cause the length deficiency.\n[BOS] Ott et al. (2018) argued that uncertainty caused by noisy training data may play a role.\n[BOS] Chen et al. (2018) showed that the consistent best string problem for RNNs is decidable.\n[BOS] We provide an alternative DFS algorithm that relies on the monotonic nature of model scores rather than consistency, and that often converges in practice.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 8, "token_end": 24, "char_start": 45, "char_end": 114, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Koehn and Knowles, 2017)": "8822680"}}}, {"token_start": 26, "token_end": 75, "char_start": 123, "char_end": 385, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Sountsov and Sarawagi (2016)": "39487"}, "Reference": {}}}, {"token_start": 82, "token_end": 111, "char_start": 423, "char_end": 561, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Murray and Chiang (2018)": "52132833"}, "Reference": {}}}, {"token_start": 114, "token_end": 142, "char_start": 582, "char_end": 712, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kumar and Sarawagi (2019)": "67855916"}, "Reference": {}}}, {"token_start": 143, "token_end": 163, "char_start": 719, "char_end": 807, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ott et al. (2018)": "4375156"}, "Reference": {}}}, {"token_start": 164, "token_end": 185, "char_start": 814, "char_end": 902, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Chen et al. (2018)": "3666178"}, "Reference": {}}}]}
{"id": "208163916_0", "paragraph": "[BOS] Related works include both deep LMs especially BERT, a representative deep learning based LM and works on Twitter classification.\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "118680003_2", "paragraph": "[BOS] Original Recently, a new coming surveillance technology called radio-frequency identification which is RFID for short has caused heated discussions on whether it should be used to track people.\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "118680003_1", "paragraph": "[BOS] Prior work (Brockett et al., 2006; Foster and Andersen, 2009; Table 9 : Comparison of recent state-of-the-art models (top) and our best single-system and ensemble models (bottom) on the CoNLL-2014 and JFLEG datsets.\n[BOS] Only systems trained with publicly available Lang-8 and CoNLL datasets are reported.\n\n", "discourse_tags": ["Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 4, "token_end": 19, "char_start": 17, "char_end": 66, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Brockett et al., 2006;": "757808"}}}]}
{"id": "118680003_0", "paragraph": "[BOS] Progress in GEC has accelerated rapidly since the CoNLL-2014 Shared Task (Ng et al., 2014) .\n[BOS] Rozovskaya and Roth (2016) combined a Phrase Based Machine Translation (PBMT) model trained on the Lang-8 dataset (Mizumoto et al., 2011) with error specific classifiers.\n[BOS] JunczysDowmunt and Grundkiewicz (2016) combined a PBMT model with bitext features and a larger language model.\n[BOS] The first Neural Machine Translation (NMT) model to reach the state of the art on CoNLL-2014 (Chollampatt and Ng, 2018) used an ensemble of four convolutional sequence-tosequence models followed by rescoring.\n[BOS] The current state of the art (F 0.5 of 56.25 on CoNLL-2014) using publicly available Lang-8 and CoNLL data was achieved by Grundkiewicz and JunczysDowmunt (2018) with a hybrid PBMT-NMT system.\n[BOS] A neural-only result with an F 0.5 of 56.1 on CoNLL-2014 was reported by using an ensemble of neural Transformer models (Vaswani et al., 2017) , where the decoder side of each model is pretrained as a language model.\n[BOS] From a modeling perspective, our approach can be viewed as a direct extension of this last work.\n[BOS] Rather than pretraining only the decoder as a language model, we pretrain on a large amount of parallel data from either Wikipedia revision histories or from round-trip translations.\n[BOS] While pretraining on out-of-domain data has been employed previously for neural machine translation (Luong and Manning, 2015) , it has not been presented in GEC thus far, perhaps due to the absence of such large datasets.\n[BOS] Tao et al. (2018b) apply iterative decoding, where two neural models, trained in left-to-right and right-to-left directions, are applied in an interleaved manner.\n[BOS] Similar to their study, we find that iterative decoding can improve the performance of GEC.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ", "Narrative_cite", "Narrative_cite", "Single_summ", "Reflection", "Reflection", "Narrative_cite", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 11, "token_end": 25, "char_start": 56, "char_end": 96, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ng et al., 2014)": null}}}, {"token_start": 27, "token_end": 70, "char_start": 105, "char_end": 275, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Rozovskaya and Roth (2016)": "18563136"}, "Reference": {"(Mizumoto et al., 2011)": "5844380"}}}, {"token_start": 71, "token_end": 100, "char_start": 282, "char_end": 392, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 119, "token_end": 133, "char_start": 481, "char_end": 518, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chollampatt and Ng, 2018)": "19236015"}}}, {"token_start": 184, "token_end": 208, "char_start": 737, "char_end": 806, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 234, "token_end": 265, "char_start": 895, "char_end": 1029, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vaswani et al., 2017)": "13756489"}}}, {"token_start": 336, "token_end": 348, "char_start": 1401, "char_end": 1453, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Luong and Manning, 2015)": null}}}, {"token_start": 371, "token_end": 411, "char_start": 1556, "char_end": 1718, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Tao et al. (2018b)": "49564245"}, "Reference": {}}}]}
{"id": "207914581_1", "paragraph": "[BOS] The main differences between previous studies and our work are: (1) we do not rely on SMT model and adapt in an end-to-end manner only requiring some preprocessing using word-alignment models; and (2) we use phrase embedding tables to represent phrases instead of keeping external phrase memory and its generation probability.\n[BOS] By using the phrase embeddings along with the continuous-output layer, we significantly reduce the computational complexity and propose an approach to overcome the phrase generation bottleneck.\n[BOS] Fertility (Brown et al., 1993) has been a core component in phrase-based SMT models (Koehn et al., 2003) .\n[BOS] Fertility gives the likelihood of each source word of being translated into n words.\n[BOS] Fertility helps in deciding which phrases should be stored in the phrase tables.\n[BOS] Tu et al. (2016) revisited fertility to model coverage in NMT to address the issue of under-translation.\n[BOS] They used a fertility vector to express how many words should be generated per source word and a coverage vector to keep track of words translated so far.\n[BOS] We use a very similar concept in this work but the fertility module is introduced with a purpose to guide the decoder to switch over generating phrases and words.\n\n", "discourse_tags": ["Reflection", "Reflection", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 99, "token_end": 162, "char_start": 539, "char_end": 823, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Koehn et al., 2003)": "8884845"}}}, {"token_start": 163, "token_end": 217, "char_start": 830, "char_end": 1095, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Tu et al. (2016)": "146843"}, "Reference": {}}}]}
{"id": "207914581_0", "paragraph": "[BOS] Multi-word Expressions for NMT There have been several studies that incorporate multi-word phrases into supervised NMT (Tang et al., 2016; Dahlmann et al., 2017) .\n[BOS] Most approaches rely on pre-defined phrase dictionaries obtained from methods such as phrase-based Stastitical MT (Koehn et al., 2003) Recent works have also explored using an additional RNN to compute phrase generation probabilities.\n[BOS] Huang et al. (2017) proposed Neural Phrase MT (NPMT) that is built upon Sleep-WAke Network (SWAN), a segmentation-based sequence modeling technique, which automatically discovers phrases given the data and appends the special symbol $ to the source and target data.\n[BOS] The model gets these segmented word/phrase sequences as input and keeps two levels of RNNs to encode and decode phrases.\n[BOS] NPMT established state of the art results for phrase-based NMT, but at a price of significant computational overhead.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 15, "token_end": 40, "char_start": 74, "char_end": 167, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Tang et al., 2016;": "18192067", "Dahlmann et al., 2017)": "8555434"}}}, {"token_start": 57, "token_end": 74, "char_start": 262, "char_end": 310, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Koehn et al., 2003)": "8884845"}}}, {"token_start": 91, "token_end": 199, "char_start": 417, "char_end": 933, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Huang et al. (2017)": null}, "Reference": {}}}]}
{"id": "203837644_2", "paragraph": "[BOS] There has been little research regarding active learning of semantic representations.\n[BOS] Among the relevant work, Siddhant and Lipton (2018) have shown that uncertainty estimation using dropout and Bayes-By-Backprop (Blundell et al., 2015) achieves good results on the SRL formulation.\n[BOS] The improvements in performance due to LTAL approaches on various tasks (Konyushkova et al., 2017; Bachman et al., 2017; Fang et al., 2017; has raised the question whether learned policies can be applied also to the field of learning semantic representations.\n\n", "discourse_tags": ["Transition", "Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 20, "token_end": 62, "char_start": 123, "char_end": 294, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Siddhant and Lipton (2018)": "52039644"}, "Reference": {"(Blundell et al., 2015)": "1806222"}}}, {"token_start": 69, "token_end": 101, "char_start": 340, "char_end": 439, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Konyushkova et al., 2017;": "5878784", "Bachman et al., 2017;": "30819066"}}}]}
{"id": "203837644_1", "paragraph": "[BOS] In a large empirical study, Lowell et al. (2019) have recently shown other limitations in active learning.\n[BOS] They investigate the performance of active learning across NLP tasks and model architectures, and demonstrate that it does not achieve consistent gains over supervised learning, mostly because the collected samples are beneficial to a specific model architecture, and does not yield better results than random selection when switching to a new architecture.\n\n", "discourse_tags": ["Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 8, "token_end": 84, "char_start": 34, "char_end": 476, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Lowell et al. (2019)": null}, "Reference": {}}}]}
{"id": "203837644_0", "paragraph": "[BOS] Active learning has shown promising results on various tasks.\n[BOS] The commonly used uncertainty criteria (Lewis and Catlett, 1994; Culotta and Mc-Callum, 2005) is focused on selecting the samples on which the confidence of the model is low.\n[BOS] Among other notable approaches, in query by committee (Seung et al., 1992) a disagreement between a set of trained models on the predicted output of an unlabeled sample is the criterion for selecting what samples to label.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 13, "token_end": 53, "char_start": 74, "char_end": 248, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lewis and Catlett, 1994;": null}}}, {"token_start": 60, "token_end": 72, "char_start": 290, "char_end": 329, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "196170603_0", "paragraph": "[BOS] Training multiple languages using a single network is a well known approach in NMT.\n[BOS] All the previous works in this line were carried out by using parallel data only.\n[BOS] Dong et al. (2015) introduced one-to-many translation using a single encoder for the source language and a decoder for each target language.\n[BOS] Firat et al. (2016) proposed multi-way multilingual NMT using multiple encoders and decoders with a single shared attention mechanism.\n[BOS] Johnson et al. (2017) came up with a simpler but effective approach that needed only a single encoder and a single decoder, in which all the parallel data were merged into a single corpus after appending some special tokens at the beginning of each sentence.\n[BOS] Our multilingual unsupervised translation approach is inspired by Artetxe et al. (2018) .\n[BOS] We use single encoder which is shared by all languages and a decoder for each language.\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ", "Single_summ", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 36, "token_end": 66, "char_start": 184, "char_end": 324, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Dong et al. (2015)": "3666937"}, "Reference": {}}}, {"token_start": 67, "token_end": 97, "char_start": 331, "char_end": 465, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Firat et al. (2016)": "6359641"}, "Reference": {}}}, {"token_start": 98, "token_end": 149, "char_start": 472, "char_end": 730, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Johnson et al. (2017)": "6053988"}, "Reference": {}}}, {"token_start": 151, "token_end": 168, "char_start": 741, "char_end": 824, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Artetxe et al. (2018)": "3515219"}}}]}
{"id": "202583694_2", "paragraph": "[BOS] Probing Models Our probes of numeracy parallel work in understanding the linguistic capabilities (literacy) of neural models (Conneau et al., 2018; Liu et al., 2019) .\n[BOS] LSTMs can remember sentence length, word order, and which words were present in a sentence (Adi et al., 2017) .\n[BOS] Khandelwal et al. (2018) show how language models leverage context, while Linzen et al. (2016) demonstrate that language models understand subjectverb agreement.\n[BOS] Spithourakis and Riedel (2018) improve the ability of language models to predict numbers, i.e., they go beyond categorical predictions over a fixed-size vocabulary.\n[BOS] They focus on improving models; our focus is probing embeddings.\n[BOS] Kotnis and Garca-Durn (2019) predict numerical attributes in knowledge bases, e.g., they develop models that try to predict the population of Paris.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Multi_summ", "Single_summ", "Reflection", "Single_summ"], "span_citation_mapping": [{"token_start": 20, "token_end": 40, "char_start": 117, "char_end": 171, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Conneau et al., 2018;": "24461982", "Liu et al., 2019)": "84841767"}}}, {"token_start": 54, "token_end": 70, "char_start": 232, "char_end": 289, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Adi et al., 2017)": "6771196"}}}, {"token_start": 72, "token_end": 88, "char_start": 298, "char_end": 365, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Khandelwal et al. (2018)": "21700944"}, "Reference": {}}}, {"token_start": 89, "token_end": 107, "char_start": 372, "char_end": 459, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Linzen et al. (2016)": "14091946"}, "Reference": {}}}, {"token_start": 108, "token_end": 151, "char_start": 466, "char_end": 667, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Spithourakis and Riedel (2018)": "29150573"}, "Reference": {}}}, {"token_start": 159, "token_end": 195, "char_start": 708, "char_end": 856, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kotnis and Garc\u00eda-Dur\u00e1n (2019)": "120189340"}, "Reference": {}}}]}
{"id": "202583694_1", "paragraph": "[BOS] In concurrent work, also explore numeracy in word vectors.\n[BOS] Their methodology is based on variants of nearest neighbors and cosine distance; we use neural network probing classifiers which can capture highly non-linear dependencies between embeddings.\n[BOS] We also explore more powerful embedding methods such as ELMo, BERT, and learned embedding methods.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": []}
{"id": "202583694_0", "paragraph": "[BOS] An open question is how the training process elicits numeracy for word vectors and contextualized embeddings.\n[BOS] Understanding this, perhaps by tracing numeracy back to the training data, is a fruitful direction to explore further (c.f., influence functions (Koh and Liang, 2017; Brunet et al., 2019) ).\n[BOS] More generally, numeracy is one type of emergent knowledge.\n[BOS] For instance, embeddings may capture the size of objects (Forbes and Choi, 2017) , speed of vehicles, and many other \"commonsense\" phenomena (Yang et al., 2018) .\n[BOS] Vendrov et al. (2016) introduce methods to encode the order of such phenomena into embeddings for concepts such as hypernymy; our work and Yang et al. (2018) show that a relative ordering naturally emerges for certain concepts.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Narrative_cite", "Multi_summ"], "span_citation_mapping": [{"token_start": 50, "token_end": 68, "char_start": 247, "char_end": 309, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Koh and Liang, 2017;": "13193974", "Brunet et al., 2019)": "52946942"}}}, {"token_start": 91, "token_end": 102, "char_start": 426, "char_end": 465, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Forbes and Choi, 2017)": "10543068"}}}, {"token_start": 110, "token_end": 123, "char_start": 502, "char_end": 545, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yang et al., 2018)": "48356558"}}}, {"token_start": 125, "token_end": 151, "char_start": 554, "char_end": 678, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Vendrov et al. (2016)": "11440692"}, "Reference": {}}}, {"token_start": 155, "token_end": 173, "char_start": 693, "char_end": 781, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Yang et al. (2018)": "48356558"}, "Reference": {}}}]}
{"id": "184483382_2", "paragraph": "[BOS] 3 Methodology and Data\n\n", "discourse_tags": ["Other"], "span_citation_mapping": []}
{"id": "184483382_1", "paragraph": "[BOS] There have been several seminars on offensive language research, such as TRAC 1 which shared task on Aggression Identification summarized in Kumar et al. (2018) (Wiegand et al., 2018) which initiate and foster research on the identification of offensive content in German language microposts.\n[BOS] Additionally, the main work of Malmasi and Zampieri (2017) and is to approach the problem of distinguishing general profanity from hate speech.\n[BOS] tried to use a Convolution-GRU for detecting hate speech on Twitter.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 22, "token_end": 43, "char_start": 107, "char_end": 189, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Kumar et al. (2018)": "59336626"}}}, {"token_start": 66, "token_end": 96, "char_start": 323, "char_end": 448, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Malmasi and Zampieri (2017)": "19182892"}, "Reference": {}}}]}
{"id": "184483382_0", "paragraph": "[BOS] Due to the universality of offensive language in social media, in order to cope with offensive language and prevent abuse in social media, research on related aspects has gradually emerged in recent years.\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "207925358_0", "paragraph": "[BOS] BioCreative IV's CHEMDNER task Table 3 shows a comparison with the previous best results.\n[BOS] (Leaman et al., 2015) and proposed a feature-based approach to improve the chemical NER performance.\n[BOS] (Lin et al., 2018) proposed a neural network approach that treats document level information to maintain tagging consistency across sentences.\n[BOS] By learning paraphrasing, our method showed the best accuracy on the CHEMDNER task.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 24, "token_end": 47, "char_start": 102, "char_end": 202, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Leaman et al., 2015)": "215196906"}, "Reference": {}}}, {"token_start": 48, "token_end": 73, "char_start": 209, "char_end": 351, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Lin et al., 2018)": null}, "Reference": {}}}]}
{"id": "198974416_0", "paragraph": "[BOS] Early studies on named entity recognition heavily relied on language-specific knowledge resources, such as hand-crafted features or gazetteers (Lafferty et al., 2001; Ratinov and Roth, 2009; Tsai et al., 2016) .\n[BOS] However, this approach was costly for new languages and domains.\n[BOS] Thus, end-toend approaches that do not rely on any external knowledge were proposed.\n[BOS] Sobhana et al. (2010) proposed to use a CRF without any external resources, to leverage the label dependencies.\n[BOS] Then, neural-based approaches, such as LSTM with a CRF (Lample et al., 2016; Lin et al., 2017; Greenberg et al., 2018) and LSTM with a CNN (Chiu and Nichols, 2016) showed a significant improvement in performance.\n[BOS] Liu et al. (2018) ; Trivedi et al. (2018) proposed a character-level LSTM to capture the underlying style and structure, such as word boundaries and spellings.\n[BOS] Finally, wordembedding ensemble techniques and preprocessing techniques, such as tokenization and normal-ization have been introduced to reduce OOV issues (Winata et al., 2018b; .\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Single_summ", "Narrative_cite", "Multi_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 19, "token_end": 53, "char_start": 113, "char_end": 215, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lafferty et al., 2001;": "277918", "Ratinov and Roth, 2009;": "1859014", "Tsai et al., 2016)": "2889848"}}}, {"token_start": 87, "token_end": 112, "char_start": 386, "char_end": 497, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Sobhana et al. (2010)": "7581530"}, "Reference": {}}}, {"token_start": 122, "token_end": 151, "char_start": 543, "char_end": 622, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lample et al., 2016;": "6042994", "Lin et al., 2017;": "1989882", "Greenberg et al., 2018)": "53082686"}}}, {"token_start": 152, "token_end": 166, "char_start": 627, "char_end": 667, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 174, "token_end": 215, "char_start": 723, "char_end": 882, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Liu et al. (2018)": "19232497", "Trivedi et al. (2018)": "51878571"}, "Reference": {}}}, {"token_start": 216, "token_end": 253, "char_start": 889, "char_end": 1065, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "102350551_2", "paragraph": "[BOS] There are two main Seq2Seq models we will compare to in this work, along with the statistical model from Narayan and Gardent (2014) .\n[BOS] Zhang and Lapata (2017) proposed DRESS (Deep REinforcement Sentence Simplification), a Seq2Seq model that uses a reinforcement learning framework at training time to reward the model for producing sentences that score high on fluency, adequacy, and simplicity.\n[BOS] This work showed stateof-the-art results on human evaluation.\n[BOS] However, the sentences generated by this model are in general longer than the reference simplifications.\n[BOS] Zhao et al. (2018) proposed DMASS (Deep Memory Augmented Sentence Simplification), a multilayer, multi-head attention transformer architecture which also integrates simplification rules.\n[BOS] This work has been shown to get state-of-the-art results in an automatic evaluation, training on the WikiLarge dataset introduced by Zhang and Lapata (2017) .\n[BOS] Zhao et al. (2018) , however, does not perform a human evaluation, and restricting evaluation to automatic metrics is generally insufficient for comparing simplification models.\n[BOS] Our model, in comparison, is able to generate shorter and simpler sentences according to Flesch-Kincaid grade level (Kincaid et al., 1975) and human judgments, and provide a comprehensive analysis using human evaluation and a qualitative error analysis.\n[BOS] Standard Seq2Seq models use cross entropy as the loss function at training time.\n[BOS] This only takes into account how similar our generated tokens are to those in the reference simple sentence, and not the complexity of said tokens.\n[BOS] Therefore, we first develop a model to predict word complexities, and incorporate these into a custom loss function.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection", "Transition", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 21, "token_end": 32, "char_start": 88, "char_end": 137, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Narayan and Gardent (2014)": "15489071"}}}, {"token_start": 34, "token_end": 97, "char_start": 146, "char_end": 474, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhang and Lapata (2017)": "7473831"}, "Reference": {}}}, {"token_start": 117, "token_end": 170, "char_start": 592, "char_end": 868, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhao et al. (2018)": "53080846"}, "Reference": {}}}, {"token_start": 174, "token_end": 187, "char_start": 886, "char_end": 941, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Zhang and Lapata (2017)": "7473831"}}}, {"token_start": 189, "token_end": 220, "char_start": 950, "char_end": 1127, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhao et al. (2018)": "53080846"}, "Reference": {}}}, {"token_start": 237, "token_end": 255, "char_start": 1223, "char_end": 1272, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kincaid et al., 1975)": "61131325"}}}]}
{"id": "102350551_1", "paragraph": "[BOS] In recent work, Seq2Seq models are widely used for sequence transduction tasks such as machine translation (Sutskever et al., 2014; Luong et al., 2015) , conversation agents (Vinyals and Le, 2015) , summarization (Nallapati et al., 2016) , etc.\n[BOS] Initial Seq2Seq models consisted of a Recurrent Neural Network (RNN) that encodes the source sentence x to a hidden vector of a fixed dimension, followed by another RNN that uses this hidden representation to generate the target sentence y.\n[BOS] The two RNNs are then trained jointly to maximize the conditional probability of the target sentence given the source sentence, i.e. P (y|x).\n[BOS] Other works have since extended this framework to include attention mechanisms (Luong et al., 2015) and transformer networks (Vaswani et al., 2017) .\n[BOS] 2 Nisioi et al. (2017) was the first major application of Seq2Seq models to text simplification, applying a standard encoder-decoder approach with attention and beam search.\n[BOS] Vu et al. (2018) extended this framework to incorporate memory augmentation, which simultaneously performs lexical and syntactic simplification, allowing them to outperform standard Seq2Seq models.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Narrative_cite", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 19, "token_end": 30, "char_start": 93, "char_end": 136, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 31, "token_end": 39, "char_start": 138, "char_end": 157, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Luong et al., 2015)": "1998416"}}}, {"token_start": 40, "token_end": 51, "char_start": 160, "char_end": 202, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vinyals and Le, 2015)": "12300158"}}}, {"token_start": 52, "token_end": 64, "char_start": 205, "char_end": 243, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Nallapati et al., 2016)": "8928715"}}}, {"token_start": 159, "token_end": 170, "char_start": 710, "char_end": 751, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Luong et al., 2015)": "1998416"}}}, {"token_start": 171, "token_end": 183, "char_start": 756, "char_end": 799, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vaswani et al., 2017)": "13756489"}}}, {"token_start": 185, "token_end": 222, "char_start": 808, "char_end": 981, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 223, "token_end": 256, "char_start": 988, "char_end": 1185, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Vu et al. (2018)": null}, "Reference": {}}}]}
{"id": "102350551_0", "paragraph": "[BOS] Text simplification has often been addressed as a monolingual translation process, which generates a simplified version of a complex text.\n[BOS] Zhu et al. (2010) employ a tree-based translation model and consider sentence splitting, deletion, reordering, and substitution.\n[BOS] Coster and Kauchak (2011) use a Phrase-Based Machine Translation (PBMT) system with support for deleting phrases, while Wubben et al. (2012) extend a PBMT system with a reranking heuristic (PBMT-R).\n[BOS] Woodsend and Lapata (2011) propose a model based on a quasisynchronous grammar, a formalism able to capture structural mismatches and complex rewrite operations.\n[BOS] Narayan and Gardent (2014) combine a sentence splitting and deletion model with PBMT-R.\n[BOS] This model has been shown to perform competitively with neural models on automatic metrics, though it is outperformed using human judgments (Zhang and Lapata, 2017) .\n\n", "discourse_tags": ["Transition", "Single_summ", "Multi_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 26, "token_end": 53, "char_start": 151, "char_end": 279, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhu et al. (2010)": "15636533"}, "Reference": {}}}, {"token_start": 54, "token_end": 80, "char_start": 286, "char_end": 398, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Coster and Kauchak (2011)": "4896510"}, "Reference": {}}}, {"token_start": 82, "token_end": 109, "char_start": 406, "char_end": 484, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wubben et al. (2012)": "141120"}, "Reference": {}}}, {"token_start": 110, "token_end": 141, "char_start": 491, "char_end": 652, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Woodsend and Lapata (2011)": "9945908"}, "Reference": {}}}, {"token_start": 142, "token_end": 179, "char_start": 659, "char_end": 843, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Narayan and Gardent (2014)": "15489071"}, "Reference": {}}}, {"token_start": 186, "token_end": 196, "char_start": 877, "char_end": 917, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhang and Lapata, 2017)": "7473831"}}}]}
{"id": "201741377_3", "paragraph": "[BOS] In the last WMT biomedical translation challenge (2018) (Neves et al., 2018) , the submission that achieved the best BLEU scores for the ES/EN and PT/EN, in both directions, were the ones submited by the UFRGS team (Soares and Becker, 2018) , followed by the TALP-UPC (Tubay and Costa-juss, 2018) in the ES/EN direction and the UHH-DS in the EN/PT directions.\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 5, "token_end": 22, "char_start": 18, "char_end": 82, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Neves et al., 2018)": "53234891"}}}, {"token_start": 28, "token_end": 65, "char_start": 118, "char_end": 246, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Soares and Becker, 2018)": "53236316"}}}, {"token_start": 69, "token_end": 85, "char_start": 265, "char_end": 302, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Tubay and Costa-juss\u00c3, 2018)": "53222957"}}}]}
{"id": "201741377_2", "paragraph": "[BOS] More recently, Tubay and Costa-juss (2018) employed multi-source language translation using romance languages to translate from Spanish, French, and Portuguese to English.\n[BOS] They used data from SciELO and Medline abstracts to train a Transformer model with individual languages to English and also with all languages concatenated to English.\n\n", "discourse_tags": ["Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 5, "token_end": 66, "char_start": 21, "char_end": 351, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "201741377_1", "paragraph": "[BOS] The OOV resolution module infers the word in the target language based on the bilingual word embedding trained on large monolingual corpora.\n[BOS] Their reported results show that both approaches can improve BLEU scores, with the best results given by the combination of OOV resolution and RNN re-ranking.\n[BOS] Similarly, Ive et al. (2016) also used the n-best output from Moses as input to a reranking model, which is based on a neural network that can handle vocabularies of arbitrary size.\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ"], "span_citation_mapping": [{"token_start": 62, "token_end": 107, "char_start": 318, "char_end": 499, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ive et al. (2016)": "17244694"}, "Reference": {}}}]}
{"id": "201741377_0", "paragraph": "[BOS] Previous participation in biomedical translation tasks include the works of Costa-Juss et al. (2016) which employed Moses Statistic Machine Translation (SMT) to perform automatic translation integrated with a neural character-based recurrent neural network for model re-ranking and bilingual word embeddings for out of vocabulary (OOV) resolution.\n[BOS] Given the 1000-best list of SMT translations, the RNN performs a rescoring and selects the translation with the highest score.\n\n", "discourse_tags": ["Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 94, "char_start": 6, "char_end": 486, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Costa-Juss\u00e0 et al. (2016)": "6413281"}, "Reference": {}}}]}
{"id": "202764316_7", "paragraph": "[BOS] 3 copycat Networks\n\n", "discourse_tags": ["Other"], "span_citation_mapping": []}
{"id": "202764316_6", "paragraph": "[BOS] Our copycat networks address this problem: they are conservative and make very few careful corrections to good quality raw MT -as a result of learning predominantly to copy, while still making substantial corrections when the raw MT is not as good.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "202764316_5", "paragraph": "[BOS] SOTA APE approaches tackle the task with the multi-source transformer architectures (Junczys-Dowmunt and Grundkiewicz, 2018; Tebbifakhr et al., 2018) .\n[BOS] Two encoders encode the source and the raw MT, respectively, and an additional targetsource multi-head attention component is stacked on top of the original target-source multi-head attention component.\n[BOS] These models are trained with millions of <source, MT, PE> triplets, which does not fit realistic scenarios where only a handful of post-editions exist.\n[BOS] Special parallel training corpora with additional MT data are released to help the development .\n[BOS] According to , these SOTA systems modify up to 30% of the NMT input, out of which only 50% are positive changes, the rest are unnecessary paraphrases or deteriorate the output.\n\n", "discourse_tags": ["Multi_summ", "Transition", "Transition", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 12, "token_end": 44, "char_start": 51, "char_end": 155, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Junczys-Dowmunt and Grundkiewicz, 2018;": "52153656", "Tebbifakhr et al., 2018)": "53248957"}}}]}
{"id": "202764316_4", "paragraph": "[BOS] Automatic Post-Editing (APE) (Simard and Foster, 2013; Chatterjee et al., 2017) aims to automatically correct errors in machine translation (MT) outputs in order to reduce the burden of human post-editors, especially with simple and repetitive corrections (e.g. typographic errors).\n[BOS] APE is usually addressed by monolingual translation models that \"translate\" from the raw MT to fixed MT (post-edits, PE).\n[BOS] Given the high quality of current neural MT (NMT) outputs, the task has become particularly challenging.\n[BOS] Human corrections to those outputs are rare and very contextdependent, which makes them difficult to capture and generalise.\n[BOS] This increases the chance of APE systems to overfit or overcorrect new inputs at test time.\n\n", "discourse_tags": ["Multi_summ", "Transition", "Transition", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 2, "token_end": 67, "char_start": 6, "char_end": 288, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Simard and Foster, 2013;": "30689464", "Chatterjee et al., 2017)": "10272774"}, "Reference": {}}}]}
{"id": "202764316_3", "paragraph": "[BOS] The latter two represent the best overall approaches on common datasets, using either an RNN-or a Transformer-based architecture.\n[BOS] We show that our copycat approach is competitive and is more abstractive (we describe this in the following sections).\n\n", "discourse_tags": ["Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "202764316_2", "paragraph": "[BOS] Reinforcement learning (RL) approaches optimise objectives for summarisation in addition to maximum likelihood.\n[BOS] Paulus et al. (2017) combine ROUGE and maximum likelihood as training objectives, use a pointer network and -to avoid repetition -introduce intra-attention to the encoder and decoder that attends over the input and continuously generated output separately.\n[BOS] Li et al. (2018) use a global summary quality estimator which is a binary classifier aiming to make the generated summaries indistinguishable from the human-written ones.\n[BOS] Pasunuru and Bansal (2018) have a loss function based on whether keywords detected as salient are included in a summary, while Hsu et al. (2018) modulate the attention based on how likely a sentence is to be included in a summary, and Chen and Bansal (2018) fol-low an extractive-abstractive hybrid architecture to first extract full sentences from a document using a sentence-level policy gradient and then compress them.\n[BOS] Gehrmann et al. (2018a) also perform content selection, but on the level of phrases by treating this process as a sequence labelling task, and without RL.\n[BOS] They first build a selection mask for the source document and then constrain a decoder on this mask.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Multi_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 21, "token_end": 70, "char_start": 124, "char_end": 380, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Paulus et al. (2017)": "21850704"}, "Reference": {}}}, {"token_start": 71, "token_end": 104, "char_start": 387, "char_end": 557, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Li et al. (2018)": "4451272"}, "Reference": {}}}, {"token_start": 105, "token_end": 131, "char_start": 564, "char_end": 683, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Pasunuru and Bansal (2018)": "4940548"}, "Reference": {}}}, {"token_start": 133, "token_end": 156, "char_start": 691, "char_end": 793, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hsu et al. (2018)": "21723747"}, "Reference": {}}}, {"token_start": 158, "token_end": 196, "char_start": 799, "char_end": 986, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 197, "token_end": 252, "char_start": 993, "char_end": 1254, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Gehrmann et al. (2018a)": "52144157"}, "Reference": {}}}]}
{"id": "202764316_1", "paragraph": "[BOS] Abstractive Summarisation SOTA approaches using sequence to sequence models have evolved from vanilla attentional models (Rush et al., 2015; Nallapati et al., 2016a) to more advanced architectures that include the use of pointer networks (Nallapati et al., 2016b; See et al., 2017; Gu et al., 2016; Paulus et al., 2017) , reinforcement learning (Paulus et al., 2017; Li et al., 2018; Pasunuru and Bansal, 2018; Chen and Bansal, 2018; Hsu et al., 2018) and content selection (Gehrmann et al., 2018a; Pasunuru and Bansal, 2018; Chen and Bansal, 2018) .\n[BOS] Nallapati et al. (2016b) and Gu et al. (2016) propose the use of pointer-generator RNN-based networks to reduce out-of-vocabulary (OOV) rate.\n[BOS] This idea was followed by See et al. (2017) , which incorporates a coverage mechanism to avoid repetition of input words by keeping track of what has already been covered.\n\n", "discourse_tags": ["Narrative_cite", "Multi_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 17, "token_end": 40, "char_start": 100, "char_end": 171, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rush et al., 2015;": "1918428", "Nallapati et al., 2016a)": "12386980"}}}, {"token_start": 49, "token_end": 84, "char_start": 227, "char_end": 325, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Nallapati et al., 2016b;": "8928715", "See et al., 2017;": null, "Gu et al., 2016;": "8174613", "Paulus et al., 2017)": "21850704"}}}, {"token_start": 85, "token_end": 127, "char_start": 328, "char_end": 457, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Paulus et al., 2017;": "21850704", "Li et al., 2018;": "4451272", "Pasunuru and Bansal, 2018;": "4940548", "Chen and Bansal, 2018;": "44129061", "Hsu et al., 2018)": "21723747"}}}, {"token_start": 128, "token_end": 158, "char_start": 462, "char_end": 554, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Gehrmann et al., 2018a;": "52144157", "Pasunuru and Bansal, 2018;": "4940548", "Chen and Bansal, 2018)": "44129061"}}}, {"token_start": 160, "token_end": 203, "char_start": 563, "char_end": 704, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Nallapati et al. (2016b)": "8928715", "Gu et al. (2016)": "8174613"}, "Reference": {}}}, {"token_start": 209, "token_end": 238, "char_start": 737, "char_end": 882, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"See et al. (2017)": null}, "Reference": {}}}]}
{"id": "202764316_0", "paragraph": "[BOS] In this section we highlight the most closely related work on abstractive summarisation and automatic post-editing.\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "199379636_1", "paragraph": "[BOS] Unsupervised-to-unsupervised transfer is less studied.\n[BOS] Dingwall and Potts (2018) proposed a GloVe model-modification that retrofits publicly available GloVe embeddings to produce specialized domain embeddings, while Bollegala and Bao (2018) propose meta-embeddings via denoising autoencoders to merge diverse (Fasttext and GloVe) embeddings spaces.\n[BOS] The later, is also a low-effort approach and closest to ours.\n[BOS] However, it focuses on embedding merging that they tuned on a single semantic similarity task, while MORTY provides an overview of tuning for 19 different settings.\n[BOS] Furthermore, MORTY requires only a single embedding space, which contributes to the literature by outlining that meta-embedding improvements may partly stem from re-encoding rather than only from semantic merging.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Multi_summ", "Reflection", "Transition"], "span_citation_mapping": [{"token_start": 13, "token_end": 91, "char_start": 67, "char_end": 428, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Dingwall and Potts (2018)": "4421001", "Bollegala and Bao (2018)": "52000553"}, "Reference": {}}}]}
{"id": "199379636_0", "paragraph": "[BOS] There is a large body of work on information transfer between supervised and unsupervised tasks.\n[BOS] First and foremost unsupervisedto-supervised transfer includes using embeddings for supervised tasks.\n[BOS] However, transfer also works vice versa, in a supervised-to-unsupervised setup to (learn to) specialize embeddings to better fit a specific supervised signal (Ruder and Plank, 2017; Ye et al., 2018) .\n[BOS] This includes injecting generally relevant semantics via retrofitting or auxiliary multi-task supervision (Faruqui et al., 2015; Kiela et al., 2018b) .\n[BOS] Supervised-to-supervised methods provide knowledge transfer between supervised tasks which is exploited successively (Kirkpatrick et al., 2017) , jointly (Kiela et al., 2018b) and in joint-succession (Hashimoto et al., 2017) .\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 46, "token_end": 82, "char_start": 263, "char_end": 415, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ruder and Plank, 2017;": "7403346", "Ye et al., 2018)": "52013137"}}}, {"token_start": 86, "token_end": 119, "char_start": 438, "char_end": 573, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Faruqui et al., 2015;": "51838647", "Kiela et al., 2018b)": "52166626"}}}, {"token_start": 135, "token_end": 148, "char_start": 676, "char_end": 725, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kirkpatrick et al., 2017)": "4704285"}}}, {"token_start": 149, "token_end": 160, "char_start": 728, "char_end": 757, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kiela et al., 2018b)": "52166626"}}}, {"token_start": 162, "token_end": 174, "char_start": 765, "char_end": 806, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hashimoto et al., 2017)": "2213896"}}}]}
{"id": "201698399_2", "paragraph": "[BOS] In regards to how structured data is represented, methods range from encoding table information, metadata and/or content, (Gur et al., 2018; Sun et al., 2018b; Petrovski et al., 2018) to encoding relations between the question and table items (Krishnamurthy et al., 2017) or KB entities (Sun et al., 2018a) .\n[BOS] We also encode the table structure and the question in an annotation graph, but use a different modelling approach.\n\n", "discourse_tags": ["Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 15, "token_end": 49, "char_start": 84, "char_end": 189, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Gur et al., 2018;": "21736196", "Sun et al., 2018b;": "52195260", "Petrovski et al., 2018)": "53246468"}}}, {"token_start": 56, "token_end": 70, "char_start": 237, "char_end": 277, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Krishnamurthy et al., 2017)": "1675452"}}}, {"token_start": 71, "token_end": 82, "char_start": 281, "char_end": 312, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sun et al., 2018a)": "52154304"}}}]}
{"id": "201698399_1", "paragraph": "[BOS] Most of the QA and semantic parsing research focuses on single turn questions.\n[BOS] We are interested in handling multiple turns and therefore in modeling context.\n[BOS] In semantic parsing tasks, logical forms (Iyyer et al., 2017; Sun et al., 2018b; Guo et al., 2018) or SQL statements (Suhr et al., 2018) from previous questions are refined to handle follow up questions.\n[BOS] In our model, we encode answers to previous questions by marking answer rows, columns and cells in the table, in a nonautoregressive fashion.\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 36, "token_end": 62, "char_start": 204, "char_end": 275, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Iyyer et al., 2017;": "2623009", "Sun et al., 2018b;": "52195260", "Guo et al., 2018)": "53976609"}}}, {"token_start": 63, "token_end": 74, "char_start": 279, "char_end": 313, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Suhr et al., 2018)": "4955031"}}}]}
{"id": "201698399_0", "paragraph": "[BOS] Semantic parsing models can be trained to produce gold logical forms using an encoder-decoder approach (Suhr et al., 2018) or by filling templates (Xu et al., 2017; Peng et al., 2017; .\n[BOS] When gold logical forms are not available, they are typically treated as latent variables or hidden states and the answers or denotations are used to search for correct logical forms (Yih et al., 2015; Long et al., 2016; Iyyer et al., 2017) .\n[BOS] In some cases, feedback from query execution is used as a reward signal for updating the model through reinforcement learning (Zhong et al., 2017; Agarwal et al., 2019) or for refining parts of the query (Wang et al., 2018) .\n[BOS] In our work, we do not use logical forms or RL, which can be hard to train, but simplify the training process by directly matching questions to table cells.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 15, "token_end": 28, "char_start": 84, "char_end": 128, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Suhr et al., 2018)": "4955031"}}}, {"token_start": 30, "token_end": 46, "char_start": 135, "char_end": 188, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Xu et al., 2017;": "10746949"}}}, {"token_start": 78, "token_end": 105, "char_start": 359, "char_end": 438, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yih et al., 2015;": "18309765", "Long et al., 2016;": "8202418", "Iyyer et al., 2017)": "2623009"}}}, {"token_start": 126, "token_end": 145, "char_start": 550, "char_end": 615, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhong et al., 2017;": "25156106", "Agarwal et al., 2019)": "67750320"}}}, {"token_start": 147, "token_end": 160, "char_start": 623, "char_end": 670, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wang et al., 2018)": "49655701"}}}]}
{"id": "196186961_2", "paragraph": "[BOS] form.\n[BOS] As far as we know, this is the first architecture that realizes deep interaction for multi-turn response selection.\n[BOS] Encouraged by the big success of deep neural architectures such as Resnet (He et al., 2016) and inception (Szegedy et al., 2015) in computer vision, researchers have studied if they can achieve similar results with deep neural networks on NLP tasks.\n[BOS] Although deep models have not yet brought breakthroughs to NLP as they do to computer vision, they have proven effective in a few tasks such as text classification (Conneau et al., 2017) , natural language inference (Kim et al., 2018; Tay et al., 2018) , and question answering (Tay et al., 2018; Kim et al., 2018) , etc.\n[BOS] In this work, we attempt to improve the accuracy of multi-turn response selection in retrieval-based dialogue systems by increasing the depth of context-response interaction in matching.\n[BOS] Through extensive studies on benchmarks, we show that depth can bring significant improvement to model performance on the task.\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite", "Narrative_cite", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 40, "token_end": 50, "char_start": 207, "char_end": 231, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(He et al., 2016)": "206594692"}}}, {"token_start": 51, "token_end": 64, "char_start": 236, "char_end": 268, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Szegedy et al., 2015)": null}}}, {"token_start": 116, "token_end": 128, "char_start": 540, "char_end": 582, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Conneau et al., 2017)": "5079983"}}}, {"token_start": 129, "token_end": 148, "char_start": 585, "char_end": 648, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kim et al., 2018;": "44132329", "Tay et al., 2018)": "52943910"}}}, {"token_start": 150, "token_end": 168, "char_start": 655, "char_end": 710, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Tay et al., 2018;": "52943910", "Kim et al., 2018)": "44132329"}}}]}
{"id": "196186961_1", "paragraph": "[BOS] The second group learns a matching model of a human input and a response candidate for response selection.\n[BOS] Along this line, the focus of research starts from single-turn response selection by setting the human input as a single message (Wang et al., 2013; Hu et al., 2014; Wang et al., 2015) , and moves to context-response matching for multi-turn response selection recently.\n[BOS] Representative methods include the dual LSTM model (Lowe et al., 2015) , the deep learning to respond architecture , the multi-view matching model (Zhou et al., 2016) , the sequential matching network , and the deep attention matching network .\n[BOS] Besides model design, some attention is also paid to the learning problem of matching models (Wu et al., 2018a) .\n[BOS] Our work belongs to the second group.\n[BOS] The proposed interaction-over-interaction network is unique in that it performs matching by stacking multiple interaction blocks, and thus extends the shallow interaction in state-of-the-art methods to a deep Figure 1 : Architecture of interaction-over-interaction network.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite", "Narrative_cite", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 40, "token_end": 68, "char_start": 216, "char_end": 303, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wang et al., 2013;": "12588798", "Hu et al., 2014;": "4497054", "Wang et al., 2015)": "14029406"}}}, {"token_start": 89, "token_end": 102, "char_start": 430, "char_end": 465, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lowe et al., 2015)": "8379583"}}}, {"token_start": 111, "token_end": 124, "char_start": 516, "char_end": 561, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhou et al., 2016)": "2867243"}}}, {"token_start": 152, "token_end": 163, "char_start": 723, "char_end": 757, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wu et al., 2018a)": "19191579"}}}]}
{"id": "196186961_0", "paragraph": "[BOS] Existing methods for building an open-domain dialogue system can be categorized into two groups.\n[BOS] The first group learns response generation models under an encoder-decoder framework.\n[BOS] On top of the basic sequence-to-sequence with attention architecture (Vinyals and Le, 2015; Shang et al., 2015; Tao et al., 2018) , various extensions have been made to tackle the \"safe response\" problem Mou et al., 2016; Zhao et al., 2017; Song et al., 2018) ; to generate responses with specific personas or emotions (Li et al., 2016a; ; and to pursue better optimization strategies (Li et al., 2017b (Li et al., , 2016b .\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 40, "token_end": 72, "char_start": 221, "char_end": 330, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vinyals and Le, 2015;": "12300158", "Shang et al., 2015;": "7356547", "Tao et al., 2018)": "51609155"}}}, {"token_start": 81, "token_end": 108, "char_start": 381, "char_end": 460, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Mou et al., 2016;": "5165773", "Zhao et al., 2017;": "14688760", "Song et al., 2018)": "51879945"}}}, {"token_start": 110, "token_end": 126, "char_start": 466, "char_end": 537, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 130, "token_end": 151, "char_start": 548, "char_end": 623, "span_type": "Reference", "span_citation_mapping": {"Dominant": {"(Li et al., 2017b": "98180"}, "Reference": {"(Li et al., , 2016b": "3147007"}}}]}
{"id": "209387678_3", "paragraph": "[BOS] Since the generation of abstractive summaries requires a powerful representation of language, we investigate the use of transfer learning.\n[BOS] Large language models based on the neural Transformer architecture (Vaswani et al., 2017) have shown promising results in language understanding tasks (Houlsby et al., 2019; Devlin et al., 2018; Chronopoulou et al., 2019) , but so far have had limited success in generation tasks (Zhang et al., 2019) .\n[BOS] Most recently, the pseudo-self attention method for fine-tuning language models to generation tasks has been introduced which may allow the application of transfer-learning to abstractive summarization (Ziegler et al., 2019) .\n[BOS] In this work, we compare this approach to strong baselines that rely on minor modifications of the Transformer (Gehrmann et al., 2018) .\n\n", "discourse_tags": ["Reflection", "Narrative_cite", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 31, "token_end": 44, "char_start": 186, "char_end": 240, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vaswani et al., 2017)": "13756489"}}}, {"token_start": 49, "token_end": 81, "char_start": 273, "char_end": 372, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Houlsby et al., 2019;": "59599816", "Devlin et al., 2018;": "52967399", "Chronopoulou et al., 2019)": "67855637"}}}, {"token_start": 90, "token_end": 101, "char_start": 414, "char_end": 451, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhang et al., 2019)": "67855893"}}}, {"token_start": 130, "token_end": 149, "char_start": 615, "char_end": 684, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ziegler et al., 2019)": null}}}, {"token_start": 166, "token_end": 181, "char_start": 765, "char_end": 827, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Gehrmann et al., 2018)": "52144157"}}}]}
{"id": "209387678_2", "paragraph": "[BOS] However, most summarization tasks use data from news domains which have mostly extractive summaries.\n[BOS] Among others, See et al. (2017) and Gehrmann et al. (2018) found that models learn to replicate this latent extraction behavior, and that the resulting summaries of copy-attention based models are over 95% extractive.\n[BOS] To address this issue, related approaches have used reinforcement learning objectives to prevent the model from reusing longer phrases from the input and to be more concise (Paulus et al., 2017; Chen and Bansal, 2018; Li et al., 2018) .\n[BOS] However, these methods often suffer from ungrammatical output or much slower training while also requiring task-specific loss functions.\n[BOS] To avoid this problem, Kim et al. (2018) and Vlske et al. (2017) created redditbased corpora with more abstractive target summaries that enable the evaluation of supervised models instead.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Multi_summ", "Transition", "Multi_summ"], "span_citation_mapping": [{"token_start": 24, "token_end": 123, "char_start": 127, "char_end": 571, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"See et al. (2017)": null, "Gehrmann et al. (2018)": "52144157"}, "Reference": {"(Paulus et al., 2017;": "21850704", "Chen and Bansal, 2018;": "44129061", "Li et al., 2018)": "4451272"}}}, {"token_start": 155, "token_end": 193, "char_start": 746, "char_end": 911, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kim et al. (2018)": "53295957", "V\u00f6lske et al. (2017)": "2204603"}, "Reference": {}}}]}
{"id": "209387678_1", "paragraph": "[BOS] Sequence-to-sequence models (S2S, Sutskever et al., 2014) are the de-facto standard for neural abstractive summarization (Rush et al., 2015; Nallapati et al., 2016) .\n[BOS] The development of models that incorporate a copy-attention mechanism for models to copy word from source documents, has further improved the performance (Gu et al., 2016; Vinyals et al., 2015; See et al., 2017) .\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 2, "token_end": 22, "char_start": 6, "char_end": 63, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Sutskever et al., 2014)": "7961699"}}}, {"token_start": 24, "token_end": 53, "char_start": 72, "char_end": 170, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Nallapati et al., 2016)": "8928715"}}}, {"token_start": 62, "token_end": 104, "char_start": 224, "char_end": 390, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Gu et al., 2016;": "8174613", "Vinyals et al., 2015;": "5692837", "See et al., 2017)": null}}}]}
{"id": "209387678_0", "paragraph": "[BOS] Throughout this study, we consider the supervised summarization problem, which aims to compress a source document of tokens x 1 , .\n[BOS] .\n[BOS] .\n[BOS] , x m of length m. The aligned summary y 1 , .\n[BOS] .\n[BOS] .\n[BOS] , y n has a length n m, and aims to convey a compressed version of the source document.\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection", "Reflection", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "170079029_3", "paragraph": "[BOS] Our work contributes to this line of research in two ways: (1) from the angle of methodology, we show how to employ derivatives to pinpoint what syntactic relations the representations learn; (2) from the perspective of tasks, we demonstrate how BiLSTM-based dependency parsers take advantage of structural information encoded in the representations.\n[BOS] In the case of constituency parsing Gaddy et al. (2018) offer such an analysis.\n[BOS] The authors show that their BiLSTM-based models implicitly learn the same information which was conventionally provided to non-neural parsers, such as grammars and lexicons.\n\n", "discourse_tags": ["Reflection", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 73, "token_end": 123, "char_start": 378, "char_end": 622, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Gaddy et al. (2018)": "5040602"}, "Reference": {}}}]}
{"id": "170079029_2", "paragraph": "[BOS] RNNs and syntax.\n[BOS] Recurrent neural networks, which BiLSTMs are a variant of, have been repeatedly analyzed to understand whether they can learn syntactic relations.\n[BOS] Such analyses differ in terms of: (1) methodology they employ to probe what type of knowledge the representations learned and (2) tasks on which the representations are trained on.\n[BOS] Shi et al. (2016) demonstrated that sequence-to-sequence machinetranslation systems capture source-language syntactic relations.\n[BOS] Linzen et al. (2016) showed that when trained on the task of number agreement prediction the representations capture a nontrivial amount of grammatical structure (although recursive neural networks are better at this task than sequential LSTMs (Kuncoro et al., 2018) ).\n[BOS] Blevins et al. (2018) found that RNN representations trained on a variety of NLP tasks (including dependency parsing) are able to induce syntactic features (e.g., constituency labels of parent or grandparent) even without explicit supervision.\n[BOS] Finally, Conneau et al. (2018) designed a set of tasks probing linguistic knowledge of sentence embedding methods.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 71, "token_end": 96, "char_start": 369, "char_end": 497, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Shi et al. (2016)": "7197724"}, "Reference": {}}}, {"token_start": 97, "token_end": 150, "char_start": 504, "char_end": 770, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Linzen et al. (2016)": "14091946"}, "Reference": {"(Kuncoro et al., 2018)": "51873108"}}}, {"token_start": 153, "token_end": 206, "char_start": 780, "char_end": 1023, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Blevins et al. (2018)": "21663989"}, "Reference": {}}}, {"token_start": 209, "token_end": 231, "char_start": 1039, "char_end": 1144, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Conneau et al. (2018)": "24461982"}, "Reference": {}}}]}
{"id": "170079029_1", "paragraph": "[BOS] None of the above mentioned efforts address the question how dependency parsers are able to compensate for the lack of structural features.\n[BOS] The very recent work by de Lhoneux et al. (2019) looked into this issue from a different perspective than ours -composition.\n[BOS] They showed that composing the structural context with recursive networks as in Dyer et al. (2015) is redundant for the K&G transition-based architecture.\n[BOS] The authors analyze components of the BiLSTMs to show which of them (forward v. backward LSTM) is responsible for capturing subtree information.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 32, "token_end": 120, "char_start": 176, "char_end": 588, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"de Lhoneux et al. (2019)": "67855541"}, "Reference": {"Dyer et al. (2015)": null}}}]}
{"id": "170079029_0", "paragraph": "[BOS] Feature extraction.\n[BOS] Kiperwasser and Goldberg (2016) and Cross and Huang (2016) first applied BiLSTMs to extract features for transition-based dependency parsers.\n[BOS] The authors demonstrated that an architecture using only a few positional features (four for the arc-hybrid system and three for arc-standard) is sufficient to achieve state-ofthe-art performance.\n[BOS] Shi et al. (2017) showed that this number can be further reduced to two features for arc-hybrid and arc-eager systems.\n[BOS] Decreasing the size of the feature set not only allows for construction of lighter and faster neural networks (Wang and Chang, 2016; Vilares and Gmez-Rodrguez, 2018) but also enables the use of exact search algorithms for several projective (Shi et al., 2017) and non-projective (Gmez-Rodrguez et al., 2018) transition systems.\n[BOS] A similar trend can be observed for graph-based dependency parsers.\n[BOS] State-of-the-art models (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016) typically use only two features of heads and dependents, possibly also incorporating their distance (Wang and Chang, 2016) .\n[BOS] Moreover, Wang and Chang (2016) show that arc-factored BiLSTM-based parsers can compete with conventional higher-order models in terms of accuracy.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Multi_summ", "Single_summ", "Narrative_cite", "Transition", "Multi_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 6, "token_end": 77, "char_start": 32, "char_end": 376, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kiperwasser and Goldberg (2016)": null, "Cross and Huang (2016)": "2412277"}, "Reference": {}}}, {"token_start": 78, "token_end": 107, "char_start": 383, "char_end": 501, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Shi et al. (2017)": "269533"}, "Reference": {}}}, {"token_start": 108, "token_end": 145, "char_start": 508, "char_end": 673, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wang and Chang, 2016;": "9289495"}}}, {"token_start": 152, "token_end": 185, "char_start": 708, "char_end": 835, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Shi et al., 2017) and": "269533"}}}, {"token_start": 209, "token_end": 251, "char_start": 940, "char_end": 1120, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kiperwasser and Goldberg, 2016;": null, "Dozat and Manning, 2016)": "7942973"}}}, {"token_start": 255, "token_end": 286, "char_start": 1139, "char_end": 1276, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wang and Chang (2016)": "9289495"}, "Reference": {}}}]}
{"id": "202787716_6", "paragraph": "[BOS] Our model differs from previous work in three main ways.\n[BOS] First, it performs word ordering on fully delexicalised data.\n[BOS] Delexicalisation has been used previously but mostly to handle rare words, e.g. named entities.\n[BOS] Here we argue that surface realisation and, in particular, word ordering works better when delexicalising all input tokens.\n[BOS] This captures the intuition that word ordering is mainly determined by the syntactic structure of the input.\n[BOS] Second, we we provide a detailed evaluation of how our model handles the three subtasks underlying surface realisation.\n[BOS] While all SR'18 participants provided descriptions of their models, not all of them performed an in-depth analysis of model performance.\n[BOS] Exceptions are works of King and White (2018) , who provided a separate evaluation for the morphological realisation module, and Puzikov and Gurevych (2018) , who evaluated both word ordering and inflection modules.\n[BOS] However, it is not clear how each of those modules affect the global performance when merged in the full pipeline.\n[BOS] In contrast, we propose a detailed incremental evaluation of each component of the full pipeline and show how each component impacts the final scores.\n[BOS] Third, we introduce a linguistic analysis, based on the dependency relations, of the word ordering component, allowing for deeper error analysis of the developed systems.\n[BOS] Furthermore, our model explicitly integrates a module for contraction handling, as done also before by Basile and Mazzei (2018) .\n[BOS] We also address all the ten languages proposed by the shared task and outline the importance of handling contractions.\n\n", "discourse_tags": ["Reflection", "Reflection", "Transition", "Reflection", "Transition", "Transition", "Transition", "Multi_summ", "Transition", "Reflection", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 151, "token_end": 169, "char_start": 777, "char_end": 876, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"King and White (2018)": "44180499"}, "Reference": {}}}, {"token_start": 171, "token_end": 192, "char_start": 882, "char_end": 968, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Puzikov and Gurevych (2018)": "51950856"}, "Reference": {}}}, {"token_start": 283, "token_end": 300, "char_start": 1488, "char_end": 1557, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Basile and Mazzei (2018)": null}}}]}
{"id": "202787716_5", "paragraph": "[BOS] One of the interesting findings of the shared task is reported by Elder and Hokamp (2018) who showed that applying standard neural encoderdecoder models to jointly learn word ordering and inflection is highly challenging; their sequence-tosequence baseline without data augmentation got 43.11 BLEU points on English.\n\n", "discourse_tags": ["Single_summ"], "span_citation_mapping": [{"token_start": 14, "token_end": 63, "char_start": 72, "char_end": 322, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Elder and Hokamp (2018)": "29169306"}, "Reference": {}}}]}
{"id": "202787716_4", "paragraph": "[BOS] For word ordering, five teams chose an approach based on neural networks, two used a classifier, and one team resorted to a language model.\n[BOS] As for the inflection subtask, five teams applied neural techniques, two used lexicon-based approaches, and one used an SMT system (Basile and Mazzei, 2018; Castro Ferreira et al., 2018; Elder and Hokamp, 2018; King and White, 2018; Madsack et al., 2018; Puzikov and Gurevych, 2018; Singh et al., 2018; Sobrevilla Cabezudo and Pardo, 2018) .\n[BOS] Overall, neural components were dominant across all the participants.\n[BOS] However, official scores of the teams that went neural greatly differ.\n[BOS] Furthermore, two teams (Elder and Hokamp, 2018; Sobrevilla Cabezudo and Pardo, 2018) applied data augmentation, which makes their results not strictly comparable to others.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Transition", "Multi_summ"], "span_citation_mapping": [{"token_start": 57, "token_end": 135, "char_start": 272, "char_end": 491, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Basile and Mazzei, 2018;": null, "Castro Ferreira et al., 2018;": "51995091", "Elder and Hokamp, 2018;": "29169306", "King and White, 2018;": "44180499", "Madsack et al., 2018;": "51989479", "Puzikov and Gurevych, 2018;": "51950856", "Singh et al., 2018;": "119313673", "Sobrevilla Cabezudo and Pardo, 2018)": "51986388"}}}, {"token_start": 165, "token_end": 204, "char_start": 666, "char_end": 825, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Elder and Hokamp, 2018;": "29169306", "Sobrevilla Cabezudo and Pardo, 2018)": "51986388"}, "Reference": {}}}]}
{"id": "202787716_3", "paragraph": "[BOS] The SR'18 shallow track received submissions from eight teams with seven of them dividing the task into two subtasks: word ordering and inflection.\n[BOS] Only Elder and Hokamp (2018) developed a joint approach, however, they participated only in the English track.\n\n", "discourse_tags": ["Transition", "Single_summ"], "span_citation_mapping": [{"token_start": 34, "token_end": 57, "char_start": 165, "char_end": 270, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Elder and Hokamp (2018)": "29169306"}, "Reference": {}}}]}
{"id": "202787716_2", "paragraph": "[BOS] Recently, Marcheggiani and Perez-Beltrachini (2018) proposed a neural end-to-end approach based on graph convolutional encoders for the SR'11 deep track.\n\n", "discourse_tags": ["Single_summ"], "span_citation_mapping": [{"token_start": 4, "token_end": 42, "char_start": 16, "char_end": 159, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Marcheggiani and Perez-Beltrachini (2018)": "53034346"}, "Reference": {}}}]}
{"id": "202787716_1", "paragraph": "[BOS] Multilingual SR'18 was preceded by the SR'11 surface realisation task for the English language only (Belz et al., 2011) .\n[BOS] The submitted systems in 2011 had grammar-based and statistical nature, mostly relying on pipelined architecture.\n\n", "discourse_tags": ["Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 20, "token_end": 32, "char_start": 84, "char_end": 125, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Belz et al., 2011)": "12040771"}}}]}
{"id": "202787716_0", "paragraph": "[BOS] Early approaches for surface realisation adopted statistical methods, including both pipelined (Bohnet et al., 2010) and joint (Song et al., 2014; Puduppully et al., 2017) architecture for word ordering and morphological generation.\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 14, "token_end": 25, "char_start": 91, "char_end": 122, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bohnet et al., 2010)": "15950784"}}}, {"token_start": 26, "token_end": 46, "char_start": 127, "char_end": 177, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Song et al., 2014;": "17044271", "Puduppully et al., 2017)": "2247313"}}}]}
{"id": "196181767_2", "paragraph": "[BOS] Universal Transformer (Dehghani et al., 2019) can be thought of variable-depth recurrent attention.\n[BOS] It obtained Turing-complete expressive power in exchange for a vast increase in the number of parameters and training time.\n[BOS] As shown in Table 4 , we have proposed an efficient method to increase the depth of recurrence in terms of the number of parameters and training time.\n[BOS] Recently, Voita et al. (2019) and Michel et al. (2019) independently reported that only a certain subset of the heads plays an important role in the Transformer.\n[BOS] They performed analyses by pruning heads from an already trained model, while we have proposed a method to assign weights to heads automatically.\n[BOS] We assume our method (multi-hop attention or attention-over-heads) selects important heads in the early stage of training, which results in faster convergence than the original Transformer.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Reflection", "Multi_summ", "Multi_summ", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 49, "char_start": 6, "char_end": 235, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Dehghani et al., 2019)": "49667762"}, "Reference": {}}}, {"token_start": 82, "token_end": 144, "char_start": 409, "char_end": 712, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Voita et al. (2019)": "162183964", "Michel et al. (2019)": "166227946"}, "Reference": {}}}]}
{"id": "196181767_1", "paragraph": "[BOS] It is well known that the Transformer is difficult to train (Popel and Bojar, 2018) .\n[BOS] As it has a large number of parameters, it takes time to converge and sometimes it does not do so at all without appropriate hyper parameter tuning.\n[BOS] Considering the experimental results of our multi-hop attention experiments, and that of the Weight Transformer, an appropriate design of the network to combine multi-head attention could result in faster and more stable convergence of the Transformer.\n[BOS] As the Transformer is used as a building block for the recently proposed pre-trained language models such as BERT (Devlin et al., 2019 ) which takes about a month for training, we think it is worthwhile to pursue this line of research including the proposed multi-hop attention.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Reflection", "Narrative_cite"], "span_citation_mapping": [{"token_start": 8, "token_end": 22, "char_start": 32, "char_end": 89, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Popel and Bojar, 2018)": "4556964"}}}, {"token_start": 111, "token_end": 129, "char_start": 585, "char_end": 648, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Devlin et al., 2019": "52967399"}}}]}
{"id": "196181767_0", "paragraph": "[BOS] The mechanism of the proposed multi-hop attention for the Transformer was inspired by the hierarchical attention in multi-source sequenceto-sequence model (Libovick and Helcl, 2017) .\n[BOS] The term \"multi-hop is borrowed from the end-to-end memory network (Sukhbaatar et al., 2015) and the title \"attention over heads\" is inspired by Attention-over-Attention neural network (Cui et al., 2017) , respectively.\n[BOS] Ahmed et al. (2018) proposed Weighted Transformer which replaces multi-head attention by multiple self-attention branches that learn to combine during the training process.\n[BOS] They reported that it slightly outperformed the baseline Transformer (0.5 BLEU points on the WMT 2014 English-to-German translation task) and converges 15-40% faster.\n[BOS] They linearly combined the multiple sources of attention, while we com-bined multiple attention non-linearly using softmax function in the second hop.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 21, "token_end": 39, "char_start": 122, "char_end": 187, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Libovick\u00fd and Helcl, 2017)": "12771016"}}}, {"token_start": 52, "token_end": 70, "char_start": 237, "char_end": 288, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sukhbaatar et al., 2015)": "1399322"}}}, {"token_start": 81, "token_end": 96, "char_start": 341, "char_end": 399, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cui et al., 2017)": "9205021"}}}, {"token_start": 100, "token_end": 200, "char_start": 422, "char_end": 924, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ahmed et al. (2018)": "29769422"}, "Reference": {}}}]}
{"id": "202777817_2", "paragraph": "[BOS] Before us, the low-resource problem has been studied in tasks such as machine translation (Gu et al., 2018b,a) , pos tagging (Kann et al., 2018) , word embedding (Jiang et al., 2018) , automatic speech recognition (Tske et al., 2014) , taskoriented dialogue systems (Tran and Nguyen, 2018; Mi et al., 2019) , etc.\n[BOS] In this work, we pay attention to low-resource open domain response generation which is untouched by existing work.\n[BOS] We propose attacking the problem with unpaired data, which is related to the effort in low-resource machine translation with monolingual data (Gulcehre et al., 2015; Sennrich et al., 2015; Zhang and Zong, 2016) .\n[BOS] Our method is unique in that rather than using the unpaired data through multitask learning (Zhang and Zong, 2016) or backtranslation (Sennrich et al., 2015) , we extract linguistic knowledge from the data as latent templates and use the templates as prior in generation.\n\n", "discourse_tags": ["Narrative_cite", "Reflection", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 17, "token_end": 30, "char_start": 76, "char_end": 116, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 31, "token_end": 41, "char_start": 119, "char_end": 150, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kann et al., 2018)": "51912665"}}}, {"token_start": 42, "token_end": 52, "char_start": 153, "char_end": 188, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Jiang et al., 2018)": null}}}, {"token_start": 53, "token_end": 65, "char_start": 191, "char_end": 239, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(T\u00fcske et al., 2014)": "3097189"}}}, {"token_start": 66, "token_end": 85, "char_start": 242, "char_end": 312, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Tran and Nguyen, 2018;": "53096901", "Mi et al., 2019)": "153311737"}}}, {"token_start": 131, "token_end": 167, "char_start": 535, "char_end": 658, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Gulcehre et al., 2015;": "15352384", "Sennrich et al., 2015;": "15600925", "Zhang and Zong, 2016)": "17667087"}}}, {"token_start": 182, "token_end": 193, "char_start": 740, "char_end": 781, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhang and Zong, 2016)": "17667087"}}}, {"token_start": 194, "token_end": 207, "char_start": 785, "char_end": 824, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sennrich et al., 2015)": "15600925"}}}]}
{"id": "202777817_1", "paragraph": "[BOS] Traditional template-based text generation (Becker, 2002; Foster and White, 2004; Gatt and Reiter, 2009 ) relies on handcrafted templates that are expensive to obtain.\n[BOS] Recently, some work explores how to automatically mine templates from plain text and how to integrate the templates into neural architectures to enhance interpretability of generation.\n[BOS] Along this line, Duan et al. (2017) mine patterns from related questions in community QA websites and leverage the patterns with a retrieval-based approach and a generation-based approach for question generation.\n[BOS] Wiseman et al. (2018) exploit a hidden semi-markov model for joint template extraction and text generation.\n[BOS] In addition to structured templates, raw text retrieved from indexes is also used as \"soft templates\" in various natural language generation tasks (Guu et al., 2018; Pandey et al., 2018; Cao et al., 2018; Peng et al., 2019) .\n[BOS] In this work, we leverage templates for open domain response generation.\n[BOS] Our idea is inspired by (Wiseman et al., 2018) , but latent templates estimated from one source are transferred to another source in order to handle the low-resource problem, and the generation model is learned by an adversarial approach rather than by maximum likelihood estimation.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Single_summ", "Single_summ", "Narrative_cite", "Reflection", "Narrative_cite"], "span_citation_mapping": [{"token_start": 2, "token_end": 26, "char_start": 6, "char_end": 109, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Becker, 2002;": "28048967", "Foster and White, 2004;": "14100944", "Gatt and Reiter, 2009": "14547126"}}}, {"token_start": 74, "token_end": 111, "char_start": 388, "char_end": 583, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Duan et al. (2017)": "427742"}, "Reference": {}}}, {"token_start": 112, "token_end": 135, "char_start": 590, "char_end": 697, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wiseman et al. (2018)": "52135124"}, "Reference": {}}}, {"token_start": 142, "token_end": 193, "char_start": 741, "char_end": 927, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Guu et al., 2018;": "2318481", "Pandey et al., 2018;": "51879612", "Cao et al., 2018;": "51878811", "Peng et al., 2019)": "104292148"}}}, {"token_start": 209, "token_end": 223, "char_start": 1015, "char_end": 1061, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wiseman et al., 2018)": "52135124"}}}]}
{"id": "202777817_0", "paragraph": "[BOS] Inspired by neural machine translation, early work applies the sequence-to-sequence with attention model (Shang et al., 2015) to open domain response generation, and gets promising results.\n[BOS] Later, the basic architecture is extended to suppress generic responses Zhao et al., 2017; Xing et al., 2017) ; to model the structure of conversation contexts ; and to incorporate different types of knowledge into generation (Li et al., 2016a; .\n[BOS] In addition to model design, how to learn a generation model (Li et al., 2016c (Li et al., , 2017 , and how to evaluate the models (Liu et al., 2016; Tao et al., 2018) , are drawing attention in the community of open domain dialogue generation.\n[BOS] In this work, we study how to learn a response generation model from limited pairs, which breaks the assumption made by existing work.\n[BOS] We propose response generation with paired and unpaired data.\n[BOS] As far as we know, this is the first work on low-resource response generation for open domain dialogue systems.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Narrative_cite", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 12, "token_end": 29, "char_start": 69, "char_end": 131, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Shang et al., 2015)": "7356547"}}}, {"token_start": 50, "token_end": 67, "char_start": 256, "char_end": 311, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Zhao et al., 2017;": "14688760", "Xing et al., 2017)": "9514751"}}}, {"token_start": 69, "token_end": 93, "char_start": 317, "char_end": 445, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 106, "token_end": 124, "char_start": 499, "char_end": 552, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Li et al., 2016c": "3147007", "(Li et al., , 2017": "98180"}}}, {"token_start": 126, "token_end": 146, "char_start": 559, "char_end": 622, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Liu et al., 2016;": "9197196", "Tao et al., 2018)": "11567842"}}}]}
{"id": "199374058_6", "paragraph": "[BOS] Yang et al. (2017) is a notable example of cross-lingual transfer learning under low-resource settings where sequence labelling models were trained to transfer knowledge between English, Spanish, and Dutch for POS tagging, chunking, and Named Entity Recognition (NER) through the use of shared and private parameters.\n[BOS] In that work, three different architectures were explored for cross-domain, cross-application, and crosslingual transfer.\n[BOS] The core of their proposed models is similar to Lample et al. (2016) , with minor differences including the incorporation of GRU instead of LSTM and a training objective based on the max-margin principle.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 2, "token_end": 89, "char_start": 6, "char_end": 451, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 99, "token_end": 107, "char_start": 506, "char_end": 526, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Lample et al. (2016)": "6042994"}}}]}
{"id": "199374058_5", "paragraph": "[BOS] A related scenario in TRL is when tasks remain the same but models are designed to transfer knowledge across languages.\n[BOS] In NLP, crosslingual transfer learning has been extensively explored in the context of representation learning where monolingual spaces are mapped into a common embedding space through methods like retrofitting (Faruqui et al., 2015) , matrix factorization (Vyas and Carpuat, 2016) or similar.\n[BOS] Outside representation learning, there have been many attempts to use TRL in NLP tasks.\n[BOS] For sequence labelling, trained POS tagging models cross-lingually without access to parallel resources.\n[BOS] The model consisted of two LSTM components where one is shared between the languages and the other is private (language-specific).\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 58, "token_end": 71, "char_start": 330, "char_end": 365, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Faruqui et al., 2015)": "51838647"}}}, {"token_start": 72, "token_end": 84, "char_start": 368, "char_end": 413, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vyas and Carpuat, 2016)": "2034481"}}}]}
{"id": "199374058_4", "paragraph": "[BOS] Transfer learning (TRL) has seen a flurry of interest with the advent of pre-trained language models, transformers, and contextualised embeddings (Howard and Ruder, 2018; Peters et al., 2018; Devlin et al., 2018) .\n[BOS] Transfer learning is particularly helpful where data scarcity can be an issue, and a related task with more data can be used to alleviate the issue.\n[BOS] Liu et al. (2018) is an example of the use of task-aware language models to enhance sequence labelling using an LSTM-CRF architecture powered by a language model.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Single_summ"], "span_citation_mapping": [{"token_start": 20, "token_end": 55, "char_start": 79, "char_end": 218, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Howard and Ruder, 2018;": "40100965", "Peters et al., 2018;": "3626819", "Devlin et al., 2018)": "52967399"}}}, {"token_start": 86, "token_end": 122, "char_start": 382, "char_end": 544, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Liu et al. (2018)": "19232497"}, "Reference": {}}}]}
{"id": "199374058_3", "paragraph": "[BOS] Using an LSTM-based model, Bingel and Sgaard (2017) performed a study to find beneficial tasks for the purpose of MTL in a sequence labelling scenario.\n[BOS] In their work, the MWE model benefited from most auxiliary tasks such as chunking, CCG parsing, and Super-sense tagging.\n[BOS] A similar finding is reported in Changpinyo et al. (2018) where performance of an MWE tagger was consistently improved when jointly trained with any of the 10 different auxiliary tasks in various MTL settings.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 4, "token_end": 68, "char_start": 15, "char_end": 284, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Bingel and S\u00f8gaard (2017)": "3127682"}, "Reference": {}}}, {"token_start": 75, "token_end": 112, "char_start": 324, "char_end": 500, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "199374058_2", "paragraph": "[BOS] is the output representation of RNN for word i and f t is the tagger/classification function.\n[BOS] This way, different tasks might be applied to different RNN layers (i.e. there are layers shared by several tasks, and layers that are specific to some tasks).\n[BOS] We use this idea here, by having some specific layers for final MWE prediction which are not shared with the auxiliary parsing task.\n\n", "discourse_tags": ["Transition", "Transition", "Reflection"], "span_citation_mapping": []}
{"id": "199374058_1", "paragraph": "[BOS] The idea of multitask learning (MTL) in neural networks was popularised by the work of Collobert et al. (2011) .\n[BOS] They improved the performance of chunking by jointly learning it with POS tagging.\n[BOS] Sgaard and Goldberg (2016) discuss the idea further by pinpointing that supervising different tasks on different layers is beneficial.\n[BOS] Specifically, in their work, for an input sequence, w 1:n they have several RNN layers l for each task, t, and their task-specific classifier is defined as:\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 47, "char_start": 6, "char_end": 207, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 48, "token_end": 101, "char_start": 214, "char_end": 457, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"S\u00f8gaard and Goldberg (2016)": "16661147"}, "Reference": {}}}]}
{"id": "199374058_0", "paragraph": "[BOS] Constant and Nivre (2016) proposed joint syntactic and lexical analysis in which the syntactic dimension of their structure is represented by a dependency tree, and the lexical dimension is represented by a forest of trees.\n[BOS] The two dimensions share token-level representations.\n[BOS] They use a transition-based system that jointly learns both lexical and syntactic analysis resulting in an improvement for the task of MWE identification.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 80, "char_start": 6, "char_end": 450, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "201630407_0", "paragraph": "[BOS] (a) Dynamic Word Embeddings from (Yao et al., 2018) has a predefined ordering of embeddings U.\n[BOS] Figure 1: Comparison of Dynamic Word Embeddings (Yao et al., 2018) and WEN which can be seen as a generalization of the former.\n\n", "discourse_tags": ["Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 5, "token_end": 25, "char_start": 10, "char_end": 100, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Yao et al., 2018)": "36748720"}, "Reference": {}}}, {"token_start": 31, "token_end": 42, "char_start": 131, "char_end": 173, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yao et al., 2018)": "36748720"}}}]}
{"id": "67855635_5", "paragraph": "[BOS] , where x i and y i are a context and its response, respectively.\n[BOS] x i consists of one or more utterances.\n[BOS] Our aim is to train a model on D to generate relevant and diverse responses given a context.\n\n", "discourse_tags": ["Other", "Other", "Reflection"], "span_citation_mapping": []}
{"id": "67855635_4", "paragraph": "[BOS] 3 The SPACEFUSION Model\n\n", "discourse_tags": ["Other"], "span_citation_mapping": []}
{"id": "67855635_3", "paragraph": "[BOS] Multi-task learning is another line of studies related to the present work (see Section 3.2).\n[BOS] Sennrich et al. (2016) use multi-task learning to improve neural machine translation by utilizing monolingual data, which usually far exceeds the amount of parallel data.\n[BOS] A similar idea is applied by Luan et al. (2017) to conversational modeling, involving two tasks: 1) a S2S model that learns a context-to-response mapping using conversation data, and 2) an AE model that utilizes speakerspecific non-conversational data.\n[BOS] The decoders of S2S and AE were shared, and the two tasks were trained alternately.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 25, "token_end": 60, "char_start": 106, "char_end": 276, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Sennrich et al. (2016)": "15600925"}, "Reference": {}}}, {"token_start": 67, "token_end": 142, "char_start": 312, "char_end": 625, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Luan et al. (2017)": "31298398"}, "Reference": {}}}]}
{"id": "67855635_2", "paragraph": "[BOS] Decoding and ranking encourage diversity during the decoding stage.\n[BOS] As \"vanilla\" beam search often produces lists of nearly identical sequences, Vijayakumar et al. (2016) propose to include a dissimilarity term in the objective of beam search decoding.\n[BOS] Li et al. (2016a) re-ranked the results obtained by beam search based on mutual information with the context using a separately trained response-to-context S2S model.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 28, "token_end": 53, "char_start": 157, "char_end": 264, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Vijayakumar et al. (2016)": "44614"}, "Reference": {}}}, {"token_start": 54, "token_end": 92, "char_start": 271, "char_end": 437, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Li et al. (2016a)": "7287895"}, "Reference": {}}}]}
{"id": "67855635_1", "paragraph": "[BOS] Variational autoencoder (VAE) models explicitly model the uncertainty of responses in latent space.\n[BOS] Bowman et al. (2016) used VAE with Long-Short Term Memory (LSTM) cells to generate sentences.\n[BOS] The basic idea of VAE is to encode the input x into a probability distribution (e.g. Gaussian) z instead of a point encoding.\n[BOS] However, it suffers from the vanishing latent variable problem (Bowman et al., 2016; Zhao et al., 2017) when applied to text generation tasks.\n[BOS] Bowman et al. (2016) ; Fu et al. (2019) proposed to tackle this problem with word dropping and specific KL annealing methods.\n[BOS] Zhao et al. (2017) proposed to add a bag-of-word loss, complementary to KL annealing.\n[BOS] Applying this to a CVAE conversation model, they showed that even greedy decoding can generate diverse responses.\n[BOS] However, as VAE/CVAE conversation models can be limited to a simple latent representations such as standard Gaussian distribution, Gu et al. (2018) proposed to enrich the latent space by leveraging a Gaussian mixture prior.\n[BOS] Our work takes a geometrical approach that is fundamentally different from probabilistic approaches to tackle the limitations of parameteric distributions in representation and difficulties in training.\n\n", "discourse_tags": ["Transition", "Single_summ", "Transition", "Narrative_cite", "Multi_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 22, "token_end": 48, "char_start": 112, "char_end": 205, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Bowman et al. (2016)": "748227"}, "Reference": {}}}, {"token_start": 83, "token_end": 113, "char_start": 356, "char_end": 486, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bowman et al., 2016;": "748227", "Zhao et al., 2017)": "14688760"}}}, {"token_start": 114, "token_end": 145, "char_start": 493, "char_end": 618, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Bowman et al. (2016)": "748227", "Fu et al. (2019)": "85501317"}, "Reference": {}}}, {"token_start": 146, "token_end": 190, "char_start": 625, "char_end": 830, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhao et al. (2017)": "14688760"}, "Reference": {}}}, {"token_start": 194, "token_end": 236, "char_start": 849, "char_end": 1060, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Gu et al. (2018)": "44129557"}, "Reference": {}}}]}
{"id": "67855635_0", "paragraph": "[BOS] Grounded conversation models utilize extra context inputs besides conversation history, such as persona (Li et al., 2016b) , textual knowledge (Ghazvininejad et al., 2017; , dialog act (Zhao et al., 2017) and emotion (Huber et al., 2018) .\n[BOS] Our approach does not depend on such extra input and thus is complementary to this line of studies.\n\n", "discourse_tags": ["Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 15, "token_end": 26, "char_start": 102, "char_end": 128, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Li et al., 2016b)": "2955580"}}}, {"token_start": 27, "token_end": 41, "char_start": 131, "char_end": 176, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 43, "token_end": 53, "char_start": 180, "char_end": 210, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhao et al., 2017)": "14688760"}}}, {"token_start": 54, "token_end": 64, "char_start": 215, "char_end": 243, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Huber et al., 2018)": null}}}]}
{"id": "197935145_3", "paragraph": "[BOS] In this work, we focus on generating more diverse and interesting endings for stories by introducing conditioning on keyphrases present in the story context and encouraging infrequent words in the outputs by modifying the training objective, thus leading to more interesting story endings.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "197935145_2", "paragraph": "[BOS] Recent works have also made advances in controllable generation of text based on constraints to make the outputs more specific.\n[BOS] have a conditional embedding matrix for valence to control the ending of the story.\n[BOS] Hu et al. (2017b) have a toggle vector to introduce constraint on the output of text generation models using Variational Auto Encoders (Doersch, 2016) .\n[BOS] Generating diverse responses based on conditioning has been done extensively in the field of dialogue systems.\n[BOS] Xing et al. (2016) ; ; propose conditioning techniques by using emotion and persona while generating responses.\n[BOS] Conditioned generation has also been studied in the field of story generation to plan and write (Yao et al., 2018; stories.\n\n", "discourse_tags": ["Transition", "Transition", "Multi_summ", "Transition", "Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 40, "token_end": 76, "char_start": 230, "char_end": 380, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hu et al. (2017b)": "20981275"}, "Reference": {"(Doersch, 2016)": "10510670"}}}, {"token_start": 96, "token_end": 119, "char_start": 506, "char_end": 617, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Xing et al. (2016)": "17075762"}, "Reference": {}}}, {"token_start": 130, "token_end": 143, "char_start": 685, "char_end": 737, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "197935145_1", "paragraph": "[BOS] Methods include using self-attention Shao et al. (2017) , Reinforcement Learning (Li et al., 2017) , GANs etc.\n[BOS] Xu et al. (2018) proposed a method called Diversity-Promoting Generative Adversarial Network, which assigns low reward for repeatedly generated text and high reward for novel and fluent text using a language model based discriminator.\n[BOS] Li et al. (2016a) propose a Maximum Mutual Information (MMI) objective function and show that this objective function leads to a decrease in the proportion of generic response sequences.\n[BOS] Nakamura et al. (2018) propose another loss function for the same objective.\n[BOS] In our models we experiment with their loss function and observe similar effects.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 5, "token_end": 16, "char_start": 28, "char_end": 61, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Shao et al. (2017)": "5586146"}}}, {"token_start": 17, "token_end": 27, "char_start": 64, "char_end": 104, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Li et al., 2017)": "98180"}}}, {"token_start": 33, "token_end": 77, "char_start": 123, "char_end": 357, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Xu et al. (2018)": "53081554"}, "Reference": {}}}, {"token_start": 78, "token_end": 115, "char_start": 364, "char_end": 550, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Li et al. (2016a)": "7287895"}, "Reference": {}}}, {"token_start": 116, "token_end": 132, "char_start": 557, "char_end": 633, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Nakamura et al. (2018)": "53791134"}, "Reference": {}}}]}
{"id": "197935145_0", "paragraph": "[BOS] There has been a surge in recent years to tackle the problem of story generation.\n[BOS] One common theme is to employ the advances in deep learning for the task.\n[BOS] Jain et al. (2017) use Seq2Seq models (Sutskever et al., 2014) to generate stories from descriptions of images.\n[BOS] leverage hierarchical decoding where a high-level decoder constructs a plan by generating a topic and a low-level decoder generates sentences based on the topic.\n[BOS] There have been a few works which try to incorporate real world knowledge during the process of story generation.\n[BOS] Guan et al. (2018) use an incremental encoding (IE) scheme and perform one hop reasoning over the ConceptNet graph ConceptNet in order to augment the representation of words in the sentences.\n[BOS] Chen et al. (2018) also tackle the problem in a similar way by including \"commonsense knowledge\" from ConceptNet as well.\n[BOS] Several prior work focus on generating more coherent stories.\n[BOS] Clark et al. (2018) A common problem with such neural approaches in general is that the generated text is very \"safe and boring\".\n[BOS] There has been a lot of recent efforts towards generating diverse outputs in problems such as dialogue systems, image captioning, story generation, etc., in order to alleviate the safe or boring text generation problem.\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ", "Transition", "Transition", "Single_summ", "Single_summ", "Transition", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 35, "token_end": 65, "char_start": 174, "char_end": 285, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Jain et al. (2017)": "643611"}, "Reference": {"(Sutskever et al., 2014)": "7961699"}}}, {"token_start": 117, "token_end": 156, "char_start": 580, "char_end": 771, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Guan et al. (2018)": "52136077"}, "Reference": {}}}, {"token_start": 157, "token_end": 185, "char_start": 778, "char_end": 899, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Chen et al. (2018)": "53282813"}, "Reference": {}}}, {"token_start": 197, "token_end": 225, "char_start": 974, "char_end": 1101, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Clark et al. (2018)": "52865081"}, "Reference": {}}}]}
{"id": "202771122_3", "paragraph": "[BOS] Other explorations with respect to the characteristics of the abstractive summarization task include copying mechanism that copies words from source sequences for composing summaries (Gu et al., 2016; Song et al., 2018b) , the selection mechanism that elaborately selects important parts of source sentences (Zhou et al., 2017; Lin et al., 2018) , the distraction mechanism that avoids repeated attention on the same area (Chen et al., 2016) , and the sequence level training that avoids exposure bias in teacher forcing methods (Ayana et al., 2016; Li et al., 2018; Edunov et al., 2018) .\n[BOS] Such methods are built on conventional attention, and are orthogonal to our proposed contrastive attention mechanism.\n\n", "discourse_tags": ["Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 18, "token_end": 46, "char_start": 107, "char_end": 226, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Gu et al., 2016;": "8174613", "Song et al., 2018b)": "46936631"}}}, {"token_start": 48, "token_end": 74, "char_start": 233, "char_end": 351, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhou et al., 2017;": "1770102", "Lin et al., 2018)": "13707541"}}}, {"token_start": 76, "token_end": 94, "char_start": 358, "char_end": 447, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chen et al., 2016)": "30064835"}}}, {"token_start": 97, "token_end": 132, "char_start": 458, "char_end": 593, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ayana et al., 2016;": "16921413", "Li et al., 2018;": "13663262", "Edunov et al., 2018)": "3718988"}}}]}
{"id": "202771122_2", "paragraph": "[BOS] The most related work to our contrastive attention mechanism is in the field of computer vision.\n[BOS] Song et al. (2018a) first propose the contrastive attention mechanism for person re-identification.\n[BOS] In their work, based on a pre-provided person and background segmentation, the two regions are contrastively attended so that they can be easily discriminated.\n[BOS] In comparison, we apply the contrastive attention mechanism for sentence level summarization by contrastively attending to relevant parts and irrelevant or less relevant parts.\n[BOS] Furthermore, we propose a novel softmax softmin functionality to train the attention mechanism, which is different to Song et al. (2018a) , who use mean squared error loss for attention training.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Reflection", "Single_summ"], "span_citation_mapping": [{"token_start": 21, "token_end": 75, "char_start": 109, "char_end": 374, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Song et al. (2018a)": "52084273"}, "Reference": {}}}, {"token_start": 124, "token_end": 147, "char_start": 660, "char_end": 759, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Song et al. (2018a)": "52084273"}}}]}
{"id": "202771122_1", "paragraph": "[BOS] In recent years, sequence-to-sequence neural framework becomes predominant on this task by encoding long source texts and decoding into short summaries together with the attention mechanism.\n[BOS] RNN is the most commonly adopted and extensively explored architecture (Chopra et al., 2016; Li et al., 2017) .\n[BOS] A CNN-based architecture is recently employed by Gehring et al. (2017) using ConvS2S, which applies CNN on both encoder and decoder.\n[BOS] Later, Wang et al. (2018) build upon ConvS2S with topic words embedding and encoding, and train the system with reinforcement learning.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 35, "token_end": 62, "char_start": 203, "char_end": 312, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chopra et al., 2016;": "133195", "Li et al., 2017)": "1508909"}}}, {"token_start": 65, "token_end": 97, "char_start": 323, "char_end": 453, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Gehring et al. (2017)": "3648736"}, "Reference": {}}}, {"token_start": 100, "token_end": 128, "char_start": 467, "char_end": 595, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wang et al. (2018)": "13663262"}, "Reference": {}}}]}
{"id": "202771122_0", "paragraph": "[BOS] Automatic summarization has been investigated in two main paradigms: the extractive method and the abstractive method.\n[BOS] The former extracts important pieces of source document and concatenates them sequentially (Jing and McKeown, 2000; Knight and Marcu, 2000; Neto et al., 2002) , while the latter grasps the core meaning of the source text and re-state it in short text as abstractive summary (Banko et al., 2000; Rush et al., 2015) .\n[BOS] In this paper, we focus on abstractive summarization, and especially on abstractive sentence summarization.\n[BOS] Previous work deals with the abstractive sentence summarization task by using either rule based methods (Dorr et al., 2003) , or statistical methods utilizing a source-summary parallel corpus to train a machine translation model (Banko et al., 2000) , or a syntax based transduction model (Cohn and Lapata, 2008; Woodsend et al., 2010) .\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Reflection", "Narrative_cite"], "span_citation_mapping": [{"token_start": 27, "token_end": 62, "char_start": 151, "char_end": 289, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Jing and McKeown, 2000;": "800331", "Knight and Marcu, 2000;": null, "Neto et al., 2002)": "1356305"}}}, {"token_start": 75, "token_end": 104, "char_start": 352, "char_end": 444, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Banko et al., 2000;": "9952653", "Rush et al., 2015)": "1918428"}}}, {"token_start": 142, "token_end": 154, "char_start": 652, "char_end": 690, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dorr et al., 2003)": "1729177"}}}, {"token_start": 168, "token_end": 180, "char_start": 770, "char_end": 816, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Banko et al., 2000)": "9952653"}}}, {"token_start": 183, "token_end": 204, "char_start": 824, "char_end": 902, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cohn and Lapata, 2008;": "2411338", "Woodsend et al., 2010)": "2292300"}}}]}
{"id": "208031706_1", "paragraph": "[BOS] For text-to-SQL parsing, the work most closely related to ours is SyntaxSQLNet (Yu et al., 2018b) , which is the state-of-the-art approach for the Spider data set (Yu et al., 2018c) .\n[BOS] SyntaxSQLNet extends prior text-to-SQL models, such as SQL-Net (Xu et al., 2017) and TypeSQL (Yu et al., 2018a) , by encoding both local information from column names and global information from table names.\n[BOS] The primary difference between Syn-taxSQLNet and our work is that we use a novel column embedding technique that additionally includes a graph of the tables, connected through shared column names.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 18, "token_end": 31, "char_start": 72, "char_end": 103, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yu et al., 2018b)": "52979524"}}}, {"token_start": 45, "token_end": 57, "char_start": 153, "char_end": 187, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yu et al., 2018c)": "52815560"}}}, {"token_start": 74, "token_end": 85, "char_start": 251, "char_end": 276, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Xu et al., 2017)": "10746949"}}}, {"token_start": 86, "token_end": 97, "char_start": 281, "char_end": 307, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yu et al., 2018a)": "13748720"}}}]}
{"id": "208031706_0", "paragraph": "[BOS] Many semantic parsers have been developed to translate natural language text into structured, symbolic forms, including abstract meaning representation (Lyu and Titov, 2018) , executable programs (e.g. Python, Lisp, Bash) (Allamanis et al., 2015; Rabinovich et al., 2017; Yin and Neubig, 2017; Liang et al., 2017; Lin et al., 2018) , and SQL queries (Dong and Lapata, 2018; Yu et al., 2018b,a; Xu et al., 2017) .\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 21, "token_end": 33, "char_start": 126, "char_end": 179, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lyu and Titov, 2018)": "46889674"}}}, {"token_start": 34, "token_end": 90, "char_start": 182, "char_end": 337, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Allamanis et al., 2015;": "2706277", "Rabinovich et al., 2017;": "13529592", "Yin and Neubig, 2017;": "12718048", "Liang et al., 2017;": "2742513", "Lin et al., 2018)": "3514435"}}}, {"token_start": 92, "token_end": 119, "char_start": 344, "char_end": 416, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dong and Lapata, 2018;": "44167998", "Xu et al., 2017)": "10746949"}}}]}
{"id": "198184826_4", "paragraph": "[BOS] However, such generative models often struggle to produce high-quality outputs.\n[BOS] Li et al. (2018) instead approaches the style transfer task by observing that there are often specific phrases that define the attribute or style of the text.\n[BOS] Their model segments in each sentence the specific phrases associated with the source style, then use a neural network to generate the target sentence with replacement phrases associated with the target style.\n[BOS] While they produce higher quality outputs than previous methods, this method requires manual annotation and may be more limited in capturing rich syntactic differences beyond the annotated phrases.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 17, "token_end": 114, "char_start": 92, "char_end": 670, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Li et al. (2018)": "4937880"}, "Reference": {}}}]}
{"id": "198184826_3", "paragraph": "[BOS] Style transfer Existing work for style transfer often takes the approach of separating content and style, for example by encoding a sentence into some latent space (Bowman et al., 2015; Hu et al., 2017; Shen et al., 2017) and then modifying or augmenting that space towards a different style.\n[BOS] Hu et al. (2017) base their method on variational autoencoders (Kingma and Welling, 2014), while Shen et al. (2017) instead propose two constrained variants of the autoencoder.\n[BOS] Yang et al. (2018) use language models as discriminators instead of a binary classifier as they hypothesize language models provide better training signal for the generator.\n[BOS] In the work perhaps most similar to the method we describe here, Prabhumoye et al. (2018) treat style transfer as a backtranslation problem, using a pivot language to first transform the original text to another language, then encoding the translation to a latent space where they use adversarial techniques to preserve content while removing style.\n\n", "discourse_tags": ["Narrative_cite", "Multi_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 22, "token_end": 52, "char_start": 127, "char_end": 227, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bowman et al., 2015;": "748227", "Hu et al., 2017;": "20981275", "Shen et al., 2017)": "7296803"}}}, {"token_start": 66, "token_end": 90, "char_start": 305, "char_end": 394, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hu et al. (2017)": "20981275"}, "Reference": {}}}, {"token_start": 92, "token_end": 110, "char_start": 402, "char_end": 481, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Shen et al. (2017)": "7296803"}, "Reference": {}}}, {"token_start": 111, "token_end": 142, "char_start": 488, "char_end": 661, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Yang et al. (2018)": "44061800"}, "Reference": {}}}, {"token_start": 156, "token_end": 213, "char_start": 733, "char_end": 1017, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Prabhumoye et al. (2018)": "13959787"}, "Reference": {}}}]}
{"id": "198184826_2", "paragraph": "[BOS] Noising and denoising To our knowledge, there has been no prior work formulating style transfer as a denoising task outside of using token corruptions to avoid copying between source and target.\n[BOS] Our style transfer method borrows techniques from the field of noising and denoising to correct errors in text.\n[BOS] We apply the noising technique in Xie et al. (2018) that requires an initial noise seed corpus instead of dictionaries or aligned embeddings.\n[BOS] Similar work for using noise to create a parallel corpus includes Ge et al. (2018) .\n\n", "discourse_tags": ["Transition", "Reflection", "Reflection", "Narrative_cite"], "span_citation_mapping": [{"token_start": 64, "token_end": 90, "char_start": 338, "char_end": 466, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Xie et al. (2018)": "21730715"}}}, {"token_start": 95, "token_end": 109, "char_start": 496, "char_end": 555, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Ge et al. (2018)": "49664123"}}}]}
{"id": "198184826_1", "paragraph": "[BOS] Machine translation Much work in style transfer builds off of work in neural machine translation, in particular recent work on machine translation without parallel data using only a dictionary or aligned word embeddings (Lample et al., 2017; Artetxe et al., 2017) .\n[BOS] These approaches also use backtranslation while introducing token-level corruptions to avoid the problem of copying during an initial autoencoder training phase.\n[BOS] They additionally use an initial dictionary or embedding alignments which may be infeasible to collect for many style transfer tasks.\n[BOS] Finally, our work also draws from work on zero-shot translation between languages given parallel corpora with a pivot language (Johnson et al., 2017) .\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 23, "token_end": 54, "char_start": 133, "char_end": 269, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lample et al., 2017;": "3518190", "Artetxe et al., 2017)": "3515219"}}}, {"token_start": 117, "token_end": 139, "char_start": 628, "char_end": 735, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Johnson et al., 2017)": "6053988"}}}]}
{"id": "198184826_0", "paragraph": "[BOS] Our work is related to broader work in training neural machine translation models in low-resource settings, work examining effective methods for applying noise to text, as well as work in style transfer.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "195063927_1", "paragraph": "[BOS] In later work, Melamud et al. (2016) propose context2vec, a model that uses a neural network architecture based on word2vec CBOW (Mikolov et al., 2013) .\n[BOS] context2vec replaces CBOW's representation of a word's surrounding context as a simple average of the embeddings of the context words in a fixed window, with a full sentence neural representation of context obtained using a bidirectional LSTM.\n[BOS] Sentential contexts and target words are embedded in the same low-dimensional space, which is optimized to reflect inter-dependencies between them.\n[BOS] This rich representation gives context2vec high performance in tasks involving context, such as lexical substitution, word sense disambiguation and sentence completion.\n[BOS] Peters et al. (2018a) propose a new type of deep contextualized word representations called ELMo (Embeddings from Language Models), where each token is assigned a representation that is a function of the entire input sentence.\n[BOS] Vectors are derived from a bidirectional LSTM that is trained with a coupled language model (LM) objective on a large test corpus.\n[BOS] ELMo representations are deep in the sense that they are a function of all of the internal layers of the biLM, which improves performance in several syntax and semantics-related tasks compared to using the top LSTM layer.\n[BOS] The best combination of layers is learnt jointly with a supervised NLP task.\n[BOS] An analysis on different tasks shows that lower layers efficiently encode syntactic information, while higher layers capture semantics (Peters et al., 2018b) .\n[BOS] The gains observed in syntactic tasks outweigh those on semantic-related tasks, such as coreference resolution, Semantic Role Labeling and word sense disambiguation.\n[BOS] In this work, we apply the ELMo vectors for the first time to the lexical substitution task and compare their performance to the contextsensitive models of Melamud et al. (2015) and Melamud et al. (2016) .\n[BOS] We also propose a way to tune the ELMo representations to the LexSub task, by using a dataset containing a high number of sentences for words in context that represent meanings close to that of their possible substitutes.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Transition", "Transition", "Single_summ", "Single_summ", "Transition", "Transition", "Narrative_cite", "Transition", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 6, "token_end": 46, "char_start": 21, "char_end": 159, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Melamud et al. (2016)": "7890036"}, "Reference": {"(Mikolov et al., 2013)": "5959482"}}}, {"token_start": 151, "token_end": 223, "char_start": 745, "char_end": 1108, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Peters et al. (2018a)": "3626819"}, "Reference": {}}}, {"token_start": 301, "token_end": 314, "char_start": 1529, "char_end": 1583, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Peters et al., 2018b)": "52098907"}}}, {"token_start": 373, "token_end": 396, "char_start": 1893, "char_end": 1967, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Melamud et al. (2015)": "2897037", "Melamud et al. (2016)": "7890036"}}}]}
{"id": "195063927_0", "paragraph": "[BOS] The lexical substitution task consists in selecting meaning-preserving substitutes for words in context.\n[BOS] Initially proposed as a testbed for word sense disambiguation systems (McCarthy and Navigli, 2007) , in recent works it is mainly seen as a way of evaluating the in-context lexical inference capacity of vector-space models without explicitly accounting for sense (Kremer et al., 2014; Melamud et al., 2015) .\n[BOS] Examples of substitutes of words in context proposed by annotators in the SemEval-2007 Lexical Substitution dataset are presented in Table 1 .\n[BOS] The main idea behind these sense-unaware models is that the basic (out-of-context) representation of a word is adapted to each specific context of use.\n[BOS] This is done by combining the basic vector of the word with the vectors of words found in its immediate context, or having a specific syntactic relation.\n[BOS] Appropriate substitutes are synonyms or paraphrases of the word that are similar to this contextualized representation.\n[BOS] Melamud et al. (2015) use word embeddings generated using the word2vec skip-gram model (Mikolov et al., 2013) .\n[BOS] word2vec learns for every word type two distinct representations, one as a target and another as a context, both embedded in the same space.\n[BOS] The context representations are generally discarded after training, considered internal to the model, and the output word embeddings represent context-insensitive target word types.\n[BOS] Melamud et al. use the context embeddings in conjunction with the target word embeddings to model word instances in context, identify appropriate substitutes by measuring their similarity to the target and the context, and obtain state-of-the-art results on the LexSub task.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Transition", "Transition", "Transition", "Multi_summ", "Multi_summ", "Multi_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 25, "token_end": 40, "char_start": 153, "char_end": 215, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(McCarthy and Navigli, 2007)": "126584"}}}, {"token_start": 69, "token_end": 88, "char_start": 374, "char_end": 423, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kremer et al., 2014;": "14141143", "Melamud et al., 2015)": "2897037"}}}, {"token_start": 204, "token_end": 296, "char_start": 1025, "char_end": 1471, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Melamud et al. (2015)": "2897037", "(Mikolov et al., 2013)": "5959482"}, "Reference": {}}}, {"token_start": 297, "token_end": 351, "char_start": 1478, "char_end": 1752, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "207901548_1", "paragraph": "[BOS] Our paper follows the modeling approach introduced by Johnson et al. (2017) , who showed that by adding a token to the source side of parallel text allows for training a single NMT model on data for multiple language pairs.\n[BOS] Their token specifies the desired target language, allowing the user control over the language of machine translation output, even for source-target language pairs that were not seen during training, which they call \"zero-shot\" translation.\n[BOS] The same approach has been successfully used in other applications, such as in distinguishing standard versus back-translated translation parallel corpora (Caswell et al., 2019) .\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 10, "token_end": 92, "char_start": 60, "char_end": 476, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Johnson et al. (2017)": "6053988"}, "Reference": {}}}, {"token_start": 110, "token_end": 126, "char_start": 593, "char_end": 660, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Caswell et al., 2019)": null}}}]}
{"id": "207901548_0", "paragraph": "[BOS] In related work on Japanese-English NMT, Yamagishi et al. (2016) use a side-constraint approach to control the voice (active or passive) of an English translation.\n[BOS] Takeno (2017) apply side constraints more broadly to control translation length, bidirectional decoding, domain adaptation, and unaligned target word generation.\n\n", "discourse_tags": ["Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 12, "token_end": 41, "char_start": 47, "char_end": 169, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 42, "token_end": 71, "char_start": 176, "char_end": 337, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Takeno (2017)": "27834461"}, "Reference": {}}}]}
{"id": "202676734_3", "paragraph": "[BOS] Different from all above works, our model focuses on patternized summary generation, which is more challenging than traditional news summarization and short sentence prototype editing.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "202676734_2", "paragraph": "[BOS] Another line of research focuses on prototype editing.\n[BOS] (Guu et al., 2018) proposed the first prototype editing model, which samples a prototype sentence from training data and then edits it into a new sentence.\n[BOS] Following this work, proposed a new paradigm for response generation, which first retrieves a prototype response from a pre-defined index and then edits the prototype response.\n[BOS] (Cao et al., 2018) applied this method on summarization, where they employed existing summaries as soft templates to generate new summary without modeling the dependency between the prototype document, summary and input document.\n[BOS] Different from these soft attention methods, (Cai et al., 2018) proposed a hard-editing skeleton-based model to promote the coherence of generated stories.\n[BOS] Template-based summarization is also a hard-editing method (Oya et al., 2014) , where a multi-sentence fusion algorithm is extended in order to generate summary templates.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 12, "token_end": 80, "char_start": 67, "char_end": 405, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Guu et al., 2018)": "2318481"}, "Reference": {}}}, {"token_start": 81, "token_end": 122, "char_start": 412, "char_end": 641, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Cao et al., 2018)": "51878811"}, "Reference": {}}}, {"token_start": 130, "token_end": 155, "char_start": 693, "char_end": 803, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Cai et al., 2018)": "52281331"}, "Reference": {}}}, {"token_start": 156, "token_end": 194, "char_start": 810, "char_end": 981, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Oya et al., 2014)": "12299544"}}}]}
{"id": "202676734_1", "paragraph": "[BOS] Text summarization can be classified into extractive and abstractive methods.\n[BOS] Extractive methods (Narayan et al., 2018b; Chen et al., 2018) directly select salient sentences from an article to compose a summary.\n[BOS] One shortcoming of these models is that they tend to suffer from redundancy.\n[BOS] Recently, with the emergence of neural network models for text generation, a vast majority of the literature on summarization (Ma et al., 2018; Gao et al., 2019a; Chen et al., 2019) is dedicated to abstractive summarization, which aims to generate new content that concisely paraphrases a document from scratch.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Transition", "Multi_summ"], "span_citation_mapping": [{"token_start": 17, "token_end": 49, "char_start": 90, "char_end": 223, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Narayan et al., 2018b;": "3510042", "Chen et al., 2018)": "52879916"}, "Reference": {}}}, {"token_start": 86, "token_end": 120, "char_start": 425, "char_end": 536, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Chen et al., 2019)": "199466316"}, "Reference": {"(Ma et al., 2018;": "44142790", "Gao et al., 2019a;": "55461757"}}}]}
{"id": "202676734_0", "paragraph": "[BOS] We detail related work on text summarization and prototype editing.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "174800109_2", "paragraph": "[BOS] Finally, another consideration with TL is the size of the target dataset.\n[BOS] For one NER task, TL gains were shown to decrease to nearly zero as the size of the target training data increased to around 50k tokens (Lee et al., 2017) .\n[BOS] Similarly, for domain adaptation, a \"phase transition\" was observed in the amount of used target data, such that using source data was not effective when the target model was trained on 3.13k or more target instances (Ben-David et al., 2010) .\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 22, "token_end": 53, "char_start": 104, "char_end": 240, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lee et al., 2017)": "6502142"}}}, {"token_start": 63, "token_end": 109, "char_start": 286, "char_end": 490, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ben-David et al., 2010)": "8577357"}}}]}
{"id": "174800109_1", "paragraph": "[BOS] Another question of interest concerns the pair of languages between which TL can be achieved.\n[BOS] Past work has shown transferring to a related language to help more than to an unrelated one for NER, POS tagging, and NMT (Zirikly and Hagiwara, 2015; Kim et al., 2017; Dabre et al., 2017) .\n[BOS] In Yang et al. (2017) it is mentioned that without additional resources, it is \"very difficult for transfer learning between languages with disparate alphabets\".\n[BOS] This background suggests TL from En-glish to Japanese to be non-trivial.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 37, "token_end": 71, "char_start": 203, "char_end": 295, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zirikly and Hagiwara, 2015;": null, "Kim et al., 2017;": "9489563", "Dabre et al., 2017)": "52141518"}}}, {"token_start": 74, "token_end": 123, "char_start": 307, "char_end": 544, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Yang et al. (2017)": "17984798"}, "Reference": {}}}]}
{"id": "174800109_0", "paragraph": "[BOS] One of the first works on cross-lingual TL for NER that did not rely on parallel corpora used a CRF and included hand-crafted features (Zirikly and Hagiwara, 2015) .\n[BOS] Currently, most work on TL is done with neural models.\n[BOS] Because neural models often consist of multiple layers, one important design decision is which layers to transfer from source to target.\n[BOS] Much related work involves only transferring a single layer or specific combination of layers.\n[BOS] In Lee et al. (2017) the authors present more thorough results combining lower and higher layers, without transferring intermediate layers though.\n[BOS] In Yang et al. (2017) it is suggested to transfer only the character embeddings and the character RNN weights between languages.\n[BOS] The reason for this is likely that many languages written in the Latin alphabet have a large charset overlap, but far less vocabulary overlap.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Transition", "Single_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 27, "token_end": 44, "char_start": 119, "char_end": 169, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zirikly and Hagiwara, 2015)": null}}}, {"token_start": 100, "token_end": 125, "char_start": 486, "char_end": 629, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Lee et al. (2017)": "6502142"}, "Reference": {}}}, {"token_start": 127, "token_end": 152, "char_start": 639, "char_end": 764, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Yang et al. (2017)": "17984798"}, "Reference": {}}}]}
{"id": "202677370_1", "paragraph": "[BOS] Recent work has developed several probing methods for (monolingual) contextual representations (Liu et al., 2019; Hewitt and Manning, 2019; Tenney et al., 2019) .\n[BOS] Wada and Iwata (2018) showed that the (contextless) input and output word vectors in a polyglot word-based language model manifest a certain level of lexical correspondence between languages.\n[BOS] Our decontextual probe demonstrated that the internal layers of polyglot language models capture crosslinguality and produce useful multilingual CWRs for downstream low-resource dependency parsing.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 10, "token_end": 44, "char_start": 60, "char_end": 166, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Tenney et al., 2019)": "155092004"}}}, {"token_start": 46, "token_end": 85, "char_start": 175, "char_end": 366, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wada and Iwata (2018)": "52179115"}, "Reference": {}}}]}
{"id": "202677370_0", "paragraph": "[BOS] In addition to the work mentioned above, much previous work has proposed techniques to transfer knowledge from a high-resource to a lowresource language for dependency parsing.\n[BOS] Many of these methods use an essentially (either lexicalized or delexicalized) joint polyglot training setup (e.g., McDonald et al., 2011; Cohen et al., 2011; Duong et al., 2015; Guo et al., 2016; Vilares et al., 2016; Falenska and  etinoglu, 2017 (2018)).\n[BOS] Some use typological information to facilitate crosslingual transfer (e.g., Naseem et al., 2012; Tckstrm et al., 2013; Zhang and Barzilay, 2015; Wang and Eisner, 2016; Rasooli and Collins, 2017; Ammar et al., 2016) .\n[BOS] Others use bitext (Zeman et al., 2018) , manually-specified rules (Naseem et al., 2012) , or surface statistics from gold universal part of speech (Wang and Eisner, 2018a,b) to map the source to target.\n[BOS] The methods examined in this work to produce multilingual CWRs do not rely on such external information about the languages, and instead use relatively abundant LM data to learn crosslinguality that abstracts away from typological divergence.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 41, "token_end": 113, "char_start": 230, "char_end": 445, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Vilares et al., 2016;": "2206746"}}}, {"token_start": 116, "token_end": 181, "char_start": 461, "char_end": 666, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"T\u00e4ckstr\u00f6m et al., 2013;": "2037646", "Zhang and Barzilay, 2015;": "9555772", "Wang and Eisner, 2016;": "10817864", "Rasooli and Collins, 2017;": "2626026"}}}, {"token_start": 185, "token_end": 196, "char_start": 686, "char_end": 713, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 197, "token_end": 211, "char_start": 716, "char_end": 762, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 213, "token_end": 232, "char_start": 768, "char_end": 848, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "202540766_3", "paragraph": "[BOS] Embeddings from Language Models (ELMo) made a significant step forward (Peters et al., 2018) .\n[BOS] ELMo uses language modeling with bidirectional recurrent neural networks (RNN) to improve word embeddings.\n[BOS] ELMo's embedding contributes to the performance of various downstream tasks.\n[BOS] OpenAI GPT (Radford et al., 2018) replaced ELMo's bidirectional RNN for language modeling with the Transformer (Vaswani et al., 2017) decoder.\n[BOS] More recently, BERT combined the approaches of Quick-Thoughts (i.e., a nextsentence prediction approach) and language modeling on top of the deep bidirectional Transformer.\n[BOS] BERT broke the records of the previous stateof-the-art methods in eleven different NLP tasks.\n[BOS] While BERT's pre-training generates generic representations that are broadly transferable to various NLP tasks, we aim to fit them for semantic equivalence assessment by injecting paraphrasal relations.\n[BOS] Liu et al. (2019) showed that BERT's performance improves when fine-tuning with a multi-task learning setting, which is applicable to our trained model for further improvement.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Single_summ", "Transition", "Transition", "Reflection", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 24, "char_start": 6, "char_end": 100, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Peters et al., 2018)": "3626819"}}}, {"token_start": 60, "token_end": 99, "char_start": 303, "char_end": 445, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Radford et al., 2018)": null}, "Reference": {"(Vaswani et al., 2017)": "13756489"}}}, {"token_start": 198, "token_end": 236, "char_start": 940, "char_end": 1116, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Liu et al. (2019)": "59523594"}, "Reference": {}}}]}
{"id": "202540766_2", "paragraph": "[BOS] The research focus of sentence representation learning has moved toward unsupervised learning in order to exploit the gigantic corpus.\n[BOS] SkipThought, which was an early learning attempt, learns to generate surrounding sentences given a sentence in a document .\n[BOS] This can be interpreted as an extension of the distributional hypothesis on sentences.\n[BOS] Quick-Thoughts, a successor of Skip-Thought, conducts classification to discriminate surrounding sentences instead of generation (Logeswaran and Lee, 2018) .\n[BOS] GenSen combines these approaches in massive multi-task learning (Subramanian et al., 2018) based on the premise that learning dependent tasks enriches sentence representations.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 63, "token_end": 94, "char_start": 370, "char_end": 525, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Logeswaran and Lee, 2018)": "3525802"}}}, {"token_start": 102, "token_end": 117, "char_start": 570, "char_end": 624, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Subramanian et al., 2018)": "4567927"}}}]}
{"id": "202540766_1", "paragraph": "[BOS] In this paper, we use structural relations in sentence pairs for sentence representations.\n[BOS] Specifically, we employ phrasal paraphrase relations that introduce the notion of a phrase to the model.\n\n", "discourse_tags": ["Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "202540766_0", "paragraph": "[BOS] Sentence representation learning is an active research area due to its importance in various downstream tasks.\n[BOS] Early studies employed supervised learning where a sentence representation is learned in an end-to-end manner using an annotated corpus.\n[BOS] Among these, the importance of phrase structures in representation learning has been discussed (Tai et al., 2015; Wu et al., 2018) .\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 50, "token_end": 74, "char_start": 294, "char_end": 396, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Tai et al., 2015;": "3033526", "Wu et al., 2018)": "53083604"}}}]}
{"id": "202769358_3", "paragraph": "[BOS] In this paper, we have presented two methods to improve the relevance of generated questions to the given passage and target answer.\n[BOS] Experiments and analyses on SQuAD show that both the partial copy mechanism and QA-based reranking improve the relevance of generated questions in terms of both BLEU and METEOR.\n\n", "discourse_tags": ["Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "202769358_2", "paragraph": "[BOS] QA and QG are closely related to each other.\n[BOS] treat QA and QG as dual tasks, and many other studies use QG to enhance QA or jointly learn QG and QA Sachan and Xing, 2018) .\n\n", "discourse_tags": ["Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 28, "token_end": 48, "char_start": 115, "char_end": 181, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Sachan and Xing, 2018)": "44130298"}}}]}
{"id": "202769358_1", "paragraph": "[BOS] After that, many QG studies have been conducted on the basis of the widely-used seq2seq architecture together with the attention and copy mechanism.\n[BOS] Song et al. (2018) propose two encoders for both the passage and the target answer.\n[BOS] Du and Cardie (2018) employ coreferences as an additional feature.\n[BOS] Kim et al. (2019) propose a model of answer separation.\n[BOS] and Kumar et al. (2018) adopt reinforcement learning to optimize the generation process.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 33, "token_end": 53, "char_start": 161, "char_end": 244, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Song et al. (2018)": "44178763"}, "Reference": {}}}, {"token_start": 54, "token_end": 69, "char_start": 251, "char_end": 317, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Du and Cardie (2018)": null}, "Reference": {}}}, {"token_start": 70, "token_end": 85, "char_start": 324, "char_end": 379, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kim et al. (2019)": "52176706"}, "Reference": {}}}, {"token_start": 87, "token_end": 103, "char_start": 390, "char_end": 474, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kumar et al. (2018)": null}, "Reference": {}}}]}
{"id": "202769358_0", "paragraph": "[BOS] The neural QG is an emerging task.\n[BOS] Unlike the extractive QA, most neural QG models are generative.\n[BOS] Du et al. (2017) pioneer the neural QG by proposing neural seq2seq models to deal with the task.\n[BOS] Unfortunately, they do not use the target answer for QG.\n[BOS] At the same time, present a similar model for QG.\n[BOS] They use answer position embeddings to represent target answers and explore a variety of lexical features.\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 27, "token_end": 67, "char_start": 117, "char_end": 276, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Du et al. (2017)": null}, "Reference": {}}}]}
{"id": "196174735_1", "paragraph": "[BOS] Reading Comprehension Machine reading comprehension (RC) aims to answer questions by comprehending evidence from passages.\n[BOS] This direction has recently drawn much attention due to the fast development of deep learning techniques and large-scale datasets.\n[BOS] The early development of the RC datasets focuses on either the cloze-style (Hermann et al., 2015; Hill et al., 2015) or quiz-style problems (Richardson et al., 2013; Lai et al., 2017) .\n[BOS] The former one aims to generate single-token answers from automatically constructed pseudo-questions while the latter requires choosing from multiple answer candidates.\n[BOS] However, such unnatural settings make them fail to serve as the standard QA benchmarks.\n[BOS] Instead, researchers started to ask human annotators to create questions and answers given passages in a crowdsourced way.\n[BOS] Such efforts give the rise of large-scale human-annotated RC datasets, many of which are quite popular in the community such as SQuAD (Rajpurkar et al., 2016) , MS MARCO (Nguyen et al., 2016) , NewsQA (Trischler et al., 2016) .\n[BOS] More recently, researchers propose even challenging datasets that require QA within dialogue or conversational context (Reddy et al., 2018; .\n[BOS] According to the difference of the answer format, these datasets can be further divided to two major categories: extractive and abstractive.\n[BOS] In the first category, the answers are in text spans of the given passages, while in the latter case, the answers may not appear in the passages.\n[BOS] It is worth mentioning that in almost all previously developed datasets, the passages are from Wikipedia, news articles or fiction stories, which are considered as the formal language.\n[BOS] Yet, there is little effort on RC over informal one like tweets.\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite", "Transition", "Transition", "Transition", "Narrative_cite", "Narrative_cite", "Transition", "Transition", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 56, "token_end": 76, "char_start": 335, "char_end": 388, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hermann et al., 2015;": "6203757", "Hill et al., 2015)": "14915449"}}}, {"token_start": 77, "token_end": 97, "char_start": 392, "char_end": 455, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Richardson et al., 2013;": "2100831", "Lai et al., 2017)": "6826032"}}}, {"token_start": 194, "token_end": 206, "char_start": 990, "char_end": 1020, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rajpurkar et al., 2016)": "11816014"}}}, {"token_start": 207, "token_end": 218, "char_start": 1023, "char_end": 1053, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Nguyen et al., 2016)": "1289517"}}}, {"token_start": 219, "token_end": 231, "char_start": 1056, "char_end": 1087, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Trischler et al., 2016)": "1167588"}}}, {"token_start": 243, "token_end": 258, "char_start": 1170, "char_end": 1234, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "196174735_0", "paragraph": "[BOS] Tweet NLP Traditional core NLP research typically focuses on English newswire datasets such as the Penn Treebank (Marcus et al., 1993) .\n[BOS] In recent years, with the increasing usage of social media platforms, several NLP techniques and datasets for processing social media text have been proposed.\n[BOS] For example, Gimpel et al. (2011) build a Twitter part-of-speech tagger based on 1,827 manually annotated tweets.\n[BOS] Ritter et al. (2011) annotated 800 tweets, and performed an empirical study for partof-speech tagging and chunking on a new Twitter dataset.\n[BOS] They also investigated the task of Twitter Named Entity Recognition, utilizing a dataset of 2,400 annotated tweets.\n[BOS] annotated 929 tweets, and built the first dependency parser for tweets, whereas Wang et al. (2014) built the Chinese counterpart based on 1,000 annotated Weibo posts.\n[BOS] To the best of our knowledge, question answering and reading comprehension over short and noisy social media data are rarely studied in NLP, and our annotated dataset is also an order of magnitude large than the above public social-media datasets.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 21, "token_end": 33, "char_start": 105, "char_end": 140, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Marcus et al., 1993)": "252796"}}}, {"token_start": 67, "token_end": 96, "char_start": 327, "char_end": 427, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Gimpel et al. (2011)": null}, "Reference": {}}}, {"token_start": 97, "token_end": 151, "char_start": 434, "char_end": 696, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ritter et al. (2011)": "12861120"}, "Reference": {}}}, {"token_start": 167, "token_end": 188, "char_start": 783, "char_end": 869, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wang et al. (2014)": "4358268"}, "Reference": {}}}]}
{"id": "209318236_0", "paragraph": "[BOS] Visual language grounding and REG Foundational work in REG has often followed the wellknown attribute selection paradigm established by (Dale and Reiter, 1995) .\n[BOS] Here, visual scenes have usually been carefully created and controlled so that the target and distractor referents and distractors would have similarities in their set of annotated attributes (e.g. type, position, size, color and so on), see Krahmer and Van Deemter (2012) .\n[BOS] In recently used image benchmarks for REG, the visual scene is typically given through a real-world image (Kazemzadeh et al., 2014; Yu et al., 2016) , which makes it very difficult to systematically control the underlying attributes of a target referent and to what extent it resembles its distractors in the scene.\n[BOS] At the same time, Yu et al. (2016) found that, in the standard version of the RefCOCO benchmark, many participants simply used location attributes like left, right relying on the 2D layout of the scene.\n[BOS] As a remedy, they propose to introduce \"taboo words\" into the reference task in order to elicit \"appearance-based\" attributes.\n[BOS] Achlioptas et al. (2019) adopt a different approach and suggest to collect data based on more abstract objects.\n[BOS] They collect a dataset of referring expressions to chairs where various properties and parts of targets and distractors are controlled in terms of their visual similarity.\n[BOS] Our work combines ideas from both paradigms: we use real-world images of objects paired with hand-drawn sketches, which allows us to integrate realistic and abstract visual inputs.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Narrative_cite", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 33, "char_start": 6, "char_end": 165, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dale and Reiter, 1995)": "7018595"}}}, {"token_start": 63, "token_end": 96, "char_start": 338, "char_end": 446, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Krahmer and Van Deemter (2012)": null}}}, {"token_start": 114, "token_end": 137, "char_start": 544, "char_end": 603, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kazemzadeh et al., 2014;": "6308361", "Yu et al., 2016)": "1688357"}}}, {"token_start": 173, "token_end": 243, "char_start": 795, "char_end": 1112, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Yu et al. (2016)": "1688357"}, "Reference": {}}}, {"token_start": 244, "token_end": 300, "char_start": 1119, "char_end": 1408, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Achlioptas et al. (2019)": "147704191"}, "Reference": {}}}]}
{"id": "196180065_3", "paragraph": "[BOS] Optimization methods for optimizing a model with respect to evaluation scores, such as reinforcement learning (Ranzato et al., 2015; Paulus et al., 2018; Chen and Bansal, 2018; Wu and Hu, 2018) and minimum risk training (Ayana et al., 2017) , have been proposed for summarization models based on neural encoder-decoders.\n[BOS] Our method is similar to that of Ayana et al. (2017) in terms of applying MRT to neural encoder-decoders.\n[BOS] There are two differences between our method and Ayana et al. 's: (i) our method uses only the part of the summary generated by a model within the length constraint for calculating the ROUGE score and (ii) it penalizes summaries that exceed the length of the reference regardless of its ROUGE score.\n\n", "discourse_tags": ["Narrative_cite", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 16, "token_end": 49, "char_start": 93, "char_end": 199, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ranzato et al., 2015;": "7147309", "Paulus et al., 2018;": "21850704", "Chen and Bansal, 2018;": "44129061", "Wu and Hu, 2018)": "4999752"}}}, {"token_start": 50, "token_end": 62, "char_start": 204, "char_end": 246, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ayana et al., 2017)": "22324032"}}}, {"token_start": 80, "token_end": 107, "char_start": 337, "char_end": 438, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Ayana et al. (2017)": "22324032"}}}, {"token_start": 114, "token_end": 121, "char_start": 483, "char_end": 506, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "196180065_2", "paragraph": "[BOS] Pointer networks (Vinyals and Le, 2015; Gulcehre et al., 2016; See et al., 2017) and copy mechanisms (Gu et al., 2016; Zeng et al., 2016) have been proposed for overcoming the unknown word problem.\n[BOS] Other methods for the improvement of abstractive summarization models include use of existing summaries as soft templates with a source text and extraction of actual fact descriptions from a source text .\n[BOS] Although summary length control of abstractive summarization has been studied, previous studies focus on incorporation of a length controlling method to neural abstractive summarization models (Kikuchi et al., 2016; Fan et al., 2018; Liu et al., 2018; Fevry and Phang, 2018; Schumann, 2018) .\n[BOS] In contrast, our research focuses on a global optimization method.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 30, "char_start": 6, "char_end": 86, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vinyals and Le, 2015;": "12300158", "Gulcehre et al., 2016;": "969555", "See et al., 2017)": null}}}, {"token_start": 31, "token_end": 48, "char_start": 91, "char_end": 143, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Gu et al., 2016;": "8174613", "Zeng et al., 2016)": "2128782"}}}, {"token_start": 113, "token_end": 161, "char_start": 543, "char_end": 711, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kikuchi et al., 2016;": "11157751", "Fan et al., 2018;": "22716243", "Liu et al., 2018;": "53079938", "Fevry and Phang, 2018;": null, "Schumann, 2018)": "52282404"}}}]}
{"id": "196180065_1", "paragraph": "[BOS] Recently, abstractive summarization models based on neural encoder-decoders have been proposed (Rush et al., 2015; Chopra et al., 2016; Zhou et al., 2017; Paulus et al., 2018) .\n[BOS] There are mainly two research directions: model architectures and optimization methods.\n\n", "discourse_tags": ["Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 4, "token_end": 51, "char_start": 16, "char_end": 181, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rush et al., 2015;": "1918428", "Chopra et al., 2016;": "133195", "Zhou et al., 2017;": "1770102", "Paulus et al., 2018)": "21850704"}}}]}
{"id": "196180065_0", "paragraph": "[BOS] There are many models for text summarization such as rule-based models (Dorr et al., 2003) and statistical models (Banko et al., 2000; Zajic et al., 2004; Filippova and Strube, 2008; Woodsend et al., 2010; Filippova and Altun, 2013) .\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 12, "token_end": 25, "char_start": 59, "char_end": 96, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dorr et al., 2003)": "1729177"}}}, {"token_start": 26, "token_end": 72, "char_start": 101, "char_end": 238, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Banko et al., 2000;": "9952653", "Zajic et al., 2004;": null, "Filippova and Strube, 2008;": "17477341", "Woodsend et al., 2010;": "2292300", "Filippova and Altun, 2013)": "9751546"}}}]}
{"id": "201676304_3", "paragraph": "[BOS] Several models have been proposed for multisource learning, in which multiple languages are used to train a model, including for machine translation (Zoph and Knight, 2016; Johnson et al., 2017; Currey and Heafield, 2018) , and NER (Tckstrm, 2012; Tsai et al., 2016; Mayhew et al., 2017; Rahimi et al., 2019) .\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 25, "token_end": 51, "char_start": 135, "char_end": 227, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zoph and Knight, 2016;": null, "Johnson et al., 2017;": "6053988", "Currey and Heafield, 2018)": null}}}, {"token_start": 53, "token_end": 87, "char_start": 234, "char_end": 314, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(T\u00e4ckstr\u00f6m, 2012;": "8033304", "Tsai et al., 2016;": "2889848", "Mayhew et al., 2017;": "5470471"}}}]}
{"id": "201676304_2", "paragraph": "[BOS] Of particular interest is the multilingual BERT model (Devlin et al., 2019) , which is trained over the concatenation of the Wikipedias in over 100 languages.\n[BOS] 1 Although BERT is not trained with explicit cross-lingual objectives, it has been shown to have emergent cross-lingual properties, as well as language identification capabilities (Wu and Dredze, 2019) .\n\n", "discourse_tags": ["Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 7, "token_end": 21, "char_start": 36, "char_end": 81, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Devlin et al., 2019)": "52967399"}}}, {"token_start": 58, "token_end": 81, "char_start": 268, "char_end": 372, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wu and Dredze, 2019)": "126167342"}}}]}
{"id": "201676304_1", "paragraph": "[BOS] Named Entity Recognition (NER), the task of detecting and classifying named entities in text, has been studied for many years.\n[BOS] Early models proposed were averaged perceptron (Ratinov and Roth, 2009) , and conditional random field (Manning et al., 2014) .\n[BOS] In recent years, neural models have proved successful, with the BiLSTM-CRF model dominant (Chiu and Nichols, 2016; Lample et al., 2016) .\n[BOS] A further increase in performance has come with contextual embeddings (Devlin et al., 2019; Akbik et al., 2018) , which are based on large language models trained over massive corpora.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 32, "token_end": 43, "char_start": 166, "char_end": 210, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ratinov and Roth, 2009)": "1859014"}}}, {"token_start": 45, "token_end": 57, "char_start": 217, "char_end": 264, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Manning et al., 2014)": "14068874"}}}, {"token_start": 71, "token_end": 94, "char_start": 337, "char_end": 408, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chiu and Nichols, 2016;": null, "Lample et al., 2016)": "6042994"}}}, {"token_start": 104, "token_end": 125, "char_start": 465, "char_end": 528, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Devlin et al., 2019;": "52967399", "Akbik et al., 2018)": "52010710"}}}]}
{"id": "201676304_0", "paragraph": "[BOS] The first shared task in Balto-slavic NLP was held in 2017, and reported in Piskorski et al. (2017) .\n[BOS] The task was somewhat different from the 2019 task in that training data was not provided to participants.\n[BOS] Approaches submitted to this task included a model based on parallel projection (Mayfield et al., 2017) and a model with language-specific features trained on found data (Marciczuk et al., 2017) .\n[BOS] There has also been follow-up work on this dataset using cross-lingual embeddings (Sharoff, 2018) .\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 3, "token_end": 32, "char_start": 10, "char_end": 105, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Piskorski et al. (2017)": "14943763"}}}, {"token_start": 62, "token_end": 76, "char_start": 272, "char_end": 330, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mayfield et al., 2017)": "12214152"}}}, {"token_start": 78, "token_end": 98, "char_start": 337, "char_end": 421, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Marci\u0144czuk et al., 2017)": "12311990"}}}, {"token_start": 112, "token_end": 123, "char_start": 487, "char_end": 527, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sharoff, 2018)": "21699523"}}}]}
{"id": "173188413_4", "paragraph": "[BOS] In this work, we have human judges evaluate both attention based machine explanations and machine explanations trained from human rationales, thus connecting learning from human explanations and machine explanations to humans.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "173188413_3", "paragraph": "[BOS] As mentioned above, there has also been some recent criticism of using attention as explanation (Jain and Wallace, 2019) , due to a lack of correlation between the attention weights and gradient based methods which are more \"faithful\" to the model's reasoning.\n[BOS] However, attention weights offer some insight into at least one point of internal representation in the model, and they also impact the training of the later features.\n[BOS] Our contribution is to measure how useful these attention based explanations are to humans in understanding a model's decision as compared to a different model architecture that explicitly learns to predict which sentences make good explanations.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 15, "token_end": 26, "char_start": 77, "char_end": 126, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Jain and Wallace, 2019)": "67855860"}}}]}
{"id": "173188413_2", "paragraph": "[BOS] None of the above related work asks human users to evaluate the generated explanations.\n[BOS] However, Nguyen (2018) does compare human and automatic evaluations of explanations.\n[BOS] That work finds that human evaluation is moderately, but statistically significantly, correlated with the automatic metrics.\n[BOS] However, it does not evaluate any explanations based on attention, nor do the explanations make use of any extra human supervision.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 20, "token_end": 78, "char_start": 109, "char_end": 453, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Nguyen (2018)": "44130060"}, "Reference": {}}}]}
{"id": "173188413_1", "paragraph": "[BOS] As discussed above, Zhang et al. (2016) demonstrate increased predictive accuracy of CNN models augmented with human rationales.\n[BOS] Here, we first reproduce their predictive results, and then focus on extracting and evaluating explanations from the models.\n[BOS] Lei et al. (2016) present a model that extracts rationales for predictions without training on rationales.\n[BOS] They compare their extracted rationales to human gold-standard ones through automated evaluations, i.e., precision and recall.\n[BOS] Bao et al. (2018) extend this work by learning a mapping from the human rationales to continuous attention.\n[BOS] They transfer this mapping to low resource target domains as an auxiliary training signal to improve classification accuracy in the target domain.\n[BOS] They compare their learned attention with human rationales by calculating their cosine distance to the 'oracle' attention.\n\n", "discourse_tags": ["Single_summ", "Reflection", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 6, "token_end": 26, "char_start": 26, "char_end": 134, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhang et al. (2016)": "6262432"}, "Reference": {}}}, {"token_start": 49, "token_end": 97, "char_start": 272, "char_end": 511, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Lei et al. (2016)": "7205805"}, "Reference": {}}}, {"token_start": 98, "token_end": 168, "char_start": 518, "char_end": 907, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Bao et al. (2018)": "52113465"}, "Reference": {}}}]}
{"id": "173188413_0", "paragraph": "[BOS] There is a growing body of research on explainable AI (Koh and Liang, 2017; Ribeiro et al., 2016; Li et al., 2016; , but it is not connected to work on learning with human rationales, which we review below.\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 10, "token_end": 35, "char_start": 45, "char_end": 119, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Koh and Liang, 2017;": "13193974", "Ribeiro et al., 2016;": "13029170"}}}]}
{"id": "201741543_0", "paragraph": "[BOS] Dungarwal et al. (Dungarwal et al., 2014) developed a statistical method for machine translation, where phrase based method for Hindi-English and factored based method for English-Hindi SMT system was used.\n[BOS] They had shown improvements to the existing SMT systems using pre-procesing and post-processing components that generated morphological inflections correctly.\n[BOS] Imankulova et al. (Imankulova et al., 2017) showed how backtranslation and filtering from monolingual data can be used to build an effective translation system for a low-resourse language pair like Japanese- Russian.\n[BOS] Sennrich et al. (Sennrich et al., 2016a) shown how back-translation of monolingual data can improve the NMT system.\n[BOS] Ramesh et al. (Ramesh and Sankaranarayanan, 2018) demonstrated how an existing model like bidirectional recurrent neural network can be used to generate parallel sentences for non-English languages like English-Tamil and English-Hindi, which belong to low-resource language pair, to improve the SMT and the NMT systems.\n[BOS] Choudhary et al. (Choudhary et al., 2018) has shown how to build NMT system for low resource parallel corpus language pair like English-Tamil using techniques like word embeddings and Byte-PairEncoding (Sennrich et al., 2016b) to handle OutOf-Vocabulary Words.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 79, "char_start": 6, "char_end": 377, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Dungarwal et al., 2014)": null}, "Reference": {}}}, {"token_start": 80, "token_end": 131, "char_start": 384, "char_end": 600, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Imankulova et al., 2017)": "5240984"}, "Reference": {}}}, {"token_start": 132, "token_end": 165, "char_start": 607, "char_end": 722, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Sennrich et al., 2016a)": "15600925"}, "Reference": {}}}, {"token_start": 166, "token_end": 237, "char_start": 729, "char_end": 1048, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 238, "token_end": 303, "char_start": 1055, "char_end": 1315, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Choudhary et al., 2018)": null}, "Reference": {"(Sennrich et al., 2016b)": "1114678"}}}]}
{"id": "195316935_1", "paragraph": "[BOS] To our knowledge, the only work that specifically uses the MTNT dataset attempts to improve the system robustness by emulating the noise in the clean data (Vaibhav et al., 2019) .\n[BOS] They introduce two techniques for noise induction, one employing hand-crafted rules, and one based on back-translation.\n[BOS] The techniques offer a similar translation quality gains as fine-tuning on MTNT data.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 12, "token_end": 88, "char_start": 61, "char_end": 403, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Vaibhav et al., 2019)": "67856759"}, "Reference": {}}}]}
{"id": "195316935_0", "paragraph": "[BOS] There have been several attempts to increase the robustness of MT systems in recent years.\n[BOS] Cheng et al. (2018) employ an adversarial training scheme in a multi-task learning setup in order to increase the system robustness.\n[BOS] For each training example, its noisy counterpart is randomly generated.\n[BOS] The network is trained to yield such input representations such that it is not possible to train a discriminator that decides (based on the input representation) which input is the noisy one.\n[BOS] This method improves both the robustness and the translation quality on the clean data.\n[BOS] attempt to make the translation more robust towards noise from homophones.\n[BOS] This type of noise is common in languages with non-phonetic writing systems and concerns words or phrases which are pronounced in the same way, but spelled differently.\n[BOS] The authors of the paper train the word embeddings to capture the phonetic information which eventually leads not only to bigger robustness but also to improved translation quality in general.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 19, "token_end": 113, "char_start": 103, "char_end": 605, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Cheng et al. (2018)": "21715690"}, "Reference": {}}}]}
{"id": "91184042_2", "paragraph": "[BOS] In our work, we demonstrate the importance of capturing non-local contextual information in the problem of paraphrase identification.\n[BOS] This relates to prior work on probing sentence representations for their linguistic properties, such as how much syntactic information is encoded in representations (Conneau et al., 2018; Tenney et 2019; Ettinger et al., 2018) .\n[BOS] There also exists prior work that directly uses structural information in modeling (Filice et al., 2015; Liu et al., 2018) .\n[BOS] All these prior approaches were evaluated on existing datasets.\n[BOS] In contrast, we perform studies on PAWS, a new dataset that emphasizes the importance of capturing structural information in representation learning.\n[BOS] While developing new models is beyond the scope of this paper, this new dataset can facilitate research in this direction.\n\n", "discourse_tags": ["Reflection", "Narrative_cite", "Narrative_cite", "Transition", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 45, "token_end": 76, "char_start": 259, "char_end": 372, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Conneau et al., 2018;": "24461982", "Tenney et": "108300988", "Ettinger et al., 2018)": "49363457"}}}, {"token_start": 84, "token_end": 106, "char_start": 415, "char_end": 503, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Filice et al., 2015;": "17630828", "Liu et al., 2018)": "52212922"}}}]}
{"id": "91184042_1", "paragraph": "[BOS] Our work closely relates to the idea of crafting adversarial examples to break NLP systems.\n[BOS] Existing approaches mostly focused on adding labelpreserving perturbations to inputs, but with the effect of distracting systems from correct answers.\n[BOS] Example perturbation rules include adding noise to inputs (Jia and Liang, 2017; Chen et al., 2018) , word replacements (Alzantot et al., 2018; Ribeiro et al., 2018) , and syntactic transformation .\n[BOS] A notable exception is Glockner et al. (2018) : they generated both entailment and contradiction examples by replacing words with their synonyms or antonyms.\n[BOS] Our work presents two main departures.\n[BOS] We propose a novel method that generates challenging examples with balanced class labels and more word reordering variations than previous work.\n[BOS] In addition, we release to public a large set of 108k example pairs with highquality human labels.\n[BOS] We believe the new dataset will benefit future research on both adversarial example generation and improvement of model robustness.\n\n", "discourse_tags": ["Reflection", "Transition", "Narrative_cite", "Single_summ", "Reflection", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 49, "token_end": 70, "char_start": 269, "char_end": 359, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Jia and Liang, 2017;": "7228830", "Chen et al., 2018)": "44220219"}}}, {"token_start": 71, "token_end": 93, "char_start": 362, "char_end": 425, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Alzantot et al., 2018;": "5076191", "Ribeiro et al., 2018)": "21740766"}}}, {"token_start": 103, "token_end": 133, "char_start": 488, "char_end": 622, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Glockner et al. (2018)": "19204066"}, "Reference": {}}}]}
{"id": "91184042_0", "paragraph": "[BOS] Existing data creation techniques have focused on collecting paraphrases, e.g. from co-captions for images (Lin et al., 2014) , tweets with shared URLs (Lan et al., 2017) , subtitles (Creutz, 2018) , and back translation .\n[BOS] Unlike all previous work, we emphasize the collection of challenging negative examples.\n\n", "discourse_tags": ["Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 9, "token_end": 33, "char_start": 56, "char_end": 131, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lin et al., 2014)": "14113767"}}}, {"token_start": 34, "token_end": 47, "char_start": 134, "char_end": 176, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lan et al., 2017)": "5821457"}}}, {"token_start": 48, "token_end": 57, "char_start": 179, "char_end": 203, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Creutz, 2018)": "21702613"}}}]}
{"id": "52055325_7", "paragraph": "[BOS] Recent Progress on CoQA Since we first released the dataset in August 2018, the progress of developing better models on CoQA has been rapid.\n[BOS] Instead of simply prepending the current question with its previous questions and answers, Huang et al. (2019) proposed a more sophisticated solution to effectively stack single-turn models along the conversational flow.\n[BOS] Others (e.g., Zhu et al., 2018) attempted to incorporate the most recent pretrained language representation model BERT (Devlin et al., 2018) 12 into CoQA and demonstrated superior results.\n[BOS] As of the time we finalized the paper (Jan 8, 2019), the state-of-art F1 score on the test set was 82.8.\n\n", "discourse_tags": ["Transition", "Single_summ", "Multi_summ", "Reflection"], "span_citation_mapping": [{"token_start": 31, "token_end": 72, "char_start": 153, "char_end": 373, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Huang et al. (2019)": "53113561"}, "Reference": {}}}, {"token_start": 80, "token_end": 117, "char_start": 394, "char_end": 568, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhu et al., 2018)": "54460803"}, "Reference": {"(Devlin et al., 2018)": "52967399"}}}]}
{"id": "52055325_6", "paragraph": "[BOS] Reasoning Our dataset is a testbed of various reasoning phenomena occurring in the context of a conversation (Section 4).\n[BOS] Our work parallels a growing interest in developing datasets that test specific reasoning abilities: algebraic reasoning (Clark, 2015) , logical reasoning , common sense reasoning (Ostermann et al., 2018) , and multi-fact reasoning (Welbl et al., 2018; Khashabi et al., 2018; Talmor and Berant, 2018) .\n\n", "discourse_tags": ["Reflection", "Narrative_cite"], "span_citation_mapping": [{"token_start": 41, "token_end": 48, "char_start": 235, "char_end": 268, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Clark, 2015)": null}}}, {"token_start": 52, "token_end": 65, "char_start": 291, "char_end": 338, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ostermann et al., 2018)": "44156126"}}}, {"token_start": 67, "token_end": 97, "char_start": 345, "char_end": 434, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Welbl et al., 2018;": "9192723", "Khashabi et al., 2018;": "5112038", "Talmor and Berant, 2018)": "3986974"}}}]}
{"id": "52055325_5", "paragraph": "[BOS] Concurrently, Saeidi et al. (2018) created a conversational QA dataset for regulatory text such as tax and visa regulations.\n[BOS] Their answers are limited to yes or no along with a positive characteristic of permitting to ask clarification questions when a given question cannot be answered.\n[BOS] Elgohary et al. (2018) proposed a sequential question answering dataset collected from Quiz Bowl tournaments, where a sequence contains multiple related questions.\n[BOS] These questions are related to the same concept while not focusing on the dialogue aspects (e.g., coreference).\n[BOS] Zhou et al. (2018) is another dialogue dataset based on a single movie-related Wikipedia article, in which two workers are asked to chat about the content.\n[BOS] Their dataset is more like chit-chat style conversations whereas our dataset focuses on multi-turn question answering.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 4, "token_end": 58, "char_start": 20, "char_end": 299, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Saeidi et al. (2018)": "52165754"}, "Reference": {}}}, {"token_start": 59, "token_end": 118, "char_start": 306, "char_end": 587, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Elgohary et al. (2018)": "52228931"}, "Reference": {}}}, {"token_start": 119, "token_end": 175, "char_start": 594, "char_end": 874, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhou et al. (2018)": "52307098"}, "Reference": {}}}]}
{"id": "52055325_4", "paragraph": "[BOS] In parallel to our work, Choi et al. (2018) also created a dataset of conversations in the form of questions and answers on text passages.\n[BOS] In our interface, we show a passage to both the questioner and the answerer, whereas their interface only shows a title to the questioner and the full passage to the answerer.\n[BOS] Because their setup encourages the answerer to reveal more information for the following questions, their average answer length is 15.1 words (our average is 2.7).\n[BOS] While the human performance on our test set is 88.8 F1, theirs is 74.6 F1.\n[BOS] Moreover, although CoQA's answers can be freeform text, their answers are restricted only to extractive text spans.\n[BOS] Our dataset contains passages from seven diverse domains, whereas their dataset is built only from Wikipedia articles about people.\n\n", "discourse_tags": ["Single_summ", "Reflection", "Reflection", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 8, "token_end": 32, "char_start": 31, "char_end": 144, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Choi et al. (2018)": "52057510"}, "Reference": {}}}]}
{"id": "52055325_3", "paragraph": "[BOS] Conversational Modeling Our focus is on questions that appear in a conversation.\n[BOS] Iyyer et al. (2017) and Talmor and Berant (2018) break down a complex question into a series of simple questions mimicking conversational QA.\n[BOS] Our work is closest to Das et al. (2017) and Saha et al. (2018) , who perform conversational QA on images and a knowledge graph, respectively, with the latter focusing on questions obtained by paraphrasing templates.\n\n", "discourse_tags": ["Reflection", "Multi_summ", "Multi_summ"], "span_citation_mapping": [{"token_start": 17, "token_end": 50, "char_start": 93, "char_end": 234, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Iyyer et al. (2017)": "2623009", "Talmor and Berant (2018)": "3986974"}, "Reference": {}}}, {"token_start": 56, "token_end": 100, "char_start": 264, "char_end": 457, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Das et al. (2017)": "47159154"}, "Reference": {}}}]}
{"id": "52055325_2", "paragraph": "[BOS] Naturalness There are various ways to curate questions: removing words from a declarative sentence to create a fill-in-the-blank question (Hermann et al., 2015) , using a hand-written grammar to create artificial questions Welbl et al., 2018) , paraphrasing artificial questions to natural questions (Saha et al., 2018; Talmor and Berant, 2018) , or, in our case, letting humans ask natural questions (Rajpurkar et al., 2016; Nguyen et al., 2016) .\n[BOS] While the former enable collecting large and cheap datasets, the latter enable collecting natural questions.\n[BOS] Recent efforts emphasize collecting questions without seeing the knowledge source in order to encourage the independence of question and documents (Joshi et al., 2017; Dunn et al., 2017; Koisk et al., 2018) .\n[BOS] Because we allow a questioner to see the passage, we incorporate measures to increase independence, although complete independence is not attainable in our setup (Section 3.1).\n[BOS] However, an advantage of our setup is that the questioner can validate the answerer on the spot resulting in high agreement data.\n\n", "discourse_tags": ["Multi_summ", "Transition", "Multi_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 13, "token_end": 39, "char_start": 62, "char_end": 166, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Hermann et al., 2015)": "6203757"}, "Reference": {}}}, {"token_start": 40, "token_end": 58, "char_start": 169, "char_end": 248, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Welbl et al., 2018)": "9192723"}, "Reference": {}}}, {"token_start": 59, "token_end": 84, "char_start": 251, "char_end": 350, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Talmor and Berant, 2018)": "3986974"}}}, {"token_start": 91, "token_end": 113, "char_start": 370, "char_end": 452, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rajpurkar et al., 2016;": "11816014", "Nguyen et al., 2016)": "1289517"}}}, {"token_start": 135, "token_end": 177, "char_start": 591, "char_end": 782, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Joshi et al., 2017;": "26501419", "Dunn et al., 2017;": "11606382"}}}]}
{"id": "52055325_1", "paragraph": "[BOS] Knowledge source We answer questions about text passages-our knowledge source.\n[BOS] Another common knowledge source is machine-friendly databases, which organize world facts in the form of a table or a graph (Berant et al., 2013; Pasupat and Liang, 2015; Bordes et al., 2015; Saha et al., 2018; Talmor and Berant, 2018) .\n[BOS] However, understanding their structure requires expertise, making it challenging to crowd-source large QA datasets without relying on templates.\n[BOS] Like passages, other human-friendly sources are images and videos (Antol et al., 2015; Das et al., 2017; Hori et al., 2018) .\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 21, "token_end": 80, "char_start": 126, "char_end": 326, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Pasupat and Liang, 2015;": "9027681", "Talmor and Berant, 2018)": "3986974"}}}, {"token_start": 110, "token_end": 142, "char_start": 507, "char_end": 609, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Antol et al., 2015;": "58727669", "Das et al., 2017;": "47159154", "Hori et al., 2018)": "49397377"}}}]}
{"id": "52055325_0", "paragraph": "[BOS] We organize CoQA's relation to existing work under the following criteria.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "166228482_3", "paragraph": "[BOS] To give the readers a better landscape of the current practice, we gather all 44 papers that have been accepted by the research track of Conference of Machine Translation (WMT) through 2017 and 2018.\n[BOS] We count different configurations used in a single paper as separate data points.\n[BOS] Hence, after removing 8 papers for which BPE is irrelevant, we still manage to obtain 42 data points, shown in Figure 1 .\n[BOS] It first comes to our attention that 30k-40k is the most popular range for the number of BPE merge operations.\n[BOS] This is mostly driven by the popularity of two configurations: 30k and 32k.\n[BOS] 80k-100k is also pretty popular, which is largely due to configurations 89.5k and 90k.\n[BOS] Upon closer examination, we realized that most papers that used 90k were following the configuration in Sennrich et al. (2017) , the winning NMT system in the WMT 2017 news translation shared task, but this setup somehow became less popular in 2018.\n[BOS] On the other hand, although we are unable to confirm a clear trend-setter, 30k-50k always seems to be a common choice.\n[BOS] Moreover, although smaller BPE size got more popular among configurations in 2018, none of the work published in WMT has ever explored BPE size lower than 6k.\n[BOS] All of the above observations support our initial claim that we as a community have not yet systematically investigated the entire range of BPE merge operations used in our experiments.\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection", "Reflection", "Reflection", "Reflection", "Reflection", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 174, "token_end": 183, "char_start": 824, "char_end": 846, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Sennrich et al. (2017)": "621915"}}}]}
{"id": "166228482_2", "paragraph": "[BOS] While some work has conducted experiments with different BPE settings, they are generally very limited in the range of configurations explored.\n[BOS] For example, Sennrich et al. (2016) , the original paper that proposed the BPE method, compared the system performance when using 60k separate BPE and 90k joint BPE.\n[BOS] They found 90k to work better and used that for their subsequent winning WMT 2017 new translation shared task submission (Sennrich et al., 2017) .\n[BOS] Wu et al. (2016) , on the other hand, found 8k-32k merge operations achieving optimal BLEU score performance for the wordpiece method.\n[BOS] Denkowski and Neubig (2017) explored several hyperparameter settings, including number of BPE merge operations, to establish strong baseline for NMT on LSTM-based architectures.\n[BOS] While Denkowski and Neubig (2017) showed that BPE models are clearly better than word-level models, their experiments on 16k and 32k BPE configuration did not show much difference.\n[BOS] They therefore recommended \"32K as a generally effective vocabulary size and 16K as a contrastive condition when building systems on less than 1 million parallel sentences\".\n[BOS] However, while studying deep character-based LSTM-based translation models, Cherry et al. (2018) also ran experiments for BPE configurations between 0-32k, and found that the system performance deteriorates with the increasing number of BPE merge operations.\n[BOS] Recently, Renduchintala et al. (2018) also showed that it is important to tune the number of BPE merge operations and found no typical optimal BPE configuration for their LSTM-based architecture while sweeping over several language pairs in the low-resource setting.\n[BOS] It should be noticed that the results from the above studies actually contradict with each other, and there is still no clear consensus as to what is the best practice for BPE application.\n[BOS] Moreover, all the work surveyed above was done with LSTM-based architectures.\n[BOS] To this day, we are not aware of any work that explored the interaction of BPE with the Transformer architecture.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Transition", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 30, "token_end": 68, "char_start": 169, "char_end": 321, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Sennrich et al. (2016)": "1114678"}, "Reference": {}}}, {"token_start": 69, "token_end": 101, "char_start": 328, "char_end": 472, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Sennrich et al., 2017)": "621915"}, "Reference": {}}}, {"token_start": 103, "token_end": 136, "char_start": 481, "char_end": 615, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wu et al. (2016)": "3603249"}, "Reference": {}}}, {"token_start": 137, "token_end": 173, "char_start": 622, "char_end": 799, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Denkowski and Neubig (2017)": "8140780"}, "Reference": {}}}, {"token_start": 175, "token_end": 245, "char_start": 812, "char_end": 1164, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Denkowski and Neubig (2017)": "8140780"}, "Reference": {}}}, {"token_start": 263, "token_end": 302, "char_start": 1249, "char_end": 1431, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Cherry et al. (2018)": "52119281"}, "Reference": {}}}, {"token_start": 305, "token_end": 359, "char_start": 1448, "char_end": 1704, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Renduchintala et al. (2018)": "52176700"}, "Reference": {}}}]}
{"id": "166228482_1", "paragraph": "[BOS] To the best of our knowledge, no prior work systematically reports findings for a wide range of systems that cover different architectures and both directions of translation for multiple language pairs.\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "166228482_0", "paragraph": "[BOS] Currently, the most common subword methods are BPE (Sennrich et al., 2016) , wordpiece (Wu et al., 2016) and subword regularization (Kudo, 2018) .\n[BOS] Subword regularization introduces Bayesian sampling method to incorporate more segmentation variety into the training corpus, thus improving the systems' ability to handle segmentation ambiguity.\n[BOS] Yet, the effect of such method is not very thoroughly tested.\n[BOS] In this work we will focus on the BPE/wordpiece method.\n[BOS] Because the two methods are very similar, throughout the rest of the paper, we will refer to the BPE/wordpiece method as BPE method unless otherwise specified.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 6, "token_end": 44, "char_start": 26, "char_end": 150, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sennrich et al., 2016)": "1114678", "(Wu et al., 2016)": "3603249", "(Kudo, 2018)": "13753208"}}}]}
{"id": "195345388_4", "paragraph": "[BOS] In order to derive a representation of the input text that better resembles the context of the input for a specific task, several approaches have been presented.\n[BOS] Akbik et al. (2018) , for example, pretrain a character-level Bi-LSTM to predict the next character for a given text corpus.\n[BOS] The pre-trained model is able to derive contextualized word embeddings by additionally utilizing the input sequence for a specific task.\n[BOS] This allows it to encode the previous as well as the following words of the given input sequence into the word itself.\n[BOS] In comparison to that, the pre-trained BERT-LM utilizes stacked attention layers (Vaswani et al., 2017) .\n[BOS] By feeding a sequence into it and extracting the output of the last sublayer for each token, the idea is to implicitly use the attention mechanism to derive a better representation for every token.\n[BOS] As is the case for the LM from Akbik et al. (2018) , the BERT embeddings are contextualized by the whole input sequence of the specific task.\n[BOS] This paper will compare the two contextualized approaches described above with the pre-defined GloVe (Pennington et al., 2014) embeddings in the light of their usefulness for AM.\n[BOS] The goal is to encode the features necessary to detect arguments by utilizing the context of a sentence.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Narrative_cite", "Transition", "Narrative_cite", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 32, "token_end": 115, "char_start": 174, "char_end": 566, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Akbik et al. (2018)": "52010710"}, "Reference": {}}}, {"token_start": 129, "token_end": 142, "char_start": 629, "char_end": 676, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vaswani et al., 2017)": "13756489"}}}, {"token_start": 188, "token_end": 199, "char_start": 912, "char_end": 939, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Akbik et al. (2018)": "52010710"}}}, {"token_start": 230, "token_end": 244, "char_start": 1120, "char_end": 1163, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Pennington et al., 2014)": "1957433"}}}]}
{"id": "195345388_3", "paragraph": "[BOS] In contrast to that, the approach presented in this paper uses attention as a separate layer that encodes all sequences before they are fed into a Bi-LSTM.\n[BOS] This might enable the recurrent part of the network to learn from better representations that are specific to the task it is trained on.\n[BOS] The aim is further to evaluate the possible applications of attention layers for the task of sequence segmentation and token classification.\n[BOS] A recurrent architecture (Ajjour et al., 2017 ) is compared to multiple modified versions that utilize the attention mechanism.\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection", "Single_summ"], "span_citation_mapping": [{"token_start": 84, "token_end": 109, "char_start": 458, "char_end": 585, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Ajjour et al., 2017": "34131024"}, "Reference": {}}}]}
{"id": "195345388_2", "paragraph": "[BOS] To the best of the authors' knowledge, the attention mechanism has not been widely utilized so far for the task of argumentative unit segmentation.\n[BOS] Stab et al. (2018) integrated the attention mechanism directly into their Bi-LSTM by calculating it at each time step t to evaluate the importance of the current hidden state h t .\n[BOS] To do that, they employed additive attention.\n[BOS] A similar approach has been applied by Morio and Fujita (2018) for a three-label classification task (claim, premise or non-argumentative).\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 31, "token_end": 80, "char_start": 160, "char_end": 392, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Stab et al. (2018)": "53083232"}, "Reference": {}}}, {"token_start": 81, "token_end": 114, "char_start": 399, "char_end": 538, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Morio and Fujita (2018)": "52155319"}, "Reference": {}}}]}
{"id": "195345388_1", "paragraph": "[BOS] While the first two of them are fully connected and work on word embeddings and task-specific features respectively, the intention for the third is to take the output of the first two as input and learn to correct their errors.\n[BOS] Even though the third Bi-LSTM did not improve on the F1-score metric, it did succeed in resolving some of the wrong consecutive token predictions, without worsening the final results.\n\n", "discourse_tags": ["Transition", "Transition"], "span_citation_mapping": []}
{"id": "195345388_0", "paragraph": "[BOS] Attention mechanisms have long been utilized in deep neural networks.\n[BOS] Some of its roots are in the salient region detection for the processing of images (Itti et al., 1998) , which takes inspiration from human perception.\n[BOS] The main idea is to focus the attention of the underlying network on pointsof-interest in the input that are often surrounded by irrelevant parts (Mnih et al., 2014) .\n[BOS] This allows the model to put more weight on the important chunks.\n[BOS] While earlier salient detectors were task-specific, newer approaches (e.g. Mnih et al., 2014) can be adapted to different tasks, like image description generation (Xu et al., 2015) , and allow for the parameters of the attention to be tuned during the training.\n[BOS] These additional tasks include sequence processing and the application of such networks to different areas of Natural Language Processing (NLP).\n[BOS] One of the first use-cases for attention mechanisms in the field of NLP was machine translation.\n[BOS] Bahdanau et al. (2014) utilized the attention to improve their NMT model.\n[BOS] A few years later, Vaswani et al. (2017) achieved new State-of-the-Art (SotA) results by presenting an encoder-decoder architecture that is based on the attention mechanism, only adding a position-wise feed-forward network and normalizations in between.\n[BOS] Devlin et al. (2018) picked up on the encoder part of this architecture to pre-train a bidirectional LM.\n[BOS] After fine-tuning, they achieved, again, a new SotA performance on different downstream NLP tasks like Part-of-speech tagging and Questions-Answering.\n[BOS] A possible way of posing the unit segmentation as NLP task is a token-based sequence labeling (Stab, 2017) .\n[BOS] While Tobias et al. (2018) used rather simple, non-recurrent classifiers to approach this problem, others mostly applied recurrent networks to the task of unit boundary prediction.\n[BOS] For example, Eger et al. (2017) reported different Long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) architectures.\n[BOS] Further, Ajjour et al. (2017) proposed a setup with three bidirectional LSTMs (Bi-LSTMs) (Schuster and Paliwal, 1997) in total as their best solution.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite", "Transition", "Narrative_cite", "Transition", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Narrative_cite", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 21, "token_end": 38, "char_start": 111, "char_end": 184, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Itti et al., 1998)": "3108956"}}}, {"token_start": 52, "token_end": 83, "char_start": 260, "char_end": 405, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mnih et al., 2014)": "17195923"}}}, {"token_start": 108, "token_end": 123, "char_start": 538, "char_end": 579, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Mnih et al., 2014)": "17195923"}}}, {"token_start": 131, "token_end": 142, "char_start": 620, "char_end": 666, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Xu et al., 2015)": "1055111"}}}, {"token_start": 205, "token_end": 224, "char_start": 1008, "char_end": 1081, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Bahdanau et al. (2014)": "61556494"}, "Reference": {}}}, {"token_start": 230, "token_end": 284, "char_start": 1107, "char_end": 1341, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Vaswani et al. (2017)": "13756489"}, "Reference": {}}}, {"token_start": 285, "token_end": 344, "char_start": 1348, "char_end": 1609, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Devlin et al. (2018)": "52967399"}, "Reference": {}}}, {"token_start": 360, "token_end": 371, "char_start": 1680, "char_end": 1722, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Stab, 2017)": null}}}, {"token_start": 374, "token_end": 408, "char_start": 1737, "char_end": 1911, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 412, "token_end": 445, "char_start": 1931, "char_end": 2048, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Eger et al. (2017)": "3221856"}, "Reference": {}}}, {"token_start": 448, "token_end": 490, "char_start": 2064, "char_end": 2205, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ajjour et al. (2017)": "34131024"}, "Reference": {"(Schuster and Paliwal, 1997)": "18375389"}}}]}
{"id": "207999127_5", "paragraph": "[BOS] Pretraining embeddings on large unlabelled corpus has been shown to improve many downstream tasks (Peters et al., 2018; Howard and Ruder, 2018; Alec et al., 2018) .\n[BOS] The recently released BERT (Devlin et al., 2018) greatly increased the F1 scores on the SQuAD 2.0 leaderboard.\n[BOS] BERT consists of stacked Transformers (Vaswani et al., 2017) , that are pre-trained on vast amounts of unlabeled data with a masked language model.\n[BOS] The masked language model helps finetuning on downstream tasks, such as SQuAD 2.0.\n[BOS] BERT models contains a special CLS token which is helpful for the SQuAD 2.0 task.\n[BOS] This CLS token is trained to predict if a pair of sentences follow each other during the pre-training, which helps encode entailment information between the sentence pair.\n[BOS] Due to a strong masked language model to help predict answers and a strong CLS token to encode entailment, BERT models are the current state-of-the art for SQuAD 2.0.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 16, "token_end": 41, "char_start": 87, "char_end": 168, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Peters et al., 2018;": "3626819", "Howard and Ruder, 2018;": "195346829", "Alec et al., 2018)": null}}}, {"token_start": 43, "token_end": 223, "char_start": 177, "char_end": 969, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Devlin et al., 2018)": "52967399"}, "Reference": {"(Vaswani et al., 2017)": "13756489"}}}]}
{"id": "207999127_4", "paragraph": "[BOS] Reader.\n[BOS] The answer sentence that is proposed by the reader and the question are passed to three combinations of differently configured verifiers for fine-grained local entailment recognition.\n[BOS] (Liu et al., 2018a) just added one layer as the unanswerable binary classifier to their SAN reader.\n[BOS] (Sun et al., 2018) proposed the U-net with a universal node that encodes the fused information from both the question and passage.\n[BOS] The summary Unode, question vector and two context vectors are passed to predict whether the question is answerable.\n[BOS] Plausible answers were used for no-answer pointer prediction, while in our approach, plausible answers were used to augment context vector for object extraction that later help the no-answer prediction.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 39, "token_end": 65, "char_start": 210, "char_end": 309, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Liu et al., 2018a)": null}, "Reference": {}}}, {"token_start": 66, "token_end": 129, "char_start": 316, "char_end": 636, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Sun et al., 2018)": null}, "Reference": {}}}]}
{"id": "207999127_3", "paragraph": "[BOS] Many MRC models have been adapted to work on SQuAD 2.0 recently (Hu et al., 2019; Liu et al., 2018a; Sun et al., 2018; Devlin et al., 2018) .\n[BOS] (Hu et al., 2019) added a separately trained answer verifier for no-answer detection with their Mnemonic\n\n", "discourse_tags": ["Narrative_cite", "Single_summ"], "span_citation_mapping": [{"token_start": 11, "token_end": 49, "char_start": 51, "char_end": 145, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hu et al., 2019;": null, "Liu et al., 2018a;": null, "Sun et al., 2018;": null, "Devlin et al., 2018)": "52967399"}}}, {"token_start": 51, "token_end": 78, "char_start": 154, "char_end": 258, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Hu et al., 2019)": null}, "Reference": {}}}]}
{"id": "207999127_2", "paragraph": "[BOS] Another similar line of work investigated pretraining relationship embeddings across word pairs on large unlabelled corpus (Jameel et al., 2018; Joshi et al., 2018) .\n[BOS] These pre-trained pairwise relational embeddings were added to the attention layers of BiDAF, where higher level abstract reasoning occurs.\n[BOS] The paper showed an impressive gain of 2.7% on the SQuAD 2.0 development set on top of their version of BiDAF.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 8, "token_end": 38, "char_start": 48, "char_end": 170, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Jameel et al., 2018;": "51879110", "Joshi et al., 2018)": "53046959"}}}]}
{"id": "207999127_1", "paragraph": "[BOS] MAC (Memory, Attention and Composition) networks (Hudson and Manning, 2018) are different models that have also been shown to learn relations from the CLEVR dataset.\n[BOS] MAC networks operate with read and write cells.\n[BOS] Each cell would compute a relation score between a knowledge base and question and write it into memory.\n[BOS] Multiple read and write cells are strung together sequentially in order to model long chains of multihop reasoning.\n[BOS] Although MAC networks do not explicitly reason between pairwise objects as relation networks do, MAC networks are an interesting way of generating multi-hop reasoning between objects within a context.\n[BOS] S and E are hidden states trained by plausible answers.\n[BOS] We then concatenate S and E with the contextual representation to feed into the object extractor.\n[BOS] After we obtain the extracted objects, we then feed into a Relation Network and pass it down for NA predictions.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 135, "char_start": 6, "char_end": 727, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "207999127_0", "paragraph": "[BOS] Relation Networks (RN) were first proposed by (Santoro et al., 2017) in order to help neural models to reason over the relationships between two objects.\n[BOS] Relation networks learn relationships between objects by learning a pairwise score for each object pair.\n[BOS] Relation networks have been applied to CLEVR (Johnson et al., 2017) as well as bAbI (Weston et al., 2015) .\n[BOS] In the CLEVR dataset, the object inputs to the relation network are visual objects in an image, extracted by a CNN, and in bAbI the object inputs are sentence encodings.\n[BOS] In both tasks, the relation network is then used to compute a relationship score over these objects.\n[BOS] Relation Networks were further applied to general reasoning by training the model on images (You et al., 2018) .\n\n", "discourse_tags": ["Single_summ", "Transition", "Narrative_cite", "Transition", "Transition", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 35, "char_start": 6, "char_end": 159, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Santoro et al., 2017)": "8528277"}, "Reference": {}}}, {"token_start": 59, "token_end": 69, "char_start": 316, "char_end": 344, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Johnson et al., 2017)": "15458100"}}}, {"token_start": 72, "token_end": 83, "char_start": 356, "char_end": 382, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Weston et al., 2015)": "3178759"}}}, {"token_start": 143, "token_end": 165, "char_start": 674, "char_end": 784, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(You et al., 2018)": null}, "Reference": {}}}]}
{"id": "208130414_1", "paragraph": "[BOS] Our methodology follows that of Aroyehun et al. (2018)\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 17, "char_start": 6, "char_end": 60, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "208130414_0", "paragraph": "[BOS] There are many different techniques have been introduced so far to identify complex words (Paetzold and Specia, 2016b; Yimam et al., 2018) .\n[BOS] It is obvious that feature-based approaches remain the best, but deep learning approaches have become more popular and achieved impressive results.\n[BOS] Gooding and Kochmar (2018) proposed a feature-based approach for monolingual English datasets.\n[BOS] The system used lexical features such as number of characters, number of syllables, number of synonyms, word n-gram, POS tags, dependency parse relations, number of words grammatically related to the target word, and Google ngram word frequencies.\n[BOS] It also used psycholinguistic features such as word familiarity rating, number of phonemes, imageability rating, concreteness rating, number of categories, samples, written frequencies, and age of acquisition.\n[BOS] The model achieved the state-of-the-art results for English datasets during the CWI Shared Task 2018 (Yimam et al., 2018) , but the limitation of this approach is that it is hard to port from one language to another.\n[BOS] Kajiwara and Komachi (2018) developed a system for multilingual and cross-lingual CWI.\n[BOS] The system was implemented using word frequencies features extracted from the learner corpus (Lang-8 corpus) Mizumoto et al. (2011) , Wikipedia and WikiNews.\n[BOS] The features contained the number of characters, the number of words, and the frequency of the target word.\n[BOS] The system achieved state-of-the-art results for both Spanish and German datasets.\n[BOS] Aroyehun et al. (2018) developed systems for both English and Spanish using binary classification and deep learning (CNN) approaches.\n[BOS] The feature-based approach used features such as word frequency of the target word from Wikipedia and Simple Wikipedia corpus, syntactic and lexical features, psycholinguistic features and entity features, and word embedding distance as a feature which is computed between the target word and the sentence.\n[BOS] The deep learning approach used GloVe word embeddings (Pennington et al., 2014) to represent target words and its context.\n[BOS] The deep learning approach is very simple and achieves better results than other deep learning approaches.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Single_summ", "Single_summ", "Single_summ", "Narrative_cite", "Single_summ", "Narrative_cite", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Narrative_cite", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 36, "char_start": 6, "char_end": 144, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Paetzold and Specia, 2016b;": "14776729", "Yimam et al., 2018)": null}}}, {"token_start": 64, "token_end": 176, "char_start": 307, "char_end": 871, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Gooding and Kochmar (2018)": "46940906"}, "Reference": {}}}, {"token_start": 194, "token_end": 209, "char_start": 958, "char_end": 999, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yimam et al., 2018)": null}}}, {"token_start": 230, "token_end": 254, "char_start": 1101, "char_end": 1187, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kajiwara and Komachi (2018)": "46937503"}, "Reference": {}}}, {"token_start": 266, "token_end": 283, "char_start": 1272, "char_end": 1325, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Mizumoto et al. (2011)": "5844380"}}}, {"token_start": 332, "token_end": 414, "char_start": 1561, "char_end": 2007, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Aroyehun et al. (2018)": "46940930"}, "Reference": {}}}, {"token_start": 420, "token_end": 433, "char_start": 2046, "char_end": 2093, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Pennington et al., 2014)": "1957433"}}}]}
{"id": "190000717_0", "paragraph": "[BOS] Cotterell and Kreutzer (2018) frame backtranslation as a variational process, with the space of source sentences as the latent space.\n[BOS] Their approach argues that the distribution of the synthetic data generator and the true translation probability should match.\n[BOS] Thus it is invaluable to clarify and investigate the sampling distributions that current state-of-the-art data generation techniques utilize.\n[BOS] A simple property is that a target sentence must be allowed to be aligned to multiple source sentences during the training phase.\n[BOS] Several efforts (Hoang et al., 2018; Edunov et al., 2018; Imamura et al., 2018) confirm that this is in fact beneficial.\n[BOS] Here, we unify these findings by re-writing the optimization criterion of NMT models to depend on a data generator, which we define for beam search, sampling and N -best list sampling approaches.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Transition", "Transition", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 52, "char_start": 6, "char_end": 272, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 103, "token_end": 132, "char_start": 563, "char_end": 655, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hoang et al., 2018;": null, "Edunov et al., 2018;": "52113461", "Imamura et al., 2018)": "51878798"}}}]}
{"id": "102353862_2", "paragraph": "[BOS] Our proposed method is inspired by the work of Lei et al. (2016) who seek to identify rationales from textual input to support sentiment classification and question retrieval.\n[BOS] Distinct from this previous work, we focus on generating generic document summaries.\n[BOS] We present a novel supervised framework encouraging the selection of consecutive sequences of words to form an extractive summary.\n[BOS] Further, we leverage reinforcement learning to explore the space of possible extractive summaries and promote those that are fluent, adequate, and competent in question answering.\n[BOS] We seek to test the hypothesis that successful summaries can serve as document surrogates to answer important questions, and moreover, ground-truth questionanswer pairs can be derived from human abstracts.\n[BOS] In the following section we describe our proposed approach in details.\n\n", "discourse_tags": ["Single_summ", "Reflection", "Reflection", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 11, "token_end": 35, "char_start": 53, "char_end": 181, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Lei et al. (2016)": "7205805"}, "Reference": {}}}]}
{"id": "102353862_1", "paragraph": "[BOS] Interestingly, studies reveal that summaries generated by recent neural abstractive systems are, in fact, quite \"extractive.\"\n[BOS] Abstractive systems often adopt the encoder-decoder architecture with an attention mechanism (Rush et al., 2015; Nallapati et al., 2016; Paulus et al., 2017; Guo et al., 2018; Gehrmann et al., 2018; Lebanoff et al., 2018; Celikyilmaz et al., 2018) .\n[BOS] The encoder condenses a source sequence to a fixed-length vector and the decoder takes the vector as input and generates a summary by predicting one word at a time.\n[BOS] See, Liu, and Manning (2017) suggest that about 35% of the summary sentences occur in the source documents, and 90% of summary n-grams appear in the source.\n[BOS] Moreover, the summaries may contain inaccurate factual details and introduce new meanings not present in the original text (Cao et al., 2018; .\n[BOS] It thus raises concerns as to whether such systems can be used in realworld scenarios to summarize materials such as legal documents.\n[BOS] In this work, we choose to focus on extractive summarization where selected word sequences can be highlighted on the source text to avoid change of meaning.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Single_summ", "Narrative_cite", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 33, "token_end": 103, "char_start": 174, "char_end": 385, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rush et al., 2015;": "15761498", "Nallapati et al., 2016;": "8928715", "Paulus et al., 2017;": "21850704", "Guo et al., 2018;": "44105751", "Gehrmann et al., 2018;": "52144157", "Lebanoff et al., 2018;": "52053741", "Celikyilmaz et al., 2018)": "4406182"}}}, {"token_start": 139, "token_end": 177, "char_start": 565, "char_end": 721, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"See, Liu, and Manning (2017)": null}, "Reference": {}}}, {"token_start": 198, "token_end": 205, "char_start": 851, "char_end": 868, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "102353862_0", "paragraph": "[BOS] Extractive summarization has seen growing popularity in the past decades (Nenkova and McKeown, 2011) .\n[BOS] The methods focus on selecting representative sentences from the document(s) and optionally deleting unimportant sentence constituents to form a summary (Knight and Marcu, 2002; Radev et al., 2004; Zajic et al., 2007; Martins and Smith, 2009; Gillick and Favre, 2009; Lin and Bilmes, 2010; Wang et al., 2013; Li et al., 2013 Li et al., , 2014 Hong et al., 2014; Yogatama et al., 2015) .\n[BOS] A majority of the methods are unsupervised.\n[BOS] They estimate sentence importance based on the sentence's length and position in the document, whether the sentence contains topical content and its relationship with other sentences.\n[BOS] The summarization objective is to select a handful of sentences to maximize the coverage of important content while minimizing summary redundancy.\n[BOS] Although unsupervised methods are promising, they cannot benefit from the large-scale training data harvested from the Web (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018) .\n[BOS] Neural extractive summarization has focused primarily on extracting sentences (Nallapati et al., 2017; Cao et al., 2017; Isonuma et al., 2017; Tarnpradab et al., 2017; Zhou et al., 2018; Kedzie et al., 2018) .\n[BOS] These studies exploit parallel training data consisting of source articles and story highlights (i.e., human abstracts) to create ground-truth labels for sentences.\n[BOS] A neural extractive summarizer learns to predict a binary label for each source sentence indicating if it is to be included in the summary.\n[BOS] These studies build distributed sentence representations using neural networks (Cheng and Lapata, 2016; Yasunaga et al., 2017) and use reinforcement learning to optimize the evaluation metric (Narayan et al., 2018b) and improve summary coherence (Wu and Hu, 2018) .\n[BOS] However, sentence extraction can be coarse and in many cases, only a part of the sentence is worthy to be added to the summary.\n[BOS] In this study, we perform finer-grained extractive summarization by allowing the system to select consecutive sequences of words rather than sentences to form a summary.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Transition", "Transition", "Transition", "Narrative_cite", "Narrative_cite", "Transition", "Transition", "Narrative_cite", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 14, "token_end": 25, "char_start": 79, "char_end": 106, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Nenkova and McKeown, 2011)": null}}}, {"token_start": 31, "token_end": 136, "char_start": 136, "char_end": 499, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Knight and Marcu, 2002;": "7793213", "Radev et al., 2004;": null, "Zajic et al., 2007;": null, "Martins and Smith, 2009;": "16148301", "Gillick and Favre, 2009;": "167874", "Lin and Bilmes, 2010;": "1803710", "Wang et al., 2013;": "1260503", "Li et al., 2013": "8928513", "Li et al., , 2014": "10112929", "Hong et al., 2014;": null, "Yogatama et al., 2015)": "12194143"}}}, {"token_start": 203, "token_end": 246, "char_start": 901, "char_end": 1083, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sandhaus, 2008;": null, "Hermann et al., 2015;": "6203757", "Grusky et al., 2018)": "13752552"}}}, {"token_start": 248, "token_end": 311, "char_start": 1092, "char_end": 1299, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Nallapati et al., 2017;": "6405271", "Cao et al., 2017;": "14651945", "Isonuma et al., 2017;": "35618061", "Tarnpradab et al., 2017;": "39210805", "Zhou et al., 2018;": "49656757", "Kedzie et al., 2018)": "53083054"}}}, {"token_start": 376, "token_end": 399, "char_start": 1645, "char_end": 1751, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cheng and Lapata, 2016;": "1499080", "Yasunaga et al., 2017)": "6532096"}}}, {"token_start": 401, "token_end": 418, "char_start": 1760, "char_end": 1840, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Narayan et al., 2018b)": "3510042"}}}, {"token_start": 419, "token_end": 429, "char_start": 1845, "char_end": 1888, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wu and Hu, 2018)": "4999752"}}}]}
{"id": "196188410_2", "paragraph": "[BOS] Low-Resource and Zero-Shot NMT: Many researchers have explored low-resource NMT using transfer learning Neubig and Hu, 2018) and data augmenting (Sennrich et al., 2016a; Zhang and Zong, 2016) approaches.\n[BOS] For zero-shot translation, and utilize a pivot-based method, which bridges the gap between sourceto-pivot and pivot-to-target two steps.\n[BOS] Multilingual translation is another direction to deal with both low-resource and zero-shot translation.\n[BOS] Gu et al. (2018) enable sharing of lexical and sentence representation across multiple languages, especially for extremely low-resource Multi-NMT.\n[BOS] Firat et al. (2016) , Lakew et al. (2017), and Johnson et al. (2017) propose to make use of multilinguality in Multi-NMT to address the zero-shot problem.\n[BOS] In this work, we propose a method for Multi-NMT to boost the accuracy of the multilingual translation, which better fits on both lowresource scenario and zero-shot scenario.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Single_summ", "Multi_summ", "Reflection"], "span_citation_mapping": [{"token_start": 22, "token_end": 31, "char_start": 92, "char_end": 130, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Neubig and Hu, 2018)": "51976920"}}}, {"token_start": 32, "token_end": 53, "char_start": 135, "char_end": 197, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sennrich et al., 2016a;": "15600925", "Zhang and Zong, 2016)": "17667087"}}}, {"token_start": 112, "token_end": 141, "char_start": 469, "char_end": 615, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Gu et al. (2018)": "3295641"}, "Reference": {}}}, {"token_start": 142, "token_end": 189, "char_start": 622, "char_end": 776, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Firat et al. (2016)": "6359641", "Lakew et al. (2017), and": null, "Johnson et al. (2017)": "6053988"}, "Reference": {}}}]}
{"id": "196188410_1", "paragraph": "[BOS] Model Compactness and Multi-NMT: To reduce the model size in NMT, weight pruning, knowledge distillation, quantization, and weight sharing (Kim and Rush, 2016; See et al., 2016; He et al., 2018; Zhou et al., 2018) have been ex-plored.\n[BOS] Due to the benefit of compactness, multilingual translation has been extensively studied in Dong et al. (2015) , and Johnson et al. (2017) .\n[BOS] Owing to excellent translation performance and ease of use, many researchers (Blackwood et al., 2018; Lakew et al., 2018) have conducted translation based on the framework of Johnson et al. (2017) and Ha et al. (2016) .\n[BOS] Zhou et al. (2019) propose to perform decoding in two translation directions synchronously, which can be applied on different target languages and is a new research area for Multi-NMT.\n[BOS] In our method, we present a compact method for Multi-NMT, which can not only compress the model but also yield superior performance.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Narrative_cite", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 16, "token_end": 61, "char_start": 67, "char_end": 219, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kim and Rush, 2016;": null, "See et al., 2016;": "2973141", "He et al., 2018;": "54088698", "Zhou et al., 2018)": "53144225"}}}, {"token_start": 76, "token_end": 100, "char_start": 282, "char_end": 385, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Dong et al. (2015)": "3666937", "Johnson et al. (2017)": "6053988"}}}, {"token_start": 113, "token_end": 154, "char_start": 459, "char_end": 611, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Blackwood et al., 2018;": "47005349", "Lakew et al., 2018)": null, "Johnson et al. (2017)": "6053988", "Ha et al. (2016)": "5234044"}}}, {"token_start": 156, "token_end": 195, "char_start": 620, "char_end": 804, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhou et al. (2019)": "116880070"}, "Reference": {}}}]}
{"id": "196188410_0", "paragraph": "[BOS] Our work is related to two lines of research, and we describe each of them as follows:\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "202775604_2", "paragraph": "[BOS] Modeling of speakers in the conversation model has been studied (Li et al., 2016b; Xing and Fernndez, 2018) .\n[BOS] They incorporate the speakers to generate the responses, but Li et al. (2016b) only considers a short context of the conversation.\n[BOS] Olabiyi et al. (2018) overcomes this, but the user information is still in the utterance level.\n[BOS] This approach tends to generate the same response for the same speaker even when the given utterances are different.\n[BOS] This is because it gives too much importance to the speaker rather than the content of the previous utterances.\n[BOS] VHUCM differs from these models in that it uses a global stochastic variable which is conditioned on the speakers and affects the context.\n\n", "discourse_tags": ["Multi_summ", "Multi_summ", "Single_summ", "Single_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 2, "token_end": 41, "char_start": 6, "char_end": 178, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Xing and Fern\u00e1ndez, 2018)": "52896891"}, "Reference": {}}}, {"token_start": 42, "token_end": 59, "char_start": 183, "char_end": 252, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 60, "token_end": 126, "char_start": 259, "char_end": 595, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Olabiyi et al. (2018)": "58675945"}, "Reference": {}}}]}
{"id": "202775604_1", "paragraph": "[BOS] Recently, latent variable models based on Conditional Variational Auto-Encoder (CVAE) (Kingma et al., 2014) or Generative Adversarial Network (GAN) (Goodfellow et al., 2014) show the better performance for generating response (Serban et al., 2017; Xu et al., 2017; Li et al., Users Dyads Conv's Utterances 27,152 107,611 770,739 6,109,469 Park et al., 2018) .\n[BOS] We adopt CVAE to VHUCM and compare the performance with GAN based model (Gu et al., 2019) .\n\n", "discourse_tags": ["Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 9, "token_end": 27, "char_start": 48, "char_end": 113, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kingma et al., 2014)": null}}}, {"token_start": 28, "token_end": 45, "char_start": 117, "char_end": 179, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Goodfellow et al., 2014)": "1033682"}}}, {"token_start": 50, "token_end": 72, "char_start": 212, "char_end": 280, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Serban et al., 2017;": "14857825", "Xu et al., 2017;": "11030403"}}}, {"token_start": 98, "token_end": 105, "char_start": 345, "char_end": 363, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Park et al., 2018)": "4829361"}}}, {"token_start": 120, "token_end": 132, "char_start": 428, "char_end": 461, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Gu et al., 2019)": "44129557"}}}]}
{"id": "202775604_0", "paragraph": "[BOS] Dialogue response generation has been extensively studied (Ratnaparkhi, 2002; Ritter et al., 2011) , and recently, neural network models, especially sequence-to-sequence models have been widely used (Sordoni et al., 2015; Serban et al., 2017; Park et al., 2018; Du et al., 2018; Gu et al., 2019) .\n[BOS] One limitation of basic seq2seq models is that they only generate responses to the immediately preceding utterances, whereas people usually respond to the entire dialogue consisting of multiple previous utterances.\n[BOS] To overcome this limitation, Hierarchical recurrent encoder-decoder (HRED) (Sordoni et al., 2015) builds one more RNN that models the dependency over the utterances in the conversation.\n[BOS] VHUCM also constructs the hierarchical RNN structure to understand the previous utterances.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 2, "token_end": 25, "char_start": 6, "char_end": 104, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ratnaparkhi, 2002;": "42820567", "Ritter et al., 2011)": "780171"}}}, {"token_start": 29, "token_end": 84, "char_start": 121, "char_end": 301, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sordoni et al., 2015;": null, "Serban et al., 2017;": "14857825", "Park et al., 2018;": "4829361", "Du et al., 2018;": "53081212", "Gu et al., 2019)": "44129557"}}}, {"token_start": 126, "token_end": 161, "char_start": 560, "char_end": 716, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Sordoni et al., 2015)": null}, "Reference": {}}}]}
{"id": "196179370_0", "paragraph": "[BOS] Recent research in CLIR and query-based summarization uses expansive, concept-based definitions of relevance.\n[BOS] For example, given the query agriculture, documents are relevant if they describe fields, pastures, or crops, even if the word agriculture is not used, and the goal of summarization is to show that the document as a whole is relevant.\n[BOS] In contrast, in this work we aim to retrieve documents that meet a more precise notion of relevance, similar to that used for keyword spotting.\n[BOS] This goal influences our retrieval approach, which seeks to account for variation in translation but does not perform more expansive embedding-based query expansion, and the summarization approach, which presents in-context search term matches rather than a narrative summary of the document as a whole.\n\n", "discourse_tags": ["Transition", "Transition", "Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "92990867_2", "paragraph": "[BOS] tic graphical modeling approaches have been used (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Takamatsu et al., 2012) as well as deep models (Lin et al., 2016; Luo et al., 2017; Lei et al., 2018; Han et al., 2018) , though these often focus on incorporating signals from other sources as opposed to manually labeled data.\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 2, "token_end": 46, "char_start": 6, "char_end": 146, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Riedel et al., 2010;": "2386383", "Hoffmann et al., 2011;": "16483125", "Surdeanu et al., 2012;": "5869747", "Takamatsu et al., 2012)": "2463401"}}}, {"token_start": 49, "token_end": 80, "char_start": 158, "char_end": 242, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lin et al., 2016;": "397533", "Luo et al., 2017;": "27410115", "Lei et al., 2018;": "52011231", "Han et al., 2018)": "44071751"}}}]}
{"id": "92990867_1", "paragraph": "[BOS] Denoising techniques for distant supervision have been applied extensively to relation extraction.\n[BOS] Here, multi-instance learning and probabilis- Figure 4 : Examples of the noisy labels (left) and the denoised labels (right) for mentions (bold).\n[BOS] The colors correspond to type classes: general (purple), finegrained (green), and ultra-fine (yellow).\n\n", "discourse_tags": ["Transition", "Transition", "Transition"], "span_citation_mapping": []}
{"id": "92990867_0", "paragraph": "[BOS] Past work on denoising data for entity typing has used multi-instance multi-label learning Schtze, 2015, 2017; Murty et al., 2018) .\n[BOS] One view of these approaches is that they delete noisily-introduced labels, but they cannot add them, or filter bad examples.\n[BOS] Other work focuses on learning type embeddings (Yogatama et al., 2015; Ren et al., 2016a,b) ; our approach goes beyond this in treating the label set in a structured way.\n[BOS] The label set of Choi et al. (2018) is distinct in not being explicitly hierarchical, making past hierarchical approaches difficult to apply.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Narrative_cite", "Single_summ"], "span_citation_mapping": [{"token_start": 5, "token_end": 35, "char_start": 19, "char_end": 136, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Murty et al., 2018)": "46896270"}}}, {"token_start": 69, "token_end": 92, "char_start": 299, "char_end": 368, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yogatama et al., 2015;": "881437"}}}, {"token_start": 109, "token_end": 136, "char_start": 454, "char_end": 595, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Choi et al. (2018)": "49212016"}, "Reference": {}}}]}
{"id": "202670634_2", "paragraph": "[BOS] Previous work on curriculum learning for MT (Kocmi and Bojar, 2017; Zhang et al., 2018b; Wang et al., 2018) proposed methods which feed easier samples to the model first and later show more complex sentences.\n[BOS] However, their focus is on improving convergence time while providing limited success on improving translation quality.\n[BOS] In contrast with their work, we train models to better handle discourse-level phenomena.\n\n", "discourse_tags": ["Multi_summ", "Multi_summ", "Reflection"], "span_citation_mapping": [{"token_start": 5, "token_end": 70, "char_start": 23, "char_end": 340, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Kocmi and Bojar, 2017;": "26468344", "Wang et al., 2018)": "20639213"}, "Reference": {}}}]}
{"id": "202670634_1", "paragraph": "[BOS] Improvements in BLEU cannot be conclusively attributed to improved anaphora resolution and therefore additional metrics are required.\n[BOS] Several works have proposed methods of evaluation and have shown that context-aware NMT achieves improvements.\n[BOS] Mller et al. (2018) propose an automatically created challenge set where a model scores German translations of an English source sentence.\n[BOS] The source sentences contain an anaphoric third person singular pronoun and the possible translations differ only in the choice of the pronoun in German.\n[BOS] Bawden et al. (2018) is an earlier work proposing a manually created challenge set for English and French.\n[BOS] Miculicich et al. (2018) evaluate their model's effectiveness on pronoun translation by computing pronoun accuracy based on alignment of hypothesized translations with the reference.\n[BOS] Voita et al. (2018) used attention scores which show a tendency of Transformerbased context-aware models to do anaphora resolution.\n[BOS] However, Mller et al. (2018) report moderate improvements of the model on their pronoun test set.\n[BOS] In order to provide a comprehensive eval-uation of our approach, we use BLEU, the pronoun challenge set from Mller et al. (2018) , and F 1 score for the ambiguous English pronoun \"it\" based on alignment.\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 44, "token_end": 97, "char_start": 263, "char_end": 561, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"M\u00fcller et al. (2018)": "52921687"}, "Reference": {}}}, {"token_start": 98, "token_end": 122, "char_start": 568, "char_end": 674, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Bawden et al. (2018)": "5016370"}, "Reference": {}}}, {"token_start": 123, "token_end": 156, "char_start": 681, "char_end": 863, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Miculicich et al. (2018)": "52044834"}, "Reference": {}}}, {"token_start": 157, "token_end": 185, "char_start": 870, "char_end": 1001, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Voita et al. (2018)": "44062236"}, "Reference": {}}}, {"token_start": 188, "token_end": 208, "char_start": 1017, "char_end": 1105, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"M\u00fcller et al. (2018)": "52921687"}, "Reference": {}}}, {"token_start": 230, "token_end": 242, "char_start": 1194, "char_end": 1240, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "202670634_0", "paragraph": "[BOS] Several works have proposed methods and models of including contextual information (Wang et al., 2017; Jean et al., 2017; Bawden et al., 2018; Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Voita et al., 2018; Stojanovski and Fraser, 2018; Miculicich et al., 2018; Zhang et al., 2018a; .\n[BOS] In general, these models make use of extra-sentential attention conditioned on the main sentence being translated and use gates to control the flow of contextual information.\n[BOS] The model we use is based on these general concepts as well.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 10, "token_end": 90, "char_start": 56, "char_end": 298, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wang et al., 2017;": "9768369", "Jean et al., 2017;": null, "Bawden et al., 2018;": "5016370", "Tiedemann and Scherrer, 2017;": "2496355", "Maruf and Haffari, 2018;": "21686013", "Voita et al., 2018;": "44062236", "Stojanovski and Fraser, 2018;": "53222571", "Miculicich et al., 2018;": "52044834"}}}]}
{"id": "202734248_3", "paragraph": "[BOS] Our model is different from the previous methods: first we model the task with the triple C, Q, R instead of C, R in the early works, and use a novel triple attention matching mechanism to model the relationships within the triple.\n[BOS] Then we represent the context from low (character) to high (context) level, which constructs the representations for the context more comprehensively.\n\n", "discourse_tags": ["Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "202734248_2", "paragraph": "[BOS] The early studies of response selection generally focus on the single-turn conversation, which use only the current query to select the response (Lu and Li, 2013; Ji et al., 2014; Wang et al., 2015) .\n[BOS] Since it is hard to get the topic and intention of the conversation by single-turn, researchers turn their attention to multi-turn conversation and model the context instead of the current query to predict the response.\n[BOS] First, Lowe et al. (2015) released the Ubuntu Dialogue dataset and proposed a neural model which matches the context and response with corresponding representations via RNNs and LSTMs.\n[BOS] Kadlec et al. (2015) evaluate the performances of various models on the dataset, such as LSTMs, Bi-LSTMs, and CNNs.\n[BOS] Later, concatenated utterances with the reformulated query and various features in a deep neural network.\n[BOS] Baudi et al. (2016) regarded the task as sentence pair scoring and implemented an RNN-CNN neural network model with attention.\n[BOS] Zhou et al. (2016) proposed a multiview model with CNN and RNN, modeling the context in both word and utterance view.\n[BOS] Further, Xu et al. (2017) proposed a deep neural network to incorporate background knowledge for conversation by LSTM with a specially designed recall gate.\n[BOS] Wu et al. (2017) proposed matching the context and response by their word and phrase representations, which had significant improvement from previous work.\n[BOS] Zhang et al. (2018) introduced a self-matching attention to route the vital information in each utterance, and used RNN to fuse the matching result.\n[BOS] Zhou et al. (2018) used self-attention and cross-attention to construct the representations at different granularities, achieving a state-of-the-art result.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 19, "token_end": 48, "char_start": 105, "char_end": 204, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lu and Li, 2013;": "14617645", "Ji et al., 2014;": "18380963", "Wang et al., 2015)": "14029406"}}}, {"token_start": 94, "token_end": 131, "char_start": 446, "char_end": 623, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Lowe et al. (2015)": "8379583"}, "Reference": {}}}, {"token_start": 132, "token_end": 185, "char_start": 630, "char_end": 857, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kadlec et al. (2015)": "1381199"}, "Reference": {}}}, {"token_start": 186, "token_end": 214, "char_start": 864, "char_end": 990, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Baudi\u0161 et al. (2016)": "14965173"}, "Reference": {}}}, {"token_start": 215, "token_end": 243, "char_start": 997, "char_end": 1114, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhou et al. (2016)": "2867243"}, "Reference": {}}}, {"token_start": 246, "token_end": 274, "char_start": 1130, "char_end": 1277, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Xu et al. (2017)": "28625162"}, "Reference": {}}}, {"token_start": 275, "token_end": 303, "char_start": 1284, "char_end": 1439, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wu et al. (2017)": "5450801"}, "Reference": {}}}, {"token_start": 304, "token_end": 336, "char_start": 1446, "char_end": 1594, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhang et al. (2018)": "49411029"}, "Reference": {}}}, {"token_start": 337, "token_end": 372, "char_start": 1601, "char_end": 1757, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhou et al. (2018)": "51877568"}, "Reference": {}}}]}
{"id": "202734248_1", "paragraph": "[BOS] In this paper, we focus on the task response selection which belongs to retrieval-based approach.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "202734248_0", "paragraph": "[BOS] Earlier works on building the conversation systems are generally based on rules or templates (Walker et al., 2001) , which are designed for the specific domain and need much human effort to collect the rules and domain knowledge.\n[BOS] As the portability and coverage of such systems are far from satisfaction, people pay more attention to the data-driven approaches for the opendomain conversation system (Ritter et al., 2011; Higashinaka et al., 2014) .\n[BOS] The main challenge for open-domain conversation is to produce a corresponding response based on the current context.\n[BOS] As mentioned previously, the retrieval-based and generation-based methods are the mainstream approaches for conversational response generation.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 13, "token_end": 24, "char_start": 80, "char_end": 120, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Walker et al., 2001)": null}}}, {"token_start": 72, "token_end": 95, "char_start": 381, "char_end": 459, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ritter et al., 2011;": "780171", "Higashinaka et al., 2014)": "17170018"}}}]}
{"id": "202626880_2", "paragraph": "[BOS] 3 Proposed Model Figure 1 shows the outline of the proposed model.\n[BOS] It first identifies keywords that strongly co-occur between utterances and their responses in a training corpus using PPMI (section 3.1).\n[BOS] The decoder then uses Gumbel-Softmax to sample words in the output layer (section 3.3).\n[BOS] Finally, it computes the proportion of output words matching the keywords, and add weights to the loss function (section 3.4).\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Transition"], "span_citation_mapping": []}
{"id": "202626880_1", "paragraph": "[BOS] (1) Here, both P (R|Q) and P (Q|R) are computed by independent Seq2Seq models.\n[BOS] Specifically, the N -best candidate responses generated by the former model are re-ranked by Equation (1).\n[BOS] MMIbidi exhibited a strong performance for diversifying responses while preserving relevance to an input utterance.\n[BOS] However, its effects depend on the diversities of the N-best candidate responses.\n[BOS] If these responses are diverse, MMI-bidi can improve futher.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Transition", "Transition"], "span_citation_mapping": []}
{"id": "202626880_0", "paragraph": "[BOS] The generic response problem has been actively studied.\n[BOS] Yao et al. (2016) and Nakamura et al. (2019) proposed models that constrain decoders to directly suppress generation of frequent words.\n[BOS] Yao et al. (2016) diversified the response by a loss function in which words with high inverse document frequency values are preferred.\n[BOS] Nakamura et al. (2019) proposed a loss function that adds weights based on the inverse of the word frequency.\n[BOS] Xing et al. (2017) proposed a model using topic words extracted from utterances.\n[BOS] Their model ensembles words predicted using the topic words and the words predicted by the decoder.\n[BOS] All of the methods described above only focus on the amount of a information in a response.\n[BOS] Therefore, generated responses tend to lack relevance to input utterances.\n[BOS] MMI-bidi (Li et al., 2016) solves this problem by approximating the PMI between the utterance Q and the generated\n\n", "discourse_tags": ["Transition", "Multi_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Transition", "Transition", "Single_summ"], "span_citation_mapping": [{"token_start": 12, "token_end": 42, "char_start": 68, "char_end": 203, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Yao et al. (2016)": "15604739", "Nakamura et al. (2019)": "53791134"}, "Reference": {}}}, {"token_start": 43, "token_end": 70, "char_start": 210, "char_end": 345, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Yao et al. (2016)": "15604739"}, "Reference": {}}}, {"token_start": 71, "token_end": 95, "char_start": 352, "char_end": 461, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Nakamura et al. (2019)": "53791134"}, "Reference": {}}}, {"token_start": 96, "token_end": 132, "char_start": 468, "char_end": 654, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Xing et al. (2017)": "9514751"}, "Reference": {}}}, {"token_start": 165, "token_end": 193, "char_start": 840, "char_end": 953, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Li et al., 2016)": "7287895"}, "Reference": {}}}]}
{"id": "201698380_3", "paragraph": "[BOS] With a large scale of corpora for training, neural network based systems have recently been developed.\n[BOS] In abstractive systems, Rush et al. (2015) proposed a local attention-based sequenceto-sequence model.\n[BOS] On top of the seq2seq framework, many other variants have been studied using convolutional networks (Cheng and Lapata, 2016; Allamanis et al., 2016) , pointer networks (See et al., 2017) , scheduled sampling (Bengio et al., 2015) , and reinforcement learning (Paulus et al., 2017) .\n[BOS] In extractive systems, different types of encoders (Cheng and Lapata, 2016; Nallapati et al., 2017; Kedzie et al., 2018) and optimization techniques (Narayan et al., 2018b) have been developed.\n[BOS] Our goal is to explore which types of systems learns which sub-aspect of summarization.\n\n", "discourse_tags": ["Transition", "Single_summ", "Narrative_cite", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 22, "token_end": 46, "char_start": 118, "char_end": 217, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 63, "token_end": 82, "char_start": 301, "char_end": 372, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cheng and Lapata, 2016;": "1499080", "Allamanis et al., 2016)": "2723946"}}}, {"token_start": 83, "token_end": 93, "char_start": 375, "char_end": 410, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(See et al., 2017)": null}}}, {"token_start": 94, "token_end": 105, "char_start": 413, "char_end": 453, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bengio et al., 2015)": "1820089"}}}, {"token_start": 107, "token_end": 118, "char_start": 460, "char_end": 504, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Paulus et al., 2017)": "21850704"}}}, {"token_start": 128, "token_end": 156, "char_start": 555, "char_end": 633, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cheng and Lapata, 2016;": "1499080", "Nallapati et al., 2017;": "6405271", "Kedzie et al., 2018)": "53083054"}}}, {"token_start": 157, "token_end": 169, "char_start": 638, "char_end": 685, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Narayan et al., 2018b)": "3510042"}}}]}
{"id": "201698380_2", "paragraph": "[BOS] Beyond the corpora themselves, a variety of summarization systems have been developed: Mihalcea and Tarau (2004); Erkan and Radev (2004) used graph-based keyword ranking algorithms.\n[BOS] Lin and Bilmes (2010) ; Carbonell and Goldstein (1998) found summary sentences which are highly relevant but less redundant.\n[BOS] Yogatama et al. (2015) used semantic volumes of bigram features for extractive summarization.\n[BOS] Internal structures of documents have been used in summarization: syntactic parse trees (Woodsend and Lapata, 2011; Cohn and Lapata, 2008) , topics (Zajic et al., 2004; Lin and Hovy, 2000) , semantic word graphs Gerani et al., 2014; Ganesan et al., 2010; Filippova, 2010; Boudin and Morin, 2013) , and abstract meaning representation .\n[BOS] Concept-based Integer-Linear Programming (ILP) solver (McDonald, 2007) is used for optimizing the summarization problem (Gillick and Favre, 2009; Banerjee et al., 2015; Boudin et al., 2015; Berg-Kirkpatrick et al., 2011) .\n[BOS] Durrett et al. (2016) optimized the problem with grammatical and anarphorcity constraints.\n\n", "discourse_tags": ["Multi_summ", "Multi_summ", "Single_summ", "Narrative_cite", "Narrative_cite", "Single_summ"], "span_citation_mapping": [{"token_start": 17, "token_end": 44, "char_start": 93, "char_end": 187, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Erkan and Radev (2004)": "506350"}, "Reference": {}}}, {"token_start": 45, "token_end": 71, "char_start": 194, "char_end": 318, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Lin and Bilmes (2010)": "1803710", "Carbonell and Goldstein (1998)": "6334682"}, "Reference": {}}}, {"token_start": 72, "token_end": 94, "char_start": 325, "char_end": 418, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Yogatama et al. (2015)": "12194143"}, "Reference": {}}}, {"token_start": 106, "token_end": 126, "char_start": 491, "char_end": 563, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Woodsend and Lapata, 2011;": "9945908", "Cohn and Lapata, 2008)": "2411338"}}}, {"token_start": 127, "token_end": 145, "char_start": 566, "char_end": 613, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zajic et al., 2004;": "62613480", "Lin and Hovy, 2000)": "8598694"}}}, {"token_start": 146, "token_end": 180, "char_start": 616, "char_end": 720, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Gerani et al., 2014;": "2767900", "Ganesan et al., 2010;": "988010", "Filippova, 2010;": "14750088", "Boudin and Morin, 2013)": "6545121"}}}, {"token_start": 187, "token_end": 204, "char_start": 767, "char_end": 837, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(McDonald, 2007)": null}}}, {"token_start": 207, "token_end": 250, "char_start": 850, "char_end": 987, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Gillick and Favre, 2009;": "167874", "Banerjee et al., 2015;": "15795297", "Boudin et al., 2015;": "6171252", "Berg-Kirkpatrick et al., 2011)": "15467396"}}}, {"token_start": 252, "token_end": 272, "char_start": 996, "char_end": 1086, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Durrett et al. (2016)": "5125975"}, "Reference": {}}}]}
{"id": "201698380_1", "paragraph": "[BOS] We analyze the sub-aspects on different domains of summarization corpora: news articles (Nallapati et al., 2016; Grusky et al., 2018; Narayan et al., 2018a) , academic papers or journals (Kang et al., 2018; Kedzie et al., 2018) , movie scripts (Gorinski and Lapata, 2015) , books (Mihalcea and Ceylan, 2007) , personal posts (Ouyang et al., 2017) , and meeting minutes (Carletta et al., 2005) as described further in 5.\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 16, "token_end": 46, "char_start": 80, "char_end": 162, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Nallapati et al., 2016;": "8928715", "Grusky et al., 2018;": "13752552", "Narayan et al., 2018a)": "52096531"}}}, {"token_start": 47, "token_end": 68, "char_start": 165, "char_end": 233, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kang et al., 2018;": "13746581", "Kedzie et al., 2018)": "53083054"}}}, {"token_start": 69, "token_end": 80, "char_start": 236, "char_end": 277, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Gorinski and Lapata, 2015)": "12131248"}}}, {"token_start": 81, "token_end": 94, "char_start": 280, "char_end": 313, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mihalcea and Ceylan, 2007)": "1843005"}}}, {"token_start": 95, "token_end": 106, "char_start": 316, "char_end": 352, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ouyang et al., 2017)": "6002018"}}}, {"token_start": 108, "token_end": 120, "char_start": 359, "char_end": 398, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Carletta et al., 2005)": "6118869"}}}]}
{"id": "201698380_0", "paragraph": "[BOS] We provide here a brief review of prior work on summarization biases.\n[BOS] Lin and Hovy (1997) studied the position hypothesis, especially in the news article writing (Hong and Nenkova, 2014; Narayan et al., 2018a) but not in other domains such as conversations (Kedzie et al., 2018) .\n[BOS] Narayan et al. (2018a) collected a new corpus to address the bias by compressing multiple contents of source document in the single target summary.\n[BOS] In the bias analysis of systems, Lin and Bilmes (2012, 2011) studied the sub-aspect hypothesis of summarization systems.\n[BOS] Our study extends the hypothesis to various corpora as well as systems.\n[BOS] With a specific focus on importance aspect, a recent work (Peyrard, 2019a) divided it into three subcategories; redundancy, relevance, and informativeness, and provided quantities of each to measure.\n[BOS] Compared to this, ours provide broader scale of sub-aspect analysis across various corpora and systems.\n\n", "discourse_tags": ["Reflection", "Single_summ", "Single_summ", "Single_summ", "Reflection", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 17, "token_end": 71, "char_start": 82, "char_end": 290, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Lin and Hovy (1997)": "5519987"}, "Reference": {"(Hong and Nenkova, 2014;": "2342155", "Narayan et al., 2018a)": "52096531", "(Kedzie et al., 2018)": "53083054"}}}, {"token_start": 73, "token_end": 104, "char_start": 299, "char_end": 446, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Narayan et al. (2018a)": "52096531"}, "Reference": {}}}, {"token_start": 105, "token_end": 132, "char_start": 453, "char_end": 573, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Lin and": "964287"}, "Reference": {}}}, {"token_start": 158, "token_end": 190, "char_start": 716, "char_end": 857, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Peyrard, 2019a)": "196208241"}, "Reference": {}}}]}
{"id": "196211466_2", "paragraph": "[BOS] To consider the multiple responses jointly, the maximum likelihood strategy is explored.\n[BOS] Zhang et al. (2018a) propose the maximum generated likelihood criteria which model a query with its multiple responses as a bag of instances and proposes to optimize the model towards the most likely answer rather than all possible responses.\n[BOS] Similarly, Rajendran et al. (2018) propose to reward the dialogue system if any valid answer is produced in the reinforcement learning phase.\n[BOS] Though considering multiple responses jointly, the maximum likelihood strategy fails to utilize all the references during training with some cases ig- Figure 2 : The overall architecture of our proposed dialogue system where the two generation steps and testing process are illustrated.\n[BOS] Given an input query x, the model aims to approximate the multiple responses in a bag {y} simultaneously with the continuous common and distinctive features, i.e., the latent variables c and z obtained from the two generation phases respectively.\n[BOS] nored.\n[BOS] In our approach, we consider multiple responses jointly and model each specific response separately by a two-step generation architecture.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Reflection", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 17, "token_end": 61, "char_start": 101, "char_end": 343, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhang et al. (2018a)": "51878364"}, "Reference": {}}}, {"token_start": 64, "token_end": 91, "char_start": 361, "char_end": 491, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Rajendran et al. (2018)": "52131263"}, "Reference": {}}}]}
{"id": "196211466_1", "paragraph": "[BOS] A few works explore to change the deterministic structure of sequence-to-sequence models by introducing stochastic latent variables.\n[BOS] VAE is one of the most popular methods (Bowman et al., 2016; Serban et al., 2017; Cao and Clark, 2017 ), where the discourse-level diversity is modeled by a Gaussian distribution.\n[BOS] However, it is observed that in the CVAE with a fixed Gaussian prior, the learned conditional posteriors tend to collapse to a single mode, resulting in a relatively simple scope (Wang et al., 2017) .\n[BOS] To tackle this, WAE (Gu et al., 2018) which adopts a Gaussian mixture prior network with Wasserstein distance and VAD (Du et al., 2018) which sequentially introduces a series of latent variables to condition each word in the response sequence are proposed.\n[BOS] Although these models overcome the deterministic structure of sequence-to-sequence model, they still ignore the correlation of multiple valid responses and each case is trained separately.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite", "Multi_summ", "Transition"], "span_citation_mapping": [{"token_start": 25, "token_end": 56, "char_start": 145, "char_end": 246, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bowman et al., 2016;": "748227", "Serban et al., 2017;": "14857825", "Cao and Clark, 2017": "17611516"}}}, {"token_start": 80, "token_end": 115, "char_start": 367, "char_end": 529, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wang et al., 2017)": "314973"}}}, {"token_start": 121, "token_end": 142, "char_start": 554, "char_end": 647, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Gu et al., 2018)": "44129557"}, "Reference": {}}}, {"token_start": 143, "token_end": 172, "char_start": 652, "char_end": 794, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Du et al., 2018)": "53081212"}, "Reference": {}}}]}
{"id": "196211466_0", "paragraph": "[BOS] Along with the flourishing development of neural networks, the sequence-to-sequence framework has been widely used for conversation response generation (Shang et al., 2015; Sordoni et al., 2015) where the mapping from a query x to a reply y is learned with the negative log likelihood.\n[BOS] However, these models suffer from the \"safe\" response problem.\n[BOS] To address this problem, various methods have been proposed.\n[BOS] Li et al. (2016a) propose a diversity-promoting objective function to encourage diverse responses during decoding.\n[BOS] Zhou et al. ( , 2018a introduce a responding mechanism between the encoder and decoder to generate various responses.\n[BOS] incorporate topic information to generate informative responses.\n[BOS] However, these models suffer from the deterministic structure when generating multiple diverse responses.\n[BOS] Besides, during the training of these models, response utterances are only used in the loss function and ignored when forward computing, which can confuse the model for pursuing multiple objectives simultaneously.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 24, "token_end": 45, "char_start": 125, "char_end": 200, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Shang et al., 2015;": "7356547", "Sordoni et al., 2015)": "94285"}}}, {"token_start": 91, "token_end": 113, "char_start": 434, "char_end": 548, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Li et al. (2016a)": "7287895"}, "Reference": {}}}, {"token_start": 114, "token_end": 136, "char_start": 555, "char_end": 672, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhou et al. ( , 2018a": "19085474"}, "Reference": {}}}]}
{"id": "184483358_2", "paragraph": "[BOS] In addition, we would also like to mention the previous editions of related workshops such as TA-COS 1 , Abusive Language Online 2 , and TRAC 3 and related shared tasks such as GermEval (Wiegand et al., 2018) and TRAC (Kumar et al., 2018) .\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 40, "token_end": 53, "char_start": 183, "char_end": 214, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 54, "token_end": 63, "char_start": 219, "char_end": 244, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kumar et al., 2018)": "59336626"}}}]}
{"id": "184483358_1", "paragraph": "[BOS] A proposal of typology of abusive language sub-tasks is presented in (Waseem et al., 2017) .\n[BOS] For studies on languages other than English see (Su et al., 2017) on Chinese and (Fier et al., 2017) on Slovene.\n[BOS] Finally, for recent discussion on identifying profanity vs. hate speech see .\n[BOS] This work high-lighted the challenges of distinguishing between profanity, and threatening language which may not actually contain profane language.\n\n", "discourse_tags": ["Single_summ", "Narrative_cite", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 2, "token_end": 27, "char_start": 6, "char_end": 96, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Waseem et al., 2017)": "8821211"}, "Reference": {}}}, {"token_start": 37, "token_end": 47, "char_start": 153, "char_end": 181, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Su et al., 2017)": "32076269"}}}, {"token_start": 48, "token_end": 62, "char_start": 186, "char_end": 217, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Fi\u0161er et al., 2017)": "28550367"}}}]}
{"id": "184483358_0", "paragraph": "[BOS] Papers published in the last two years include the surveys by (Schmidt and Wiegand, 2017) and (Fortuna and Nunes, 2018) , the paper by presenting the Hate Speech Detection dataset used in (Malmasi and Zampieri, 2017 ) and a few other recent papers such as (ElSherief et al., 2018; Gambck and Sikdar, 2017; Zhang et al., 2018) .\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 11, "token_end": 32, "char_start": 57, "char_end": 125, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Schmidt and Wiegand, 2017)": "9626793", "(Fortuna and Nunes, 2018)": "52184457"}}}, {"token_start": 38, "token_end": 55, "char_start": 156, "char_end": 221, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Malmasi and Zampieri, 2017": "19182892"}}}, {"token_start": 60, "token_end": 91, "char_start": 240, "char_end": 331, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(ElSherief et al., 2018;": "4809781", "Gamb\u00e4ck and Sikdar, 2017;": null, "Zhang et al., 2018)": "46939253"}}}]}
{"id": "202784760_1", "paragraph": "[BOS] Visual QA systems answer queries based on the contexts from provided images (Antol et al., 2015; .\n[BOS] Jiang et al. (2018) propose the visual memex QA task which addresses similar domains given a dataset composed of multiple photo albums.\n[BOS] We extend the problem domain to the conversational settings where the focus is the increased engagement with users through natural multi-modal interactions.\n[BOS] Our work also extends the QA capability by utilizing semantic and structural contexts from memory and knowledge graphs, instead of relying solely on meta information and multi-modal content available in photo albums.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 22, "char_start": 6, "char_end": 101, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 25, "token_end": 53, "char_start": 111, "char_end": 246, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Jiang et al. (2018)": null}, "Reference": {}}}]}
{"id": "202784760_0", "paragraph": "[BOS] End-to-end dialog systems: There have been a number of studies on end-to-end dialog systems, often focused on task or goal oriented dialog systems such as conversational recommendations Sun and Zhang, 2018) , information querying (Williams et al., 2017; de Vries et al., 2018; Reddy et al., 2018) , etc.\n[BOS] Many of the public datasets are collected via bootstrapped simulations , Wizard-of-Oz setup Wei et al., 2018; Moon et al., 2019a) , or online corpus (Li et al., 2016) .\n[BOS] In our work, we propose a unique setup for dialog systems called memory-grounded conversations, where the focus is on grounding human conversations with past user memories for both the goal-oriented task (memory recall QA) and the more open-ended dialogs (proactive memory reference).\n[BOS] Our Memory Dialog dataset uses the popular Wizard-of-Oz setup between role-playing human annotators, where the reference memories are bootstrapped through memory graph generator.\n[BOS] QA Systems: Structured QA systems have been very popular due to the popularity of the factretrieval assistant products, which solve factretrieval QA queries with large-scale common fact knowledge graphs (Bordes et al., 2015; Xu et al., 2016; Dubey et al., 2018) .\n[BOS] Most of the work typically utilize an entity linking system and a QA model for predicting graph operations e.g. through template matching approaches, etc.\n[BOS] For QA systems with unstructured knowledge sources (e.g. machine reading comprehension), the approaches that utilize Memory Networks with explicit memory slots (Weston et al., 2014; Sukhbaatar et al., 2016) are widely used for their capability of transitive reasoning.\n[BOS] In our work, we utilize Memory Graph Networks (MGN) (Moon et al., 2019b) to store graph nodes as memory slots and expand slots via graph traversals, to effectively handle complex memory recall queries and to identify relevant memories to surface next.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Reflection", "Transition", "Narrative_cite", "Transition", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 37, "token_end": 46, "char_start": 161, "char_end": 212, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Sun and Zhang, 2018)": null}}}, {"token_start": 47, "token_end": 74, "char_start": 215, "char_end": 302, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Williams et al., 2017;": "13214003", "de Vries et al., 2018;": "49669712", "Reddy et al., 2018)": "52055325"}}}, {"token_start": 86, "token_end": 114, "char_start": 362, "char_end": 445, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Wei et al., 2018;": "53080145", "Moon et al., 2019a)": "196176000"}}}, {"token_start": 116, "token_end": 126, "char_start": 451, "char_end": 482, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Li et al., 2016)": "2955580"}}}, {"token_start": 254, "token_end": 285, "char_start": 1129, "char_end": 1228, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bordes et al., 2015;": "9605730", "Xu et al., 2016;": "139787", "Dubey et al., 2018)": "6263477"}}}, {"token_start": 338, "token_end": 363, "char_start": 1515, "char_end": 1604, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Sukhbaatar et al., 2016)": "1399322"}}}, {"token_start": 380, "token_end": 397, "char_start": 1697, "char_end": 1745, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Moon et al., 2019b)": "209063721"}}}]}
{"id": "184482898_1", "paragraph": "[BOS] More recently several researchers tried to address the problem by using state-of-the-art neural network based models.\n[BOS] Among several prominent works, Badjatiya et al. (2017) employed multiple deep learning architectures including CNNs, LSTMs, and fastText to learn semantic word embeddings for hate speech detection.\n[BOS] Golem et al. (2018) utilized the combination of traditional shallow machine learning models and deep learning models for hate speech detection.\n[BOS] Pitsilis et al. (2018) utilized the ensemble of recurrent neural network (RNN) classifiers and incorporated various features associated with user-related information.\n[BOS] Zhang et al. (2018) introduced a new method by combining a convolutional neural network (CNN) and gated recurrent unit (GRU) models.\n[BOS] Djuric et al. (2015) used the comment embeddings for detecting hate speech.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 31, "token_end": 68, "char_start": 161, "char_end": 327, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Badjatiya et al. (2017)": "2880908"}, "Reference": {}}}, {"token_start": 69, "token_end": 96, "char_start": 334, "char_end": 477, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Golem et al. (2018)": "53064798"}, "Reference": {}}}, {"token_start": 97, "token_end": 129, "char_start": 484, "char_end": 650, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Pitsilis et al. (2018)": "12985409"}, "Reference": {}}}, {"token_start": 130, "token_end": 160, "char_start": 657, "char_end": 789, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhang et al. (2018)": "46939253"}, "Reference": {}}}, {"token_start": 161, "token_end": 179, "char_start": 796, "char_end": 871, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Djuric et al. (2015)": "2039295"}, "Reference": {}}}]}
{"id": "184482898_0", "paragraph": "[BOS] Early studies on hate speech detection focused mainly on lexicon-based approaches (Kwok and Wang, 2013; Gitari et al., 2015) .\n[BOS] However, these approaches prone to failure for detecting hate speech in a microblogging platform where rare terms are evolving incessantly.\n[BOS] Besides some researchers tackled the problem by employing feature (e.g., N-gram, TF-IDF) based supervised learning approach using SVM and Naive-Bayes classifier (Gaydhani et al., 2018; Unsvg and Gambck, 2018) .\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 12, "token_end": 33, "char_start": 63, "char_end": 130, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kwok and Wang, 2013;": "14964721", "Gitari et al., 2015)": "16011169"}}}, {"token_start": 96, "token_end": 119, "char_start": 423, "char_end": 493, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Gaydhani et al., 2018;": "52816907"}}}]}
{"id": "174800297_5", "paragraph": "[BOS] The switch to Spearman correlation appears to have occurred in Gabrilovich and Markovitch (2007) , who employed it without comment.\n[BOS] Agirre et al. (2009) did provide a justification, saying, \"In our belief Pearson is less informative, as the Pearson correlation suffers much when the scores of two systems are not linearly correlated, something which happens often due to the different nature of the techniques applied.\"\n[BOS] Unfortunately, Agirre et al. (2009) mischaracterized the popularity of Spearman correlation by claiming that all researchers have used Spearman in evaluating WordSim-353 dataset (Finkelstein et al., 2002) .\n[BOS] This likely stems from a misinterpretation of Gabrilovich and Markovitch's Table 4 , which compares their methodology with earlier studies using Spearman correlation.\n[BOS] The latter authors apparently recomputed word relatedness with the associated algorithms, as the cited studies report Pearson correlation values.\n[BOS] Willmott (1981; 1982) specifically argues that Pearson correlation should not be used to evaluate model performance, and that RMSE is superior at comparing observed and simulated data.\n[BOS] 3 However, as far as we know, no previous work has seriously considered evaluating the performance of computed word similarity scores using RMSE.\n[BOS] Reliance on Spearman correlation may lead to incorrect conclusions regarding the quality of word vectors.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 2, "token_end": 28, "char_start": 6, "char_end": 137, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Gabrilovich and Markovitch (2007)": "5291693"}, "Reference": {}}}, {"token_start": 29, "token_end": 85, "char_start": 144, "char_end": 430, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Agirre et al. (2009)": "5944731"}, "Reference": {}}}, {"token_start": 87, "token_end": 134, "char_start": 438, "char_end": 642, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Agirre et al. (2009)": "5944731"}, "Reference": {"(Finkelstein et al., 2002)": "12956853"}}}, {"token_start": 136, "token_end": 190, "char_start": 651, "char_end": 969, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 191, "token_end": 225, "char_start": 976, "char_end": 1160, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Willmott (1981;": null, "1982)": null}, "Reference": {}}}]}
{"id": "174800297_4", "paragraph": "[BOS] More generally, there does not seem to be a strong theoretical reason to prefer correlationbased measures over residual-based ones.\n[BOS] Although the current practice is to report the Spearman's rank correlation coefficient between the vector cosine similarities and human word similarity judgments, for over a decade the standard was to report Pearson product-moment correlation coefficient.\n[BOS] When Resnik (1995) pioneered the technique of comparing computed measures of similarity with human similarity ratings, he used (Pearson) correlation as \"one reasonable way to judge [computational measures of semantic similarity]\".\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ"], "span_citation_mapping": [{"token_start": 69, "token_end": 110, "char_start": 406, "char_end": 634, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Resnik (1995)": "1752785"}, "Reference": {}}}]}
{"id": "174800297_3", "paragraph": "[BOS] BabelNet (Navigli and Ponzetto, 2012) and ConceptNet (Speer et al., 2017) are knowledge resources derived from a number of collaborativelyconstructed sources, such as Wikipedia and Wiktionary.\n[BOS] Though their collaborative nature likely makes them less accurate than hand-curated resources such as WordNet, they have potential in improving the quality of word vectors (e.g. Speer and Chin, 2016) .\n[BOS] As we observed with FN-ANNO, RMSE may be a more informative measure of comparison than correlation in future retrofitting experiments involving heterogeneous resources.\n\n", "discourse_tags": ["Multi_summ", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 51, "char_start": 6, "char_end": 198, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Navigli and Ponzetto, 2012)": null, "(Speer et al., 2017)": "15206880"}, "Reference": {}}}, {"token_start": 73, "token_end": 94, "char_start": 321, "char_end": 404, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Speer and Chin, 2016)": "1758220"}}}]}
{"id": "174800297_2", "paragraph": "[BOS] Grouping nouns using the FrameNet annotation data led to large drops in correlation against word similarity benchmarks.\n[BOS] However, these same data yielded large gains in RMSE performance.\n[BOS] It might be inferred that semantic resources which have a similar stochastic component may result lower correlation.\n[BOS] The PPDB is automatically generated, introducing a similar element of randomness, but this is curtailed by its conservative criteria: paraphrases must be attested as translation equivalents.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Transition"], "span_citation_mapping": []}
{"id": "174800297_1", "paragraph": "[BOS] PropBank (Bonial et al., 2014 ) is a large semantically-annotated corpus.\n[BOS] The semantic roles (\"rolesets\") in PropBank are defined with respect to individual verb and noun word senses.\n[BOS] The types of words that fill these roles are presumably less varied than those that fill the semantically broader FrameNet frame elements.\n[BOS] Additionally, PropBank is considerably larger than FrameNet.\n[BOS] Consequently, we might predict that retrofitting word vectors to PropBank would yield stronger gains in word similarity judgment than to the FrameNet annotation data.\n[BOS] We leave this task for future research.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 83, "char_start": 6, "char_end": 407, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Bonial et al., 2014": "12691423"}, "Reference": {}}}]}
{"id": "174800297_0", "paragraph": "[BOS] Our novel treatment of FrameNet groups nouns using its collection of sense-annotated sentences.\n[BOS] Although all of the frame elements in these sentences were annotated by hand, the words filling the FEs are not, adding a component of randomness.\n[BOS] Especially with more semantically general frames, frame elements can be realized by a large number of words.\n[BOS] This contrasts with FrameNet frames, in which the placement of word senses are painstakingly deliberated, and a particular sense can only be put into one frame.\n\n", "discourse_tags": ["Reflection", "Reflection", "Transition", "Transition"], "span_citation_mapping": []}
{"id": "201657646_1", "paragraph": "[BOS] Although much research has focused on response generation in a chit-chat setting, models trained on large datasets of human-human interactions of diverse speaker characteristics often tend to generate responses which are too vague and generic (common for most speakers) or inconsistent in personality (switching between different speakers' characteristics).\n[BOS] Recently, (Zhang et al., 2018) presented the CONVAI2 challenge containing persona descriptions and over 10K real human chit-chats where speakers were required to converse based on their assigned persona.\n[BOS] (Li et al., 2016a ) learned speaker persona embeddings from a single-speaker setting (e.g. Twitter posts) or a speaker-address style (human-human conversations) to generate personalized responses given a single utterance input.\n[BOS] Another related work (Raghu et al., 2018) applies hierarchical memory network for task oriented dialog problem.\n[BOS] In this work, we compare our model with (Zhang et al., 2018) which uses a memoryaugmented sequence-to-sequence response generator grounded on the dialogue history and persona.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 65, "token_end": 107, "char_start": 380, "char_end": 573, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Zhang et al., 2018)": "6869582"}, "Reference": {}}}, {"token_start": 108, "token_end": 158, "char_start": 580, "char_end": 807, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Li et al., 2016a": "2955580"}, "Reference": {}}}, {"token_start": 162, "token_end": 181, "char_start": 835, "char_end": 925, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Raghu et al., 2018)": null}, "Reference": {}}}, {"token_start": 191, "token_end": 221, "char_start": 972, "char_end": 1107, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Zhang et al., 2018)": "6869582"}, "Reference": {}}}]}
{"id": "201657646_0", "paragraph": "[BOS] Earlier work on data-driven, end-to-end approaches to conversational response generation treated the task as statistical machine translation, where the goal is to generate a response given the previous dialogue turn (Ritter et al., 2011; Vinyals and Le, 2015) .\n[BOS] While these studies resulted in a paradigm change compared to earlier work, they do not include mechanisms to represent conversation context.\n[BOS] To tackle this problem and have a better representation of conversation context as input to generation, (Serban et al., 2016) proposed hierarchical recurrent encoder-decoder (HRED) networks.\n[BOS] HRED combines two RNNs, one at the token level, modeling individual turns, and one at the dialogue level, inputting turn representations from the tokenlevel RNNs.\n[BOS] However, utterances generated by such neural response generation systems are often generic and contentless (Vinyals and Le, 2015) .\n[BOS] To improve the diversity and content of generated responses, HRED was later extended with a latent variable that aims to model the higher level aspects (such as topic) of the generated responses, resulting in the VHRED approach (Serban et al., 2017) .\n[BOS] Another challenge for dialogue response generation is the integration of knowledge into the generated responses.\n[BOS] (Liu et al., 2018) extracted facts relevant to a dialogue from knowledge using string matching, named entity recognition and linking, found additional entities from knowledge that are most relevant to the facts by a neural similarity scorer, and used these as input context features for the dialogue generation RNN.\n[BOS] (Ghazvininejad et al., 2018) used end-to-end memory networks to base the generated responses on knowledge, where an attention over the knowledge relevant to the conversation context is estimated, and multiple knowledge representations are included as input during the decoding of responses.\n[BOS] In this work, we use end-to-end memory networks as a baseline.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Single_summ", "Single_summ", "Narrative_cite", "Narrative_cite", "Transition", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 33, "token_end": 58, "char_start": 169, "char_end": 265, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ritter et al., 2011;": "780171", "Vinyals and Le, 2015)": "12300158"}}}, {"token_start": 89, "token_end": 158, "char_start": 449, "char_end": 781, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Serban et al., 2016)": "6126582"}, "Reference": {}}}, {"token_start": 161, "token_end": 184, "char_start": 797, "char_end": 917, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vinyals and Le, 2015)": "12300158"}}}, {"token_start": 226, "token_end": 238, "char_start": 1139, "char_end": 1175, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Serban et al., 2017)": "14857825"}}}, {"token_start": 257, "token_end": 316, "char_start": 1303, "char_end": 1618, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Liu et al., 2018)": "51881445"}, "Reference": {}}}, {"token_start": 317, "token_end": 374, "char_start": 1625, "char_end": 1915, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Ghazvininejad et al., 2018)": "15442925"}, "Reference": {}}}]}
{"id": "196208296_1", "paragraph": "[BOS] A few other toolkits have a dialog emphasis.\n[BOS] DeepPavlov (Burtsev et al., 2018 ) is a deep learning library with a focus on task-oriented dialogue.\n[BOS] It provides demos and pre-trained models for tasks such as question answering and sentiment classification.\n[BOS] Affiliated with DeepPavlov is the ConvAI2 challenge (Dinan et al., 2019) , a general dialogue competition featuring a synthetic personalized conversational dataset.\n[BOS] ParlAI (Miller et al., 2017 ) is a library centered around taskoriented dialogue, compiling a number of popular datasets for NLP tasks as well as pre-trained models for knowledge-grounded dialog agents trained on crowd-sourced data.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 13, "token_end": 61, "char_start": 57, "char_end": 272, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Burtsev et al., 2018": "51871042"}, "Reference": {}}}, {"token_start": 70, "token_end": 98, "char_start": 313, "char_end": 443, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 99, "token_end": 151, "char_start": 450, "char_end": 682, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Miller et al., 2017": "3677429"}, "Reference": {}}}]}
{"id": "196208296_0", "paragraph": "[BOS] Several NLP-oriented toolkits have been opensourced.\n[BOS] Tensor2Tensor (Vaswani et al., 2018) , maintained by Google Brain, extends TensorFlow with an array of state-of-the-art baseline deep learning models.\n[BOS] It places a strong emphasis on sequence modeling baselines.\n[BOS] AllenNLP ) is a PyTorch library developed by AI2 for natural language processing tasks, notable for an open-source release of ELMo .\n[BOS] OpenNMT (Klein et al., 2017 ) is a popular neural machine translation toolkit originally developed for LuaTorch that now has implementations in PyTorch and TensorFlow.\n[BOS] MarianNMT (Junczys-Dowmunt et al., 2018 ) is another framework for neural machine translation developed between the Adam Mickiewicz University in Pozna and the University of Edinburgh.\n[BOS] It is built in C++ and designed for fast training in multi-GPU systems.\n[BOS] Texar (Hu et al., 2018) is a text generation toolkit affiliated with Carnegie Mellon University, featuring a similar emphasis on modularity to ICE-CAPS.\n[BOS] It includes reinforcement learning capabilities alongside its sequence modelling tools.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 16, "token_end": 67, "char_start": 65, "char_end": 281, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Vaswani et al., 2018)": "3988816"}, "Reference": {}}}, {"token_start": 100, "token_end": 135, "char_start": 427, "char_end": 594, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Klein et al., 2017": "16538528"}, "Reference": {}}}, {"token_start": 136, "token_end": 198, "char_start": 601, "char_end": 863, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Junczys-Dowmunt et al., 2018": "4623739"}, "Reference": {}}}, {"token_start": 199, "token_end": 247, "char_start": 870, "char_end": 1116, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Hu et al., 2018)": "51993096"}, "Reference": {}}}]}
{"id": "196172757_2", "paragraph": "[BOS] Unsupervised cross-lingual embedding.\n[BOS] The endeavors to explore unsupervised cross-lingual embedding are mainly divided into two categories.\n[BOS] One line focuses on designing heuristics or utilizing the structural similarity of monolingual embeddings.\n[BOS] For instance, Hoshen and Wolf (2018) present a non-adversarial method based on the principal component analysis.\n[BOS] Both Aldarmaki et al. (2018) and Artetxe et al. (2018a) take advantage of geometric properties across languages to perform word retrieval to learn the initial word mapping.\n[BOS] Cao and Zhao (2018) formulate this problem as point set registration to adopt a point set registration method.\n[BOS] However, these methods usually require plenty of random restarts or additional skills to achieve satisfactory performance.\n[BOS] Another line strives to learn unsupervised word mapping by direct distribution-matching.\n[BOS] For example, Lample et al. (2018) and Zhang et al. (2017a) completely eliminate the need for any supervision signal by aligning the distribution of transferred embedding and target embedding with GAN.\n[BOS] Furthermore, Zhang et al. (2017b) and adopt the Earth Mover's distance and Sinkhorn distance as the optimized distance metrics, respectively.\n[BOS] There are also some attempts on distant language pairs.\n[BOS] For instance, Kementchedjhieva et al. (2018) generalize Procrustes analysis by projecting the two languages into a latent space and Nakashole (2018) propose to learn neighborhood sensitive mapping by training non-linear functions.\n[BOS] As for the hubness problem, propose a latent-variable model learned with Viterbi EM algorithm.\n[BOS] Recently, Alaux et al. (2018) work on the problem of aligning more than two languages simultaneously by a formulation ensuring composable mappings.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Single_summ", "Multi_summ", "Single_summ", "Transition", "Transition", "Multi_summ", "Multi_summ", "Transition", "Multi_summ", "Transition", "Single_summ"], "span_citation_mapping": [{"token_start": 50, "token_end": 71, "char_start": 285, "char_end": 383, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 72, "token_end": 111, "char_start": 390, "char_end": 562, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Aldarmaki et al. (2018)": "249573", "Artetxe et al. (2018a)": "21728524"}, "Reference": {}}}, {"token_start": 112, "token_end": 133, "char_start": 569, "char_end": 679, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Cao and Zhao (2018)": "51604690"}, "Reference": {}}}, {"token_start": 175, "token_end": 214, "char_start": 923, "char_end": 1110, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Lample et al. (2018)": "3470398", "Zhang et al. (2017a)": "26873455"}, "Reference": {}}}, {"token_start": 217, "token_end": 246, "char_start": 1130, "char_end": 1258, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhang et al. (2017b)": "32923127"}, "Reference": {}}}, {"token_start": 261, "token_end": 287, "char_start": 1341, "char_end": 1454, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kementchedjhieva et al. (2018)": "52154332"}, "Reference": {}}}, {"token_start": 288, "token_end": 307, "char_start": 1459, "char_end": 1557, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 332, "token_end": 360, "char_start": 1675, "char_end": 1812, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Alaux et al. (2018)": "53216389"}, "Reference": {}}}]}
{"id": "196172757_1", "paragraph": "[BOS] Supervised cross-lingual embedding.\n[BOS] Inspired by the isometric observation between monolingual word embeddings of two different languages, Mikolov et al. (2013b) propose to learn cross-lingual word mapping by minimizing mean squared error.\n[BOS] Latter, Dinu and Baroni (2015) investigate the hubness problem and Faruqui and Dyer (2014) incorporates the semantics of a word in multiple languages into its embedding.\n[BOS] Furthermore, Xing et al. (2015) propose to impose the orthogonal constraint to the linear mapping and Artetxe et al. (2016) (2017) present a self-learning framework to perform iterative refinement, which is also adopted in some unsupervised settings and plays a crucial role in improving performance.\n\n", "discourse_tags": ["Transition", "Single_summ", "Multi_summ", "Multi_summ"], "span_citation_mapping": [{"token_start": 10, "token_end": 50, "char_start": 48, "char_end": 250, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Mikolov et al. (2013b)": "1966640"}, "Reference": {}}}, {"token_start": 53, "token_end": 66, "char_start": 265, "char_end": 319, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Dinu and Baroni (2015)": "17910711"}, "Reference": {}}}, {"token_start": 67, "token_end": 90, "char_start": 324, "char_end": 426, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Faruqui and Dyer (2014)": "3792324"}, "Reference": {}}}, {"token_start": 93, "token_end": 111, "char_start": 446, "char_end": 530, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Xing et al. (2015)": "3144258"}, "Reference": {}}}, {"token_start": 112, "token_end": 152, "char_start": 535, "char_end": 733, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Artetxe et al. (2016)": "1040556"}, "Reference": {}}}]}
{"id": "196172757_0", "paragraph": "[BOS] This paper is mainly related to the following two lines of work.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "202540407_1", "paragraph": "[BOS] Our work is also related to the structure modeling for SANs, as the proposed model utilizes the dependency tree to generate structural representations.\n[BOS] Recently, Hao et al. (2019c,b) integrate the recurrence into the SANs and empirically demonstrate that the hybrid models achieve better performances by modeling structure of sentences.\n[BOS] Hao et al. (2019a) further make use of the multi-head attention to form the multi-granularity self-attention, to capture the different granularity phrases in source sentences.\n[BOS] The difference is that we treat the position representation as a medium to transfer the structure information from the dependency tree into the SANs.\n[BOS] Table 1 : Impact of the position encoding components on ChineseEnglish NIST02 development dataset using Transformer-Base model.\n[BOS] \"Abs.\"\n[BOS] and \"Rel.\"\n[BOS] denote absolute and relative position encoding, respectively.\n[BOS] \"Spd.\"\n[BOS] denotes the decoding speed (sentences/second) on a Tesla M40, the speed of structural position encoding strategies include the step of dependency parsing.\n\n", "discourse_tags": ["Reflection", "Single_summ", "Single_summ", "Reflection", "Other", "Other", "Other", "Other", "Other", "Other"], "span_citation_mapping": [{"token_start": 30, "token_end": 64, "char_start": 174, "char_end": 348, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 65, "token_end": 104, "char_start": 355, "char_end": 530, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hao et al. (2019a)": "202539179"}, "Reference": {}}}]}
{"id": "202540407_0", "paragraph": "[BOS] There has been growing interest in improving the representation power of SANs (Dou et al., 2018 (Dou et al., , 2019 Yang et al., 2018; Wu et al., 2018; Yang et al., 2019a,b; Sukhbaatar et al., 2019) .\n[BOS] Among these approaches, a straightforward strategy is that augmenting the SANs with position representations (Shaw et al., 2018; Ma et al., 2019; Bello et al., 2019; , as the position representations involves elementwise attention computation.\n[BOS] In this work, we propose to augment SANs with structural position representations to model the latent structure of the input sentence.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 10, "token_end": 68, "char_start": 55, "char_end": 204, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dou et al., 2018": "53083122", "(Dou et al., , 2019": "69911484", "Yang et al., 2018;": "53081403", "Wu et al., 2018;": "53083604", "Sukhbaatar et al., 2019)": "159041867"}}}, {"token_start": 81, "token_end": 110, "char_start": 283, "char_end": 377, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Shaw et al., 2018;": "3725815", "Ma et al., 2019;": "174799954"}}}]}
{"id": "182952400_2", "paragraph": "[BOS] Multitask representation learning in NLP is well studied, and again can be traced back at least as far as Collobert et al. (2011 ).\n[BOS] Luong et al. (2016 show promising results combining translation and parsing; Subramanian et al. (2018) benefit from multitask learning in sentence-to-vector encoding; and Bingel and Sgaard (2017) and Changpinyo et al. (2018) offer studies of when multitask learning is helpful for lower-level NLP tasks.\n\n", "discourse_tags": ["Narrative_cite", "Multi_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 31, "char_start": 6, "char_end": 134, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 34, "token_end": 48, "char_start": 144, "char_end": 219, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 49, "token_end": 70, "char_start": 221, "char_end": 309, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Subramanian et al. (2018)": "4567927"}, "Reference": {}}}, {"token_start": 72, "token_end": 108, "char_start": 315, "char_end": 447, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Bingel and S\u00f8gaard (2017)": "3127682"}, "Reference": {}}}]}
{"id": "182952400_1", "paragraph": "[BOS] The successes of sentence encoder pretraining have sparked a line of work analyzing these models (Zhang and Bowman, 2018; Peters et al., 2018b; Tenney et al., 2019b; Peters et al., 2019; Tenney et al., 2019a; Liu et al., 2019, i.a.)\n[BOS] .\n[BOS] Our work also attempts to better understand what is learned by pretrained encoders, but we study this question entirely through the lens of pretraining and fine-tuning tasks, rather than architectures or specific linguistic capabilities.\n[BOS] Some of our experiments resemble those of Yogatama et al. (2019) , who also empirically investigate transfer performance with limited amounts of data and find similar evidence of catastrophic forgetting.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Reflection", "Single_summ"], "span_citation_mapping": [{"token_start": 6, "token_end": 77, "char_start": 23, "char_end": 238, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Tenney et al., 2019b;": "108300988", "Tenney et al., 2019a;": "155092004"}}}, {"token_start": 130, "token_end": 161, "char_start": 539, "char_end": 700, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "182952400_0", "paragraph": "[BOS] Work on reusable sentence encoders can be traced back at least as far as the multitask model of Collobert et al. (2011) .\n[BOS] Several works focused on learning reusable sentence-to-vector encodings, where the pretrained encoder produces a fixed-size representation for each input sentence (Dai and Le, 2015; Kiros et al., 2015; Hill et al., 2016; Conneau et al., 2017) .\n[BOS] More recent reusable sentence encoders such as CoVe (McCann et al., 2017) and GPT (Radford et al., 2018) instead represent sentences as sequences of vectors.\n[BOS] These methods work well, but most use distinct pretraining objectives, and none offers a substantial investigation of the choice of objective like we conduct here.\n[BOS] We build on two methods for pretraining sentence encoders on language modeling: ELMo and BERT.\n[BOS] ELMo consists of a forward and backward LSTM (Hochreiter and Schmidhuber, 1997), the hidden states of which are used to produce a contextual vector representation for each token in the inputted sequence.\n[BOS] ELMo is adapted to target tasks by freezing the model weights and only learning a set of task-specific scalar weights that are used to compute a linear combination of the LSTM layers.\n[BOS] BERT consists of a pretrained Transformer (Vaswani et al., 2017) , and is adapted to downstream tasks by fine-tuning the entire model.\n[BOS] Follow-up work has explored parameterefficient fine-tuning (Stickland and Murray, 2019; Houlsby et al., 2019) and better target task adaptation via multitask fine-tuning (Phang et al., 2018; Liu et al., 2019) , but work in this area is nascent.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Narrative_cite", "Transition", "Reflection", "Narrative_cite", "Transition", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 18, "token_end": 30, "char_start": 83, "char_end": 125, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 48, "token_end": 92, "char_start": 217, "char_end": 376, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 102, "token_end": 113, "char_start": 432, "char_end": 458, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 114, "token_end": 125, "char_start": 463, "char_end": 489, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 190, "token_end": 207, "char_start": 839, "char_end": 899, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 273, "token_end": 286, "char_start": 1239, "char_end": 1284, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vaswani et al., 2017)": "13756489"}}}, {"token_start": 308, "token_end": 332, "char_start": 1389, "char_end": 1470, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Stickland and Murray, 2019;": "59608712"}}}, {"token_start": 338, "token_end": 360, "char_start": 1509, "char_end": 1569, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "173991071_2", "paragraph": "[BOS] Our work shows that apart from strengthening the target-side decoder, direct supervision over the in-domain unseen words is essential for domain adaptation.\n[BOS] Similar to this, a variety of methods focus on solving OOV problems in translation.\n[BOS] Daum III and Jagarlamudi (2011) induce lexicons for unseen words and construct phrase tables for statistical machine translation.\n[BOS] However, it is nontrivial to integrate lexicon into NMT models that lack explicit use of phrase tables.\n[BOS] With regard to NMT, Arthur et al. (2016) use a lexicon to bias the probability of the NMT system and show promising improvements.\n[BOS] Luong and Manning (2015) propose to emit OOV target words by their corresponding source words and do post-translation for those OOV words with a dictionary.\n[BOS] Fadaee et al. (2017) propose an effective data augmentation method that generates sentence pairs containing rare words in synthetically created contexts, but this requires parallel training data not available in the fully unsupervised adaptation setting.\n[BOS] Arcan and Buitelaar (2017) leverage a domainspecific lexicon to replace unknown words after decoding.\n[BOS] Zhao et al. (2018) design a contextual memory module in an NMT system to memorize translations of rare words.\n[BOS] Kothur et al. (2018) treats an annotated lexicon as parallel sentences and continues training the NMT system on the lexicon.\n[BOS] Though all these works leverage a lexicon to address the problem of OOV words, none specifically target translating in-domain OOV words under a domain adaptation setting.\n\n", "discourse_tags": ["Reflection", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 50, "token_end": 98, "char_start": 259, "char_end": 498, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Daum\u00e9 III and Jagarlamudi (2011)": "272933"}, "Reference": {}}}, {"token_start": 105, "token_end": 130, "char_start": 525, "char_end": 634, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Arthur et al. (2016)": "10086161"}, "Reference": {}}}, {"token_start": 131, "token_end": 165, "char_start": 641, "char_end": 797, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Luong and Manning (2015)": null}, "Reference": {}}}, {"token_start": 166, "token_end": 209, "char_start": 804, "char_end": 1058, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Fadaee et al. (2017)": "3291104"}, "Reference": {}}}, {"token_start": 210, "token_end": 232, "char_start": 1065, "char_end": 1166, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 233, "token_end": 258, "char_start": 1173, "char_end": 1282, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhao et al. (2018)": "53080443"}, "Reference": {}}}, {"token_start": 259, "token_end": 285, "char_start": 1289, "char_end": 1413, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kothur et al. (2018)": "51799910"}, "Reference": {}}}]}
{"id": "173991071_1", "paragraph": "[BOS] Prior work on using monolingual data to do data augmentation could be easily adapted to the domain adaptation setting.\n[BOS] Early studies on databased methods such as self-enhancing (Schwenk, 2008; Lambert et al., 2011) translate monolingual source sentences by a statistical machine translation system, and continue training the system on the synthetic parallel data.\n[BOS] Recent databased methods such as back-translation (Sennrich et al., 2016a) and copy-based methods (Currey et al., 2017) mainly focus on improving fluency of the output sentences and translation of identical words, while our method targets OOV word translation.\n[BOS] In addition, there have been several attempts to do data augmentation using monolingual source sentences (Zhang and Zong, 2016; ChineaRios et al., 2017 ).\n[BOS] Besides, model-based methods change model architectures to leverage monolingual corpus by introducing an extra learning objective, such as auto-encoder objective (Cheng et al., 2016) and language modeling objective (Ramachandran et al., 2017) .\n[BOS] Another line of research on using monolingual data is unsupervised machine translation (Artetxe et al., 2018; Lample et al., 2018b,a; Yang et al., 2018) .\n[BOS] These methods use word-for-word translation as a component, but require a careful design of model architectures, and do not explicitly tackle the domain adaptation problem.\n[BOS] Our proposed data-based method does not depend on model architectures, which makes it orthogonal to these model-based methods.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Multi_summ", "Narrative_cite", "Narrative_cite", "Narrative_cite", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 24, "token_end": 72, "char_start": 131, "char_end": 375, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Schwenk, 2008;": "10849474", "Lambert et al., 2011)": "11120415"}, "Reference": {}}}, {"token_start": 79, "token_end": 131, "char_start": 415, "char_end": 642, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Sennrich et al., 2016a)": "15600925", "(Currey et al., 2017)": "40575489"}, "Reference": {}}}, {"token_start": 142, "token_end": 165, "char_start": 701, "char_end": 800, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhang and Zong, 2016;": "17667087"}}}, {"token_start": 191, "token_end": 203, "char_start": 949, "char_end": 992, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cheng et al., 2016)": null}}}, {"token_start": 204, "token_end": 218, "char_start": 997, "char_end": 1052, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ramachandran et al., 2017)": "3488076"}}}, {"token_start": 230, "token_end": 261, "char_start": 1115, "char_end": 1213, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Artetxe et al., 2018;": "3515219", "Yang et al., 2018)": "53080443"}}}]}
{"id": "173991071_0", "paragraph": "[BOS] There is much work on supervised domain adaptation setting where we have large out-of-domain parallel data and much smaller in-domain parallel data.\n[BOS] Luong and Manning (2015) propose training a model on an out-of-domain corpus and do finetuning with small sized in-domain parallel data to mitigate the domain shift problem.\n[BOS] Instead of naively mixing out-of-domain and in-domain data, Britz et al. (2017) circumvent the domain shift problem by jointly learning domain discrimination and the translation.\n[BOS] Joty et al. (2015) and Wang et al. (2017) address the domain adaptation problem by assigning higher weight to out-ofdomain parallel sentences that are close to the indomain corpus.\n[BOS] Our proposed method focuses on solving the adaptation problem with no in-domain parallel sentences, a strict unsupervised setting.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Multi_summ", "Reflection"], "span_citation_mapping": [{"token_start": 32, "token_end": 72, "char_start": 161, "char_end": 334, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Luong and Manning (2015)": null}, "Reference": {}}}, {"token_start": 73, "token_end": 111, "char_start": 341, "char_end": 519, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Britz et al. (2017)": "30042437"}, "Reference": {}}}, {"token_start": 112, "token_end": 154, "char_start": 526, "char_end": 706, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Joty et al. (2015)": "9074757", "Wang et al. (2017)": "9990193"}, "Reference": {}}}]}
{"id": "153312318_6", "paragraph": "[BOS] Another form of content transfer bridges across modalities: text generation given schematized or semi-structured information.\n[BOS] Recent research has addressed neural natural language generation techniques given a range of structured sources: selecting relevant database records and generating natural language descriptions of them (Mei et al., 2016) , selecting and describing slot-value pairs for task-specific dialog response generation (Wen et al., 2015) , and even generating Wikipedia biography abstracts given Infobox information (Lebret et al., 2016) .\n[BOS] Our task, while grounded in external content, is different in that it leverages linguistic grounding as well as prior text context when generating text.\n[BOS] This challenging setting enables a huge range of grounded generation tasks: there are vast amounts of unstructured textual data.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 39, "token_end": 59, "char_start": 251, "char_end": 358, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mei et al., 2016)": "1354459"}}}, {"token_start": 60, "token_end": 82, "char_start": 361, "char_end": 466, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wen et al., 2015)": "739696"}}}, {"token_start": 84, "token_end": 103, "char_start": 473, "char_end": 566, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lebret et al., 2016)": "1238927"}}}]}
{"id": "153312318_5", "paragraph": "[BOS] is a substantial difference.\n[BOS] Also our datasets are substantially larger, enabling generative models to be used in this space, where prior update summarization techniques have been primarily extractive (Fisher and Roark, 2008; Li et al., 2015) .\n[BOS] For any generation task, it is important to address both the content ('what' is being said) as well its style ('how' it is being said).\n[BOS] Recently, a great deal of research has focused on the 'how' (Li et al., 2018; Shen et al., 2017) , including efforts to collect a parallel dataset that differs in formality (Rao and Tetreault, 2018) , to control author characteristics in the generated sentences (Prabhumoye et al., 2018) , to control the perceived personality traits of dialog responses (Zhang et al., 2018) .\n[BOS] We believe this research thread is complementary to our efforts on generating the 'what'.\n\n", "discourse_tags": ["Reflection", "Narrative_cite", "Transition", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 26, "token_end": 51, "char_start": 144, "char_end": 254, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Fisher and Roark, 2008;": "16246383", "Li et al., 2015)": "15631314"}}}, {"token_start": 100, "token_end": 118, "char_start": 459, "char_end": 501, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Li et al., 2018;": "4937880", "Shen et al., 2017)": "7296803"}}}, {"token_start": 120, "token_end": 140, "char_start": 514, "char_end": 603, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rao and Tetreault, 2018)": "4859003"}}}, {"token_start": 142, "token_end": 161, "char_start": 609, "char_end": 692, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Prabhumoye et al., 2018)": "13959787"}}}, {"token_start": 163, "token_end": 179, "char_start": 698, "char_end": 779, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhang et al., 2018)": "6869582"}}}]}
{"id": "153312318_4", "paragraph": "[BOS] it's always the goal of the foreign-entry film award executive to be as possible.\"\n[BOS] Table 5 : Example generations from the CIG system, paired with the human generated updates.\n\n", "discourse_tags": ["Other", "Other"], "span_citation_mapping": []}
{"id": "153312318_3", "paragraph": "[BOS] Another closely related task is update summarization (Dang and Owczarzak, 2008) , where systems attempt to provide a brief summary of the novel information in a new article assuming the user has read a known set of prior documents.\n[BOS] Our focus on curating an authoritative resource on march 9, 2014, manning signed a one-year contract with the cincinnati bengals.\n[BOS] 4. on oct 10, 2013, barrett signed with the memphis grizzlies.\n[BOS] on feb 9, 2013, barrett signed with the memphis grizzlies.\n[BOS] 5. some people think elvis is still alive, but most of us think he's dead and gone.\"\n[BOS] some people think elvis, but most of us think he's dead and gone.\"\n[BOS] 6. it's always the goal of the foreign-language film award executive committee to be as inclusive as possible.\"\n\n", "discourse_tags": ["Single_summ", "Reflection", "Other", "Other", "Other", "Other", "Other"], "span_citation_mapping": [{"token_start": 2, "token_end": 51, "char_start": 6, "char_end": 237, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Dang and Owczarzak, 2008)": null}, "Reference": {}}}]}
{"id": "153312318_2", "paragraph": "[BOS] Looking beyond the clear task similarity of authoring Wikipedia style content, there are several crucial differences in our approach.\n[BOS] First, the goal of that paper is to author the whole page, starting from nothing more than a set of primary sources, such as news articles.\n[BOS] In practice, however, Wikipedia articles often contain information outside these primary sources, including common sense knowledge, framing statements to set the article in context, and inferences made from those primary sources.\n[BOS] Our task restricts the focus to content where a human editor explicitly decided to cite some external source.\n[BOS] Hence, it is much more likely that the resulting summary can be derived from the external source content.\n[BOS] Furthermore, we focus on the act of adding information to existing articles, rather than writing a complete article without any context.\n[BOS] These two scenarios are clearly useful yet complementary: sometimes people want to produce a new reference text where nothing existed before; in other cases the goal is to maintain and update an existing reference.\n\n", "discourse_tags": ["Reflection", "Transition", "Transition", "Reflection", "Reflection", "Reflection", "Transition"], "span_citation_mapping": []}
{"id": "153312318_1", "paragraph": "[BOS] Reference Update anne kirkbride died of breast cancer in a manchester hospital on 19 january 2015, aged 60.\n[BOS] Generated Update she was diagnosed with non-hodgkin's lymphoma.\n[BOS] culties of purely abstractive methods given quite large input token sequences.\n\n", "discourse_tags": ["Other", "Other", "Transition"], "span_citation_mapping": []}
{"id": "153312318_0", "paragraph": "[BOS] The proposed content transfer task is clearly related to a long series of papers in summarization, including recent work with neural techniques (Rush et al., 2015; Nallapati et al., 2016) .\n[BOS] In particular, one recent paper casts the the task of generating an entire Wikipedia article as a multidocument summarization problem (Liu et al., 2018) .\n[BOS] Their best-performing configuration was a two-stage extractive-abstractive framework; a multi-stage approach helped circumvent the diffiDocument (News Article) anne kirkbride, who portrayed bespectacled, gravelly-voiced deirdre barlow in coronation street for more that four decades, has died.\n[BOS] the 60-year-old, whose first appearance in the soap opera was in 1972, died in a manchester hospital after a short illness.... kirkbride had left the soap opera after she was diagnosed with non-hodgkin's lymphoma in 1993 but returned some months later after treatment and spoke candidly about how she had struggled with depression following the diagnosis... Curated Text (Wikipedia Context) in 1993, kirkbride was diagnosis with non-hodgkin's lymphoma.\n[BOS] she spoke to the british press about her bout of depression following the diagnosis.\n[BOS] she was cured within a year of being diagnosed.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Other", "Other", "Other", "Other"], "span_citation_mapping": [{"token_start": 24, "token_end": 44, "char_start": 132, "char_end": 193, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rush et al., 2015;": "1918428", "Nallapati et al., 2016)": "8928715"}}}, {"token_start": 65, "token_end": 78, "char_start": 300, "char_end": 354, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Liu et al., 2018)": "3608234"}}}]}
{"id": "174798398_2", "paragraph": "[BOS] Cross-sentence generating LVMs.\n[BOS] There exists some prior work on cross-sentence generating LVMs.\n[BOS] Shen et al. (2017) introduce a similar data generation assumption to ours and apply the idea to unaligned style transfer and natural language generation.\n[BOS] Serban et al. (2017) use latent variable models for machine translation and dialogue generation.\n[BOS] Kang et al. (2018) propose a data augmentation framework for natural language inference that generates a sentence, however unlabeled data are not considered in their work.\n[BOS] Deudon (2018) build a sentence-reformulating deep generative model whose objective is to measure the semantic similarity between a sentence pair.\n[BOS] However their work cannot be applied to a multi-class classification problem, and the generative objective is only used in pre-training, not considering the joint optimization of the generative and the discriminative objective.\n[BOS] To the best of our knowledge, our work is the first work on introducing the concept of crosssentence generating LVM to the semi-supervised text matching problem.\n[BOS] Table 5 : Results of evaluation of generated artificial datasets.\n[BOS] distinct-1 and distinct-2 compute the ratio of the number of unique unigrams or bigrams to that of the total generated tokens .\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection", "Other", "Other"], "span_citation_mapping": [{"token_start": 24, "token_end": 54, "char_start": 114, "char_end": 267, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Shen et al. (2017)": "7296803"}, "Reference": {}}}, {"token_start": 55, "token_end": 74, "char_start": 274, "char_end": 370, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Serban et al. (2017)": "14857825"}, "Reference": {}}}, {"token_start": 75, "token_end": 106, "char_start": 377, "char_end": 548, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 107, "token_end": 175, "char_start": 555, "char_end": 934, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "174798398_1", "paragraph": "[BOS] When the use of external corpora is allowed, the performance can further be increased.\n[BOS] Dai and Le (2015); Ramachandran et al. (2017) train an encoder-decoder network on large corpora and fine-tune the learned encoder on a specific task.\n[BOS] Recently, there have been remarkable improvements in pre-trained language representations (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018) , where language models trained on extremely large data brought a huge performance boost.\n[BOS] These methods are orthogonal to our work, and additional enhancements are expected when they are used together with our model.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 19, "token_end": 57, "char_start": 99, "char_end": 248, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ramachandran et al. (2017)": "3488076"}, "Reference": {}}}, {"token_start": 66, "token_end": 95, "char_start": 308, "char_end": 409, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Peters et al., 2018;": "3626819", "Radford et al., 2018;": null}}}]}
{"id": "174798398_0", "paragraph": "[BOS] Semi-supervised learning for text classification.\n[BOS] Using unlabeled data for text classification is an important subject and there exists much previous research (Zhu et al., 2003; Nigam et al., 2006; Zhu, 2008 , to name but a few).\n[BOS] Notably, the work of Xu et al. (2017) applies the semi-supervised VAE (Kingma et al., 2014) to the single-sentence text classification problem.\n[BOS] Zhao et al. (2018) ; Shen et al. (2018a) present VAE models for the semisupervised text sequence matching, while their models have drawbacks as mentioned in 3.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Multi_summ", "Multi_summ"], "span_citation_mapping": [{"token_start": 12, "token_end": 46, "char_start": 68, "char_end": 219, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhu et al., 2003;": "1052837", "Nigam et al., 2006;": null, "Zhu, 2008": null}}}, {"token_start": 60, "token_end": 92, "char_start": 269, "char_end": 391, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Xu et al. (2017)": "2060721"}, "Reference": {}}}, {"token_start": 93, "token_end": 131, "char_start": 398, "char_end": 557, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhao et al. (2018)": "21140441", "Shen et al. (2018a)": "1240989"}, "Reference": {}}}]}
{"id": "201666897_5", "paragraph": "[BOS] There are many prevailing unsupervised techniques for domain adaptation.\n[BOS] Our proposed approach is inspired by the seminal work of Ganin et al. (2016) to validate its potential of solving domain adaptation problem on a new task, without any supervision for the target domain.\n[BOS] There are also other more advanced methods, such as MMD-based adaptation (Long et al., 2017) , residual transfer network (Long et al., 2016) , and maximum classifier discrepancy (Saito et al., 2018 ) that can be explored for future work.\n\n", "discourse_tags": ["Transition", "Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 23, "token_end": 53, "char_start": 142, "char_end": 286, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ganin et al. (2016)": "2871880"}, "Reference": {}}}, {"token_start": 64, "token_end": 77, "char_start": 345, "char_end": 385, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 78, "token_end": 89, "char_start": 388, "char_end": 433, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 91, "token_end": 102, "char_start": 440, "char_end": 490, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "201666897_4", "paragraph": "[BOS] Domain Adaptation Domain adaptation aims to make a machine learning model generalizable to other domains, especially without any annotated data in the target domain (or with only limited data) (Ganin and Lempitsky, 2015) .\n[BOS] One line of research on domain adaptation focuses on transiting the feature distribution from the source domain to the target domain (Gong et al., 2012; Long et al., 2015) .\n[BOS] Another school of research focuses on learning domain-invariant representations (Glorot et al., 2011 ) (e.g., via adversarial learning (Ganin et al., 2016; Tzeng et al., 2017) ).\n[BOS] Domain adaptation has been successfully applied to many tasks, such as image classification (Tzeng et al., 2017) , speech recognition (Doulaty et al., 2015) , sentiment classification (Ganin et al., 2016; , machine translation (Johnson et al., 2017; Zoph et al., 2016) , relation extraction (Fu et al., 2017) , and paraphrase identification (Shah et al., 2018) .\n[BOS] Compared to these areas, the application to MRC presents additional challenges, since besides missing labeled data (i.e., answer spans), the questions in the target domain are also unavailable.\n[BOS] To our best knowledge, we are the first to investigate the usage of adversarial domain adaptation for the MRC task.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Narrative_cite", "Narrative_cite", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 4, "token_end": 45, "char_start": 24, "char_end": 226, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ganin and Lempitsky, 2015)": "6755881"}}}, {"token_start": 56, "token_end": 85, "char_start": 288, "char_end": 406, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Gong et al., 2012;": "6742009"}}}, {"token_start": 94, "token_end": 106, "char_start": 462, "char_end": 515, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Glorot et al., 2011": "18235792"}}}, {"token_start": 114, "token_end": 134, "char_start": 529, "char_end": 590, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ganin et al., 2016;": "2871880", "Tzeng et al., 2017)": "4357800"}}}, {"token_start": 149, "token_end": 160, "char_start": 671, "char_end": 712, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Tzeng et al., 2017)": "4357800"}}}, {"token_start": 161, "token_end": 173, "char_start": 715, "char_end": 756, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Doulaty et al., 2015)": "17829458"}}}, {"token_start": 174, "token_end": 184, "char_start": 759, "char_end": 803, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 186, "token_end": 204, "char_start": 807, "char_end": 868, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Johnson et al., 2017;": "6053988", "Zoph et al., 2016)": "16631020"}}}, {"token_start": 205, "token_end": 215, "char_start": 871, "char_end": 908, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Fu et al., 2017)": "31779941"}}}, {"token_start": 217, "token_end": 229, "char_start": 915, "char_end": 960, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Shah et al., 2018)": "52176498"}}}]}
{"id": "201666897_3", "paragraph": "[BOS] To the best of our knowledge, SynNet (Golub et al., 2017) is the only work that also studied domain adaptation for MRC.\n[BOS] Compared with SynNet, the key difference in our model is adversarial learning, which enables domain-invariant representation learning for better model adaptation to low-resource domains.\n[BOS] Our approach is also related to multi-task learning (Xu et al., 2019; Caruana, 1997; Liu et al., 2015 and semisupervised learning for MRC.\n[BOS] In this work, we focus on purely unsupervised domain adaptation.\n\n", "discourse_tags": ["Single_summ", "Reflection", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 9, "token_end": 32, "char_start": 36, "char_end": 125, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Golub et al., 2017)": "20298653"}, "Reference": {}}}, {"token_start": 73, "token_end": 98, "char_start": 357, "char_end": 426, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Xu et al., 2019;": "84846285", "Caruana, 1997;": null, "Liu et al., 2015": "11754890"}}}]}
{"id": "201666897_2", "paragraph": "[BOS] Although important for the MRC task, where annotated data are limited in real-life applications, this problem has not yet been well investigated.\n[BOS] There were some relevant studies along this line.\n[BOS] For example, Chung et al. (2018) adapted a pretrained model to TOEFL and MCTest dataset, and Wiese et al. (2017) applied transfer learning to the biomedical domain.\n[BOS] However, both studies assumed that annotated data in the target domain (either questions or question-answer pairs) are available.\n\n", "discourse_tags": ["Transition", "Transition", "Multi_summ", "Transition"], "span_citation_mapping": [{"token_start": 43, "token_end": 62, "char_start": 227, "char_end": 301, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Chung et al. (2018)": "3453374"}, "Reference": {}}}, {"token_start": 64, "token_end": 80, "char_start": 307, "char_end": 378, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wiese et al. (2017)": "1915951"}, "Reference": {}}}]}
{"id": "201666897_1", "paragraph": "[BOS] to answer a question by extracting a text snippet within a given passage as the answer.\n[BOS] A large number of deep learning models have been proposed to tackle this task (Seo et al., 2017; Xiong et al., 2017; Shen et al., 2017; Yu et al., 2018) .\n[BOS] However, the success of these methods largely relies on large-scale humanannotated datasets (such as SQuAD (Rajpurkar et al., 2016) , NewsQA (Trischler et al., 2016) and MS MARCO (Bajaj et al., 2016) ).\n[BOS] Different from previous work that focused on improving the state of the art on particular MRC datasets, we study the MRC task from a different angle, and aim at addressing a critical yet challenging problem: how to transfer an MRC model learned from a high-resource domain to other lowresource domains in an unsupervised manner.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 23, "token_end": 67, "char_start": 102, "char_end": 252, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Seo et al., 2017;": "8535316", "Xiong et al., 2017;": "3714278", "Shen et al., 2017;": "6300274", "Yu et al., 2018)": "3453374"}}}, {"token_start": 90, "token_end": 102, "char_start": 362, "char_end": 392, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 103, "token_end": 115, "char_start": 395, "char_end": 426, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Trischler et al., 2016)": "1167588"}}}, {"token_start": 116, "token_end": 129, "char_start": 431, "char_end": 460, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bajaj et al., 2016)": "1289517"}}}]}
{"id": "201666897_0", "paragraph": "[BOS] Machine Reading Comprehension The MRC task has recently attracted a lot of attention in the community.\n[BOS] An MRC system is required Figure 1: t-SNE plot of encoded feature representations from (a) SynNet (Golub et al., 2017) and (b) the proposed AdaMRC.\n[BOS] We sampled 100 data points, each from the development set of the source and the target domains.\n[BOS] Blue: SQuAD.\n[BOS] Red: NewsQA.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 41, "token_end": 52, "char_start": 206, "char_end": 233, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Golub et al., 2017)": "20298653"}}}]}
{"id": "184482696_1", "paragraph": "[BOS] Many of the last approaches rely on neural network models.\n[BOS] For instance, the work of (Ganesan et al., 2018 ) presents a Multi-Layer Feedforward Neural Networks.\n[BOS] Moreover, (Park and Fung, 2017) proposes to implement three models based on Convolutional Neural Networks (CNN) to classify sexist and racist abusive language: Char-CNN, WordCNN, and HybridCNN.\n[BOS] It work reports that can boost the performance of simpler models.\n[BOS] Also, (Pitsilis et al., 2018) proposes a detection scheme that is an ensemble of Recurrent Neural Network classifiers.\n[BOS] It incorporates various features associated with user-related information.\n[BOS] They report that the scheme can successfully distinguish racism and sexism messages from normal text.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 20, "token_end": 39, "char_start": 97, "char_end": 172, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Ganesan et al., 2018": null}, "Reference": {}}}, {"token_start": 42, "token_end": 99, "char_start": 189, "char_end": 444, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Park and Fung, 2017)": "687037"}, "Reference": {}}}, {"token_start": 102, "token_end": 157, "char_start": 457, "char_end": 758, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Pitsilis et al., 2018)": "12985409"}, "Reference": {}}}]}
{"id": "184482696_0", "paragraph": "[BOS] Some approaches have been proposed to tackle the problem of offensive language detection.\n[BOS] It is the case of recent works (Waseem et al., 2017; ElSherief et al., 2018; Gambck and Sikdar, 2017; Zhang et al., 2018) and surveys (Schmidt and Wiegand, 2017) and (Fortuna and Nunes, 2018) .\n[BOS] There are even studies on languages other than English such as (Su et al., 2017) on Chinese and (Fier et al., 2017) on Slovene.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 23, "token_end": 61, "char_start": 120, "char_end": 223, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Waseem et al., 2017;": "8821211", "ElSherief et al., 2018;": "4809781", "Gamb\u00e4ck and Sikdar, 2017;": null, "Zhang et al., 2018)": "46939253"}}}, {"token_start": 62, "token_end": 82, "char_start": 228, "char_end": 293, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Schmidt and Wiegand, 2017)": "9626793", "(Fortuna and Nunes, 2018)": "52184457"}}}, {"token_start": 95, "token_end": 105, "char_start": 365, "char_end": 393, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Su et al., 2017)": "32076269"}}}, {"token_start": 106, "token_end": 120, "char_start": 398, "char_end": 429, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Fi\u0161er et al., 2017)": "28550367"}}}]}
{"id": "203610361_5", "paragraph": "[BOS] Strongly related to our work is which presents HotpotQA, a novel dataset for multi-hop QA.\n[BOS] The authors highlight the importance of identifying supporting facts for improving reasoning and explainability of current systems.\n[BOS] We compare the proposed architecture with the baseline described in their paper.\n[BOS] The model is based on a state-of-the-art MC model (Seo et al., 2016) that adopts a sequential reading strategy to identifying supporting facts from large collections of documents.\n\n", "discourse_tags": ["Transition", "Transition", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 64, "token_end": 82, "char_start": 352, "char_end": 396, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Seo et al., 2016)": "8535316"}}}]}
{"id": "203610361_4", "paragraph": "[BOS] Another line of research focuses on narrowing down the context for later Machine Comprehension models by selecting relevant passages as supporting facts.\n[BOS] Work in that direction includes (Watanabe et al., 2017) which present a neural information retrieval system to retrieve a sufficiently small paragraph and (Geva and Berant, 2018) which employ a Deep Q-Network (DQN) to solve the task by learning to navigate over an intra-document tree.\n[BOS] A similar approach is chosen by (Clark and Gardner, 2017) .\n[BOS] However, instead of operating on document structure, they adopt a sampling technique to make the model more robust towards multi-paragraph documents.\n[BOS] These approaches are not directly comparable to our work since they focus either on single paragraphs or intra-document (local) structure.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Single_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 31, "token_end": 52, "char_start": 198, "char_end": 316, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Watanabe et al., 2017)": "12626900"}, "Reference": {}}}, {"token_start": 53, "token_end": 88, "char_start": 321, "char_end": 451, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Geva and Berant, 2018)": "47017117"}, "Reference": {}}}, {"token_start": 90, "token_end": 131, "char_start": 460, "char_end": 673, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Clark and Gardner, 2017)": "223637"}, "Reference": {}}}]}
{"id": "203610361_3", "paragraph": "[BOS] The proposed approach is similar to (De Cao et al., 2018) and (Song et al., 2018) , where the aim is to answer complex questions that require the integration of multiple text passages.\n[BOS] However, our research is focused on the identification of supporting facts instead of answer retrieval.\n\n", "discourse_tags": ["Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 8, "token_end": 17, "char_start": 42, "char_end": 63, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(De Cao et al., 2018)": "52116920"}}}, {"token_start": 18, "token_end": 26, "char_start": 68, "char_end": 87, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Song et al., 2018)": "52172080"}}}]}
{"id": "203610361_2", "paragraph": "[BOS] Our research is in line with recent trends in Question Answering prone to explore messagepassing architectures over graph-structured representation of documents to enhance performance and overcome challenges involved in dealing with unstructured text.\n[BOS] fuse text corpus with manually-curated knowledge bases to create heterogeneous graphs of KB facts and text sentences.\n[BOS] Their model, GRAFT-Net, built upon Graph Convolutional Networks (Schlichtkrull et al., 2018) , is used to propagate information between heterogeneous nodes in the graph and perform binary classification on entity nodes to select the answer.\n[BOS] Differently from the proposed approach, the latter work focuses on links between whole paragraphs and external entities in a Knowledge Base.\n[BOS] Moreover, GRAFT-Net is designed for single-hop Question Answering, assuming that the question is always about a single entity.\n\n", "discourse_tags": ["Reflection", "Transition", "Narrative_cite", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 72, "token_end": 86, "char_start": 423, "char_end": 480, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Schlichtkrull et al., 2018)": "207908316"}}}]}
{"id": "203610361_1", "paragraph": "[BOS] Research efforts towards the creation of message-passing architectures with relational inductive bias (Battaglia et al., 2018) have enabled machine learning algorithms to incorporate graphical structures in their training process.\n[BOS] These models, trained over explicit entities and relations, have the potential to boost generalisation, interpretability and abstract reasoning capabilities.\n[BOS] A variety of Graph Neural Network architectures have already demonstrated remarkable results in a large set of applications ranging from Computer Vision, Physical Systems and Protein-Protein In-teraction (Zhou et al., 2018) .\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 6, "token_end": 26, "char_start": 35, "char_end": 132, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Battaglia et al., 2018)": "46935302"}}}, {"token_start": 93, "token_end": 108, "char_start": 582, "char_end": 630, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhou et al., 2018)": "56517517"}}}]}
{"id": "203610361_0", "paragraph": "[BOS] State-of-the-art approaches for Open-Domain Question Answering over large collections of documents employ a combination of character-level models, self-attention (Wang et al., 2017) , and biattention (Seo et al., 2016) to operate over unstructured paragraphs without exploiting any structured text representation.\n[BOS] Despite these methods have demonstrated impressive results reaching in some cases super-human performances (Seo et al., 2016; Chen et al., 2017; , recent studies have raised important concerns related to generalisation (Wiese et al., 2017; complex reasoning (Welbl et al., 2018) and explainability .\n[BOS] Specifically, the lack of structured representation makes it hard for current Machine Comprehension models to find meaningful patterns in large corpora, generalise beyond the training domain and justify the answer.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 30, "token_end": 41, "char_start": 153, "char_end": 187, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wang et al., 2017)": "12501880"}}}, {"token_start": 43, "token_end": 55, "char_start": 194, "char_end": 224, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Seo et al., 2016)": "8535316"}}}, {"token_start": 79, "token_end": 98, "char_start": 408, "char_end": 469, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Seo et al., 2016;": "8535316"}}}, {"token_start": 108, "token_end": 118, "char_start": 530, "char_end": 564, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 119, "token_end": 130, "char_start": 566, "char_end": 604, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Welbl et al., 2018)": null}}}]}
{"id": "184482872_0", "paragraph": "[BOS] In this part, we briefly review the prior work in language representation as well as the semisupervised learning method we used.\n[BOS] (Kiros et al., 2015) tried to learn sentence embedding by reconstructing the surrounding sentences of an encoded passage.\n[BOS] (Peters et al., 2018) proposed to extract context-sensitive features from a language model.\n[BOS] (Devlin et al., 2018) jointly conditioned on both left and right context and obtained state-of-the-art results on eleven natural language processing tasks.\n[BOS] (Triguero et al., 2015) provided a survey of selflabeled methods for semi-supervised classification.\n[BOS] (Zhu and Goldberg, 2009) showed selflabeled techniques are typically divided into selftraining and co-training.\n[BOS] (Lin et al., 2018) proposed semi-supervised learning to leverage a small amount of user-comment data to train a model and then expand the dataset by that trained model.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 27, "token_end": 52, "char_start": 141, "char_end": 262, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Kiros et al., 2015)": "9126867"}, "Reference": {}}}, {"token_start": 53, "token_end": 73, "char_start": 269, "char_end": 360, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Peters et al., 2018)": "3626819"}, "Reference": {}}}, {"token_start": 74, "token_end": 108, "char_start": 367, "char_end": 522, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Devlin et al., 2018)": "52967399"}, "Reference": {}}}, {"token_start": 109, "token_end": 132, "char_start": 529, "char_end": 629, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Triguero et al., 2015)": "1955810"}, "Reference": {}}}, {"token_start": 133, "token_end": 155, "char_start": 636, "char_end": 747, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Zhu and Goldberg, 2009)": null}, "Reference": {}}}, {"token_start": 156, "token_end": 193, "char_start": 754, "char_end": 922, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Lin et al., 2018)": "53099675"}, "Reference": {}}}]}
{"id": "202538740_1", "paragraph": "[BOS] Concurrently with our work, Song et al. (2019) extend the approach of Lample and Conneau (2019) by using separate encoder and decoder parameters (as in our model) and pre-training them jointly in an autoregressive version of masked language modeling, although with monolingual data.\n[BOS] While this work demonstrates that pretraining CMLMs can improve autoregressive machine translation, it does not try to leverage the parallel and bi-directional nature of CMLMs to generate text in a non-left-to-right manner.\n\n", "discourse_tags": ["Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 111, "char_start": 6, "char_end": 518, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Song et al. (2019)": "146808476"}, "Reference": {"Lample and Conneau (2019)": null}}}]}
{"id": "202538740_0", "paragraph": "[BOS] Training Masked Language Models with Translation Data Recent work by Lample and Conneau (2019) shows that training a masked language model on sentence-pair translation data, as a pre-training step, can improve performance on cross-lingual tasks, including autoregressive machine translation.\n[BOS] Our training scheme builds on their work, with the following differences: we use separate model parameters for source and target texts (encoder and decoder), and we also use a different masking scheme.\n[BOS] Specifically, we mask a varying percentage of tokens, only from the target, and do not replace input tokens with noise.\n[BOS] Most importantly, the goal of our work is different; we do not use CMLMs for pre-training, but to directly generate text with mask-predict decoding.\n\n", "discourse_tags": ["Single_summ", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 59, "char_start": 6, "char_end": 297, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Lample and Conneau (2019)": null}, "Reference": {}}}]}
{"id": "166228668_0", "paragraph": "[BOS] We introduce several related works about data augmentation for NMT.\n[BOS] Artetxe et al. (2017) and Lample et al. (2017) randomly shuffle (swap) the words in a sentence, with constraint that the words will not be shuffled further than a fixed small window size.\n[BOS] Iyyer et al. (2015) and Lample et al. (2017) randomly drop some words in the source sentence for learning an autoencoder to help train the unsupervised NMT model.\n[BOS] In Xie et al. (2017) , they replace the word with a placeholder token or a word sampled from the frequency distribution of vocabulary, showing that data noising is an effective regularizer for NMT.\n[BOS] Fadaee et al. (2017) propose to replace a common word by low-frequency word in the target sentence, and change its corresponding word in the source sentence to improve translation quality of rare words.\n[BOS] Most recently, Kobayashi (2018) propose an approach to use the prior knowledge from a bi-directional language model to replace a word token in the sentence.\n[BOS] Our work differs from their work that we use a soft distribution to replace the word representation instead of a word token.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Multi_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 15, "token_end": 63, "char_start": 80, "char_end": 267, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Artetxe et al. (2017)": "3515219", "Lample et al. (2017)": "3518190"}, "Reference": {}}}, {"token_start": 64, "token_end": 104, "char_start": 274, "char_end": 436, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Iyyer et al. (2015)": null, "Lample et al. (2017)": "3518190"}, "Reference": {}}}, {"token_start": 106, "token_end": 148, "char_start": 446, "char_end": 640, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Xie et al. (2017)": "10635893"}, "Reference": {}}}, {"token_start": 149, "token_end": 191, "char_start": 647, "char_end": 849, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Fadaee et al. (2017)": null}, "Reference": {}}}, {"token_start": 195, "token_end": 223, "char_start": 871, "char_end": 1012, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kobayashi (2018)": "21725995"}, "Reference": {}}}]}
{"id": "196178045_3", "paragraph": "[BOS] There are some other directions to enhance generative models by adding additional information.\n[BOS] Some of them introduce the knowledge base to conversation models, which provide task-specific knowledge (Madotto et al., 2018; Wu et al., 2019) or lexical knowledge to text generation (Young et al., 2018; Parthasarathi and Pineau, 2018) .\n[BOS] Some other directions are to maintain sample-level temporary memory.\n[BOS] Weston et al. (2014) , Shang et al. (2015) , , Tian et al. (2017) and Le et al. (2018) memorize the previous utterances in a multi-round session.\n[BOS] Unlike them, our model brings in the corpus-level memory and does not rely on any external resource.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Multi_summ", "Reflection"], "span_citation_mapping": [{"token_start": 30, "token_end": 52, "char_start": 187, "char_end": 250, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Madotto et al., 2018;": "5068596", "Wu et al., 2019)": "58006691"}}}, {"token_start": 53, "token_end": 76, "char_start": 254, "char_end": 343, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Young et al., 2018;": null, "Parthasarathi and Pineau, 2018)": "52281610"}}}, {"token_start": 91, "token_end": 137, "char_start": 427, "char_end": 572, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Shang et al. (2015)": "7356547", "Tian et al. (2017)": "13100783", "Le et al. (2018)": null}, "Reference": {}}}]}
{"id": "196178045_2", "paragraph": "[BOS] Recently, several researchers (Song et al., 2018; Li et al., 2017; Zhuang et al., 2017) propose to merge retrieval-based models and generative models.\n[BOS] Cai et al. (2018) generate the response skeleton from the retrieved results and revise the skeletons to get the response.\n[BOS] Guu et al. (2018) use the Seq2Seq model to edit a prototype from the retrieved results for text generation and leverage context lexical differences to edit prototypes for conversation.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 5, "token_end": 40, "char_start": 24, "char_end": 156, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Song et al., 2018;": "51879945", "Li et al., 2017;": "26030198", "Zhuang et al., 2017)": "2699619"}}}, {"token_start": 41, "token_end": 67, "char_start": 163, "char_end": 284, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Cai et al. (2018)": "52281331"}, "Reference": {}}}, {"token_start": 68, "token_end": 104, "char_start": 291, "char_end": 475, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Guu et al. (2018)": "2318481"}, "Reference": {}}}]}
{"id": "196178045_1", "paragraph": "[BOS] The retrieval-based models are another branch in building dialogue systems.\n[BOS] Ji et al. (2014) propose to apply information retrieval techniques to search for related queries and replies.\n[BOS] Zhou et al. (2016) and improve it by neural networks.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 16, "token_end": 38, "char_start": 88, "char_end": 197, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ji et al. (2014)": "18380963"}, "Reference": {}}}, {"token_start": 39, "token_end": 53, "char_start": 204, "char_end": 257, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhou et al. (2016)": "2867243"}, "Reference": {}}}]}
{"id": "196178045_0", "paragraph": "[BOS] Generative models build dialogue systems via end-to-end training.\n[BOS] Ritter et al. (2011) first regard response generation as query-to-response translation.\n[BOS] Following that, Shang et al. (2015) implement an end-to-end dialogue system borrowing the Seq2Seq model, while Li et al. (2016b) replace the maximum likelihood criterion with maximum mutual information (MMI) to deal with the universal response issue of the seq2seq.\n\n", "discourse_tags": ["Transition", "Single_summ", "Multi_summ"], "span_citation_mapping": [{"token_start": 16, "token_end": 36, "char_start": 78, "char_end": 165, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ritter et al. (2011)": null}, "Reference": {}}}, {"token_start": 40, "token_end": 63, "char_start": 188, "char_end": 275, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Shang et al. (2015)": "7356547"}, "Reference": {}}}, {"token_start": 65, "token_end": 99, "char_start": 283, "char_end": 437, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "203689636_1", "paragraph": "[BOS] Ensemble learning has received more attention in the field of neural machine translation.\n[BOS] A common approach is to combine predictions of several models in beam search during decoding (Denkowski and Neubig, 2017) .\n[BOS] Another approach is to train several models and then distill them into a single model (Denkowski and Neu-big, 2017) .\n[BOS] The simplest approach to distillation is to average the parameters of the different models.\n[BOS] While these techniques could be applied in MI, the focus of this paper is to explore ensemble methods which do not require any changes to the underlying model architecture.\n[BOS] Therefore, such methods fall outside of the scope of our work.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite", "Transition", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 22, "token_end": 41, "char_start": 126, "char_end": 223, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Denkowski and Neubig, 2017)": "8140780"}}}, {"token_start": 47, "token_end": 69, "char_start": 255, "char_end": 347, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "203689636_0", "paragraph": "[BOS] Following Kann and Schtze (2016) and many others, we explore learning of MI systems in the context of bidirectional LSTM encoder-decoder models with attention.\n[BOS] Several papers have employed straightforward majority voting for the task of MI (Kann and Schtze, 2016; Makarov and Clematide, 2018; Kementchedjhieva et al., 2018; Sharma et al., 2018) .\n[BOS] However, work on more advanced ensembling methods is scarce for the MI task.\n[BOS] Najafi et al. (2018) and Silfverberg et al. (2017) explored weighted variants of majority voting.\n[BOS] Both of these approaches are based on weighting models according to their performance on a heldout development set.\n[BOS] Silfverberg et al. (2017) use sampling-based methods for finding good weighting coefficients for the component models in an ensemble.\n[BOS] Najafi et al. (2018) instead simply weight models according to their accuracy on the development set.\n[BOS] We opt for using the latter weighing scheme in our experiments because Silfverberg et al. (2017) report that the samplingbased method can sometimes overfit the development set which leads to poor performance on the test set.\n[BOS] Najafi et al. (2018) combined different types of models, both neural and non-neural, in their ensemble but we apply their technique in a purely neural setting.\n\n", "discourse_tags": ["Reflection", "Narrative_cite", "Transition", "Multi_summ", "Multi_summ", "Single_summ", "Single_summ", "Reflection", "Single_summ"], "span_citation_mapping": [{"token_start": 3, "token_end": 11, "char_start": 16, "char_end": 38, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Kann and Sch\u00fctze (2016)": "16406435"}}}, {"token_start": 36, "token_end": 86, "char_start": 172, "char_end": 356, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kann and Sch\u00fctze, 2016;": "16406435", "Makarov and Clematide, 2018;": "53102983", "Kementchedjhieva et al., 2018;": "52164624"}}}, {"token_start": 105, "token_end": 153, "char_start": 448, "char_end": 667, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Najafi et al. (2018)": "53092701", "Silfverberg et al. (2017)": "19265222"}, "Reference": {}}}, {"token_start": 154, "token_end": 182, "char_start": 674, "char_end": 807, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Silfverberg et al. (2017)": "19265222"}, "Reference": {}}}, {"token_start": 183, "token_end": 205, "char_start": 814, "char_end": 915, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Najafi et al. (2018)": "53092701"}, "Reference": {}}}, {"token_start": 218, "token_end": 251, "char_start": 993, "char_end": 1146, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 252, "token_end": 288, "char_start": 1153, "char_end": 1312, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Najafi et al. (2018)": "53092701"}, "Reference": {}}}]}
{"id": "170078852_4", "paragraph": "[BOS] Finally, paraphrasing can also impact downstream tasks, e.g. to generate additional training data by paraphrasing training sentences (Marton et al., 2009; Zhang et al., 2015; Yu et al., 2018) .\n[BOS] We evaluate this impact for classification tasks.\n\n", "discourse_tags": ["Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 18, "token_end": 51, "char_start": 70, "char_end": 197, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Marton et al., 2009;": "2695216", "Zhang et al., 2015;": "368182", "Yu et al., 2018)": "4842909"}}}]}
{"id": "170078852_3", "paragraph": "[BOS] In addition to generation, probabilistic models can be assessed through scoring.\n[BOS] For a sentence pair (x, y), the model estimate of P (y|x) can be used to discriminate between paraphrase and non-paraphrase pairs (Dolan and Brockett, 2005) .\n[BOS] The correlation of model scores with human judgments (Cer et al., 2017) can also be assessed.\n[BOS] We report both types of evaluation.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 40, "token_end": 62, "char_start": 166, "char_end": 249, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dolan and Brockett, 2005)": "16639476"}}}, {"token_start": 70, "token_end": 85, "char_start": 295, "char_end": 351, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cer et al., 2017)": "4421747"}}}]}
{"id": "170078852_2", "paragraph": "[BOS] Although less common, monolingual neural sequence models have also been proposed.\n[BOS] In supervised settings, Prakash et al. (2016) ; Gupta et al. (2018) learn sequence-to-sequence models on paraphrase data.\n[BOS] In unsupervised settings, Bowman et al. (2016) apply a VAE to paraphrase detection while Li et al. (2017) train a paraphrase generator with adversarial training.\n[BOS] Paraphrase Evaluation Evaluation can be performed by human raters, evaluating both text fluency and semantic similarity.\n[BOS] Automatic evaluation is more challenging but necessary for system development and larger scale statistical analysis (Callison-Burch, 2007; Madnani and J. Dorr, 2010) .\n[BOS] Automatic evaluation and generation are actually linked: if an automated metric would reliably assess the semantic similarity and fluency of a pair of sentences, one would generate by searching the space of sentences to maximize that metric.\n[BOS] Automated evaluation can report the overlap with a reference paraphrase, like for translation (Papineni et al., 2002) or summarization (Lin, 2004) .\n[BOS] BLEU, METEOR and TER metrics have been used (Prakash et al., 2016; Gupta et al., 2018) .\n[BOS] These metrics do not evaluate whether the generated paraphrase differs from the input sentence and large amount of input copying is not penalized.\n[BOS] Galley et al. (2015) compare overlap with multiple references, weighted by quality; while Sun and Zhou (2012) explicitly penalize overlap with the input sentence.\n[BOS] Grangier and Auli (2018) alternatively compare systems which have first been calibrated to a reference level of overlap with the input.\n[BOS] We follow this strategy and calibrate the generation overlap to match the average overlap observed in paraphrases from humans.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Multi_summ", "Transition", "Multi_summ", "Transition", "Narrative_cite", "Multi_summ", "Transition", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 21, "token_end": 50, "char_start": 118, "char_end": 215, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Prakash et al. (2016)": "9385494", "Gupta et al. (2018)": "12737290"}, "Reference": {}}}, {"token_start": 55, "token_end": 91, "char_start": 248, "char_end": 383, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Bowman et al. (2016)": "748227", "Li et al. (2017)": "21646317"}, "Reference": {}}}, {"token_start": 125, "token_end": 149, "char_start": 599, "char_end": 682, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Callison-Burch, 2007;": null, "Madnani and J. Dorr, 2010)": "17652653"}}}, {"token_start": 192, "token_end": 226, "char_start": 939, "char_end": 1085, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Papineni et al., 2002)": "11080756", "(Lin, 2004)": null}}}, {"token_start": 228, "token_end": 254, "char_start": 1094, "char_end": 1180, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Prakash et al., 2016;": "9385494", "Gupta et al., 2018)": "12737290"}}}, {"token_start": 284, "token_end": 301, "char_start": 1342, "char_end": 1424, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 303, "token_end": 318, "char_start": 1432, "char_end": 1504, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Sun and Zhou (2012)": "434479"}, "Reference": {}}}, {"token_start": 319, "token_end": 346, "char_start": 1511, "char_end": 1646, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Grangier and Auli (2018)": "4382470"}, "Reference": {}}}]}
{"id": "170078852_1", "paragraph": "[BOS] Machine translation later emerged as a dominant method for paraphrase generation.\n[BOS] Bannard and Callison-Burch (2005) identify equivalent English phrases mapping to the same non-English phrases from an MT phrase table.\n[BOS] Kok and Brockett (2010) performs random walks across multiple phrase tables.\n[BOS] Translation-based paraphrasing has recently benefited from neural networks for MT Vaswani et al., 2017) .\n[BOS] Neural MT can generate paraphrase pairs by translating one side of a parallel corpus Iyyer et al., 2018) .\n[BOS] Paraphrase generation with pivot/round-trip neural translation has also been used (Mallinson et al., 2017; Yu et al., 2018) .\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Narrative_cite", "Single_summ", "Multi_summ"], "span_citation_mapping": [{"token_start": 17, "token_end": 46, "char_start": 94, "char_end": 228, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Bannard and Callison-Burch (2005)": "15728911"}, "Reference": {}}}, {"token_start": 47, "token_end": 64, "char_start": 235, "char_end": 311, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kok and Brockett (2010)": "6650023"}, "Reference": {}}}, {"token_start": 76, "token_end": 89, "char_start": 377, "char_end": 421, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 91, "token_end": 115, "char_start": 430, "char_end": 534, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Iyyer et al., 2018)": "4956100"}, "Reference": {}}}, {"token_start": 117, "token_end": 150, "char_start": 543, "char_end": 666, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mallinson et al., 2017;": "17246494", "Yu et al., 2018)": "4842909"}}}]}
{"id": "170078852_0", "paragraph": "[BOS] Paraphrase Generation Paraphrases express the same content with alternative surface forms.\n[BOS] Their automatic generation has been studied for decades: rule-based (McKeown, 1980; Meteer and Shaked, 1988) and data-driven methods (Madnani and J. Dorr, 2010) have been explored.\n[BOS] Data-driven approaches have considered different source of training data, including multiple translations of the same text (Barzilay and McKeown, 2001; Pang et al., 2003) or alignments of comparable corpora, such as news from the same period (Dolan et al., 2004; Barzilay and Lee, 2003) .\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 28, "token_end": 46, "char_start": 160, "char_end": 211, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(McKeown, 1980;": "462954", "Meteer and Shaked, 1988)": "5076418"}}}, {"token_start": 47, "token_end": 63, "char_start": 216, "char_end": 263, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 81, "token_end": 106, "char_start": 374, "char_end": 460, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Barzilay and McKeown, 2001;": "9842595", "Pang et al., 2003)": "11728052"}}}, {"token_start": 107, "token_end": 136, "char_start": 464, "char_end": 576, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dolan et al., 2004;": "10181753", "Barzilay and Lee, 2003)": "6387310"}}}]}
{"id": "196173919_3", "paragraph": "[BOS] beddings worked the best.\n[BOS] Subsequent work by Zhu et al. (2018) obtained state-of-the-art results on the same task using contextual embeddings (ELMo) that were pre-trained on a large in-domain corpus made of medical articles from Wikipedia and clinical notes from MIMIC-III (Johnson et al., 2016) .\n[BOS] More recently, these embeddings were outperformed by BERT representations pre-trained on MIMIC-III, proving once more the value of large in-domain corpora (Si et al., 2019) .\n[BOS] 2 While interesting for the clinical domain, these strategies may not always be applicable to other specialized fields since large in-domain corpora like MIMIC-III will rarely be available.\n[BOS] To deal with this issue, we explore embedding combinations 3 .\n[BOS] In this respect, we consider both static forms of combination explored in (Yin and Schtze, 2016; Muromgi et al., 2017; Bollegala et al., 2018) and more dynamic modes of combination that can be found in and (Kiela et al., 2018) .\n[BOS] In this work, we show in particular how a combination of general-domain contextual embeddings, fine-tuning, and in-domain static embeddings trained on a small corpus can be employed to reach a similar performance using resources that are available for any domain.\n\n", "discourse_tags": ["Transition", "Single_summ", "Narrative_cite", "Transition", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 12, "token_end": 72, "char_start": 57, "char_end": 307, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhu et al. (2018)": "53029378"}, "Reference": {"(Johnson et al., 2016)": "33285731"}}}, {"token_start": 99, "token_end": 113, "char_start": 447, "char_end": 488, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Si et al., 2019)": "67856085"}}}, {"token_start": 170, "token_end": 203, "char_start": 796, "char_end": 904, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yin and Sch\u00fctze, 2016;": "2820782", "Murom\u00e4gi et al., 2017;": "17731007"}}}, {"token_start": 205, "token_end": 224, "char_start": 914, "char_end": 988, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "196173919_2", "paragraph": "[BOS] The patient had headache that was relieved only with oxycodone .\n[BOS] A CT scan of the head showed microvascular ischemic changes .\n[BOS] A followup MRI which also showed similar changes .\n[BOS] This was most likely due to her multiple myeloma with hyperviscosity .\n[BOS] (Roberts, 2016) .\n\n", "discourse_tags": ["Other", "Other", "Other", "Other", "Narrative_cite"], "span_citation_mapping": [{"token_start": 55, "token_end": 60, "char_start": 279, "char_end": 294, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Roberts, 2016)": "14700403"}}}]}
{"id": "196173919_1", "paragraph": "[BOS] DERMOPLAST TOPICAL TP Q12H PRN Pain DOCUSATE SODIUM 100 MG PO BID PRN Constipation IBUPROFEN 400-600 MG PO Q6H PRN Pain\n\n", "discourse_tags": ["Other"], "span_citation_mapping": []}
{"id": "196173919_0", "paragraph": "[BOS] Former work by Roberts (2016) analyzed the trade-off between corpus size and similarity when training word embeddings for a clinical entity recognition task.\n[BOS] The author's conclusion was that while embeddings trained with word2vec on indomain texts performed generally better, a combination of both in-domain and general domain em-3.\n[BOS] Echocardiogram on **DATE [Nov 6 2007] , showed ejection fraction of 55% , mild mitral insufficiency , and 1+ tricuspid insufficiency with mild pulmonary hypertension .\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Other"], "span_citation_mapping": [{"token_start": 3, "token_end": 68, "char_start": 13, "char_end": 344, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Roberts (2016)": "14700403"}, "Reference": {}}}]}
{"id": "203693444_1", "paragraph": "[BOS] Our contribution consists of an investigation of the presence of gender bias in pretrained embeddings for Swedish.\n[BOS] We are less interested in bias as a theoretical construct, and more interested in the effects of gender bias in actual applications where pretrained embeddings are employed.\n[BOS] Our experiments are therefore tightly tied to a real-world use case where gender bias would have potentially serious ramifications.\n[BOS] We also provide further evidence of the inability of the debiasing method proposed by Bolukbasi et al. (2016b) to handle the type of bias we are concerned with.\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 87, "token_end": 104, "char_start": 502, "char_end": 555, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Bolukbasi et al. (2016b)": "1704893"}}}]}
{"id": "203693444_0", "paragraph": "[BOS] Research regarding bias and stereotypes expressed in text and subsequently incorporated in learned language models is currently a vivid field.\n[BOS] Caliskan et al. (2017) show that learned embeddings exhibit every linguistic bias documented in the field of psychology (such as that flowers are more pleasant than insects, musical instruments are preferred to weapons, and personal names are used to infer race).\n[BOS] Garg et al. (2018) show that temporal changes of the embeddings can be used to quantify gender and ethnic stereotypes over time, and Zhao et al. (2017) suggest that biases might in fact be amplified by embedding models.\n[BOS] Several researchers have also investigated ways to counter stereotypes and biases in learned language models.\n[BOS] While the seminal work by Bolukbasi et al. (2016a Bolukbasi et al. ( , 2016b concerns the identification and mitigation of gender bias in pretrained word embeddings, Zhao et al. (2018) provide insights into the possibilities of learning embeddings that are gender neutral.\n[BOS] Bordia and Bowman (2019) outline a way of training a recurrent neural network for word-based language modelling such that the model is gender neutral.\n[BOS] Park et al. (2018) discuss different ways of mitigating gender bias, in the context of abusive language detection, ranging from debiasing a model by using the hard debiased word embeddings produced by Bolukbasi et al. (2016b) , to manipulating the data prior to training a model by swapping masculine and feminine mentions, and employing transfer learning from a model learned from less biased text.\n[BOS] Gonen and Goldberg (2019) contest the approaches to debiasing word embeddings presented by Bolukbasi et al. (2016b) and Zhao et al. (2018) , arguing that while the bias is reduced when measured according to its definition, i.e., dampening the impact of the general gender direction in the vector space, \"the actual effect is mostly hiding the bias, not removing it\".\n[BOS] Further, Gonen and Gold-berg (2019) claim that a lot of the supposedly removed bias can be recovered due to the geometry of the vector representation of the gender neutralized words.\n\n", "discourse_tags": ["Transition", "Single_summ", "Multi_summ", "Transition", "Multi_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 26, "token_end": 77, "char_start": 155, "char_end": 418, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Caliskan et al. (2017)": "23163324"}, "Reference": {}}}, {"token_start": 78, "token_end": 105, "char_start": 425, "char_end": 552, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Garg et al. (2018)": null}, "Reference": {}}}, {"token_start": 107, "token_end": 126, "char_start": 558, "char_end": 644, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 150, "token_end": 184, "char_start": 793, "char_end": 931, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Bolukbasi et al. (2016a": null, "Bolukbasi et al. ( , 2016b": "1704893"}, "Reference": {}}}, {"token_start": 185, "token_end": 205, "char_start": 933, "char_end": 1039, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhao et al. (2018)": "52161864"}, "Reference": {}}}, {"token_start": 206, "token_end": 238, "char_start": 1046, "char_end": 1196, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Bordia and Bowman (2019)": "102352788"}, "Reference": {}}}, {"token_start": 239, "token_end": 328, "char_start": 1203, "char_end": 1602, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Park et al. (2018)": "52070035"}, "Reference": {"Bolukbasi et al. (2016b)": "1704893"}}}, {"token_start": 329, "token_end": 414, "char_start": 1609, "char_end": 1973, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Gonen and Goldberg (2019)": "73729169"}, "Reference": {"Bolukbasi et al. (2016b)": "1704893", "Zhao et al. (2018)": "52161864"}}}, {"token_start": 419, "token_end": 457, "char_start": 1991, "char_end": 2164, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "155092969_5", "paragraph": "[BOS] Certifiable Robustness Finally, an alternative approach for creating models that are free of artifacts is to alter the training process.\n[BOS] In particular, model robustness research in computer vision has begun to transition from an empirical arms race between attackers and defenders to more theoretically sound robustness methods.\n[BOS] For instance, convex relaxations can train models that are provably robust to adversarial examples (Raghunathan et al., 2018; Wong and Kolter, 2018) .\n[BOS] Despite these method's impressive (and rapidly developing) results, they largely focus on adversarial perturbations bounded to an L  ball.\n[BOS] This is due to the difficulties in formalizing attacks and defenses for more complex threat models, of which the discrete nature of NLP is included.\n[BOS] Future work can look to generalize these methods to other classes of model vulnerabilities and artifacts.\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite", "Transition", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 62, "token_end": 95, "char_start": 361, "char_end": 495, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Raghunathan et al., 2018;": "11217889", "Wong and Kolter, 2018)": "3659467"}}}]}
{"id": "155092969_4", "paragraph": "[BOS] Interpretations Another technique for probing models is to use interpretation methods.\n[BOS] Interpretations, however, have a problem of faithfulness (Rudin, 2018) : they approximate (often locally) a complex model with a simpler, interpretable model (often a linear model).\n[BOS] Since interpretations are inherently an approximation, they can never be completely faithful-there are cases where the original model and the simple model behave differently (Ghorbani et al., 2019) .\n[BOS] These cases might also be especially important as they usually reflect the counter-intuitive brittleness of the complex models (e.g., in adversarial examples).\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 23, "token_end": 55, "char_start": 143, "char_end": 280, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rudin, 2018)": "53747370"}}}, {"token_start": 76, "token_end": 95, "char_start": 406, "char_end": 484, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ghorbani et al., 2019)": "22172746"}}}]}
{"id": "155092969_3", "paragraph": "[BOS] Adversarial Evaluation Instead of validating a dataset, one can alternatively probe the model directly.\n[BOS] For example, models can be stress tested using adversarial examples (Jia and Liang, 2017; Wallace et al., 2019) and challenge sets (Glockner et al., 2018; Naik et al., 2018) .\n[BOS] These tests can reveal strikingly simple model limitations, e.g., basic paraphrases can fool textual entailment and visual question answering systems (Iyyer et al., 2018; Ribeiro et al., 2018) , while common typos drastically degrade neural machine translation quality (Belinkov and Bisk, 2018) .\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 29, "token_end": 47, "char_start": 163, "char_end": 227, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Jia and Liang, 2017;": "7228830", "Wallace et al., 2019)": "155100063"}}}, {"token_start": 48, "token_end": 68, "char_start": 232, "char_end": 289, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Glockner et al., 2018;": "19204066", "Naik et al., 2018)": "46932607"}}}, {"token_start": 84, "token_end": 117, "char_start": 364, "char_end": 490, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Iyyer et al., 2018;": "4956100", "Ribeiro et al., 2018)": "21740766"}}}, {"token_start": 119, "token_end": 138, "char_start": 499, "char_end": 592, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Belinkov and Bisk, 2018)": "3513372"}}}]}
{"id": "155092969_2", "paragraph": "[BOS] Adversarial Annotation Rather than using partial-input baselines as post-hoc tests, a natural idea is to incorporate them into the data generation process to reject bad examples.\n[BOS] For example, the SWAG (Zellers et al., 2018 ) dataset consists of multiple-choice answers that are selected adversarially against an ensemble of partial-input and heuristic classifiers.\n[BOS] However, since these classifiers can be easily fooled if they rely on superficial patterns, the resulting dataset may still contain artifacts.\n[BOS] In particular, a much stronger model (BERT) that sees the full-input easily solves the dataset.\n[BOS] This demonstrates that using partial-input baselines as adversaries may lead to datasets that are just difficult enough to fool the baselines but not difficult enough to ensure that no model can cheat.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Transition", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 41, "token_end": 76, "char_start": 208, "char_end": 376, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Zellers et al., 2018": "52019251"}, "Reference": {}}}]}
{"id": "155092969_1", "paragraph": "[BOS] Hypothesis Testing Validating datasets with partial-input baselines is a form of hypothesistesting: one hypothesizes trivial solutions to the dataset (i.e., a spurious correlation between labels and a part of the input) and verifies if these hypotheses are true.\n[BOS] While it is tempting to hypothesize other ways a model can cheat, it is infeasible to enumerate over all of them.\n[BOS] In other words, if we could write down all the necessary tests for test-driven development (Beck, 2002) of a machine learning model, we would already have a rule-based system that can solve our task.\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 96, "token_end": 105, "char_start": 462, "char_end": 498, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Beck, 2002)": "109097665"}}}]}
{"id": "155092969_0", "paragraph": "[BOS] Partial-input baselines are valuable sanity checks for datasets, but as we illustrate, their implications should be understood carefully.\n[BOS] This section discusses methods for validating and creating datasets in light of possible artifacts from the annotation process, as well as empirical results that corroborate the potential pitfalls highlighted in this paper.\n[BOS] Furthermore, we discuss alternative approaches for developing robust NLP models.\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "202770347_0", "paragraph": "[BOS] The proposed task is motivated by prior knowledge-grounded conversation tasks (Ghazvininejad et al., 2018; Zhou et al., 2018b) , but further requires the capability to adapt to dynamic knowledge graphs.\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 8, "token_end": 35, "char_start": 40, "char_end": 132, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ghazvininejad et al., 2018;": "15442925", "Zhou et al., 2018b)": "51608183"}}}]}
{"id": "147703948_1", "paragraph": "[BOS] There have been several studies for NMT using dependency syntax.\n[BOS] Hashimoto and Tsuruoka (2017) propose to combine the head information with sequential words together as source encoder inputs, where their input trees are latent dependency graphs.\n[BOS] Recently, there are several studies by using convolutional neural structures to represent source dependency trees, where tree nodes are modeled individually (Chen et al., 2017b; Bastings et al., 2017) .\n[BOS] Wu et al. (2017b) build a syntax enhanced encoder by multiple Bi-RNNs over several different word sequences based on different traversing orders over dependency trees, i.e., the original sequential order and several tree-based orders.\n[BOS] All these methods require certain extra efforts to encode the source dependency syntax over a baseline Seq2Seq NMT.\n\n", "discourse_tags": ["Transition", "Single_summ", "Multi_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 15, "token_end": 49, "char_start": 77, "char_end": 257, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hashimoto and Tsuruoka (2017)": "1423962"}, "Reference": {}}}, {"token_start": 58, "token_end": 91, "char_start": 309, "char_end": 464, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chen et al., 2017b;": "43165539", "Bastings et al., 2017)": "6206777"}}}, {"token_start": 93, "token_end": 143, "char_start": 473, "char_end": 707, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wu et al. (2017b)": "11432578"}, "Reference": {}}}]}
{"id": "147703948_0", "paragraph": "[BOS] tactic tags.\n[BOS] The method is as effective as Tree-RNN approaches yet more effective.\n[BOS] Noticeably, all these studies focus on constituent trees.\n\n", "discourse_tags": ["Transition", "Transition", "Transition"], "span_citation_mapping": []}
{"id": "102350997_2", "paragraph": "[BOS] The variational approximation uses amortized inference (Kingma and Welling, 2014; Mnih and Gregor, 2014; Rezende et al., 2014) , in which an inference network is used to obtain the variational posterior for each observed x.\n[BOS] Since our inference network is structured (i.e., a CRF), it is also related to CRF autoencoders (Ammar et al., 2014) and structured VAEs (Johnson et al., 2016; Krishnan et al., 2017) , which have been used previously for unsupervised (Cai et al., 2017; Drozdov et al., 2019; Li et al., 2019) and semi-supervised (Yin et al., 2018; Corro and Titov, 2019) parsing.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 6, "token_end": 36, "char_start": 41, "char_end": 132, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kingma and Welling, 2014;": null, "Mnih and Gregor, 2014;": "1981188", "Rezende et al., 2014)": "16895865"}}}, {"token_start": 76, "token_end": 89, "char_start": 315, "char_end": 352, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ammar et al., 2014)": "4977771"}}}, {"token_start": 90, "token_end": 109, "char_start": 357, "char_end": 418, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Johnson et al., 2016;": "5578295", "Krishnan et al., 2017)": "2901305"}}}, {"token_start": 116, "token_end": 143, "char_start": 457, "char_end": 527, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cai et al., 2017;": "28556787", "Drozdov et al., 2019;": "102350747", "Li et al., 2019)": "53306585"}}}, {"token_start": 144, "token_end": 163, "char_start": 532, "char_end": 589, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yin et al., 2018;": "49325612", "Corro and Titov, 2019)": "50783524"}}}]}
{"id": "102350997_1", "paragraph": "[BOS] work (Cho et al., 2014; Chung et al., 2017; Shen et al., 2018 Shen et al., , 2019 .\n[BOS] In contrast, Buys and Blunsom (2018) make Markov assumptions and perform exact marginalization over latent dependency trees.\n[BOS] Our work is also related to the recent line of work on learning latent trees as part of a deep model through supervision on other tasks, typically via differentiable structured hidden layers (Kim et al., 2017; Bradbury and Socher, 2017; Tran and Bisk, 2018; Peng et al., 2018; Niculae et al., 2018; , policy gradient-based approaches (Yogatama et al., 2017; Williams et al., 2018; Havrylov et al., 2019) , or differentiable relaxations (Choi et al., 2018; Maillard and Clark, 2018) .\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 3, "token_end": 32, "char_start": 11, "char_end": 87, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cho et al., 2014;": null, "Chung et al., 2017;": "1463401", "Shen et al., 2018": "3347806", "Shen et al., , 2019": "53034786"}}}, {"token_start": 37, "token_end": 59, "char_start": 109, "char_end": 220, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Buys and Blunsom (2018)": "44118631"}, "Reference": {}}}, {"token_start": 89, "token_end": 130, "char_start": 378, "char_end": 524, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kim et al., 2017;": "6961760", "Bradbury and Socher, 2017;": "9683221", "Tran and Bisk, 2018;": "44108645", "Peng et al., 2018;": "44143441"}}}, {"token_start": 132, "token_end": 164, "char_start": 528, "char_end": 630, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yogatama et al., 2017;": "423406", "Williams et al., 2018;": "3527842", "Havrylov et al., 2019)": "67856687"}}}, {"token_start": 166, "token_end": 185, "char_start": 636, "char_end": 708, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Choi et al., 2018;": "24462966", "Maillard and Clark, 2018)": "46935811"}}}]}
{"id": "102350997_0", "paragraph": "[BOS] There has been much work on incorporating tree structures into deep models for syntax-aware language modeling, both for unconditional (Emami and Jelinek, 2005; Buys and Blunsom, 2015; Dyer et al., 2016) and conditional (Yin and Neubig, 2017; Alvarez-Melis and Jaakkola, 2017; Rabinovich et al., 2017; Aharoni and Goldberg, 2017; Eriguchi et al., 2017; Wang et al., 2018; Gu et al., 2018) cases.\n[BOS] These approaches generally rely on annotated parse trees during training and maximizes the joint likelihood of sentence-tree pairs.\n[BOS] Prior work on combining language modeling and unsupervised tree learning typically embed soft, tree-like structures as hidden layers of a deep net- 23 The main time bottleneck is the dynamic compution graph, since the dynamic programming algorithm can be batched (however the latter is a significant memory bottleneck).\n[BOS] We manually batch the SHIFT and REDUCE operation as much as possible, though recent work on auto-batching could potentially make this easier/faster.\n[BOS] 24 Many prior works that induce trees directly from words often employ additional heuristics based on punctuation (Seginer, 2007; Ponvert et al., 2011; Spitkovsky et al., 2013; Parikh et al., 2014) , as punctuation (e.g. comma) is usually a reliable signal for start/end of constituent spans.\n[BOS] In contrast the URNNG still has to learn to rely on punctuation.\n[BOS] We also reiterate that punctuation is used during training but ignored during evaluation (except in Table 4 ), as with the prior works mentioned above.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Reflection", "Narrative_cite", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 23, "token_end": 51, "char_start": 126, "char_end": 208, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Emami and Jelinek, 2005;": "2503423", "Buys and Blunsom, 2015;": "16833500", "Dyer et al., 2016)": "1949831"}}}, {"token_start": 52, "token_end": 115, "char_start": 213, "char_end": 393, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yin and Neubig, 2017;": "12718048", "Alvarez-Melis and Jaakkola, 2017;": "56763307", "Rabinovich et al., 2017;": "13529592", "Aharoni and Goldberg, 2017;": null, "Eriguchi et al., 2017;": "14519034", "Wang et al., 2018;": "52112576", "Gu et al., 2018)": "52167541"}}}, {"token_start": 245, "token_end": 278, "char_start": 1128, "char_end": 1223, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Seginer, 2007;": "2862221", "Ponvert et al., 2011;": null, "Spitkovsky et al., 2013;": "422387", "Parikh et al., 2014)": "500031"}}}]}
{"id": "202234053_9", "paragraph": "[BOS] The scoring rules are set heuristically such that they model relative fact importance in different interactions.\n[BOS] Next, we sort the fact triples in descending order of their scores, and take the top N f facts from the sorted list as the related facts for subsequent processing.\n\n", "discourse_tags": ["Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "202234053_8", "paragraph": "[BOS]  Score+1, if the subject occurs in q or p.\n\n", "discourse_tags": ["Other"], "span_citation_mapping": []}
{"id": "202234053_7", "paragraph": "[BOS]  Score+2, if the subject and the object both occur in p.\n\n", "discourse_tags": ["Other"], "span_citation_mapping": []}
{"id": "202234053_6", "paragraph": "[BOS]  Score+4, if the subject occurs in q, and the object occurs in p.\n\n", "discourse_tags": ["Other"], "span_citation_mapping": []}
{"id": "202234053_5", "paragraph": "[BOS] For each instance (q, p), we first extract facts with the subject or object that occurs in question q or passage p. Scores are added to each extracted fact according to the following rules:\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "202234053_4", "paragraph": "[BOS] Due to the size of a knowledge base and the large amount of unnecessary information, we need an effective way of extracting a set of candidate facts which provide novel information while being related to a given question and passage.\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "202234053_3", "paragraph": "[BOS] 3 Knowledge-aware Answer Generation Knowledge-aware answer generation is a question answering paradigm, where a QA model is expected to generate an abstractive answer to a given question by leveraging both the contextual passage and external knowledge.\n[BOS] More formally, given a knowledge base K and two sequences of input words: question .\n[BOS] .\n[BOS] , w r Nr }.\n[BOS] The knowledge base K contains a set of facts, each of which is represented as a triple f = (subject, relation, object) where subject and object can be multi-word expressions and relation is a relation type, e.g., (bridge, U sedF or, cross water).\n\n", "discourse_tags": ["Transition", "Transition", "Other", "Other", "Transition"], "span_citation_mapping": []}
{"id": "202234053_2", "paragraph": "[BOS] Some work on knowledge-enhanced natural language (NLU) understanding can be adapted to the question answering task.\n[BOS] CRWE (Weissenborn, 2017) dynamically integrates background knowledge in a NLU model in the form of free-text statements, and yields refined word representations to a task-specific NLU architecture that reprocesses the task inputs with these representations.\n[BOS] In contrast, KBLSTM (Yang and Mitchell, 2017) leverages continuous representations of knowledge bases to enhance the learning of recurrent neural networks for machine reading.\n[BOS] Furthermore, Bauer et al. (2018) proposed MHPGM, a QA architecture that fills in the gaps of inference with commonsense knowledge.\n[BOS] The model, however, does not allow an answer word to come directly from knowledge.\n[BOS] We adapt these knowledge-enhanced NLU architectures to answer generation, as baselines for our experiments.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 25, "token_end": 76, "char_start": 128, "char_end": 385, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 80, "token_end": 109, "char_start": 405, "char_end": 567, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Yang and Mitchell, 2017)": "19968363"}, "Reference": {}}}, {"token_start": 112, "token_end": 158, "char_start": 587, "char_end": 793, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Bauer et al. (2018)": "52290656"}, "Reference": {}}}]}
{"id": "202234053_1", "paragraph": "[BOS] The role of knowledge in certain types of QA tasks has been remarked on.\n[BOS] Mihaylov and Frank (2018) showed improvements on a cloze-style task by incorporating commonsense knowledge via a context-to-commonsense attention.\n[BOS] Zhong et al. (2018) proposed commonsense-based pre-training to improve answer selection.\n[BOS] Long et al. (2017) made use of knowledge in the form of entity descriptions to predict missing entities in a given document.\n[BOS] There have also been a few studies on incorporating knowledge into QA models without passage reading.\n[BOS] GenQA (Yin et al., 2016) combines knowledge retrieval and seq2seq learning to produce fluent answers, but it only deals with simple questions containing one single fact.\n[BOS] COREQA (He et al., 2017) extends it with a copy mechanism to learn to copy words from a given question.\n[BOS] Moreover, Fu and Feng (2018) introduced a new attention mechanism that attends across the generated history and memory to explicitly avoid repetition, and incorporated knowledge to enrich generated answers.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Transition", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 19, "token_end": 52, "char_start": 85, "char_end": 231, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Mihaylov and Frank (2018)": "29151507"}, "Reference": {}}}, {"token_start": 53, "token_end": 73, "char_start": 238, "char_end": 326, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhong et al. (2018)": "52188664"}, "Reference": {}}}, {"token_start": 74, "token_end": 100, "char_start": 333, "char_end": 457, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Long et al. (2017)": "2215426"}, "Reference": {}}}, {"token_start": 119, "token_end": 155, "char_start": 572, "char_end": 741, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Yin et al., 2016)": "14039866"}, "Reference": {}}}, {"token_start": 156, "token_end": 182, "char_start": 748, "char_end": 851, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(He et al., 2017)": "43225062"}, "Reference": {}}}, {"token_start": 185, "token_end": 218, "char_start": 868, "char_end": 1064, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Fu and Feng (2018)": "44092258"}, "Reference": {}}}]}
{"id": "202234053_0", "paragraph": "[BOS] There have been several attempts at using machine reading to generate natural answers in the QA field.\n[BOS] Tan et al. (2018) took a generative approach where they added a decoder on top of their extractive model to leverage the extracted evidence for answer synthesis.\n[BOS] However, this model still relies heavily on the extraction to perform the generation and thus needs to have start and end labels (a span) for every QA pair.\n[BOS] Mitra (2017) proposed a seq2seq-based model that learns alignment between a question and passage words to produce rich question-aware passage representation by which it directly decodes an answer.\n[BOS] Gao et al. (2019) focused on product-aware answer generation based on large-scale unlabeled e-commerce reviews and product attributes.\n[BOS] Furthermore, natural answer generation can be reformulated as query-focused summarization which is addressed by Nema et al. (2017) .\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 21, "token_end": 86, "char_start": 115, "char_end": 439, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Tan et al. (2018)": "19090838"}, "Reference": {}}}, {"token_start": 87, "token_end": 126, "char_start": 446, "char_end": 642, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 127, "token_end": 156, "char_start": 649, "char_end": 783, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Gao et al. (2019)": null}, "Reference": {}}}, {"token_start": 157, "token_end": 184, "char_start": 790, "char_end": 920, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Nema et al. (2017)": "5673925"}, "Reference": {}}}]}
{"id": "204795082_6", "paragraph": "[BOS] In the next section, Section 3, we describe how our system works.\n[BOS] In Section 4 we present a preliminary evaluation followed by discussion and plans for future work directions.\n\n", "discourse_tags": ["Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "204795082_5", "paragraph": "[BOS] It is unclear how well these two latter approaches potentially scale beyond bigrams or trigrams.\n[BOS] Further, they assume that the length of the input/output phrases is known in advance.\n[BOS] However, the task that we are aiming for is to develop a system that can take any query phrase of arbitrary (sub-sentence) length as input.\n[BOS] As output it should suggest phrases that it identifies in a large document corpus which express the same or similar information/meaning.\n[BOS] Here the idea is that we only apply upper and lower thresholds when it comes to the length of the output phrase suggestions.\n[BOS] In addition, we do not want to be concerned with knowledge about word classes in the input and output phrases.\n[BOS] We are not aware of previous work presenting a solution to this task.\n\n", "discourse_tags": ["Transition", "Transition", "Reflection", "Transition", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "204795082_4", "paragraph": "[BOS] Dinu and Baroni (2014) also apply vector composition and decomposition in a recursive manner for longer phrases (t  3).\n[BOS] Their focus is on mapping between unigrams, bigrams and tri-grams.\n[BOS] As output their system produce one vector per word which represent the (to be) generated phrase.\n[BOS] Here the evaluation primarily assumes that t = 1, i.e. the nearest neighbouring word in the semantic model, belonging to the expected word class, is extracted per vector to form the output phrase.\n[BOS] However, no solution is presented for when t > 1 other than independent ranked lists of semantically similar words to each vector.\n[BOS] Turney (2014) explores an approach targeting retrieval of multiple phrases for a single query (i.e. t > 1), evaluated on unigram to bigram and bigram to unigram extraction.\n[BOS] Here he applies a supervised ranking algorithm to rank the generated output candidates.\n[BOS] For each input query, the evaluation checks whether or not the correct/expected output (phrase) is among the list of top hundred candidates.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Transition", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 107, "char_start": 6, "char_end": 504, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 133, "token_end": 219, "char_start": 648, "char_end": 1061, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "204795082_3", "paragraph": "[BOS] In the works mentioned so far, the focus is on distributional semantics for representing and calculating semantic similarity and relatedness between predefined lexical units and/or of predefined length (words/n-grams, collocations, clauses, sentences, etc.).\n[BOS] Dinu and Baroni (2014) and Turney (2014) take things a step further and approach the more complex and challenging task of using semantic models to enable phrase generation.\n[BOS] Their aim is similar to ours: given an input query (phrase) consisting of k words, generate as output t phrases consisting of l words that each expresses its meaning.\n[BOS] Their approaches rely on applying a set of separately trained vector composition and decomposition functions able to compose a single vector from a vector pair, or decompose a vector back into estimates of its constituent vectors, possibly in the semantic space of another domain or language.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Multi_summ", "Multi_summ"], "span_citation_mapping": [{"token_start": 52, "token_end": 173, "char_start": 271, "char_end": 915, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Dinu and Baroni (2014)": "772413"}, "Reference": {}}}]}
{"id": "204795082_2", "paragraph": "[BOS] A relatively straight forward approach to identify and represent common phrases as vectors in a semantic space is to first use some type of collocation detection.\n[BOS] Here the aim is to identify sequences of words that co-occur more often than what is expected by chance in a large corpus.\n[BOS] One can then train a semantic model where identified phrases are treated as individual tokens, on the same level as words, like it is done in Mikolov et al. (2013) .\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 86, "token_end": 95, "char_start": 446, "char_end": 467, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Mikolov et al. (2013)": "16447573"}}}]}
{"id": "204795082_1", "paragraph": "[BOS] Further, there are several relatively recent works focusing on using and/or representing n-gram information as semantic vectors (see e.g. Bojanowski et al. (2016) ; Zhao et al. (2017) ; Poliak et al. (2017) ; Gupta et al. (2019) ), possibly to further represent clauses, sentences and/or documents (see e.g. Le and Mikolov (2014) ; Pagliardini et al. (2018) ) in semantic vector spaces.\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 16, "token_end": 66, "char_start": 82, "char_end": 236, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Bojanowski et al. (2016)": "207556454", "Zhao et al. (2017)": "1823746", "Poliak et al. (2017)": "6292995", "Gupta et al. (2019)": "131774183"}}}, {"token_start": 73, "token_end": 104, "char_start": 277, "char_end": 365, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Le and Mikolov (2014)": "2407601", "Pagliardini et al. (2018)": "16251657"}}}]}
{"id": "204795082_0", "paragraph": "[BOS] Unsupervised methods for capturing and modeling word-level semantics as vectors, or embeddings, have been popular since the introduction of Latent Semantic Analysis (LSA) (Deerwester et al., 1990) around the beginning of the 1990s.\n[BOS] Such word vector representations, where the underlying training heuristic is typically based on the distributional hypothesis (Harris, 1954) , usually with some form of dimension reduction, have shown to capture word similarity (synonymy and relatedness) and analogy (see e.g. Agirre et al. (2009); Mikolov et al. (2013) ).\n[BOS] Methods and toolkits like Word2Vec (Mikolov et al., 2013) and GloVe (Pen-nington et al., 2014) are nowadays commonly used to (pre-)train word embeddings for further use in various NLP tasks, including supervised text classification with neural networks.\n[BOS] However, recent methods such as ELMo (Peters et al., 2017) and BERT (Devlin et al., 2018) use deep neural networks to represent context sensitive word embeddings, which achieves state-of-the-art performance when used in supervised text classification and similar.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Narrative_cite", "Multi_summ"], "span_citation_mapping": [{"token_start": 25, "token_end": 42, "char_start": 146, "char_end": 202, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 66, "token_end": 74, "char_start": 344, "char_end": 384, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Harris, 1954)": null}}}, {"token_start": 86, "token_end": 123, "char_start": 448, "char_end": 564, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Agirre et al. (2009);": "5944731", "Mikolov et al. (2013)": "16447573"}}}, {"token_start": 131, "token_end": 144, "char_start": 600, "char_end": 631, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mikolov et al., 2013)": "16447573"}}}, {"token_start": 145, "token_end": 158, "char_start": 636, "char_end": 668, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 194, "token_end": 245, "char_start": 866, "char_end": 1097, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Peters et al., 2017)": "7197241", "(Devlin et al., 2018)": "52967399"}, "Reference": {}}}]}
{"id": "189927897_3", "paragraph": "[BOS] Multimodal domain: With the interplay between visual and textual modalities, an obvious downstream application for persona based text generation is image captioning.\n[BOS] Chandrasekaran et al. (2018) worked on generating witty captions for images by both retrieving and generating with an encoder-decoder architecture.\n[BOS] This work used external resources to gather a list of words that are related to puns from web which the decoder attempts to generate conditioned on phonological similarity.\n[BOS] Wang and Wen (2015) studied the statistical correlation of words associated with specific memes.\n[BOS] These ideas have also recently penetrated into visual dialog setting.\n[BOS] Shuster et al. (2018) have collected a grounded conversational dataset with 202k dialogs where humans are asked to portray a personality in the collection process.\n[BOS] They have also set up various baselines with different techniques to fuse the modalities including multimodal sum combiner and multimodal attention combiner.\n[BOS] We use this dataset to learn personas which are adapted to our storytelling model.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Transition", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 31, "token_end": 93, "char_start": 178, "char_end": 504, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Chandrasekaran et al. (2018)": "30046385"}, "Reference": {}}}, {"token_start": 94, "token_end": 112, "char_start": 511, "char_end": 607, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 126, "token_end": 186, "char_start": 690, "char_end": 1017, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Shuster et al. (2018)": "53297976"}, "Reference": {}}}]}
{"id": "189927897_2", "paragraph": "[BOS] Persona Based Dialog: Persona based generation of responses has been studied by NLP community in dialog domain.\n[BOS] (Li et al., 2016) encoded personas of individuals in contextualized embeddings that capture the background information and style to maintain consistency in the responses given.\n[BOS] The embeddings for the speaker information are learnt jointly with the word embeddings.\n[BOS] Following this work, (Zhou et al., 2018) proposed Emotional Chatting Machine that generates responses in an emotional tone in addition to conditioning the content.\n[BOS] The key difference between former and latter work is that the latter captures dynamic change in emotion as the conversation proceeds, while the user persona remains the same in the former case.\n[BOS] release a huge dataset of conversations conditioned on the persona of the two people interacting.\n[BOS] This work shows that conditioning on the profile information improves the dialogues which is measured by next utterance prediction.\n[BOS] In these works, the gold value of the target response was known.\n[BOS] For our work, we do not have gold values of stories in different personas.\n[BOS] Hence we leverage annotated data from a different task and transfer that knowledge to steer our generation process.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Transition", "Single_summ", "Single_summ", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 25, "token_end": 73, "char_start": 124, "char_end": 394, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Li et al., 2016)": "2955580"}, "Reference": {}}}, {"token_start": 78, "token_end": 105, "char_start": 422, "char_end": 564, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Zhou et al., 2018)": "2024574"}, "Reference": {}}}]}
{"id": "189927897_1", "paragraph": "[BOS] Style Transfer: One line of research that is closely related to our task is style transfer in text.\n[BOS] Recently generative models have gained popularity in attempting to solve style transfer in text with non-parallel data (Hu et al., 2017; Li et al., 2018) .\n[BOS] Some of this work has also focused on transferring author attributes (Prabhumoye et al., 2018) , transferring multiple attributes (Lample et al., 2019; Logeswaran et al., 2018) and collecting parallel dataset for formality (Rao and Tetreault, 2018) .\n[BOS] Although our work can be viewed as another facet of style transfer, we have strong grounding of the stories in the sequence of images.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 33, "token_end": 57, "char_start": 185, "char_end": 265, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hu et al., 2017;": "20981275", "Li et al., 2018)": "4937880"}}}, {"token_start": 67, "token_end": 82, "char_start": 312, "char_end": 368, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Prabhumoye et al., 2018)": "13959787"}}}, {"token_start": 83, "token_end": 106, "char_start": 371, "char_end": 450, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lample et al., 2019;": "53334018", "Logeswaran et al., 2018)": "53217784"}}}, {"token_start": 107, "token_end": 122, "char_start": 455, "char_end": 522, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rao and Tetreault, 2018)": "4859003"}}}]}
{"id": "189927897_0", "paragraph": "[BOS] Visual Story Telling: Last decade witnessed enormous interest in research at the intersection of multiple modalities, especially vision and language.\n[BOS] Mature efforts in image captioning (Hossain et al., 2019) paved way into more advanced tasks like visual question answering (Wu et al., 2017) and visual dialog (Das et al., 2017) , (Mostafazadeh et al., 2017) .\n[BOS] As an obvious next step from single shot image captioning lies the task of describing a sequence of images which are related to one another to form a story like narrative.\n[BOS] This task was introduced as visual story telling by Huang et al. (2016) , differentiating descriptions of images in isolation (image captions) and stories in sequences.\n[BOS] The baseline model that we are leveraging to generate personality conditioned story generation is based on the model proposed by Kim et al. (2018) for the visual story telling challenge.\n[BOS] Another simple yet effective technique is late fusion model by Smilevski et al. (2018) .\n[BOS] In addition to static images, Gella et al. (2018) have also collected a dataset of describing stories from videos uploaded on social media.\n[BOS] Chandu et al. (2019) recently introduced a dataset for generating textual cooking recipes from a sequence of images and proposed two models to incorporate structure in procedural text generation from images.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Transition", "Single_summ", "Narrative_cite", "Narrative_cite", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 30, "token_end": 45, "char_start": 180, "char_end": 219, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hossain et al., 2019)": null}}}, {"token_start": 53, "token_end": 64, "char_start": 260, "char_end": 303, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wu et al., 2017)": null}}}, {"token_start": 65, "token_end": 88, "char_start": 308, "char_end": 370, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Das et al., 2017)": null, "(Mostafazadeh et al., 2017)": "9142609"}}}, {"token_start": 127, "token_end": 159, "char_start": 567, "char_end": 725, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Huang et al. (2016)": "2574224"}, "Reference": {}}}, {"token_start": 169, "token_end": 188, "char_start": 777, "char_end": 878, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Kim et al. (2018)": "44064084"}}}, {"token_start": 203, "token_end": 217, "char_start": 967, "char_end": 1011, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Smilevski et al. (2018)": "21691163"}}}, {"token_start": 219, "token_end": 249, "char_start": 1020, "char_end": 1159, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Gella et al. (2018)": "53080252"}, "Reference": {}}}, {"token_start": 250, "token_end": 288, "char_start": 1166, "char_end": 1373, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Chandu et al. (2019)": "196171135"}, "Reference": {}}}]}
{"id": "202565872_2", "paragraph": "[BOS] Persona-grounded conversation modeling Li et al. (2016b) ; Luan et al. (2017) aim to generate responses mimicking a speaker.\n[BOS] It is closely related to the present task, since persona is, broadly speaking, the manifestation of a type of style.\n[BOS] Li et al. (2016b) feeds a speaker ID to the decoder to promote generation of response for that target speaker.\n[BOS] However non-conversational data cannot be used.\n[BOS] Luan et al. (2017) applied a multi-task learning approach to utilize non-conversational data.\n[BOS] A S2S model, taking in conversational data, and an autoencoder (AE), taking in nonconversational data, share the decoder and are trained alternately.\n[BOS] However, Gao et al. (2019b) observed that sharing the decoder may not truly allow S2S and AE to share the latent space, and thus S2S may not fully utilize what is learned by AE.\n[BOS] Unlike Li et al. (2016b) using labelled persona IDs, have proposed using a self-supervised method to extract persona features from conversation history.\n[BOS] This allows modeling persona dynamically, which agrees with the fact that even the same person can speak in different style in different scenarios.\n[BOS] Zhang et al. (2017) aggregates the strengths of each specific task, and induces regularization effects as the model is trained to learn a more universal representation.\n[BOS] However a simple multi-task approach (Luan et al., 2017) may learn separate representations for each dataset (Gao et al., 2019b) .\n[BOS] To address this, in previous work (Gao et al., 2019b) , we proposed the SPACEFUSION model featuring a regularization technique that explicitly encourages alignment of latent spaces for a universal representation.\n[BOS] SPACEFUSION, however, is only designed for paired samples.\n[BOS] We generalize SPACEFUSION to non-parallel datasets in this paper.\n\n", "discourse_tags": ["Multi_summ", "Multi_summ", "Single_summ", "Transition", "Single_summ", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Narrative_cite", "Single_summ", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 59, "char_start": 6, "char_end": 253, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Luan et al. (2017)": "31298398"}, "Reference": {}}}, {"token_start": 60, "token_end": 85, "char_start": 260, "char_end": 370, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 97, "token_end": 120, "char_start": 431, "char_end": 524, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Luan et al. (2017)": "31298398"}, "Reference": {}}}, {"token_start": 162, "token_end": 206, "char_start": 696, "char_end": 864, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 207, "token_end": 266, "char_start": 871, "char_end": 1177, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 267, "token_end": 298, "char_start": 1184, "char_end": 1352, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhang et al. (2017)": "5258986"}, "Reference": {}}}, {"token_start": 299, "token_end": 333, "char_start": 1359, "char_end": 1489, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Luan et al., 2017)": "31298398"}}}, {"token_start": 338, "token_end": 375, "char_start": 1513, "char_end": 1708, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "202565872_1", "paragraph": "[BOS] Stylized response generation is a relatively new task.\n[BOS] Akama et al. (2017) use a stylized conversation corpus to fine-tune a conversation model pretrained on a background conversation dataset.\n[BOS] However, stylized texts are usually in non-conversational format, as in the present setting.\n[BOS] Niu and Bansal (2018) proposed a method that takes the weighted average of the token probability distribution predicted by a S2S trained on background conversational dataset and that predicted by a LM trained on style dataset as the token probability.\n[BOS] They observed reduced relevance and attributed this to the fact that the LM was not trained to attend to conversation context and S2S was not trained to learn style during training.\n[BOS] In contrast, we jointly learn from conversation and style datasets during training.\n[BOS] Niu and Bansal (2018) have proposed label-fine-tuning, but this is limited to scenarios where a reasonable portion of the conversational dataset is in the target style, which is not always the case.\n\n", "discourse_tags": ["Transition", "Single_summ", "Transition", "Single_summ", "Single_summ", "Reflection", "Single_summ"], "span_citation_mapping": [{"token_start": 13, "token_end": 42, "char_start": 67, "char_end": 204, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Akama et al. (2017)": "5661541"}, "Reference": {}}}, {"token_start": 64, "token_end": 147, "char_start": 310, "char_end": 749, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Niu and Bansal (2018)": "13690180"}, "Reference": {}}}, {"token_start": 163, "token_end": 207, "char_start": 846, "char_end": 1044, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Niu and Bansal (2018)": "13690180"}, "Reference": {}}}]}
{"id": "202565872_0", "paragraph": "[BOS] Text style transfer is a related but distinct task.\n[BOS] It usually preserves the content (Yang et al., 2018; Hu et al., 2017; Fu et al., 2018; Gong et al., 2019) .\n[BOS] In contrast, content of conversational responses in a given context can be semantically diverse.\n[BOS] Various approaches have been proposed for non-parallel data setup.\n[BOS] Fu et al. (2018) proposed to use separate decoders for different styles and a classifier to measure style strength.\n[BOS] proposed to map texts of two different styles into a shared latent space where the \"content\" information is preserved and \"style\" information is discarded.\n[BOS] An adversarial discriminator is used to align the latent spaces of two different styles.\n[BOS] However, Yang et al. (2018) point out the difficulty of training an adversarial discriminator and proposed instead the use of language models as discriminator.\n[BOS] Like ; Yang et al. (2018) , we align latent spaces for different styles.\n[BOS] However we also align latent spaces encoded by different models (S2S and AE).\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Transition", "Single_summ", "Transition", "Transition", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 14, "token_end": 49, "char_start": 67, "char_end": 169, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yang et al., 2018;": "44061800", "Hu et al., 2017;": "20981275", "Fu et al., 2018;": "6484065", "Gong et al., 2019)": "85517799"}}}, {"token_start": 82, "token_end": 106, "char_start": 354, "char_end": 469, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Fu et al. (2018)": "6484065"}, "Reference": {}}}, {"token_start": 157, "token_end": 187, "char_start": 742, "char_end": 892, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Yang et al. (2018)": "44061800"}, "Reference": {}}}, {"token_start": 190, "token_end": 206, "char_start": 906, "char_end": 971, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Yang et al. (2018)": "44061800"}, "Reference": {}}}]}
{"id": "174802888_2", "paragraph": "[BOS] The knowledge transfer between the PRPN and the Tree-LSTM applies a simple imitation learning procedure, where an agent learns from a teacher (a human or a well-trained model) based on demonstrations (i.e., predictions of the teacher).\n[BOS] Typical approaches to imitation learning include behavior cloning (step-by-step supervised learning) and inverse reinforcement learning (Hussein et al., 2017) .\n[BOS] If the environment/simulator is available, the agent can refine its policy after learning from demonstrations (Gao et al., 2018) .\n[BOS] Our work also adopts a two-step strategy: learning from demonstrations and refining policy.\n[BOS] Policy refinement is needed in our approach because the teacher is imperfect, and experiments show the benefit of policy refinement in our case.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 74, "token_end": 87, "char_start": 353, "char_end": 406, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hussein et al., 2017)": "15333444"}}}, {"token_start": 98, "token_end": 116, "char_start": 462, "char_end": 543, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Gao et al., 2018)": "3307812"}}}]}
{"id": "174802888_1", "paragraph": "[BOS] Unsupervised parsing is also related to differentiation through discrete variables, where researchers have proposed to use reinforcement learning with sampling (Williams, 1992) , neural attention for marginalization (Deng et al., 2018) , and proximal gradient methods (Jang et al., 2017; Peng et al., 2018) .\n[BOS] Our work follows the framework of Mou et al. (2017) , who couple neural and symbolic systems for table querying by pretraining an reinforcement learning executor with neural attention.\n[BOS] We extend this idea to syntactic parsing and show the relationship between parsing and downstream tasks.\n[BOS] Such a framework couples diverse models at the intermediate output level (latent trees in our case); its flexibility allows us to make use of heterogeneous models, such as the PRPN and the Tree-LSTM.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 19, "token_end": 28, "char_start": 129, "char_end": 182, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Williams, 1992)": "19115634"}}}, {"token_start": 29, "token_end": 42, "char_start": 185, "char_end": 241, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Deng et al., 2018)": "49669485"}}}, {"token_start": 44, "token_end": 63, "char_start": 248, "char_end": 312, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Jang et al., 2017;": "2428314", "Peng et al., 2018)": "44143441"}}}, {"token_start": 71, "token_end": 101, "char_start": 355, "char_end": 505, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Mou et al. (2017)": "46540"}, "Reference": {}}}]}
{"id": "174802888_0", "paragraph": "[BOS] Recursive neural networks are a type of neural network which incorporates syntactic structures for sentence-level understanding tasks.\n[BOS] Typically, recursive neural network models assume that an annotated treebank or a pretrained syntactic parser is available (Socher et al., 2013; Tai et al., 2015; Kim et al., 2019a) , but recent work pays more attention to learning syntactic structures in an unsupervised manner.\n[BOS] Yogatama et al. (2017) propose to use reinforcement learning, and Maillard et al. (2017) introduce the Tree-LSTM to jointly learn sentence embeddings and syntax trees, later combined with a Straight-Through Gumbel-Softmax estimator by Choi et al. (2018) .\n[BOS] In addition to sentence classification tasks, recent research has focused on unsupervised structure learning for language modeling (Shen et al., 2018 (Shen et al., , 2019 Drozdov et al., 2019; Kim et al., 2019b) .\n[BOS] In our work, we explore the possibility for combining the merits of both sentence classification and language modeling.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Multi_summ", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 25, "token_end": 68, "char_start": 158, "char_end": 328, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Socher et al., 2013;": "990233", "Tai et al., 2015;": "3033526", "Kim et al., 2019a)": "52175957"}}}, {"token_start": 85, "token_end": 99, "char_start": 433, "char_end": 493, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Yogatama et al. (2017)": "28631039"}, "Reference": {}}}, {"token_start": 101, "token_end": 146, "char_start": 499, "char_end": 686, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Maillard et al. (2017)": "28631039"}, "Reference": {"Choi et al. (2018)": "24462966"}}}, {"token_start": 160, "token_end": 201, "char_start": 772, "char_end": 906, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Shen et al., 2018": "3347806", "(Shen et al., , 2019": "53034786", "Drozdov et al., 2019;": "102350747", "Kim et al., 2019b)": "102350997"}}}]}
{"id": "189761997_1", "paragraph": "[BOS] At inference time, Freitag and Al-Onaizan (2016) use uniform ensembles of general and finetuned models.\n[BOS] Our Bayesian Interpolation experiments extend previous work by Allauzen and Riley (2011) on Bayesian Interpolation for language model combination.\n\n", "discourse_tags": ["Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 30, "char_start": 6, "char_end": 109, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 38, "token_end": 56, "char_start": 176, "char_end": 262, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Allauzen and Riley (2011)": null}}}]}
{"id": "189761997_0", "paragraph": "[BOS] Transfer learning has been applied to NMT in many forms.\n[BOS] Luong and Manning (2015) use transfer learning to adapt a general model to indomain data.\n[BOS] Zoph et al. (2016) use multilingual transfer learning to improve NMT for lowresource languages.\n[BOS] Chu et al. (2017) introduce mixed fine-tuning, which carries out transfer learning to a new domain combined with some original domain data.\n[BOS] Kobus et al. (2017) train a single model on multiple domains using domain tags.\n[BOS] Khan et al. (2018) sequentially adapt across multiple biomedical domains to obtain one singledomain model.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 15, "token_end": 37, "char_start": 69, "char_end": 158, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Luong and Manning (2015)": null}, "Reference": {}}}, {"token_start": 38, "token_end": 60, "char_start": 165, "char_end": 260, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zoph et al. (2016)": "16631020"}, "Reference": {}}}, {"token_start": 61, "token_end": 90, "char_start": 267, "char_end": 406, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Chu et al. (2017)": "35273027"}, "Reference": {}}}, {"token_start": 91, "token_end": 110, "char_start": 413, "char_end": 492, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kobus et al. (2017)": "7497218"}, "Reference": {}}}, {"token_start": 111, "token_end": 131, "char_start": 499, "char_end": 605, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Khan et al. (2018)": "53235792"}, "Reference": {}}}]}
{"id": "196184953_3", "paragraph": "[BOS] Multihead att.\n\n", "discourse_tags": ["Other"], "span_citation_mapping": []}
{"id": "196184953_2", "paragraph": "[BOS] Multihead att.\n\n", "discourse_tags": ["Other"], "span_citation_mapping": []}
{"id": "196184953_1", "paragraph": "[BOS] 3 Problem setting and dataset NLG tasks can be divided into high entropy (story generation, chit-chat dialog) and low entropy (summarization, machine translation) tasks.\n[BOS] We focus on the high entropy task of chit-chat dialog to study the use and effect of various types of contexts: facts, history and previous tokens.\n[BOS] Table 1 shows a typical dialog from PersonaChat (Zhang et al., 2018b) , one of the largest multi-turn open-domain dialog dataset available.\n[BOS] PersonaChat consists of crowdsourced conversations between real human beings who were asked to chit-chat.\n[BOS] Each participant was given a set of 4-5 profile sentences that define his/her persona Multihead att.\n\n", "discourse_tags": ["Transition", "Reflection", "Narrative_cite", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 76, "token_end": 91, "char_start": 352, "char_end": 405, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhang et al., 2018b)": null}}}]}
{"id": "196184953_0", "paragraph": "[BOS] Unsupervised pretraining for transfer learning has a long history in natural language processing, and a common thread has been to reduce the amount of task-specific architecture added on top of pretrained modules.\n[BOS] Most early methods (Mikolov et al., 2013; Pennington et al., 2014) focused on learning word representations using shallow models, with complex recurrent or convolutional networks later added on top for specific tasks.\n[BOS] With increased computing capacities, it has now become feasible to pretrain deep neural language models.\n[BOS] Dai and Le (2015); Ramachandran et al. (2016) proposed unsupervised pretraining of a language model for transfer learning and to initialize encoder and decoder in a seq2seq model for machine translation tasks.\n[BOS] Works in zero-shot machine translation used large corpora of monolingual data to improve performances for lowresource languages (Johnson et al., 2017; Wada and Iwata, 2018; Lample and Conneau, 2019) .\n[BOS] Most of the work transfering large-scale language models from and for monolingual NLG tasks focus on classification and natural language understanding (Kiros et al., 2015; Jozefowicz et al., 2016) .\n[BOS] Recently, Radford et al. (2019) studied large-scale language models for various generation tasks in the zero-shot setting focusing on summarization and translation and Wolf et al. (2019) presented early work on chit-chat.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Transition", "Multi_summ", "Narrative_cite", "Narrative_cite", "Multi_summ"], "span_citation_mapping": [{"token_start": 42, "token_end": 85, "char_start": 231, "char_end": 443, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mikolov et al., 2013;": "16447573", "Pennington et al., 2014)": "1957433"}}}, {"token_start": 105, "token_end": 150, "char_start": 561, "char_end": 770, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ramachandran et al. (2016)": "3488076"}, "Reference": {}}}, {"token_start": 158, "token_end": 198, "char_start": 816, "char_end": 975, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Johnson et al., 2017;": "6053988", "Wada and Iwata, 2018;": "52179115", "Lample and Conneau, 2019)": "58981712"}}}, {"token_start": 221, "token_end": 246, "char_start": 1085, "char_end": 1180, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kiros et al., 2015;": null, "Jozefowicz et al., 2016)": "260422"}}}, {"token_start": 250, "token_end": 281, "char_start": 1199, "char_end": 1352, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Radford et al. (2019)": "160025533"}, "Reference": {}}}, {"token_start": 282, "token_end": 299, "char_start": 1357, "char_end": 1410, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wolf et al. (2019)": "59222757"}, "Reference": {}}}]}
{"id": "174798410_1", "paragraph": "[BOS] Recent work has also investigated the inductive biases that different sequence models learn.\n[BOS] For example, Tran et al. (2018) find that recurrent models are better at modeling hierarchical structure while Tang et al. (2018) find that feedforward architectures like the transformer and convolutional models are not better than RNNs at modeling long-distance agreement.\n[BOS] Transformers however excel at word-sense disambiguation.\n[BOS] We analyze whether the choice of architecture and the use of an attention mechanism affect the way in which dialog systems use information available to them.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 20, "token_end": 37, "char_start": 118, "char_end": 209, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Tran et al. (2018)": "3785155"}, "Reference": {}}}, {"token_start": 38, "token_end": 68, "char_start": 216, "char_end": 378, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Tang et al. (2018)": "52100282"}, "Reference": {}}}]}
{"id": "174798410_0", "paragraph": "[BOS] Since this work aims at investigating and gaining an understanding of the kinds of information a generative neural response model learns to use, the most relevant pieces of work are where sim- Khandelwal et al. (2018) .\n[BOS] They empirically demonstrate that models are sensitive to perturbations only in the nearby context and typically use only about 150 words of context.\n[BOS] On the other hand, in conditional language modeling tasks like machine translation, models are adversely affected by both synthetic and natural noise introduced anywhere in the input (Belinkov and Bisk, 2017) .\n[BOS] Understanding what information is learned or contained in the representations of neural networks has also been studied by \"probing\" them with linear or deep models (Adi et al., 2016; Subramanian et al., 2018; Conneau et al., 2018) .\n[BOS] Several works have recently pointed out the presence of annotation artifacts in common text and multi-modal benchmarks.\n[BOS] For example, Gururangan et al. (2018) demonstrate that hypothesisonly baselines for natural language inference obtain results significantly better than random guessing.\n[BOS] Kaushik and Lipton (2018) report that reading comprehension systems can often ignore the entire question or use only the last sentence of a document to answer questions.\n[BOS] Anand et al. (2018) show that an agent that does not navigate or even see the world around it can answer questions about it as well as one that does.\n[BOS] These pieces of work suggest that while neural methods have the potential to learn the task specified, its design could lead them to do so in a manner that doesn't use all of the available information within the task.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Narrative_cite", "Transition", "Single_summ", "Single_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 11, "token_end": 71, "char_start": 59, "char_end": 381, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Khandelwal et al. (2018)": "21700944"}, "Reference": {}}}, {"token_start": 78, "token_end": 111, "char_start": 410, "char_end": 596, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Belinkov and Bisk, 2017)": "3513372"}, "Reference": {}}}, {"token_start": 122, "token_end": 167, "char_start": 667, "char_end": 835, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Adi et al., 2016;": "6771196", "Subramanian et al., 2018;": "4567927", "Conneau et al., 2018)": "24461982"}}}, {"token_start": 193, "token_end": 221, "char_start": 983, "char_end": 1138, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Gururangan et al. (2018)": "4537113"}, "Reference": {}}}, {"token_start": 222, "token_end": 255, "char_start": 1145, "char_end": 1314, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kaushik and Lipton (2018)": "52011616"}, "Reference": {}}}, {"token_start": 256, "token_end": 291, "char_start": 1321, "char_end": 1470, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Anand et al. (2018)": "53289270"}, "Reference": {}}}]}
{"id": "189927896_2", "paragraph": "[BOS] TriviaQA (Joshi et al., 2017 ) includes a small portion of questions that require cross-sentence inference.\n[BOS] Welbl et al. (2017) uses Wikipedia articles as the context and subject-relation pairs as the query, and construct the multi-hop QAngaroo dataset by traversing a directed bipartite graph.\n[BOS] It is designed in a way such that the evidence required to answer a query could be spread across multiple documents that are not directly related to the query.\n[BOS] HotpotQA (Yang et al., 2018 ) is a more recent multi-hop dataset that has crowd-sourced questions with diverse syntactic and semantic features.\n[BOS] HotpotQA and QAngaroo also differ in their types of multi-hop reasoning covered.\n[BOS] Because of the knowledge-base domain and the triplet format used in the construction, QAngaroo's questions usually require inferring the desired property of a query subject by finding a bridge entity that connects the query to the answer.\n[BOS] HotpotQA includes three more types of question, each requiring a different reasoning paradigm.\n[BOS] Some examples require inferring the bridge entity from the question (Type I in Yang et al. (2018) ), while others demand checking facts or comparing subjects' properties from two different documents (Type II and comparison question).\n[BOS] Jia and Liang (2017) first applied adversarial evaluation to QA models on the SQuAD (Rajpurkar et al., 2016) dataset by generating a sentence that only resembles the question syntactically and appending it to the paragraph.\n[BOS] They report that the performances of state-of-the-art QA models (Seo et al., 2017; Hu et al., 2018; drop significantly when evaluated on the adversarial data.\n[BOS] Wang and Bansal (2018) further improves the AddSent adversary and proposed AddSentDiverse that employs a diverse vocabulary for the question conversion procedure.\n[BOS] They show that models trained with such adversarial examples can be robust against a wide range of adversarial evaluation samples.\n[BOS] Our paper shares the spirit with these two works as we also try to investigate models' over-stability to semantics-altering perturbations.\n[BOS] However, our study also differs from the previous works (Jia and Liang, 2017; Wang and Bansal, 2018) in two points.\n[BOS] First, we generate adversarial documents by replacing the answer and bridge entities in the supporting documents instead of converting the question into a statement.\n[BOS] Second, our adversarial documents still preserve words with common semantic meaning to the question so that it can distract models that are exploiting the reasoning shortcut in the context.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Transition", "Transition", "Single_summ", "Narrative_cite", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 27, "char_start": 6, "char_end": 113, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Joshi et al., 2017": "26501419"}, "Reference": {}}}, {"token_start": 28, "token_end": 101, "char_start": 120, "char_end": 472, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Welbl et al. (2017)": "9192723"}, "Reference": {}}}, {"token_start": 102, "token_end": 135, "char_start": 479, "char_end": 622, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Yang et al., 2018": "52822214"}, "Reference": {}}}, {"token_start": 227, "token_end": 243, "char_start": 1098, "char_end": 1159, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Yang et al. (2018)": "52822214"}}}, {"token_start": 268, "token_end": 358, "char_start": 1302, "char_end": 1690, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Jia and Liang (2017)": "7228830"}, "Reference": {"(Rajpurkar et al., 2016)": "11816014", "(Seo et al., 2017;": "8535316", "Hu et al., 2018;": "13559921"}}}, {"token_start": 359, "token_end": 414, "char_start": 1697, "char_end": 1996, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wang and Bansal (2018)": "4940747"}, "Reference": {}}}, {"token_start": 452, "token_end": 466, "char_start": 2204, "char_end": 2248, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Jia and Liang, 2017;": "7228830", "Wang and Bansal, 2018)": "4940747"}}}]}
{"id": "189927896_1", "paragraph": "[BOS] Earlier attempts in multi-hop QA focused on reasoning about the relations in a knowledge base (Jain, 2016; Lin et al., 2018) or tables (Yin et al., 2015) .\n[BOS] The bAbI dataset (Weston et al., 2016) uses synthetic contextx and requires the model to combine multiple pieces of evidence in the text-based context.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ"], "span_citation_mapping": [{"token_start": 17, "token_end": 31, "char_start": 85, "char_end": 130, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Jain, 2016;": "18780529", "Lin et al., 2018)": "52143467"}}}, {"token_start": 32, "token_end": 41, "char_start": 134, "char_end": 159, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yin et al., 2015)": "6715526"}}}, {"token_start": 43, "token_end": 77, "char_start": 168, "char_end": 319, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Weston et al., 2016)": "3178759"}, "Reference": {}}}]}
{"id": "189927896_0", "paragraph": "[BOS] Multi-hop Reading Comprehension The last few years have witnessed significant progress on large-scale QA datasets including cloze-style blank-filling tasks (Hermann et al., 2015) , opendomain QA (Yang et al., 2015) , QA with answer span prediction (Rajpurkar et al., 2016 (Rajpurkar et al., , 2018 , and generative QA (Nguyen et al., 2016) .\n[BOS] However, all of the above datasets are confined to a singledocument context per question domain.\n\n", "discourse_tags": ["Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 22, "token_end": 39, "char_start": 130, "char_end": 184, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hermann et al., 2015)": "6203757"}}}, {"token_start": 40, "token_end": 51, "char_start": 187, "char_end": 220, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yang et al., 2015)": "1373518"}}}, {"token_start": 52, "token_end": 76, "char_start": 223, "char_end": 303, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rajpurkar et al., 2016": "11816014", "(Rajpurkar et al., , 2018": "47018994"}}}, {"token_start": 78, "token_end": 88, "char_start": 310, "char_end": 345, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Nguyen et al., 2016)": "1289517"}}}]}
{"id": "207926704_1", "paragraph": "[BOS] MRQA19, the Machine Reading for Question Answering workshop, introduced a shared task, which tests whether existing machine reading comprehension systems can generalize beyond the datasets on which they were trained.\n[BOS] The task provides six large-scale datasets for training, and evaluates generalization to ten different hidden test datasets.\n[BOS] However these datasets were modified from there original version, and context was limited to 800 tokens.\n[BOS] In addition this shared task only tests for generalization with no intra-domain evaluation.\n[BOS] In contrast, our evaluation server simply provides a single-model evaluation on many different datasets, with no prescriptions about training regimes.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Transition", "Reflection"], "span_citation_mapping": []}
{"id": "207926704_0", "paragraph": "[BOS] Generalization and multi-dataset evaluation Recently there has been some work aimed at exploring the relation and differences between multiple reading comprehension datasets.\n[BOS] MULTIQA (Talmor and Berant, 2019) investigates over ten RC datasets, training on one or more source RC datasets, and evaluating generalization, as well as transfer to a target RC dataset.\n[BOS] This work analyzes the factors that contribute to generalization, and shows that training on a source RC dataset and transferring to a target dataset substantially improves performance.\n[BOS] MultiQA also provides a single format including a model and infrastructure for training and comparing question answering datasets.\n[BOS] We provide no training mechanism, instead focusing on very simple evaluation that is compatible with any training regime, including evaluating on hidden test sets.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 28, "token_end": 120, "char_start": 187, "char_end": 703, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Talmor and Berant, 2019)": "173188058"}, "Reference": {}}}]}
{"id": "104292554_4", "paragraph": "[BOS] abilities of words from a vocabulary V g , but it can also generate words that are names of entities of a specific type.\n[BOS] Each entity type has a separate vocabulary {V 1 , ..., V K } collected from a KB.\n[BOS] KALM learns to predict from context whether to expect an entity from a given type and generalizes over entity types.\n\n", "discourse_tags": ["Transition", "Transition", "Transition"], "span_citation_mapping": []}
{"id": "104292554_3", "paragraph": "[BOS] To the best of our knowledge, KALM is the first unsupervised neural NER approach.\n[BOS] As we discuss in Section 5.4, KALM achieves results comparable to supervised CRF-biLSTM models.\n\n", "discourse_tags": ["Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "104292554_2", "paragraph": "[BOS] Most unsupervised NER models are rule-based (Collins and Singer, 1999; Etzioni et al., 2005; Nadeau et al., 2006) and require feature engineering or parallel corpora (Munro and Manning, 2012) .\n[BOS] Yang and Mitchell (2017) incorporate a KB to the CRF-biLSTM model (Lample et al., 2016) by embedding triples from a KB obtained using TransE (Bordes et al., 2013) .\n[BOS] Peters et al. (2017) add pre-trained language model embeddings as knowledge to the input of a CRF-biLSTM model, while still requiring labels in training.\n\n", "discourse_tags": ["Narrative_cite", "Multi_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 7, "token_end": 35, "char_start": 39, "char_end": 119, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Collins and Singer, 1999;": "859162", "Etzioni et al., 2005;": "7162988", "Nadeau et al., 2006)": "17071982"}}}, {"token_start": 36, "token_end": 51, "char_start": 124, "char_end": 197, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Munro and Manning, 2012)": "18853476"}}}, {"token_start": 53, "token_end": 97, "char_start": 206, "char_end": 368, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Yang and Mitchell (2017)": "19968363"}, "Reference": {"(Lample et al., 2016)": "6042994", "(Bordes et al., 2013)": null}}}, {"token_start": 99, "token_end": 133, "char_start": 377, "char_end": 530, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Peters et al. (2017)": "7197241"}, "Reference": {}}}]}
{"id": "104292554_1", "paragraph": "[BOS] Unsupervised predictive learning has been proven effective in improving text understanding.\n[BOS] ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018) used different unsupervised objectives to pre-train text models which have advanced the state-of-the-art for many NLP tasks.\n[BOS] Similar to these approaches KALM is trained end-to-end using a predictive objective on large corpus of text.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Transition"], "span_citation_mapping": [{"token_start": 15, "token_end": 63, "char_start": 104, "char_end": 286, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Peters et al., 2018)": "3626819", "(Devlin et al., 2018)": "52967399"}, "Reference": {}}}]}
{"id": "104292554_0", "paragraph": "[BOS] Our work draws inspiration from Ahn et al. (2016) , who propose to predict whether the word to generate has an underlying fact or not.\n[BOS] Their model can generate knowledge-related words by copying from the description of the predicted fact.\n[BOS] While theoretically interesting, their model functions only in a very constrained setting as it requires extra information: a shortlist of candidate entities that are mentioned in the text.\n[BOS] Several efforts successfully extend LMs with entities from a knowledge base and their types, but require that entity models are trained separately from supervised entity labels.\n[BOS] Parvez et al. (2018) and Xin et al. (2018) explicitly model the type of the next word in addition to the word itself.\n[BOS] In particular, Parvez et al. (2018) use two LSTMbased language models, an entity type model and an entity composite (entity type) model.\n[BOS] Xin et al. (2018) use a similarly purposed entity typing module and a LM-enhancement module.\n[BOS] Instead of entity type generation, Gu et al. (2018) propose to explicitly decompose word generation into sememe (a semantic language unit of meaning) generation and sense generation, but requires sememe labels.\n[BOS] Yang et al. (2016) propose a pointer-network LM that can point to a 1-D or 2-D database record during inference.\n[BOS] At each time step, the model decides whether to point to the database or the general vocabulary.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Transition", "Multi_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 7, "token_end": 84, "char_start": 38, "char_end": 446, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ahn et al. (2016)": "2600027"}, "Reference": {}}}, {"token_start": 114, "token_end": 145, "char_start": 637, "char_end": 754, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Parvez et al. (2018)": "44097540", "Xin et al. (2018)": "53080998"}, "Reference": {}}}, {"token_start": 149, "token_end": 179, "char_start": 776, "char_end": 897, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Parvez et al. (2018)": "44097540"}, "Reference": {}}}, {"token_start": 180, "token_end": 202, "char_start": 904, "char_end": 996, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Xin et al. (2018)": "53080998"}, "Reference": {}}}, {"token_start": 209, "token_end": 244, "char_start": 1038, "char_end": 1213, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Gu et al. (2018)": "53080999"}, "Reference": {}}}, {"token_start": 245, "token_end": 295, "char_start": 1220, "char_end": 1435, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Yang et al. (2016)": "1899153"}, "Reference": {}}}]}
{"id": "59604441_0", "paragraph": "[BOS] Memory networks provide a general architecture for online updates to a set of distinct memories Sukhbaatar et al., 2015) .\n[BOS] Henaff et al. (2017) used memories to track the states of multiple entities in a text, but they predefined the alignment of entities to memories, rather than learning to align entities with memories using gates.\n[BOS] The incorporation of entities into language models has also been explored in prior work (Yang et al., 2017; Kobayashi et al., 2017) ; similarly, Dhingra et al. (2018) augment the gated recurrent unit (GRU) architecture with additional edges between coreferent mentions.\n[BOS] In general, this line of prior work assumes that coreference information is available at test time (e.g., from a coreference resolution system), rather than determining coreference in an online fashion.\n[BOS] Ji et al. (2017) propose a generative entity-aware language model that incorporates coreference as a discrete latent variable.\n[BOS] For this reason, importance sampling is required for inference, and the model cannot be trained on unlabeled data.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ", "Transition", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 2, "token_end": 27, "char_start": 6, "char_end": 126, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Sukhbaatar et al., 2015)": "1399322"}}}, {"token_start": 29, "token_end": 71, "char_start": 135, "char_end": 346, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Henaff et al. (2017)": "11243593"}, "Reference": {}}}, {"token_start": 73, "token_end": 101, "char_start": 357, "char_end": 484, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Kobayashi et al., 2017)": "855563"}}}, {"token_start": 104, "token_end": 132, "char_start": 498, "char_end": 622, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Dhingra et al. (2018)": "4957155"}, "Reference": {}}}, {"token_start": 176, "token_end": 201, "char_start": 838, "char_end": 964, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ji et al. (2017)": "5564363"}, "Reference": {}}}]}
{"id": "198985828_2", "paragraph": "[BOS] 1 See also the detailed criticism of analogical inference with word embeddings in general in (Rogers et al., 2017) .\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 9, "token_end": 26, "char_start": 43, "char_end": 120, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rogers et al., 2017)": "28702383"}}}]}
{"id": "198985828_1", "paragraph": "[BOS] Using distributional word representations to trace diachronic semantic shifts (including those reflecting social and cultural events) has received substantial attention in the recent years.\n[BOS] Our work shares some of the workflow with Kutuzov et al. (2017) .\n[BOS] They used a supervised approach to analogical reasoning, applying 'semantic directions' learned on the previous year's armed conflicts data to the subsequent year.\n[BOS] We extend their research by significantly reformulating the analogy task, making it more realistic, and finding ways to cope with false positives (insurgent armed groups predicted for locations where no armed conflicts are registered this year).\n[BOS] In comparison to their work, we also use newer and larger corpora of news texts and the most recent version of the UCDP dataset.\n[BOS] For brevity, we do not describe the emerging field of diachronic word embeddings in details, referring the interested readers to the recent surveys of Kutuzov et al. (2018) and Tang (2018) .\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 38, "token_end": 82, "char_start": 230, "char_end": 437, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Kutuzov et al. (2017)": null}}}, {"token_start": 183, "token_end": 200, "char_start": 971, "char_end": 1019, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Kutuzov et al. (2018)": "47019063", "Tang (2018)": null}}}]}
{"id": "198985828_0", "paragraph": "[BOS] The issue of linguistic regularity manifested in relational similarity has been studied for a long time.\n[BOS] Due to the long-standing criticism of strictly binary relation structure, SemEval-2012 offered the task to detect the degree of relational similarity (Jurgens et al., 2012) .\n[BOS] This means that multiple correct answers exist, but they should be ranked differently.\n[BOS] Somewhat similar improvements to the well-known word analogies dataset from (Mikolov et al., 2013b) were presented in the BATS analogy test set (Gladkova et al., 2016) , also featuring multiple correct answers.\n[BOS] 1 Our One-to-X analogy setup extends this by introducing the possibility of the correct answer being 'None'.\n[BOS] In the cases when correct answers exist, they are equally ranked, but their number can be different.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Single_summ", "Reflection", "Transition"], "span_citation_mapping": [{"token_start": 33, "token_end": 57, "char_start": 191, "char_end": 289, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Jurgens et al., 2012)": "5278106"}}}, {"token_start": 83, "token_end": 126, "char_start": 439, "char_end": 601, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Gladkova et al., 2016)": "15829781"}, "Reference": {"(Mikolov et al., 2013b)": "16447573"}}}]}
{"id": "209892309_1", "paragraph": "[BOS] Our work focuses on training a model using a larger dataset and fine-tune using another related low-resource dataset rather than multi-task learning.\n[BOS] We also evaluate how additional training examples impact transfer learning models.\n\n", "discourse_tags": ["Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "209892309_0", "paragraph": "[BOS] Encoder-decoder architectures based on neural networks have been successfully applied to semantic parsing (Dong and Lapata, 2016; Jia and Liang, 2016; Ling et al., 2016; Dong and Lapata, 2018) .\n[BOS] Since then, several ideas such as including attention mechanism (Dong and Lapata, 2016) , multi-task learning (Susanto and Lu, 2017; Herzig and Berant, 2017; Fan et al., 2017) , data augmentation (Jia and Liang, 2016; Koisk et al., 2016) and two-steps (coarse-to-fine) decoder (Dong and Lapata, 2018) have been applied to semantic parsing models with the aim of boosting performance.\n[BOS] Similar to our work, others tried to overcome the lack of annotated data by leveraging existing datasets from related domains.\n[BOS] Previous works from Herzig and Berant (2017) and Fan et al. (2017) used a multi-task framework to jointly learn the neural semantic parsing model and encourage parameter sharing between different datasets.\n[BOS] The model proposed by Herzig and Berant (2017) used multiple knowledge bases in different domains to enhance the model performance.\n[BOS] On the other hand, the work from Fan et al. (2017) leveraged access to a very large labeled dataset to help a small one.\n[BOS] However, their models are trained using proprietary datasets, which are not publicly available, thus making model comparison unfeasible.\n[BOS] The work proposed by Damonte et al. (2019) investigates the possibility of transfer learning to tackle the issue of lacking annotated data on neural semantic parsing.\n[BOS] They used more complex model and data sets compared to our work.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Transition", "Multi_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 15, "token_end": 45, "char_start": 95, "char_end": 198, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dong and Lapata, 2016;": "15412473", "Jia and Liang, 2016;": "7218315", "Ling et al., 2016;": "14434979", "Dong and Lapata, 2018)": "44167998"}}}, {"token_start": 55, "token_end": 65, "char_start": 251, "char_end": 294, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dong and Lapata, 2016)": "15412473"}}}, {"token_start": 66, "token_end": 93, "char_start": 297, "char_end": 382, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Susanto and Lu, 2017;": "20138082", "Herzig and Berant, 2017;": "14215409", "Fan et al., 2017)": "5955929"}}}, {"token_start": 94, "token_end": 111, "char_start": 385, "char_end": 444, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Jia and Liang, 2016;": "7218315"}}}, {"token_start": 112, "token_end": 131, "char_start": 449, "char_end": 507, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 170, "token_end": 211, "char_start": 730, "char_end": 935, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Herzig and Berant (2017)": "14215409", "Fan et al. (2017)": "5955929"}, "Reference": {}}}, {"token_start": 212, "token_end": 237, "char_start": 942, "char_end": 1073, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Herzig and Berant (2017)": "14215409"}, "Reference": {}}}, {"token_start": 243, "token_end": 293, "char_start": 1099, "char_end": 1343, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Fan et al. (2017)": "5955929"}, "Reference": {}}}, {"token_start": 294, "token_end": 341, "char_start": 1350, "char_end": 1587, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Damonte et al. (2019)": "75135250"}, "Reference": {}}}]}
{"id": "202771074_1", "paragraph": "[BOS] In computer vision, Zhou et al. (2018) analyze the relationship between individual units of a CNN and label prediction.\n[BOS] To the best of our knowledge, however, in the field of NLP, there has been little work on analyzing the affinity between labels and neurons of recurrent networks.\n[BOS] This paper aims to address this problem.\n\n", "discourse_tags": ["Single_summ", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 3, "token_end": 26, "char_start": 9, "char_end": 125, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "202771074_0", "paragraph": "[BOS] Recently, work has been done to analyze continuous representations in NLP.\n[BOS] Shi et al. (2016) and Qian et al. (2016) analyze linguistic properties carried by representation vectors using external supervision.\n[BOS] and further analyze linguistic information in individual neurons from neural machine translation representations in an unsupervised manner.\n[BOS] For LSTMs of language models, Karpathy et al. (2015) identify individual neurons that trigger for specific information, such as bracket and sequence length, and Radford et al. (2017) discover neurons that encode sentiment information.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Multi_summ", "Multi_summ"], "span_citation_mapping": [{"token_start": 17, "token_end": 44, "char_start": 87, "char_end": 219, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Shi et al. (2016)": "7197724", "Qian et al. (2016)": "17886097"}, "Reference": {}}}, {"token_start": 65, "token_end": 96, "char_start": 376, "char_end": 527, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Karpathy et al. (2015)": "988348"}, "Reference": {}}}, {"token_start": 98, "token_end": 113, "char_start": 533, "char_end": 606, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Radford et al. (2017)": "14838925"}, "Reference": {}}}]}
{"id": "208049931_1", "paragraph": "[BOS] Recently, motivated by the success of crosslingual embeddings (Artetxe et al., 2016; Zhang et al., 2017; Conneau et al., 2017) , several works have tried to train NMT or SMT models using unsupervised setting, in which the model only has access to unlabeled data.\n[BOS] For example, Lample et al. (2018a) propose a model that consists of a single encoder and a single decoder for both languages, respectively responsible for encoding source and target sentences to a shared latent space and to decode from that latent space to the source or target domain.\n[BOS] Different from (Lample et al., 2018a) , Artetxe et al. (2018b) introduce a shared encoder but two independent decoders with one for each language.\n[BOS] Both of these two works mentioned above utilize denoising auto-encoding to reconstruct their noisy inputs and incorporate back-translation into cross-language training procedure.\n[BOS] Further, Yang et al. (2018) extend the single encoder by using two independent encoders but sharing some partial weights, which are responsible for alleviating the weakness in keeping language-specific characteristics of the shared encoder.\n[BOS] And the entire system is fine-tuned by introducing two global GANs with one for each language.\n[BOS] More recently, Artetxe et al. (2018a) and Lample et al. (2018b) propose an alternative approach based on phrase-based statistical machine translation, which profits from the modular architecture of SMT.\n[BOS] In addition, Lample et al. (2018b) also introduce a novel cross-lingual embedding training method which is particularly suitable for related languages (e.g., English-French and English-German).\n[BOS] Ren et al. (2019) introduce SMT models as posterior regularization, in which SMT and NMT models boost each other through iterative back-translation in a unified EM training algorithm.\n[BOS] Wu et al. (2019) propose an alternative for back-translation, , extract-edit, to extract and then edit real sentences from the target monolingual corpora.\n[BOS] Lample and Conneau (2019) and Song et al. (2019) propose to pretrain cross-lingual language models for the initialization stage of unsupervised neural machine translation, which is critical to the performance of their proposed model.\n[BOS] In contrast to theirs, we propose an effective training method for unsupervised NMT that models future rewards to optimize the global word predictions via neural policy reinforcement learning, which can be applied to arbitrary architectures and language pairs easily.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Multi_summ", "Multi_summ", "Single_summ", "Single_summ", "Multi_summ", "Single_summ", "Single_summ", "Single_summ", "Multi_summ", "Reflection"], "span_citation_mapping": [{"token_start": 9, "token_end": 39, "char_start": 44, "char_end": 132, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Artetxe et al., 2016;": "1040556", "Zhang et al., 2017;": "26873455", "Conneau et al., 2017)": "3470398"}}}, {"token_start": 71, "token_end": 124, "char_start": 288, "char_end": 560, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Lample et al. (2018a)": "3518190"}, "Reference": {}}}, {"token_start": 127, "token_end": 193, "char_start": 582, "char_end": 898, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Lample et al., 2018a)": "3518190", "Artetxe et al. (2018b)": "3515219"}, "Reference": {}}}, {"token_start": 196, "token_end": 259, "char_start": 914, "char_end": 1246, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Yang et al. (2018)": "13748556"}, "Reference": {}}}, {"token_start": 263, "token_end": 306, "char_start": 1268, "char_end": 1455, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Artetxe et al. (2018a)": "52166727", "Lample et al. (2018b)": "5033497"}, "Reference": {}}}, {"token_start": 310, "token_end": 352, "char_start": 1475, "char_end": 1655, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Lample et al. (2018b)": "5033497"}, "Reference": {}}}, {"token_start": 353, "token_end": 392, "char_start": 1662, "char_end": 1845, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ren et al. (2019)": "58004593"}, "Reference": {}}}, {"token_start": 393, "token_end": 428, "char_start": 1852, "char_end": 2006, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wu et al. (2019)": "102353583"}, "Reference": {}}}, {"token_start": 429, "token_end": 479, "char_start": 2013, "char_end": 2246, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Lample and Conneau (2019)": "58981712", "Song et al. (2019)": "146808476"}, "Reference": {}}}]}
{"id": "208049931_0", "paragraph": "[BOS] In order to reduce the exposure bias and optimize the metrics used to evaluate sequence modeling tasks (like BLEU, ROUGE or METEOR) directly, reinforcement learning (RL) has been widely used in many of recent works on machine translation (Ranzato et al., 2016; Shen et al., 2016; He et al., 2017; Bahdanau et al., 2017; Li et al., 2017) , text summarization (Paulus et al., 2018; Wu and Hu, 2018; Li et al., 2018; , dialogue generation (Li et al., 2016) , and question answering .\n[BOS] However, our proposed method is the first use in combination with reinforcement learning for unsupervised NMT to explicitly enhance back-translation.\n\n", "discourse_tags": ["Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 46, "token_end": 88, "char_start": 224, "char_end": 342, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ranzato et al., 2016;": "7147309", "Shen et al., 2016;": "3913537", "He et al., 2017;": "4021462", "Bahdanau et al., 2017;": "14096841", "Li et al., 2017)": "14635535"}}}, {"token_start": 89, "token_end": 113, "char_start": 345, "char_end": 418, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Paulus et al., 2018;": "21850704", "Wu and Hu, 2018;": "4999752"}}}, {"token_start": 115, "token_end": 125, "char_start": 422, "char_end": 459, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Li et al., 2016)": "3147007"}}}]}
{"id": "186206212_1", "paragraph": "[BOS] Unlike popular Reading Comprehension datasets such as MovieQA (Tapaswi et al., 2016) and SQuAD (Rajpurkar et al., 2018 (Rajpurkar et al., , 2016 , which are created by crowdsourcing, we work with authentic user-generated data.\n[BOS] This means that the data is collected from sources where users spontaneously created content for their own purposes.\n[BOS] Since there is no guarantee that reviews contain text related to the question, there is no span data that can be reliably used to provide the answer.\n[BOS] This, together with the considerable volume of review text, contributes to the difficulty of the task.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 9, "token_end": 21, "char_start": 60, "char_end": 90, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Tapaswi et al., 2016)": "1017389"}}}, {"token_start": 22, "token_end": 43, "char_start": 95, "char_end": 150, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rajpurkar et al., 2018": "47018994", "(Rajpurkar et al., , 2016": "11816014"}}}]}
{"id": "186206212_0", "paragraph": "[BOS] A number of studies have explored the use of customer reviews in retrieval and question answering.\n[BOS] Using Amazon data, Yu et al. (2018) develop a framework which returns a ranked list of sentences from reviews or existing question-answer pairs for a given question.\n[BOS] Xu et al. (2019) create a new dataset comprising Amazon laptop reviews and questions and Yelp restaurant reviews and questions, where reviews are used to answer questions in multiple-turn dialogue form.\n[BOS] Bogdanova et al. (2017) and Bogdanova and Foster (2016) ) do not use review data but also focus on QA over usergenerated content, attempting to find similar questions or rank answers in user fora.\n[BOS] We use the same Amazon data as Yu et al. (2018) but consider a wider set of domains (they consider only two), and attempt to directly answer yes/no questions.\n[BOS] To the best of our knowledge, the novelty in our work lies in trying to directly answer customer questions using user-generated reviews.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Multi_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 20, "token_end": 54, "char_start": 111, "char_end": 276, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Yu et al. (2018)": "52011507"}, "Reference": {}}}, {"token_start": 55, "token_end": 97, "char_start": 283, "char_end": 485, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Xu et al. (2019)": "59600068"}, "Reference": {}}}, {"token_start": 98, "token_end": 145, "char_start": 492, "char_end": 688, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Bogdanova et al. (2017)": "3228703", "Bogdanova and Foster (2016)": "6073237"}, "Reference": {}}}, {"token_start": 150, "token_end": 173, "char_start": 711, "char_end": 803, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Yu et al. (2018)": "52011507"}}}]}
{"id": "207852642_3", "paragraph": "[BOS] (2) only numbers in the top half and the bottom half (with the same test set) are comparable.\n[BOS] The original test set has 630 MRs and 4,352 slots in total.\n[BOS] The cleaned test set has 1,847 MRs and 11,547 slots; however, for the runs with SC-LSTM these counts are 1,800 and 11,101, respectively, since some items had to be dropped due to preprocessing issues.\n\n", "discourse_tags": ["Other", "Other", "Other"], "span_citation_mapping": []}
{"id": "207852642_2", "paragraph": "[BOS] Furthermore, in accordance with our observations, related work also reports a relation between hallucinations and data diversity: Rohrbach et al. (2018) observe an increase for \"novel compositions of objects at test time\", i.e. non-overlapping test and training sets (cf.\n[BOS] Section 3); whereas Lee et al. (2019) reports data augmentation as one of the most efficient counter measures.\n[BOS] In future work, we plan to experimentally manipulate these factors to disentangle the relative contributions of data cleanliness and diversity.\n[BOS] Table 5 : Absolute numbers of errors (added slots/missed slots/wrong slot values) and numbers of completely correct instances in all our experiments (compare to Tables 2 and 3 in the paper).\n[BOS] Note that (1) the numbers are averages over 5 runs with different random network initializations, hence the non-integer values;\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Reflection", "Other", "Other"], "span_citation_mapping": [{"token_start": 23, "token_end": 57, "char_start": 136, "char_end": 272, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Rohrbach et al. (2018)": "52176506"}, "Reference": {}}}, {"token_start": 66, "token_end": 86, "char_start": 304, "char_end": 394, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Lee et al. (2019)": "53593076"}, "Reference": {}}}]}
{"id": "207852642_1", "paragraph": "[BOS] Contemporaneous with our work is the effort of Nie et al. (2019) , who focus on automatic data cleaning using a NLU iteratively bootstrapped from the noisy data.\n[BOS] Their analysis similarly finds that omissions are more common than hallucinations.\n[BOS] Correcting for missing slots, i.e. forcing the generator to verbalise all slots during training, leads to the biggest performance improvement.\n[BOS] This phenomenon is also observed by Duek et al. (2018 Duek et al. ( , 2019 for systems in the E2E NLG challenge, but stands in contrast to work on related tasks, which mostly reports on hallucinations (i.e. adding information not grounded in the input), as observed for image captioning (Rohrbach et al., 2018) , sports report generation (Wiseman et al., 2017) , machine translation (Koehn and Knowles, 2017; Lee et al., 2019) , and question answering (Feng et al., 2018) .\n[BOS] These previous works suggest that the most likely case of hallucinations is an over-reliance on language priors, i.e. memorising 'which words go together'.\n[BOS] Similar priors could equally exist in the E2E data for omitting a slot; this might be connected with the fact that the E2E test set MRs tend to be longer than training MRs (6.91 slots on average for test MRs vs. 5.52 for training MRs) and that a large part of them is 'saturated', i.e. contains all possible 8 attributes.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Multi_summ", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 2, "token_end": 81, "char_start": 6, "char_end": 405, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Nie et al. (2019)": "196183567"}, "Reference": {}}}, {"token_start": 88, "token_end": 208, "char_start": 448, "char_end": 883, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Du\u0161ek et al. (2018": "52908627", "Du\u0161ek et al. ( , 2019": null}, "Reference": {"(Rohrbach et al., 2018)": "52176506", "(Wiseman et al., 2017)": "23892230", "(Koehn and Knowles, 2017;": "8822680", "Lee et al., 2019)": "53593076"}}}]}
{"id": "207852642_0", "paragraph": "[BOS] We present a detailed study of semantic errors in NNLG outputs and how these relate to noise in training data.\n[BOS] We found that even imperfectly cleaned input data significantly improves semantic accuracy for seq2seq-based generators (up to 97% relative error reduction with the reranker), while only causing a slight decrease in fluency.\n\n", "discourse_tags": ["Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "91184245_3", "paragraph": "[BOS] The most related approach is the IRN model (Zhou et al., 2018) , composed of an input module, a memory-based reasoning module, and an answer module.\n[BOS] At each hop, it predicts a relation path using the reasoning module, and also optimizes it using intermediate results.\n[BOS] However, UHop has demonstrated the ability to process large-scale knowledge graphs in experiments conducted on Freebase (Bordes et al., 2015) .\n[BOS] In contrast, IRN consumes memory linearly to the size of the knowledge graph, resulting in a limited workspace, e.g., they use a subset of Freebase in their experiments.\n[BOS] Also, IRN still uses a constraint for the number of maximum hops in the experiments, while UHop needs no such limit.\n[BOS] Most importantly, as UHop is a framework which facilitates the use of different models, we can expect the performance of UHop to remain competitive with the state of the art over time.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Narrative_cite", "Transition", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 8, "token_end": 62, "char_start": 39, "char_end": 279, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Zhou et al., 2018)": "3806582"}, "Reference": {}}}, {"token_start": 70, "token_end": 93, "char_start": 321, "char_end": 427, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bordes et al., 2015)": "9605730"}}}]}
{"id": "91184245_2", "paragraph": "[BOS] For the solution, we turn to the field of multihop knowledge based reasoning.\n[BOS] Early methods include the Path-Ranking Algorithm and its variants.\n[BOS] (Lao et al., 2011; Gardner et al., 2014 Gardner et al., , 2013 Toutanova et al., 2015) The drawback of these methods is that they use random walks independent of the type of input.\n[BOS] DeepPath (Xiong et al., 2017) and MINERVA (Das et al., 2017) tackle this issue by framing the multi-hop reasoning problem as a Markov decision process, efficiently searching for paths using reinforcement learning; others propose an algorithm for learning logical rules, a variational auto-encoder view of the knowledge graph (Chen et al., 2018b; Zhang et al., 2018b) , and reward shaping technique (Lin et al., 2018) for further improvement.\n[BOS] The major difference between UHop and these methods is that they do not utilize annotated relations and hence require REINFORCE training (Williams, 1992) for optimization.\n[BOS] As some datasets are already annotated with relations and paths, direct learning using an intermediate reward is more reasonable.\n[BOS] Hence UHop adopts a novel comparative termination decision module to control the search process of the relation path.\n\n", "discourse_tags": ["Reflection", "Transition", "Narrative_cite", "Multi_summ", "Narrative_cite", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 32, "token_end": 84, "char_start": 163, "char_end": 343, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lao et al., 2011;": "1619841", "Gardner et al., 2014": "577805", "Gardner et al., , 2013": "5679499", "Toutanova et al., 2015)": "2127100"}}}, {"token_start": 85, "token_end": 131, "char_start": 350, "char_end": 562, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Xiong et al., 2017)": "20667722", "(Das et al., 2017)": "13206339"}, "Reference": {}}}, {"token_start": 135, "token_end": 168, "char_start": 582, "char_end": 716, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chen et al., 2018b;": "4669223", "Zhang et al., 2018b)": "8953884"}}}, {"token_start": 170, "token_end": 181, "char_start": 723, "char_end": 766, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lin et al., 2018)": "52143467"}}}, {"token_start": 197, "token_end": 216, "char_start": 858, "char_end": 969, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "91184245_1", "paragraph": "[BOS] Embedding-based methods first allocate candidates from the knowledge graph, represent these candidates as distributed embedding vectors, and choose or rank these vectors.\n[BOS] Here the candidates can be either entities or relations.\n[BOS] Some use embedding-based models to predict answers directly (Dong et al., 2015; Bast and Haussmann, 2015; Hao et al., 2017; Zhou et al., 2018; Lukovnikov et al., 2017) , whereas others focus on extracting relation paths and require further procedures to select the answer entity (Bordes et al., 2015; Xu et al., 2016; Yin et al., 2016; Yu et al., 2017; Zhang et al., 2018a; Yu et al., 2018; Chen et al., 2018a; Shen et al., 2018) .\n[BOS] Our work follows the latter methods in focusing on predicting relation paths, but we seek to eliminate the need to assume in advance a maximum number of hops.\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 43, "token_end": 93, "char_start": 255, "char_end": 413, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dong et al., 2015;": "12926055", "Bast and Haussmann, 2015;": "495573", "Hao et al., 2017;": "3896491", "Zhou et al., 2018;": "3806582", "Lukovnikov et al., 2017)": "12983389"}}}, {"token_start": 98, "token_end": 170, "char_start": 440, "char_end": 675, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bordes et al., 2015;": "9605730", "Xu et al., 2016;": "139787", "Yin et al., 2016;": "557620", "Yu et al., 2017;": "7752968", "Zhang et al., 2018a;": "3122402", "Yu et al., 2018;": "3674497", "Chen et al., 2018a;": "51875514", "Shen et al., 2018)": "46921712"}}}]}
{"id": "91184245_0", "paragraph": "[BOS] State-of-the-art KBQA methods are in general based on either semantic parsing, or on embedding (Zhou et al., 2018) .\n[BOS] Semantic parsing methods learn semantic parsers which parse natural language input queries into logical forms, and then use the logical forms to query the KG for answers (Berant et al., 2013; Yih et al., 2015 Yih et al., , 2016 Iyyer et al., 2017; Peng et al., 2017; Sorokin and Gurevych, 2018) .\n[BOS] These systems are effective and provide deep interpretation of the question, but require expensive data annotation, or require training using reinforcement learning.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 9, "token_end": 32, "char_start": 23, "char_end": 120, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhou et al., 2018)": "3806582"}}}, {"token_start": 34, "token_end": 112, "char_start": 129, "char_end": 423, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Berant et al., 2013;": "6401679", "Yih et al., 2015": "18309765", "Yih et al., , 2016": null, "Iyyer et al., 2017;": "2623009", "Peng et al., 2017;": "31484634", "Sorokin and Gurevych, 2018)": "51974493"}}}]}
{"id": "104292422_3", "paragraph": "[BOS] Generation of adversarial examples has also been used to increase the robustness of NLP systems as part of the Build it, Break It, The Language Edition Workshop (Ettinger et al., 2017) .\n[BOS] In this workshop, builders designed systems for Sentiment Analysis and Question-Answer Driven Semantic Role Labeling tasks and were evaluated on the accuracy of their models on adversarial test cases designed by breakers.\n[BOS] Whereas Build It Break It adversarial generation required submissions to match the format of a starter dataset and offered limited adversarial access to the target NLP systems, the AQuA construction procedure allows for entirely new questions and provide adversaries with a target model throughout the submission process, allowing workers to experiment.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 30, "token_end": 43, "char_start": 141, "char_end": 190, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ettinger et al., 2017)": "25422730"}}}]}
{"id": "104292422_2", "paragraph": "[BOS] State-of-the-art models for natural language inference have rapidly improved and approach human performance, which leaves little room for continued improvement on current benchmarks.\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "104292422_1", "paragraph": "[BOS] Earlier work has established multiple benchmarks for natural language inference and linguistic entailment with the release SNLI (Bowman et al., 2015) and MultiNLI datasets (Williams et al., 2018) .\n[BOS] In these tasks, systems must identify whether a hypothesis agrees with or contradicts a provided premise.\n[BOS] In these datasets, determining entailment solely relies upon the provided premise and does not require a questionanswering system to utilize external knowledge.\n[BOS] More recently, the SWAG dataset (Zellers et al., 2018) directly targets natural language inference that leverages commonsense knowledge.\n[BOS] SWAG multiple choice completion questions are constructed using a video caption as the ground truth with incorrect counterfactuals created using adversarially filtered generations from an LSTM language model.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 19, "token_end": 30, "char_start": 129, "char_end": 155, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bowman et al., 2015)": "14604520"}}}, {"token_start": 31, "token_end": 42, "char_start": 160, "char_end": 201, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Williams et al., 2018)": "3432876"}}}, {"token_start": 96, "token_end": 157, "char_start": 508, "char_end": 840, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Zellers et al., 2018)": "52019251"}, "Reference": {}}}]}
{"id": "104292422_0", "paragraph": "[BOS] Prior work in question-answering has largely focused on the development of reading comprehension-based question-answering and resulted in the creation of several large datasets for factoid extraction such as SQuAD (Rajpurkar et al., 2016 (Rajpurkar et al., , 2018 and the Google Natural Questions datasets (Kwiatkowski et al., 2019) .\n[BOS] In these tasks, extraction of correct answers from the provided context requires little external world knowledge, understanding of intents, or other commonsense knowledge.\n\n", "discourse_tags": ["Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 37, "token_end": 58, "char_start": 214, "char_end": 269, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rajpurkar et al., 2016": "11816014", "(Rajpurkar et al., , 2018": "47018994"}}}, {"token_start": 60, "token_end": 76, "char_start": 278, "char_end": 338, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kwiatkowski et al., 2019)": null}}}]}
{"id": "125969731_2", "paragraph": "[BOS] More closely related to the idioms that we use for decoding is Allamanis and Sutton (2014) , who develop a system (HAGGIS) to automatically mine idioms from large code bases.\n[BOS] They focused on finding idioms that are interesting and explainable, e.g., those that can be included as preset code templates in programming IDEs.\n[BOS] Instead, we learn idiomatic structures that are frequently used and can be easily associated with natural language phrases in our dataset.\n[BOS] The production of large subtrees in a single step directly translates to a large speedup in training and inference.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 15, "token_end": 56, "char_start": 69, "char_end": 254, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Allamanis and Sutton (2014)": "2923536"}, "Reference": {}}}]}
{"id": "125969731_1", "paragraph": "[BOS] Another different but related method to produce source code is with the help of sketches, which are code snippets containing slots in the place of low-level information such as variable names and arguments.\n[BOS] Dong and Lapata (2018) generate sketches as intermediate representations to convert NL to logical forms; Hayati et al. (2018) retrieve sketches from a large training corpus and later modify them for the current input; Murali et al. (2018) use a combination of neural learning and type-guided combinatorial search to convert existing sketches into executable programs, whereas Nye et al. (2019) additionally also generate the sketches before synthesising programs.\n[BOS] While we don't explicitly generate sketches, we find that our idiom-based decoder learns to generate commonly used programming sketches with slots, and fills them in during subsequent decoding timesteps.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Reflection"], "span_citation_mapping": [{"token_start": 43, "token_end": 62, "char_start": 219, "char_end": 322, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Dong and Lapata (2018)": "44167998"}, "Reference": {}}}, {"token_start": 63, "token_end": 87, "char_start": 324, "char_end": 435, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hayati et al. (2018)": "52136345"}, "Reference": {}}}, {"token_start": 88, "token_end": 116, "char_start": 437, "char_end": 585, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Murali et al. (2018)": "4852673"}, "Reference": {}}}, {"token_start": 118, "token_end": 138, "char_start": 595, "char_end": 682, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Nye et al. (2019)": "62841423"}, "Reference": {}}}]}
{"id": "125969731_0", "paragraph": "[BOS] Neural encoder-decoder models have proved effective in mapping NL to logical forms (Dong and Lapata, 2016) and also for directly producing general purpose programs (Iyer et al., 2017 (Iyer et al., , 2018 .\n[BOS] Ling et al. (2016) use a sequence-tosequence model with attention and a copy mechanism to generate source code.\n[BOS] Instead of directly generating a sequence of code tokens, recent methods focus on constrained decoding mechanisms to generate syntactically correct output using a decoder that is either grammar-aware or has a dynamically determined modular structure paralleling the structure of the abstract syntax tree (AST) of the code (Rabinovich et al., 2017; Yin and Neubig, 2017) .\n[BOS] Iyer et al. (2018) use a similar decoding approach but use a specialized context encoder for the task of context-dependent code generation.\n[BOS] We augment these neural encoder-decoder models with the ability to decode in terms of frequently occurring higher level idiomatic structures to achieve gains in accuracy and training time.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Multi_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 11, "token_end": 24, "char_start": 61, "char_end": 112, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dong and Lapata, 2016)": "15412473"}}}, {"token_start": 27, "token_end": 49, "char_start": 126, "char_end": 209, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Iyer et al., 2017": "497108", "(Iyer et al., , 2018": "52125417"}}}, {"token_start": 51, "token_end": 76, "char_start": 218, "char_end": 329, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ling et al. (2016)": "14434979"}, "Reference": {}}}, {"token_start": 91, "token_end": 149, "char_start": 418, "char_end": 705, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Rabinovich et al., 2017;": "13529592", "Yin and Neubig, 2017)": "12718048"}, "Reference": {}}}, {"token_start": 151, "token_end": 180, "char_start": 714, "char_end": 853, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Iyer et al. (2018)": "52125417"}, "Reference": {}}}]}
{"id": "202773073_0", "paragraph": "[BOS] External Knowledge Enhanced MRC Models There are several models that use knowledge for machine comprehension (Yang and Mitchell, 2017; Mihaylov and Frank, 2018; Weissenborn, 2017; Bauer et al., 2018; Pan et al., 2019) .\n[BOS] Mihaylov and Frank (2018) relies on the ability of the attention mechanism to retrieve relevant pieces of knowledge, and Bauer et al. (2018) employs multihop commonsense paths to help multi-hop reasoning.\n[BOS] They treat retrieved knowledge triples as sequences and use sequence modeling methods to compress the representation of knowledge, which are not based on graph structure.\n[BOS] On the contrary, we organize knowledge as sub-graphs, then update the representation of nodes on sub-graphs with graph neural network.\n\n", "discourse_tags": ["Narrative_cite", "Multi_summ", "Multi_summ", "Reflection"], "span_citation_mapping": [{"token_start": 13, "token_end": 54, "char_start": 79, "char_end": 223, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yang and Mitchell, 2017;": "19968363", "Mihaylov and Frank, 2018;": "29151507", "Weissenborn, 2017;": "195345850", "Bauer et al., 2018;": "52290656", "Pan et al., 2019)": "59600051"}}}, {"token_start": 56, "token_end": 79, "char_start": 232, "char_end": 347, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Mihaylov and Frank (2018)": "29151507"}, "Reference": {}}}, {"token_start": 81, "token_end": 129, "char_start": 353, "char_end": 613, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Bauer et al. (2018)": "52290656"}, "Reference": {}}}]}
{"id": "209070321_6", "paragraph": "[BOS] The use of expected head vectors as input of the step encoder is related to the syntactic head attention of the SRL neural architecture in (Strubell et al., 2018) .\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 23, "token_end": 38, "char_start": 118, "char_end": 168, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Strubell et al., 2018)": "5068376"}}}]}
{"id": "209070321_5", "paragraph": "[BOS] Our encoder-decoder architecture together with independence assumptions made in the probabilistic model which decomposes a derivation score in several subtasks can be seen as auxiliary tasks as in (Coavoux et al., 2018) .\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": [{"token_start": 30, "token_end": 45, "char_start": 181, "char_end": 225, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Coavoux et al., 2018)": "52113954"}}}]}
{"id": "209070321_4", "paragraph": "[BOS] Machine Learning Aspects Self-attention networks have been used in parsing, see for instance (Kitaev and Klein, 2018) , whether based on dependencies or syntagms.\n[BOS] Curiously we found few models of transition-based parsing based on these networks, and bidirectional recurrent network are still preferred in most architectures, where they are believed to capture some information about the sequential nature of transition-based algorithms.\n[BOS] Instead we present a non-sequential model of transition-based parsing where representation vectors are obtained via unrolled iterative estimation (Greff et al., 2017) .\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 17, "token_end": 26, "char_start": 90, "char_end": 123, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kitaev and Klein, 2018)": "19206893"}}}, {"token_start": 102, "token_end": 116, "char_start": 571, "char_end": 621, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Greff et al., 2017)": "8581028"}}}]}
{"id": "209070321_3", "paragraph": "[BOS] Compared to joint parsing systems working on both constituents and dependencies, our approach doesn't require external linguistic knowledge such as head percolation rules.\n[BOS] On the other hand, since derivations don't add new information, but merely offer a new vision of the problem, the potential accuracy gain is lower.\n\n", "discourse_tags": ["Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "209070321_2", "paragraph": "[BOS] We depart from both in the following way.\n[BOS] In most works on maximum subgraph selection, the class of valid subgraph is a class defined only by properties on dependencies.\n[BOS] Here we represent derivations instead of derived structures.\n[BOS] In that respect, we are closer to approaches developed for mildly-context sensitive formalisms such as Tree Adjoining Grammars which work primarily on the derivation tree (Corro et al., 2017) and consider the derived tree, i.e. the parse structure, as a byproduct.\n[BOS] Compared to other dynamic programming approaches to arc-hybrid parsing, we therefore work on a richer model, and have more expressive power to take a representation of states into account in the scoring scheme.\n[BOS] This comes at a cost since the time complexity of our parsing algorithm is O(n 4 ), an order of magnitude higher.\n[BOS] The stack information (size) is minimal and is used to parameterize access to information available from step embeddings.\n\n", "discourse_tags": ["Reflection", "Transition", "Reflection", "Narrative_cite", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 75, "token_end": 85, "char_start": 410, "char_end": 446, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Corro et al., 2017)": "26604137"}}}]}
{"id": "209070321_1", "paragraph": "[BOS] There is a long line of research which solve the different variants of transition-based dependency parsing algorithms with dynamic programming.\n[BOS] Recent work showed that this can be performed efficiently, in O(n 3 ), for arc-hybrid parsers (Gmez-Rodrguez et al., 2008) and have since been extended with non-linear classifiers (Kiperwasser and Goldberg, 2016; Shi et al., 2017) to reach state-of-the-art parsing accuracy.\n\n", "discourse_tags": ["Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 44, "token_end": 62, "char_start": 231, "char_end": 278, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 68, "token_end": 89, "char_start": 313, "char_end": 386, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kiperwasser and Goldberg, 2016;": "1642392"}}}]}
{"id": "209070321_0", "paragraph": "[BOS] Derivation Parsing Maximum subgraph selection has played a central role in dependency parsing since the MST reduction by McDonald et al. (2005) and can also be traced back to the parsingas-intersection tradition in phrase-based parsingsee for instance (Billot and Lang, 1989 ) -where the goal is to find, starting from a generic grammar, a graph-structure (a shared forest) that recognizes the input presented as a string or an automaton.\n[BOS] In dependency parsing, this approach has since been extended to more complex dependencies such as non-crossing and 1-endpoint-crossing dependencies (Kuhlmann and Jonsson, 2015; Cao et al., 2017) .\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 17, "token_end": 27, "char_start": 110, "char_end": 149, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"McDonald et al. (2005)": "6681594"}}}, {"token_start": 41, "token_end": 55, "char_start": 221, "char_end": 280, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Billot and Lang, 1989": "57821518"}}}, {"token_start": 108, "token_end": 135, "char_start": 549, "char_end": 645, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kuhlmann and Jonsson, 2015;": "12233172", "Cao et al., 2017)": "26322548"}}}]}
{"id": "202537879_3", "paragraph": "[BOS] agate to subsequent modules.\n[BOS] Here we study the usage of LSTM networks 4 on the TempRel extraction problem as an end-to-end approach that only takes a sequence of word embeddings as input (assuming that the position of events are known).\n[BOS] Conceptually, we need to feed those word embeddings to LSTMs and obtain a vector representation for a particular pair of events, which is followed by a fully-connected, feed-forward neural network (FFNN) to generate confidence scores for each output label.\n[BOS] Based on the confidence scores, global inference is performed via integer linear programming (ILP), which is a standard procedure used in many existing works to enforce the transitivity property of time (Chambers and Jurafsky, 2008b; Do et al., 2012; Ning et al., 2017 ).\n[BOS] An overview of the proposed network structure and corresponding parameters can be found in Fig.\n[BOS] 1 .\n[BOS] Below we also explain the main components.\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection", "Narrative_cite", "Reflection", "Other", "Reflection"], "span_citation_mapping": [{"token_start": 108, "token_end": 168, "char_start": 518, "char_end": 786, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Do et al., 2012;": "7359050", "Ning et al., 2017": "28982109"}}}]}
{"id": "202537879_2", "paragraph": "[BOS] A recent annotation scheme, Ning et al. (2018c) , introduced the notion of multi-axis to represent the temporal structure of text, and identified that one of the sources of confusions in human annotation is asking annotators for TempRels across different axes.\n[BOS] When annotating only sameaxis TempRels, along with some other improvements to the annotation guidelines, MATRES was able to achieve much higher IAAs.\n[BOS] 3 This dataset opens up opportunities to study neural methods for this problem.\n[BOS] In Sec.\n[BOS] 3, we will explain our proposed LSTM system, and also highlight the major differences from previous neural attempts.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Transition", "Other", "Reflection"], "span_citation_mapping": [{"token_start": 3, "token_end": 88, "char_start": 8, "char_end": 422, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ning et al. (2018c)": null}, "Reference": {}}}]}
{"id": "202537879_1", "paragraph": "[BOS] Since TempRel is a specific relation type, it is natural to borrow recent neural relation extraction approaches (Zeng et al., 2014; Zhang and Wang, 2015; Xu et al., 2016) .\n[BOS] There have indeed been such attempts, e.g., in clinical narratives (Dligach et al., 2017; Lin et al., 2017; Tourille et al., 2017) and in newswire (Cheng and Miyao, 2017; Meng and Rumshisky, 2018; Leeuwenberg and Moens, 2018) .\n[BOS] However, their improvements over feature-based methods were moderate (Lin et al. (2017) even showed negative results).\n[BOS] Given the low IAAs in those datasets, it was unclear whether it was simply due to the low data quality or neural methods inherently do not work well for this task.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 17, "token_end": 42, "char_start": 80, "char_end": 176, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zeng et al., 2014;": "12873739", "Zhang and Wang, 2015;": "11717703", "Xu et al., 2016)": "952356"}}}, {"token_start": 57, "token_end": 84, "char_start": 232, "char_end": 315, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dligach et al., 2017;": "37827595", "Lin et al., 2017;": "37827595", "Tourille et al., 2017)": "35796099"}}}, {"token_start": 86, "token_end": 113, "char_start": 323, "char_end": 410, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Meng and Rumshisky, 2018;": null, "Leeuwenberg and Moens, 2018)": "52111780"}}}, {"token_start": 118, "token_end": 134, "char_start": 434, "char_end": 506, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lin et al. (2017)": "37827595"}}}]}
{"id": "202537879_0", "paragraph": "[BOS] Early computational attempts to TempRel extraction include Mani et al. (2006); Chambers et al. (2007); Bethard et al. (2007); Verhagen and Pustejovsky (2008) , which aimed at building classic learning algorithms (e.g., perceptron, SVM, and logistic regression) using hand-engineered features extracted for each pair of events.\n[BOS] The frontier was later pushed forward through continuous efforts in a series of SemEval workshops (Verhagen et al., 2007 (Verhagen et al., , 2010 UzZaman et al., 2013; Bethard et al., 2015 Bethard et al., , 2016 , and significant progresses were made in terms of data annotation (Styler IV et al., 2014; Cassidy et al., 2014; Mostafazadeh et al., 2016; O'Gorman et al., 2016) , structured inference (Chambers and Jurafsky, 2008a; Do et al., 2012; Chambers et al., 2014; Ning et al., 2018a) , and structured machine learning (Yoshikawa et al., 2009; Leeuwenberg and Moens, 2017; Ning et al., 2017) .\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 6, "token_end": 47, "char_start": 38, "char_end": 163, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Mani et al. (2006);": "18281724", "Bethard et al. (2007);": "195914489", "Verhagen and Pustejovsky (2008)": "10348932"}}}, {"token_start": 96, "token_end": 141, "char_start": 419, "char_end": 550, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Verhagen et al., 2007": "39011", "(Verhagen et al., , 2010": "12126440", "UzZaman et al., 2013;": "640783", "Bethard et al., 2015": "209538", "Bethard et al., , 2016": "8559209"}}}, {"token_start": 150, "token_end": 192, "char_start": 602, "char_end": 714, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Styler IV et al., 2014;": "10260215", "Mostafazadeh et al., 2016;": "8387007", "O'Gorman et al., 2016)": "15139323"}}}, {"token_start": 193, "token_end": 227, "char_start": 717, "char_end": 828, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Do et al., 2012;": "7359050", "Ning et al., 2018a)": "51878335"}}}, {"token_start": 229, "token_end": 257, "char_start": 835, "char_end": 935, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yoshikawa et al., 2009;": "6945139", "Leeuwenberg and Moens, 2017;": "17894632", "Ning et al., 2017)": "28982109"}}}]}
{"id": "174802848_1", "paragraph": "[BOS] Another direction of attempts is the sentencelevel training with the thinking that the sentencelevel metric, e.g., BLEU, brings a certain degree of flexibility for generation and hence is more robust to mitigate the exposure bias problem.\n[BOS] To avoid the problem of exposure bias, Ranzato et al. (2015) presented a novel algorithm Mixed Incremental Cross-Entropy Reinforce (MIXER) for sequence-level training, which directly optimized the sentence-level BLEU used at inference.\n[BOS] Shen et al. (2016) introduced the Minimum Risk Training (MRT) into the end-to-end NMT model, which optimized model parameters by minimizing directly the expected loss with respect to arbitrary evaluation metrics, e.g., sentence-level BLEU.\n[BOS] Shao et al. (2018) proposed to eliminate the exposure bias through a probabilistic n-gram matching objective, which trains NMT NMT under the greedy decoding strategy.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 50, "token_end": 98, "char_start": 254, "char_end": 486, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ranzato et al. (2015)": "7147309"}, "Reference": {}}}, {"token_start": 99, "token_end": 154, "char_start": 493, "char_end": 732, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Shen et al. (2016)": "3913537"}, "Reference": {}}}, {"token_start": 155, "token_end": 190, "char_start": 739, "char_end": 905, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Shao et al. (2018)": "52186491"}, "Reference": {}}}]}
{"id": "174802848_0", "paragraph": "[BOS] Some other researchers have noticed the problem of exposure bias in NMT and tried to solve it.\n[BOS] Venkatraman et al. (2015) proposed DATA AS DEMONSTRATOR (DAD) which initialized the training examples as the paired two adjacent ground truth words and at each step added the predicted word paired with the next ground truth word as a new training example.\n[BOS] further developed the method by sampling as context from the previous ground truth word and the previous predicted word with a changing probability, not treating them equally in the whole training process.\n[BOS] This is similar to our method, but they do not include the sentence-level oracle to relieve the overcorrection problem and neither the noise perturbations on the predicted distribution.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 22, "token_end": 74, "char_start": 107, "char_end": 362, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Venkatraman et al. (2015)": "15080650"}, "Reference": {}}}]}
{"id": "196201920_1", "paragraph": "[BOS] Our work is also inspired by Clark and Manning (2016a) and Yin et al. (2018) , which resolve coreferences with reinforcement learning techniques.\n[BOS] They view the mention-ranking model as an agent taking a series of actions, where each action links each mention to a candidate antecedent.\n[BOS] They also use pretraining for initialization.\n[BOS] Nevertheless, their models assume mentions are given while our work is end-to-end.\n[BOS] Furthermore, we add entropy regularization to encourage more exploration (Mnih et al. ; Eysenbach et al., 2019 ) and prevent our model from prematurely converging to a sub-optimal (or bad) local optimum.\n\n", "discourse_tags": ["Multi_summ", "Multi_summ", "Multi_summ", "Multi_summ", "Reflection"], "span_citation_mapping": [{"token_start": 8, "token_end": 91, "char_start": 35, "char_end": 438, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Clark and Manning (2016a)": "2012188", "Yin et al. (2018)": "47019459"}, "Reference": {}}}, {"token_start": 96, "token_end": 119, "char_start": 465, "char_end": 557, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mnih et al.;": "6875312", "Eysenbach et al., 2019": "3521071"}}}]}
{"id": "196201920_0", "paragraph": "[BOS] Closely related to our work are the end-to-end coreference models developed by Lee et al. (2017) and .\n[BOS] Different from previous pipeline approaches, Lee et al. used neural networks to learn mention representations and calculate mention and antecedent scores without using syntactic parsers.\n[BOS] However, their models optimize a heuristic loss based on local decisions rather than the actual coreference evaluation metrics, while our reinforcement model directly optimizes the evaluation metrics based on the rewards calculated from sequences of actions.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 9, "token_end": 26, "char_start": 42, "char_end": 102, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Lee et al. (2017)": "1222212"}}}, {"token_start": 32, "token_end": 58, "char_start": 139, "char_end": 301, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "208139384_0", "paragraph": "[BOS] Discourse parsing, especially in the form of RST parsing, has been the target of research over a long period of time, including pre-neural feature engi-neering approaches (Hernault et al., 2010; Feng and Hirst, 2012; Ji and Eisenstein, 2014) .\n[BOS] Two approaches have been proposed to construct discourse parses: (1) bottom-up construction, where EDU merge operations are applied to single units; and (2) transition parser approaches, where the discourse tree is constructed as a sequence of parser actions.\n[BOS] Neural sequence models have also been proposed.\n[BOS] In early work, Li et al. (2016a) applied attention in an encoder-decoder framework and slightly improved on a classical featureengineering approach.\n[BOS] The current state of the art is a neural transition-based discourse parser (Yu et al., 2018) which incorporates implicit syntax features obtained from a bi-affine dependency parser (Dozat and Manning, 2017) .\n[BOS] In this work, we employ this discourse parser to generate discourse representations.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Single_summ", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 27, "token_end": 60, "char_start": 134, "char_end": 247, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hernault et al., 2010;": null, "Feng and Hirst, 2012;": "11919464", "Ji and Eisenstein, 2014)": "16391334"}}}, {"token_start": 126, "token_end": 152, "char_start": 591, "char_end": 724, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Li et al. (2016a)": "18483718"}, "Reference": {}}}, {"token_start": 161, "token_end": 175, "char_start": 765, "char_end": 823, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yu et al., 2018)": "52009900"}}}, {"token_start": 183, "token_end": 198, "char_start": 884, "char_end": 937, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dozat and Manning, 2017)": "7942973"}}}]}
{"id": "189858502_3", "paragraph": "[BOS] The role of discourse markers in the identification of claims and premises was discussed by Eckle-Kohler et al. (2015) , who found such markers to be moderately useful for identifying argumentative sentences.\n[BOS] Also Daxenberger et al. (2017) noted that claims share lexical clues across different datasets.\n[BOS] They also concluded from their experiments that typical argumentation mining datasets were too small to unleash the power of recent DNN-based classifiers; methods based on feature engineering still worked best.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 43, "char_start": 6, "char_end": 214, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Eckle-Kohler et al. (2015)": "88666"}, "Reference": {}}}, {"token_start": 45, "token_end": 104, "char_start": 226, "char_end": 533, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Daxenberger et al. (2017)": "11014757"}, "Reference": {}}}]}
{"id": "189858502_2", "paragraph": "[BOS] Developing a system for mining comparative sentences (with potential argumentation support for a preference) from the web might utilize specialized jargon like hashtags for argumentative tweets (Dusmanu et al., 2017) but at the same time faces the challenges recognized for general web argument mining (najder, 2017) : web text is typically not well formulated, misses argument structures, and contains poorly formulated claims.\n[BOS] In contrast to the use of dependency parses for mining comparative sentences in the biomedical domain, such syntactic features are often impossible to derive for noisy web text and were even shown to not really help in identifying argument structures from well-formulated texts like persuasive essays or Wikipedia articles (Aker et al., 2017; Stab and Gurevych, 2014) ; simpler structural features such as punctuation subsumed syntactic features in the above studies.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 27, "token_end": 44, "char_start": 166, "char_end": 222, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dusmanu et al., 2017)": "31107411"}}}, {"token_start": 54, "token_end": 65, "char_start": 280, "char_end": 322, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 133, "token_end": 158, "char_start": 724, "char_end": 808, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Aker et al., 2017;": "26570934", "Stab and Gurevych, 2014)": "71907"}}}]}
{"id": "189858502_1", "paragraph": "[BOS] Mining and categorizing comparative sentences from the web could support search engines in answering comparative queries (with potential argumentation justifying the preference in the mined sentence itself or in its context) but also has opinion mining (Ganapathibhotla and Liu, 2008) as another important application.\n[BOS] Still, previous work on recognizing comparative sentences has mostly been conducted in the biomedical domain.\n[BOS] For instance, Fiszman et al. (2007) identify sentences explicitly comparing elements of drug therapy via manually developed comparative and direction patterns informed by a lot of domain knowledge.\n[BOS] Later, Park and Blake (2012) trained a highprecision Bayesian Network classifier for toxicol-1 zenodo.org/record/3237552 2 github.com/uhh-lt/comparative ogy publications that used lexical clues (comparatives and domain-specific vocabulary) but also paths between comparison targets in dependency parses.\n[BOS] More recently, Gupta et al. (2017) described a system for the biomedical domain that also combines manually collected patterns for lexical matches and dependency parses in order to identify comparison targets and comparison type using the as gradable, non-gradable, superlativetaxonomy of Jindal and Liu (2006) .\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 41, "token_end": 55, "char_start": 244, "char_end": 290, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ganapathibhotla and Liu, 2008)": "8985962"}}}, {"token_start": 82, "token_end": 114, "char_start": 461, "char_end": 644, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Fiszman et al. (2007)": "1447797"}, "Reference": {}}}, {"token_start": 117, "token_end": 186, "char_start": 658, "char_end": 954, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 190, "token_end": 250, "char_start": 976, "char_end": 1271, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {"Jindal and Liu (2006)": "11160367"}}}]}
{"id": "189858502_0", "paragraph": "[BOS] A number of online comparison portals like GoCompare or Compare.com provide access to structured databases where products of the same class can be ranked along with their aspects.\n[BOS] Other systems like Diffen.com and Versus.com try to compare any pair of items on arbitrary properties.\n[BOS] They reach high coverage through the integration of a large number of structured resources such as databases and semi-structured resources like Wikipedia, but still list aspects side by side without providing further verbal explanationsnone of the portals aim at extracting comparisons from text.\n[BOS] Promising data sources for textual comparisons are question answering portals like Quora or Yahoo!\n[BOS] Answers that contain a lot of \"How does X compare to Y?\n[BOS] \"-questions with human answers but the web itself is an even larger source of textual comparisons.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Transition", "Transition", "Transition"], "span_citation_mapping": []}
{"id": "202779832_1", "paragraph": "[BOS] Some previous studies have been about using the results of traditional retrieval systems for informative response generation.\n[BOS] introduced an extra encoder for the retrieved response.\n[BOS] The encoder's output, together with that of the query encoder, is utilized to feed the decoder.\n[BOS] Weston et al. (2018) simply concatenated the original query and the retrieved response as the input to the encoder.\n[BOS] Instead of solely using the retrieved response, Wu et al. (2019) further introduced to encodes the lexical differences between the current query and the retrieved query.\n[BOS] Besides, Pandey et al. (2018) proposed to weight different training instances by context similarity, yet their work is done in close domain conversation.\n[BOS] The idea of editing some prototype materials rather than generating from scratch has also been explored in other text generation tasks.\n[BOS] For examples, Guu et al. (2018) proposed a prototypethen-edit model for unconditional text generation.\n[BOS] Wiseman et al. (2017 Wiseman et al. ( , 2018 used either fixed template or learned templates for data-to-text generation.\n[BOS] conditioned the next sentence generation on a skeleton that is extracted from the source input and the already generated text in storytelling.\n[BOS] Also for storytelling, Clark et al. (2018) proposed to extract the entities in sentences and use them as additional input.\n[BOS] Gu et al. (2018) uses retrieved translation as a reference to the generative translation model.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Single_summ", "Single_summ", "Single_summ", "Transition", "Single_summ", "Multi_summ", "Transition", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 53, "token_end": 77, "char_start": 302, "char_end": 417, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Weston et al. (2018)": "52006529"}, "Reference": {}}}, {"token_start": 86, "token_end": 110, "char_start": 472, "char_end": 593, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wu et al. (2019)": null}, "Reference": {}}}, {"token_start": 113, "token_end": 141, "char_start": 609, "char_end": 753, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Pandey et al. (2018)": "51879612"}, "Reference": {}}}, {"token_start": 168, "token_end": 188, "char_start": 916, "char_end": 1004, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Guu et al. (2018)": "2318481"}, "Reference": {}}}, {"token_start": 189, "token_end": 219, "char_start": 1011, "char_end": 1132, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wiseman et al. (2017": "23892230", "Wiseman et al. ( , 2018": "52135124"}, "Reference": {}}}, {"token_start": 246, "token_end": 273, "char_start": 1288, "char_end": 1410, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Clark et al. (2018)": "52865081"}, "Reference": {}}}, {"token_start": 274, "token_end": 293, "char_start": 1417, "char_end": 1512, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Gu et al. (2018)": "19206366"}, "Reference": {}}}]}
{"id": "202779832_0", "paragraph": "[BOS] Open domain dialog system has been a long goal for the NLP community since ELIZA (Weizenbaum, 1966) .\n[BOS] Early data-driven work uses information retrieval techniques (Ji et al., 2014; Hu et al., 2014) .\n[BOS] Recently, end-to-end neural sequence generation (Vinyals and Le, 2015; Serban et al., 2016; Li et al., 2016a; Sordoni et al., 2015) has attracted the most attention.\n[BOS] A major issue of such end-to-end sequence generation method is the safe response problem.\n[BOS] The generated responses tend to be universal and unengaging (e.g., \"I don't know\", \"I think so\" etc.).\n[BOS] One of the rea- sons is that for most queries, the set of possible responses is considerably large and the query alone cannot specify an informative response.\n[BOS] Various approaches (Li et al., 2016b; Xing et al., 2017; Ghazvininejad et al., 2018; Zhou et al., 2018; Liu et al., 2018; Tian et al., 2019; have been proposed for this problem.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Narrative_cite", "Transition", "Transition", "Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 17, "token_end": 27, "char_start": 81, "char_end": 105, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Weizenbaum, 1966)": "1896290"}}}, {"token_start": 35, "token_end": 53, "char_start": 142, "char_end": 209, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ji et al., 2014;": "18380963", "Hu et al., 2014)": "4497054"}}}, {"token_start": 57, "token_end": 99, "char_start": 228, "char_end": 349, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vinyals and Le, 2015;": "12300158", "Serban et al., 2016;": "6126582", "Li et al., 2016a;": "7287895", "Sordoni et al., 2015)": "94285"}}}, {"token_start": 192, "token_end": 244, "char_start": 760, "char_end": 899, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Li et al., 2016b;": "2955580", "Xing et al., 2017;": "9514751", "Ghazvininejad et al., 2018;": "15442925", "Zhou et al., 2018;": "51608183", "Liu et al., 2018;": "53081475"}}}]}
{"id": "209082217_9", "paragraph": "[BOS] (2) Cross-lingual Lexical Substitution : 3 Methods\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "209082217_8", "paragraph": "[BOS] source query: the national [coach] of the Irish teams ... answer: allenatore (Italian); Fubaltrainer; Nationaltrainer; Trainer (German); entrenador(Spanish) ...\n\n", "discourse_tags": ["Other"], "span_citation_mapping": []}
{"id": "209082217_7", "paragraph": "[BOS] (1) Cross-lingual Word Sense Disambigution:\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "209082217_6", "paragraph": "[BOS] Our new task is also related to Cross-lingual Word Sense Disambiguation (Lefever and Hoste, 2009 ): given a source language word in context, a system needs to provide the correct sense labels as clustered translation words in a number of target languages.\n[BOS] Another related task is Cross-lingual Lexical Substitution (Sinha et al., 2009) : the model must provide plausible target language translations for the source language lexical item in the source language context.\n[BOS] In contrast, our BTSR task: (1) directly evaluates token-level word representations without the need to predict sense labels from a sense inventory and (2) it contextualizes both the source query and the target candidates ensuring full sense disambiguation.\n[BOS] The core differences between the three tasks are illustrated in the following examples below:\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 9, "token_end": 26, "char_start": 38, "char_end": 102, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lefever and Hoste, 2009": "2085863"}}}, {"token_start": 61, "token_end": 76, "char_start": 292, "char_end": 347, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sinha et al., 2009)": "7275077"}}}]}
{"id": "209082217_5", "paragraph": "[BOS] Cross-lingual Word Sense Disambiguation.\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "209082217_4", "paragraph": "[BOS] Another task used for evaluating contextualized embeddings is Sentence Retrieval (Aldarmaki and Diab, 2019) : given a query source sentence, the task is to retrieve the corresponding parallel sentence in the target language.\n[BOS] Sentences can be represented as averages of contextualized embeddings of their constituent words.\n[BOS] As the task does not explicitly evaluate at the word level, even if a system cannot accurately capture polysemy, it can rely on other words in the sentence to retrieve the correct parallel sentence.\n[BOS] Therefore, Sentence Retrieval may lead to superficially high scores.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 113, "char_start": 6, "char_end": 614, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Aldarmaki and Diab, 2019)": null}, "Reference": {}}}]}
{"id": "209082217_3", "paragraph": "[BOS] The only context-aware dataset for evaluating cross-lingual embeddings on the word level is Bilingual Contextual Word Similarity (BCWS) (Chi and Chen, 2018) .\n[BOS] It challenges a system to predict similarity scores between cross-lingual word pairs with sentential context provided in both languages.\n[BOS] However, BCWS does not explicitly test for the retrieval of meaning-equivalent cross-lingual contextualized embeddings, which is explicitly tested in our test.\n[BOS] Also, BCWS is only available for one language pair: English-Chinese.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 2, "token_end": 61, "char_start": 6, "char_end": 307, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Chi and Chen, 2018)": "52289318"}, "Reference": {}}}]}
{"id": "209082217_2", "paragraph": "[BOS] Evaluation of (Contextualized) Cross-lingual Embeddings.\n[BOS] The traditional task to evaluate cross-lingual embeddings is Bilingual Dictionary Induction (BDI) (Vuli and Moens, 2013; Mikolov et al., 2013a; Gouws et al., 2015) : given a source query word, the task is to retrieve the translation word in the target language.\n[BOS] The test words in BDI are out-of-context and polysemy cannot be addressed properly.\n[BOS] The same issue is found in another relevant lexical task, Cross-lingual Semantic Similarity.\n[BOS] (Camacho-Collados et al., 2017) .\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 26, "token_end": 61, "char_start": 130, "char_end": 232, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vuli\u0107 and Moens, 2013;": "16470894", "Mikolov et al., 2013a;": "1966640", "Gouws et al., 2015)": "7021865"}}}, {"token_start": 123, "token_end": 136, "char_start": 526, "char_end": 557, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Camacho-Collados et al., 2017)": "7665329"}}}]}
{"id": "209082217_1", "paragraph": "[BOS] However, projection-based cross-lingual embeddings are still predominantly concerned with static word embeddings Mohiuddin and Joty, 2019) .\n[BOS] Learning crosslingual contextualized embeddings is still a large unexplored area with only two concurrent papers at the moment.\n[BOS] First, Aldarmaki and Diab (2019) adopt the same projection-based approach as our paper to align contextualized embeddings on the token-level using parallel data.\n[BOS] They find that context-aware mapping using parallel data outperforms context-independent mappings from static dictionaries on a parallel Sentence Retrieval task.\n[BOS] Second, Schuster et al. (2019) introduce anchor embeddings as the average of contextualized embeddings of a word to perform alignment for contextualized models, and show its effectiveness in cross-lingual dependency parsing.\n[BOS] These two studies are not directly comparable, whereas our paper provides a comprehensive and systematic comparison of various methods for learning cross-lingual contextualized embeddings and introduces a new and more challenging evaluation task.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 17, "token_end": 31, "char_start": 96, "char_end": 144, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Mohiuddin and Joty, 2019)": "102350797"}}}, {"token_start": 59, "token_end": 119, "char_start": 294, "char_end": 616, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Aldarmaki and Diab (2019)": null}, "Reference": {}}}, {"token_start": 122, "token_end": 164, "char_start": 631, "char_end": 847, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Schuster et al. (2019)": "67856005"}, "Reference": {}}}]}
{"id": "209082217_0", "paragraph": "[BOS] Cross-lingual Word Embeddings.\n[BOS] We conduct our experiments using a popular projection-based approach that learns an orthogonal mapping between pretrained embeddings (Xing et al., 2015; Artetxe et al., 2016) .\n[BOS] The orthogonality of the mapping is crucial as it preserves monolingual invariance and is empirically proven to be more robust (Smith et al., 2017; Xing et al., 2015) .\n[BOS] This projection-based method can be applied post-hoc on pretrained monolingual embeddings with an exact analytical solution.\n[BOS] Moreover, its performance is often competitive to that of jointly trained crosslingual models using additional bilingual signals in the form of parallel or comparable corpora .\n\n", "discourse_tags": ["Reflection", "Reflection", "Narrative_cite", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 27, "token_end": 48, "char_start": 154, "char_end": 217, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Xing et al., 2015;": "3144258", "Artetxe et al., 2016)": "1040556"}}}, {"token_start": 61, "token_end": 88, "char_start": 286, "char_end": 392, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Smith et al., 2017;": "11591887", "Xing et al., 2015)": "3144258"}}}]}
{"id": "202769374_5", "paragraph": "[BOS] Unlike all of the above approaches, (Zhelezniak et al., 2019a) see each word embedding itself as D (e.g. 300) observations from some scalar random variable.\n[BOS] They cast semantic similarity as correlations between these random variables and study their properties using simple tools from univariate statistics.\n[BOS] While they consider correlations between individual word vectors and averaged word vectors, they do not formally explore correlations between word vector sets.\n[BOS] We review their framework in Section 3 and then proceed to formalise and generalise it to the case of sets of word embeddings.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 9, "token_end": 90, "char_start": 42, "char_end": 485, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Zhelezniak et al., 2019a)": "159041087"}, "Reference": {}}}]}
{"id": "202769374_4", "paragraph": "[BOS] Cosine similarity between covariances is an instance of the RV coefficient and its uncentered version was applied in the context of word embeddings before (Botev et al., 2017) .\n[BOS] We arrive at a similar coefficient (but with different centering) as a special case of CKA, which in the general case makes no parametric assumptions about disbtributions whatsoever.\n[BOS] In particular our version is suitable for comparing sets containing just one word vector, whereas the method of Nikolentzos et al. (2017) ; Torki (2018) requires at least two vectors in each set.\n[BOS] Very recently, Kornblith et al. (2019) used CKA to compare representations between layers of the same or different neural networks.\n[BOS] This is again an instance of treating such representations as observations from a D-variate distribution, where D is the dimension of the hidden layer in question.\n[BOS] Our use of CKA is completely different from theirs.\n\n", "discourse_tags": ["Narrative_cite", "Reflection", "Multi_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 26, "token_end": 38, "char_start": 138, "char_end": 181, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 97, "token_end": 125, "char_start": 481, "char_end": 574, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Nikolentzos et al. (2017)": "7102885", "Torki (2018)": "51882191"}, "Reference": {}}}, {"token_start": 129, "token_end": 188, "char_start": 596, "char_end": 882, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "202769374_3", "paragraph": "[BOS] Approaches that see word embeddings as statistical objects are very closely related to our work.\n[BOS] Virtually all of them treat word embeddings as observations from some D-variate parametric family, where D is the embedding dimension.\n[BOS] Arora et al. (2016 Arora et al. ( , 2017 introduce a latent discourse model and show the maximum likelihood estimate (MLE) for the discourse vector to be the weighted average of word embeddings in a sentence, where the weights are given by smooth inverse frequencies (SIF).\n[BOS] Nikolentzos et al. (2017) ; Torki (2018) treat sets of word embeddings as observations from D-variate Gaussians, and compare such sets with cosine similarity between the parameters (means and covariances) estimated by maximum likelihood.\n[BOS] Vargas et al. (2019) measure semantic similarity through penalised likelihood ratio between the joint and factorised models and explore Gaussian and von Mises-Fisher likelihoods.\n\n", "discourse_tags": ["Transition", "Transition", "Multi_summ", "Multi_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 45, "token_end": 105, "char_start": 250, "char_end": 523, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 106, "token_end": 159, "char_start": 530, "char_end": 767, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Nikolentzos et al. (2017)": "7102885", "Torki (2018)": "51882191"}, "Reference": {}}}, {"token_start": 160, "token_end": 196, "char_start": 774, "char_end": 952, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Vargas et al. (2019)": "140439681"}, "Reference": {}}}]}
{"id": "202769374_2", "paragraph": "[BOS] Some approaches do not seek to build an explicit representation and instead focus directly on designing a similarity function between sets.\n[BOS] Word Mover's Distance (WMD) (Kusner et al., 2015) is an instance of the Earth Mover's Distance (EMD) computed between normalised BoW, with the cost matrix given by Euclidean distances between word embeddings.\n[BOS] In the soft cardinality framework of (Jimenez et al., 2010 (Jimenez et al., , 2015 , the contribution of a word to the cardinality of a set depends on its similarities to other words in the same set.\n[BOS] Such sets are then compared using an appropriately defined Jaccard index or related measures.\n[BOS] DynaMax (Zhelezniak et al., 2019b) uses universe-constrained fuzzy sets designed explicitly for similarity computations.\n\n", "discourse_tags": ["Transition", "Single_summ", "Narrative_cite", "Transition", "Single_summ"], "span_citation_mapping": [{"token_start": 25, "token_end": 77, "char_start": 152, "char_end": 360, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 80, "token_end": 127, "char_start": 374, "char_end": 566, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 145, "token_end": 172, "char_start": 673, "char_end": 793, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Zhelezniak et al., 2019b)": "108299957"}, "Reference": {}}}]}
{"id": "202769374_1", "paragraph": "[BOS] Perhaps the most obvious such approaches are based on elementary pooling operations such as average-, max-and min-pooling (Mitchell and Lapata, 2008; De Boom et al., 2015 .\n[BOS] While seemingly over-simplistic, numerous studies have confirmed their impressive performance on the downstream tasks (Arora et al., 2017; Wieting et al., 2016; Wieting and Gimpel, 2018; Zhelezniak et al., 2019b) One step further, Zhao and Mao (2017) ; Zhelezniak et al. (2019b) introduce fuzzy bags-of-words (FBoW) where degrees of membership in a fuzzy set are given by the similarities between word embeddings.\n[BOS] Zhelezniak et al. (2019b) show a close connection between FBoW and max-pooled word vectors.\n\n", "discourse_tags": ["Narrative_cite", "Multi_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 16, "token_end": 41, "char_start": 98, "char_end": 176, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mitchell and Lapata, 2008;": "18597583"}}}, {"token_start": 59, "token_end": 99, "char_start": 286, "char_end": 397, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Wieting et al., 2016;": "5882977", "Wieting and Gimpel, 2018;": "5003931", "Zhelezniak et al., 2019b)": "108299957"}}}, {"token_start": 103, "token_end": 150, "char_start": 416, "char_end": 598, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhao and Mao (2017)": "206686029", "Zhelezniak et al. (2019b)": "108299957"}, "Reference": {}}}, {"token_start": 151, "token_end": 177, "char_start": 605, "char_end": 696, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhelezniak et al. (2019b)": "108299957"}, "Reference": {}}}]}
{"id": "202769374_0", "paragraph": "[BOS] Several lines of research seek to combine the strength of pretrained word embeddings and the elegance of set-or bag-of-words (BoW) representations.\n[BOS] Any method that determines semantic similarity between sentences by comparing the corresponding sets of word embeddings is directly related to our work.\n\n", "discourse_tags": ["Transition", "Reflection"], "span_citation_mapping": []}
{"id": "186206777_1", "paragraph": "[BOS] Some other work attempts to incorporate external knowledge for abstractive summarization.\n[BOS] For example, proposed to enrich their encoder with handcrafted features such as named entities and part-of-speech (POS) tags.\n[BOS] Guu et al. (2018) also attempted to encode humanwritten sentences to improve neural text generation.\n[BOS] Similar to our work, Cao et al. (2018a) proposed to retrieve a related summary from the training set as soft template to assist with the summarization.\n[BOS] However, their approach tends to oversimplify the role of the template, by directly concatenating a template after the source article encoding.\n[BOS] In contrast, our bi-directional selective mechanism exhibits a novel attempt to selecting key information from the article and the template in a mutual manner, offering greater flexibility in using the template.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 46, "token_end": 67, "char_start": 234, "char_end": 334, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Guu et al. (2018)": "2318481"}, "Reference": {}}}, {"token_start": 73, "token_end": 130, "char_start": 362, "char_end": 642, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Cao et al. (2018a)": "51878811"}, "Reference": {}}}]}
{"id": "186206777_0", "paragraph": "[BOS] Abstractive sentence summarization, a task analogous to headline generation or sentence compression, aims to generate a brief summary given a short source article.\n[BOS] Early studies in this problem mainly focus on statistical or linguistic-rule-based methods, including those based on extractive and compression (Jing and McKeown, 2000; Knight and Marcu, 2002; Clarke and Lapata, 2010) , templates (Zhou and Hovy, 2004) and statistical machine translation (Banko et al., 2000) .\n[BOS] The advent of large-scale summarization corpora accelerates the development of various neural network methods.\n[BOS] Rush et al. (2015) first applied an attention-based sequence-to-sequence model for abstractive summarization, which includes a convolutional neural network (CNN) encoder and a feed-forward network decoder.\n[BOS] Chopra et al. (2016) replaced the decoder with a recurrent neural network (RNN).\n[BOS] further changed the sequence-to-sequence model to a fully RNN-based model.\n[BOS] Besides, Gu et al. (2016) found that this task benefits from copying words from the source articles and proposed the CopyNet correspondingly.\n[BOS] With a similar purpose, proposed to use a switch gate to control when to copy from the source article and when to generate from the vocabulary.\n[BOS] Zhou et al. (2017) employed a selective gate to filter out unimportant information when encoding.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 53, "token_end": 81, "char_start": 293, "char_end": 393, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Jing and McKeown, 2000;": "800331", "Knight and Marcu, 2002;": "7793213", "Clarke and Lapata, 2010)": "16531053"}}}, {"token_start": 82, "token_end": 91, "char_start": 396, "char_end": 427, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhou and Hovy, 2004)": null}}}, {"token_start": 92, "token_end": 104, "char_start": 432, "char_end": 484, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Banko et al., 2000)": "9952653"}}}, {"token_start": 126, "token_end": 170, "char_start": 610, "char_end": 815, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Rush et al. (2015)": "1918428"}, "Reference": {}}}, {"token_start": 171, "token_end": 211, "char_start": 822, "char_end": 983, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Chopra et al. (2016)": "133195"}, "Reference": {}}}, {"token_start": 214, "token_end": 269, "char_start": 999, "char_end": 1281, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Gu et al. (2016)": "8174613"}, "Reference": {}}}, {"token_start": 270, "token_end": 290, "char_start": 1288, "char_end": 1385, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhou et al. (2017)": "1770102"}, "Reference": {}}}]}
{"id": "195833740_0", "paragraph": "[BOS] EFP is one of the fundamental tasks in Information Extraction.\n[BOS] The early work on this problem has employed the rule-based approaches (Nairn et al., 2006; Saur, 2008; Lotan et al., 2013) or the machine learning approaches (with manually designed features) (Diab et al., 2009; Prabhakaran et al., 2010; De Marneffe et al., 2012; Lee et al., 2015) , or the hybrid approaches of both (Saur and Pustejovsky, 2012; Qian et al., 2015) .\n[BOS] Recently, deep learning has been applied to solve EFP.\n[BOS] (Qian et al., 2018) employ Generative Adversarial Networks (GANs) for EFP while (Rudinger et al., 2018) utilize LSTMs for both sequential and dependency representations of the input sentences.\n[BOS] Finally, deep learning has also been considered for the related tasks of EFP, including event detection (Nguyen and Grishman, 2015b; Nguyen et al., 2016b; Lu and Nguyen, 2018; Nguyen and Nguyen, 2019) , event realis classification (Mitamura et al., 2015; Nguyen et al., 2016g) , uncertainty detection (Adel and Schtze, 2017) , modal sense classification (Marasovic and Frank, 2016) and entity detection (Nguyen et al., 2016d) .\n[BOS] the current event mention has happened.\n[BOS] There are three major components in the EFP model proposed in this work, i.e., (i) sentence encoding, (ii) structure induction, and (iii) prediction.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Multi_summ", "Narrative_cite", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 24, "token_end": 51, "char_start": 123, "char_end": 197, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Saur\u00ed, 2008;": null}}}, {"token_start": 53, "token_end": 97, "char_start": 205, "char_end": 356, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Prabhakaran et al., 2010;": "5730838", "De Marneffe et al., 2012;": "12046735"}}}, {"token_start": 100, "token_end": 124, "char_start": 366, "char_end": 439, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Saur\u00ed and Pustejovsky, 2012;": "2239324"}}}, {"token_start": 139, "token_end": 160, "char_start": 509, "char_end": 582, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Qian et al., 2018)": "51605619"}, "Reference": {}}}, {"token_start": 161, "token_end": 185, "char_start": 589, "char_end": 701, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Rudinger et al., 2018)": "4717344"}, "Reference": {}}}, {"token_start": 203, "token_end": 236, "char_start": 796, "char_end": 908, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 237, "token_end": 258, "char_start": 911, "char_end": 984, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 259, "token_end": 271, "char_start": 987, "char_end": 1032, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 272, "token_end": 284, "char_start": 1035, "char_end": 1089, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 285, "token_end": 296, "char_start": 1094, "char_end": 1133, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "207999240_0", "paragraph": "[BOS] The benefit of using in-domain embeddings for the biomedical domain has already been proven effective.\n[BOS] For example, (Pakhomov et al., 2016) and (Wang et al., 2018) found that using clinical notes or biomedical articles for training word embeddings has generally a positive impact on down-stream NLP tasks.\n[BOS] (Nikfarjam et al., 2015) trained embeddings on user-generated medical content and used them successfully on the pharmacovigilance task; however, they trained the embeddings an adverse reaction mining corpus, hence making them too task-specific to be considered useful on generic UG-BioNLP tasks.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 24, "token_end": 68, "char_start": 128, "char_end": 317, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Pakhomov et al., 2016)": "43063924", "(Wang et al., 2018)": "3610459"}, "Reference": {}}}, {"token_start": 69, "token_end": 132, "char_start": 324, "char_end": 619, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Nikfarjam et al., 2015)": "8319232"}, "Reference": {}}}]}
{"id": "204877385_1", "paragraph": "[BOS] Claim Validation Reasoning about the validity of a particular claim can be separated into three sub-tasks: document retrieval to find documents related to the claim, ED to find the relevant pieces of evidence that support or contradict the claim, and Textual Entailment (TE) to determine whether the claim follows from the evidence.\n[BOS] The FEVER shared tasks follows this approach (Thorne et al., 2018; Thorne and Vlachos, 2019) .\n[BOS] Other approaches, such as TwoWingOS (Yin and Roth, 2018) and DeClarE (Popat et al., 2018) combine the ED and TE models into a single end-toend method.\n[BOS] Ma et al. (2019) used two pre-trained models, one for ED and one for TE which are then jointly fine-tuned.\n[BOS] While presenting promising results, all of these approaches rely on static models that are trained beforehand and do not learn from the user.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Multi_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 65, "token_end": 90, "char_start": 349, "char_end": 437, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Thorne et al., 2018;": "53645946", "Thorne and Vlachos, 2019)": "76666238"}}}, {"token_start": 97, "token_end": 133, "char_start": 472, "char_end": 596, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Yin and Roth, 2018)": "51969223", "(Popat et al., 2018)": "52215843"}, "Reference": {}}}, {"token_start": 134, "token_end": 164, "char_start": 603, "char_end": 709, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ma et al. (2019)": "196209746"}, "Reference": {}}}]}
{"id": "204877385_0", "paragraph": "[BOS] This paper touches three areas of research, namely the overarching field of claim validation, the task domain (ED and AM) with small data, and the interaction of Natural Language Processing (NLP) components with users.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "174801260_0", "paragraph": "[BOS] Our work builds on the existing body of literature in both fast decoding methods for neural generation models as well as syntax-based MT; we review each area below.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "102352698_3", "paragraph": "[BOS] Many state-of-the-art RE models rely upon domain-specific external syntactic tools to construct dependency paths between the entities in a sentence (Li and Ji, 2014; Xu et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2017) .\n[BOS] These systems suffer from cascading errors from these tools and are hard to generalize to different domains.\n[BOS] To make the model more general, we combine the multitask learning framework with ELMo embeddings (Peters et al., 2018) without relying on external syntactic tools and risking the cascading errors that accompany them, and improve the interaction between tasks through dynamic graph propagation.\n[BOS] While the performance of DyGIE benefits from ELMo, it advances over some systems (Luan et al., 2018a; Sanh et al., 2019) that also incorporate ELMo.\n[BOS] The analyses presented here give insights into the benefits of joint modeling.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Reflection", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 22, "token_end": 80, "char_start": 102, "char_end": 348, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Li and Ji, 2014;": "20744", "Xu et al., 2015;": "12203896", "Miwa and Bansal, 2016;": "2476229", "Zhang et al., 2017)": "2204967"}}}, {"token_start": 96, "token_end": 107, "char_start": 436, "char_end": 473, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Peters et al., 2018)": "3626819"}}}, {"token_start": 154, "token_end": 179, "char_start": 736, "char_end": 803, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Luan et al., 2018a;": "52118895", "Sanh et al., 2019)": "53436546"}}}]}
{"id": "102352698_2", "paragraph": "[BOS] Neural graph-based models have achieved significant improvements over traditional featurebased approaches on several graph modeling tasks.\n[BOS] Knowledge graph completion (Yang et al., 2015; Bordes et al., 2013 ) is one prominent example.\n[BOS] For relation extraction tasks, graphs have been used primarily as a means to incorporate pipelined features such as syntactic or discourse relations (Peng et al., 2017; Song et al., 2018; .\n[BOS] Christopoulou et al. (2018) models all possible paths between entities as a graph, and refines pair-wise embeddings by performing a walk on the graph structure.\n[BOS] All these previous works assume that the nodes of the graph (i.e. the entity candidates to be considered during relation extraction) are predefined and fixed throughout the learning process.\n[BOS] On the other hand, our framework does not require a fixed set of entity boundaries as an input for graph construction.\n[BOS] Motivated by state-ofthe-art span-based approaches to coreference resolution (Lee et al., 2017 and semantic role labeling , the model uses a beam pruning strategy to dynamically select high-quality spans, and constructs a graph using the selected spans as nodes.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite", "Single_summ", "Transition", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 23, "token_end": 41, "char_start": 151, "char_end": 217, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yang et al., 2015;": "2768038", "Bordes et al., 2013": null}}}, {"token_start": 68, "token_end": 86, "char_start": 368, "char_end": 438, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Peng et al., 2017;": "2797612"}}}, {"token_start": 89, "token_end": 124, "char_start": 448, "char_end": 608, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Christopoulou et al. (2018)": "51877560"}, "Reference": {}}}]}
{"id": "102352698_1", "paragraph": "[BOS] Entity recognition has commonly been cast as a sequence labeling problem, and has benefited substantially from the use of neural architectures (Collobert et al., 2011; Lample et al., 2016; Ma and Hovy, 2016; Luan et al., 2017b Luan et al., , 2018b .\n[BOS] However, most systems based on sequence labeling suffer from an inability to extract entities with overlapping spans.\n[BOS] Recently Katiyar and Cardie (2018) and Wang and Lu (2018) have presented methods enabling neural models to extract overlapping entities, applying hypergraph-based representations on top of sequence labeling systems.\n[BOS] Our framework offers an alternative approach, forgoing sequence labeling entirely and simply considering all possible spans as candidate entities.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Multi_summ", "Reflection"], "span_citation_mapping": [{"token_start": 23, "token_end": 66, "char_start": 128, "char_end": 253, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Collobert et al., 2011;": "351666", "Lample et al., 2016;": "6042994", "Ma and Hovy, 2016;": "10489017", "Luan et al., 2017b": "28752386", "Luan et al., , 2018b": "44155611"}}}, {"token_start": 89, "token_end": 129, "char_start": 395, "char_end": 601, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Katiyar and Cardie (2018)": "44170949", "Wang and Lu (2018)": "52916675"}, "Reference": {}}}]}
{"id": "102352698_0", "paragraph": "[BOS] Previous studies have explored joint modeling (Miwa and Bansal, 2016; Zhang et al., 2017; Singh et al., 2013; Yang and Mitchell, 2016) ) and multi-task learning (Peng and Dredze, 2015; Peng et al., 2017; Luan et al., 2018a Luan et al., , 2017a as methods to share representational strength across related information extraction tasks.\n[BOS] The most similar to ours is the work in Luan et al. (2018a) that takes a multi-task learning approach to entity, relation, and coreference extraction.\n[BOS] In this model, the different tasks share span representations that only incorporate broader context indirectly via the gradients passed back to the LSTM layer.\n[BOS] In contrast, DYGIE uses dynamic graph propagation to explicitly incorporate rich contextual information into the span representations.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 6, "token_end": 37, "char_start": 37, "char_end": 140, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Miwa and Bansal, 2016;": "2476229", "Zhang et al., 2017;": "2204967", "Singh et al., 2013;": "13965810", "Yang and Mitchell, 2016)": "2367456"}}}, {"token_start": 39, "token_end": 76, "char_start": 147, "char_end": 249, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Peng and Dredze, 2015;": "12266201", "Peng et al., 2017;": "2797612", "Luan et al., 2018a": "52118895", "Luan et al., , 2017a": "31298398"}}}, {"token_start": 99, "token_end": 154, "char_start": 387, "char_end": 663, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Luan et al. (2018a)": "52118895"}, "Reference": {}}}]}
{"id": "196210081_1", "paragraph": "[BOS] Our work is also inspired by several works in other areas.\n[BOS] Zhang et al. (2018) introduces document context into Transformer on document-level Neural Machine Translation (NMT) task.\n[BOS] devises the incremental encoding scheme based on rnn for story ending generation task.\n[BOS] In our work, we design an Incremental Transformer to achieve a knowledge-aware context representation using an incremental encoding scheme.\n[BOS] Xia et al. (2017) first proposes Deliberation Network based on rnn on NMT task.\n[BOS] Our Deliberation Decoder is different in two aspects: 1) We clearly devise the two decoders targeting context and knowledge respectively; 2) Our second pass decoder directly fine tunes the first pass result, while theirs uses both the hidden states and results from the first pass.\n\n", "discourse_tags": ["Reflection", "Single_summ", "Single_summ", "Reflection", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 15, "token_end": 40, "char_start": 71, "char_end": 192, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhang et al. (2018)": "52938038"}, "Reference": {}}}, {"token_start": 82, "token_end": 103, "char_start": 438, "char_end": 517, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Xia et al. (2017)": "13481571"}, "Reference": {}}}]}
{"id": "196210081_0", "paragraph": "[BOS] The closest work to ours lies in the area of opendomain dialogue system incorporating unstructured knowledge.\n[BOS] Ghazvininejad et al. (2018) uses an extended Encoder-Decoder where the decoder is provided with an encoding of both the context and the external knowledge.\n[BOS] Parthasarathi and Pineau (2018) uses an architecture containing a Bag-of-Words Memory Network fact encoder and an RNN decoder.\n[BOS] Dinan et al. (2018) combines Memory Network architectures to retrieve, read and condition on knowledge, and Transformer architectures to provide text representation and generate outputs.\n[BOS] Different from these works, we greatly enhance the Transformer architectures to handle the document knowledge in multi-turn dialogue from two aspects: 1) using attention mechanism to combine document knowledge and context utterances; and 2) exploiting incremental encoding scheme to encode multi-turn knowledge aware conversations.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 21, "token_end": 56, "char_start": 122, "char_end": 277, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ghazvininejad et al. (2018)": "15442925"}, "Reference": {}}}, {"token_start": 57, "token_end": 87, "char_start": 284, "char_end": 410, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Parthasarathi and Pineau (2018)": "52281610"}, "Reference": {}}}, {"token_start": 88, "token_end": 120, "char_start": 417, "char_end": 603, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Dinan et al. (2018)": "53218829"}, "Reference": {}}}]}
{"id": "160009452_5", "paragraph": "[BOS] (2018), and restore wiki links using the DBpedia Spotlight API (Daiber et al., 2013) following Bjerva et al. (2016); van Noord and Bos (2017b) .\n[BOS] We add polarity attributes based on the rules observed from the training data.\n[BOS] More details of preand post-processing are provided in the Appendix.\n[BOS] We conduct experiments on two AMR general releases (available to all LDC subscribers): AMR 2.0 (LDC2017T10) and AMR 1.0 (LDC2014T12).\n[BOS] Our model is trained using ADAM (Kingma and Ba, 2014) for up to 120 epochs, with early stopping based on the development set.\n[BOS] Full model training takes about 19 hours on AMR 2.0 and 7 hours on AMR 1.0, using two GeForce GTX TI-TAN X GPUs.\n[BOS] At training, we have to fix BERT parameters due to the limited GPU memory.\n[BOS] We leave fine-tuning BERT for future work.\n[BOS] Table 1 lists the hyper-parameters used in our full model.\n[BOS] Both encoder and decoder embedding layers have GloVe and POS tag embeddings as well as CharCNN, but their parameters are not tied.\n[BOS] We apply dropout (dropout rate = 0.33) to the outputs of each module.\n\n", "discourse_tags": ["Narrative_cite", "Reflection", "Reflection", "Reflection", "Reflection", "Reflection", "Reflection", "Reflection", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 13, "token_end": 47, "char_start": 47, "char_end": 148, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Daiber et al., 2013)": "207206865", "Bjerva et al. (2016);": "2986409", "van Noord and Bos (2017b)": "22984360"}}}, {"token_start": 132, "token_end": 141, "char_start": 484, "char_end": 510, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "160009452_4", "paragraph": "[BOS] Attention-based parsing with Seq2Seq-style models have been considered (Barzdins and Gosko, 2016; Peng et al., 2017b) , but are limited by the relatively small amount of labeled AMR data.\n[BOS] Konstas et al. (2017) overcame this by making use of millions of unlabeled data through self-training, while van Noord and Bos (2017b) showed significant gains via a characterlevel Seq2Seq model and a large amount of silverstandard AMR training data.\n[BOS] In contrast, our approach supported by extended pointer generator can be effectively trained on the limited amount of labeled AMR data, with no data augmentation.\n\n", "discourse_tags": ["Narrative_cite", "Multi_summ", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 34, "char_start": 6, "char_end": 123, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Barzdins and Gosko, 2016;": "16086013", "Peng et al., 2017b)": "7730925"}}}, {"token_start": 50, "token_end": 75, "char_start": 200, "char_end": 301, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Konstas et al. (2017)": "8066499"}, "Reference": {}}}, {"token_start": 77, "token_end": 109, "char_start": 309, "char_end": 450, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "160009452_3", "paragraph": "[BOS] Grammar-based approaches are represented by Artzi et al. (2015) ; Peng et al. (2015) who leveraged external semantic resources, and employed CCG-based or SHRG-based grammar induction approaches converting logical forms into AMRs.\n[BOS] Pust et al. (2015) recast AMR parsing as a machine translation problem, while also drawing features from external semantic resources.\n\n", "discourse_tags": ["Multi_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 53, "char_start": 6, "char_end": 235, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Artzi et al. (2015)": "5499420", "Peng et al. (2015)": "7138313"}, "Reference": {}}}, {"token_start": 54, "token_end": 82, "char_start": 242, "char_end": 375, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Pust et al. (2015)": "6971327"}, "Reference": {}}}]}
{"id": "160009452_2", "paragraph": "[BOS] Transition-based approaches began with Wang et al. (2015 Wang et al. ( , 2016 , who incrementally transform dependency parses into AMRs using transitonbased models, which was followed by a line of research, such as Puzikov et al. (2016); Brandt et al. (2016) ; Goodman et al. (2016) ; Damonte et al. (2017) ; Ballesteros and Al-Onaizan (2017); Groschwitz et al. (2018) .\n[BOS] A pre-trained aligner, e.g. Pourdamghani et al. (2014) ; Liu et al. (2018) , is needed for most parsers to generate training data (e.g., oracles for a transition-based parser).\n[BOS] Our approach makes no significant use of external semantic resources, 3 and is aligner-free.\n\n", "discourse_tags": ["Multi_summ", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 110, "char_start": 6, "char_end": 374, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wang et al. (2015": "15344879", "Wang et al. ( , 2016": "16454657"}, "Reference": {"Puzikov et al. (2016);": "5459392", "Brandt et al. (2016)": "18037181", "Goodman et al. (2016)": "17055413", "Damonte et al. (2017)": null, "Groschwitz et al. (2018)": "44066560"}}}, {"token_start": 113, "token_end": 152, "char_start": 385, "char_end": 512, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Pourdamghani et al. (2014)": "217895", "Liu et al. (2018)": "52939688"}}}]}
{"id": "160009452_1", "paragraph": "[BOS] Alignment-based approaches were first explored by JAMR (Flanigan et al., 2014) , a pipeline of concept and relation identification with a graphbased algorithm.\n[BOS] improved this by jointly learning concept and relation identification with an incremental model.\n[BOS] Both approaches rely on features based on alignments.\n[BOS] Lyu and Titov (2018) treated alignments as latent variables in a joint probabilistic model, leading to a substantial reported improvement.\n[BOS] Our approach re-quires no explicit alignments, but implicitly learns a source-side copy mechanism using attention.\n\n", "discourse_tags": ["Single_summ", "Transition", "Transition", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 10, "token_end": 36, "char_start": 56, "char_end": 165, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Flanigan et al., 2014)": "5000956"}, "Reference": {}}}, {"token_start": 62, "token_end": 88, "char_start": 335, "char_end": 473, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Lyu and Titov (2018)": "46889674"}, "Reference": {}}}]}
{"id": "160009452_0", "paragraph": "[BOS] AMR parsing approaches can be categorized into alignment-based, transition-based, grammarbased, and attention-based approaches.\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "174804052_0", "paragraph": "[BOS] Type inventories for the task of fine-grained entity typing (Ling and Weld, 2012; Gillick et al., 2014; Yosef et al., 2012) have grown in size and complexity (Del Corro et al., 2015; Murty et al., 2017; Choi et al., 2018) .\n[BOS] Systems have tried to incorporate hierarchical information on the type distribution in different manners.\n[BOS] Shimaoka et al. (2017) encode the hierarchy through a sparse matrix.\n[BOS] Xu and Barbosa (2018) model the relations through a hierarchy-aware loss function.\n[BOS] Ma et al. (2016) and Abhishek et al. (2017) learn embeddings for labels and feature representations into a joint space in order to facilitate information sharing among them.\n[BOS] Our work resembles Xiong et al. (2019) since they derive hierarchical information in an unrestricted fashion, through type co-occurrence statistics from the dataset.\n[BOS] These models operate under Euclidean assumptions.\n[BOS] Instead, we impose a hyperbolic geometry to enrich the hierarchical information.\n[BOS] Hyperbolic spaces have been applied mostly on complex and social networks modeling (Krioukov et al., 2010; Verbeek and Suri, 2016) .\n[BOS] In the field of Natural Language Processing, they have been employed to learn embeddings for Question Answering (Tay et al., 2018) , in Neural Machine Translation (Gulcehre et al., 2019) , and to model language (Leimeister and Wilson, 2018; Tifrea et al., 2019) .\n[BOS] We build upon the work of Nickel and Kiela (2017) on modeling hierarchical link structure of symbolic data and adapt it with the parameterization method proposed by Dhingra et al. (2018) to cope with feature representations of text.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Single_summ", "Single_summ", "Multi_summ", "Single_summ", "Single_summ", "Reflection", "Narrative_cite", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 9, "token_end": 39, "char_start": 39, "char_end": 129, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ling and Weld, 2012;": "9345159", "Gillick et al., 2014;": null, "Yosef et al., 2012)": "10145463"}}}, {"token_start": 42, "token_end": 69, "char_start": 144, "char_end": 227, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Del Corro et al., 2015;": "9899754", "Murty et al., 2017;": "26965064", "Choi et al., 2018)": "49212016"}}}, {"token_start": 88, "token_end": 105, "char_start": 348, "char_end": 416, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Shimaoka et al. (2017)": "5204434"}, "Reference": {}}}, {"token_start": 106, "token_end": 124, "char_start": 423, "char_end": 505, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Xu and Barbosa (2018)": null}, "Reference": {}}}, {"token_start": 125, "token_end": 163, "char_start": 512, "char_end": 685, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ma et al. (2016)": "5288798", "Abhishek et al. (2017)": "14148547"}, "Reference": {}}}, {"token_start": 164, "token_end": 204, "char_start": 692, "char_end": 913, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Xiong et al. (2019)": "71148539"}, "Reference": {}}}, {"token_start": 226, "token_end": 250, "char_start": 1053, "char_end": 1137, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Krioukov et al., 2010;": null, "Verbeek and Suri, 2016)": "19894963"}}}, {"token_start": 266, "token_end": 279, "char_start": 1224, "char_end": 1276, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Tay et al., 2018)": "207603084"}}}, {"token_start": 281, "token_end": 296, "char_start": 1282, "char_end": 1332, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Gulcehre et al., 2019)": "43968607"}}}, {"token_start": 299, "token_end": 320, "char_start": 1342, "char_end": 1407, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Leimeister and Wilson, 2018;": "52165083", "Tifrea et al., 2019)": null}}}, {"token_start": 328, "token_end": 343, "char_start": 1442, "char_end": 1522, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 348, "token_end": 362, "char_start": 1545, "char_end": 1602, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Dhingra et al. (2018)": "48360418"}}}]}
{"id": "59553499_0", "paragraph": "[BOS] We divide reading comprehension data sets into three categories based on the types of answers: extractive, abstractive, and multiple choice.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "208004937_1", "paragraph": "[BOS] We have used the document retrieval module developed by (Hanselowski et al., 2018) which achieved the highest evidence recall in the first fever challenge (Thorne et al., 2018c) .\n[BOS] They use the MediaWiki API 3 which queries the Wikipedia search engine.\n[BOS] Every noun phrase is considered to be a possible entity mention and is fed into the MediWiki API, yielding up to seven Wikipedia pages per claim.\n[BOS] Nie et al. (2018) propose a 'two-hop' evidence enhancement process, that is they gather all hyperlinks in their already retrieved evidence sentences and apply their sentence selection module on all sentences found in these documents retrieved by following the hyperlinks.\n[BOS] A 0.8% increase in FEVER score (using oracle lables) is reported by using this strategy.\n[BOS] Malon (2018) use the open-GPT model (Radford et al., 2018) for sentence selection and entailment classification.\n[BOS] We have trained similar models, but used BERT instead.\n[BOS] BERT is a noisy autoencoder pre-trained on masked language modeling tasks and was the state of the art on a number of natural language understanding (NLU) tasks (Devlin et al., 2018) during the builder phase of FEVER 2.0, e.g. the NLU benchmark GLUE (Wang et al., 2018) and on SQuAD (Rajpurkar et al., 2016) , a question answering dataset.\n[BOS] Classification in BERT is achieved by training a special '[CLS]' token which is prepended to every sequence (or sequence pair), gather the '[CLS]' token's hidden representation and perform classification on top of that.\n[BOS] We used the cased English version of BERT BASE for all our experiments.\n[BOS] Hanselowski et al. (2018) use the hinge loss function 4 to maximize the margin between positive and (sampled) negative evidence sentences.\n[BOS] Thus, we adapted BERT for sentence selection to be trained with the hinge loss as well.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Single_summ", "Transition", "Single_summ", "Reflection", "Narrative_cite", "Transition", "Reflection", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 6, "token_end": 22, "char_start": 23, "char_end": 88, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hanselowski et al., 2018)": "52162540"}}}, {"token_start": 23, "token_end": 43, "char_start": 95, "char_end": 183, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Thorne et al., 2018c)": "53645946"}}}, {"token_start": 92, "token_end": 143, "char_start": 422, "char_end": 693, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Nie et al. (2018)": "53713656"}, "Reference": {}}}, {"token_start": 167, "token_end": 196, "char_start": 795, "char_end": 907, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Malon (2018)": "53590103"}, "Reference": {"(Radford et al., 2018)": "49313245"}}}, {"token_start": 235, "token_end": 252, "char_start": 1093, "char_end": 1157, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Devlin et al., 2018)": "52967399"}}}, {"token_start": 268, "token_end": 280, "char_start": 1206, "char_end": 1244, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wang et al., 2018)": "5034059"}}}, {"token_start": 282, "token_end": 294, "char_start": 1252, "char_end": 1282, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rajpurkar et al., 2016)": "11816014"}}}, {"token_start": 362, "token_end": 392, "char_start": 1625, "char_end": 1763, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hanselowski et al. (2018)": "52162540"}, "Reference": {}}}]}
{"id": "208004937_0", "paragraph": "[BOS] Most work on the FEVER dataset is based on the baseline system proposed in the dataset description (Thorne et al., 2018a) , using a pipeline consisting of document retrieval, sentence selection and RTE.\n[BOS] We implemented such a pipeline as well and have built on several ideas found in the first FEVER challenge.\n\n", "discourse_tags": ["Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 17, "token_end": 29, "char_start": 85, "char_end": 127, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Thorne et al., 2018a)": "4711425"}}}]}
{"id": "59599804_2", "paragraph": "[BOS] Syntactic Compression Prior to the explosion of neural models for summarization, syntactic compression was relatively more common.\n[BOS] Martins and Smith (2009) cast joint extraction and compression as an ILP and used dependency parsing information in their model.\n[BOS] Woodsend and Lapata (2011) induced a quasi-synchronous grammar from Wikipedia for compression.\n[BOS] Several systems explored the usage of constituency parses (Berg-Kirkpatrick et al., 2011; Wang et al., 2013; Li et al., 2014) as well as RST-based approaches (Hirao et al., 2013; Durrett et al., 2016) .\n[BOS] Our approach follows in this vein but could be combined with more sophisticated neural text compression methods as well.\n[BOS] Filippova et al. (2015) presented an LSTM approach to deletionbased sentence compression.\n[BOS] Miao and Blunsom (2016) proposed a deep generative model for text compression.\n[BOS] explored the compression module after the extraction model but the separation of these two modules hurt the performance.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Narrative_cite", "Reflection", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 23, "token_end": 48, "char_start": 143, "char_end": 271, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 49, "token_end": 68, "char_start": 278, "char_end": 372, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 73, "token_end": 106, "char_start": 408, "char_end": 504, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Berg-Kirkpatrick et al., 2011;": "15467396", "Wang et al., 2013;": "1260503", "Li et al., 2014)": "10112929"}}}, {"token_start": 109, "token_end": 130, "char_start": 516, "char_end": 579, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hirao et al., 2013;": "18505561", "Durrett et al., 2016)": "5125975"}}}, {"token_start": 153, "token_end": 173, "char_start": 715, "char_end": 804, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Filippova et al. (2015)": "1992250"}, "Reference": {}}}, {"token_start": 174, "token_end": 213, "char_start": 811, "char_end": 1016, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Miao and Blunsom (2016)": "10480989"}, "Reference": {}}}]}
{"id": "59599804_1", "paragraph": "[BOS] cle and joint learning are used in our best model.\n[BOS] Additional improvements to extractive modeling might therefore be expected to stack with our approach.\n\n", "discourse_tags": ["Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "59599804_0", "paragraph": "[BOS] Neural Extractive Summarization Neural networks have shown to be effective in extractive summarization.\n[BOS] Past approaches have structured the decision either as binary classification over sentences (Cheng and Lapata, 2016; Nallapati et al., 2017) or classification followed by ranking (Narayan et al., 2018) .\n[BOS] used a seq-to-seq decoder instead.\n[BOS] For our model, text compression forms a module largely orthogonal to the extraction module, although the joint ora- Table 8 : The compressions actually used by our model on CNN; average lengths and the fraction of that constituency type among compressions taken by our model.\n[BOS] Comp Acc indicates how frequently that compression was taken by the oracle; note that error, especially keeping constituents that we shouldn't, may have minimal impact on summary quality.\n[BOS] Dedup indicates the percentage of chosen compressions which arise from deduplication as opposed to model prediction.\n[BOS] Many PPs are removed in this process contrary to what the oracle states.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Reflection", "Transition", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 24, "token_end": 50, "char_start": 137, "char_end": 256, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cheng and Lapata, 2016;": "1499080", "Nallapati et al., 2017)": "6405271"}}}, {"token_start": 51, "token_end": 64, "char_start": 260, "char_end": 317, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Narayan et al., 2018)": "3510042"}}}]}
{"id": "67855882_4", "paragraph": "[BOS] Their parser outperforms swap-based systems.\n[BOS] However, they only experiment with a linear classifier, and assume access to gold part-of-speech (POS) tags for most of their experiments.\n[BOS] All these proposals use a lexicalized model, as defined in the introduction: they assign heads to new constituents and use them as features to inform parsing decisions.\n[BOS] Previous work on unlexicalized transition-based parsing models only focused on projective constituency trees (Dyer et al., 2016; Liu and Zhang, 2017) .\n[BOS] In particular, Cross and Huang (2016b) introduced a system that does not require explicit binarization.\n[BOS] Their system decouples the construction of a tree and the labeling of its nodes by assigning types (structure or label) to each action, and alternating between a structural action for even steps and labeling action for odd steps.\n[BOS] This distinction arguably makes each decision simpler.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Narrative_cite", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 77, "token_end": 108, "char_start": 394, "char_end": 526, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dyer et al., 2016;": "1949831", "Liu and Zhang, 2017)": "31713824"}}}, {"token_start": 113, "token_end": 185, "char_start": 550, "char_end": 935, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "67855882_3", "paragraph": "[BOS] 3. the GAP action pops an element from S and adds it to D, making the next element of S available for a reduction.\n\n", "discourse_tags": ["Other"], "span_citation_mapping": []}
{"id": "67855882_2", "paragraph": "[BOS] 2. reductions are applied to the respective tops of S and D;\n\n", "discourse_tags": ["Other"], "span_citation_mapping": []}
{"id": "67855882_1", "paragraph": "[BOS] In contrast, Coavoux and Crabb (2017a) extended a shift-reduce transition system to handle discontinuous constituents.\n[BOS] Their system allows binary reductions to apply to the top element in the stack, and any other element in the stack (instead of the two top elements in standard shift-reduce parsing).\n[BOS] The second constituent for a reduction is chosen dynamically, with an action called GAP that gives access to older elements in the stack and can be performed several times before a reduction.\n[BOS] In practice, they made the following modifications over a standard shift-reduce system: 1.\n[BOS] The stack, that stores subtrees being constructed, is split into two parts S and D;\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 5, "token_end": 118, "char_start": 19, "char_end": 605, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "67855882_0", "paragraph": "[BOS] Several approaches to discontinuous constituency parsing have been proposed.\n[BOS] Hall and Nivre (2008) reduces the problem to non-projective dependency parsing, via a reversible transformation, a strategy developed by Fernndez-Gonzlez and Martins (2015) and Corro et al. (2017) .\n[BOS] Chart parsers are based on probabilistic Linear Context-Free Rewriting Systems (LCFRS) (Evang and Kallmeyer, 2011; Kallmeyer and Maier, 2010) , the Data-Oriented Parsing (DOP) framework (van Cranenburgh and Bod, 2013; van Cranenburgh et al., 2016) , or pseudo-projective parsing (Versley, 2016) .\n[BOS] Some transition-based discontinuous constituency parsers use the swap action, adapted from dependency parsing (Nivre, 2009 ) either with an easy-first strategy (Versley, 2014a,b) or with a shift-reduce strategy (Maier, 2015; Maier and Lichte, 2016; Stanojevi and Garrido Alhama, 2017) .\n[BOS] Nevertheless, the swap strategy tends to produce long derivations (in number of actions) to construct discontinuous constituents; as a result, the choice of an oracle that minimizes the number of swap actions has a substantial positive effect in accuracy (Maier and Lichte, 2016; Stanojevi and Garrido Alhama, 2017) .\n\n", "discourse_tags": ["Transition", "Multi_summ", "Narrative_cite", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 14, "token_end": 60, "char_start": 89, "char_end": 285, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hall and Nivre (2008)": "38327080"}, "Reference": {"Fern\u00e1ndez-Gonz\u00e1lez and Martins (2015)": "5754528"}}}, {"token_start": 68, "token_end": 101, "char_start": 321, "char_end": 435, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Evang and Kallmeyer, 2011;": null}}}, {"token_start": 103, "token_end": 131, "char_start": 442, "char_end": 541, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 133, "token_end": 143, "char_start": 547, "char_end": 588, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 156, "token_end": 168, "char_start": 662, "char_end": 719, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 172, "token_end": 185, "char_start": 737, "char_end": 775, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 186, "token_end": 221, "char_start": 779, "char_end": 881, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Maier, 2015;": "6147316", "Maier and Lichte, 2016;": "6492742"}}}, {"token_start": 249, "token_end": 291, "char_start": 1037, "char_end": 1205, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Maier and Lichte, 2016;": "6492742"}}}]}
{"id": "196180577_2", "paragraph": "[BOS] The aim of this paper is to demonstrate the necessity of modeling cross-sentence context for GEC.\n[BOS] It is beyond the scope of this paper to comprehensively evaluate all approaches to exploit document-level context, and we leave it to future work to evaluate more sophisticated models such as memory networks to capture entire documentlevel context or incorporate external knowledge sources.\n\n", "discourse_tags": ["Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "196180577_1", "paragraph": "[BOS] On the other hand, there are a number of studies recently on integrating cross-sentence context for neural machine translation (NMT).\n[BOS] There are three major approaches for document-level NMT: (1) translating an extended source (context concatenated with the source) to a single or extended target (Tiedemann and Scherrer, 2017; Bawden et al., 2018) ; (2) using an additional encoder to capture document-wide context (Jean et al., 2017; Wang et al., 2017; Bawden et al., 2018; Voita et al., 2018; Miculicich et al., 2018; ; and (3) using discrete (Kuang et al., 2018) or continuous cache (Tu et al., 2018; Maruf and Haffari, 2018) mechanisms during translation for storing and retrieving document-level information.\n[BOS] We investigated the first two approaches in this paper as we believe that most of the ambiguities can be resolved by considering a few previous sentences.\n[BOS] Since GEC is a monolingual rewriting task, most of the disambiguating information is in the source sentence itself, unlike bilingual NMT.\n[BOS] All approaches for document-level NMT extended recurrent models or Transformer models.\n[BOS] There is no prior work that extends convolutional sequence-to-sequence models for document-level NMT.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Reflection", "Transition", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 45, "token_end": 81, "char_start": 207, "char_end": 359, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Tiedemann and Scherrer, 2017;": "2496355", "Bawden et al., 2018)": "5016370"}}}, {"token_start": 85, "token_end": 136, "char_start": 366, "char_end": 530, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Jean et al., 2017;": null, "Wang et al., 2017;": "9768369", "Bawden et al., 2018;": "5016370", "Voita et al., 2018;": "44062236"}}}, {"token_start": 143, "token_end": 154, "char_start": 548, "char_end": 580, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kuang et al., 2018)": "49214109"}}}, {"token_start": 154, "token_end": 174, "char_start": 581, "char_end": 651, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Tu et al., 2018;": "7421176", "Maruf and Haffari, 2018)": "21686013"}}}]}
{"id": "196180577_0", "paragraph": "[BOS] Sophisticated sequence-to-sequence architectures (Gehring et al., 2017; Vaswani et al., 2017) 2018b), rescoring (Chollampatt and Ng, 2018a) , iterative decoding strategies (Ge et al., 2018; Lichtarge et al., 2018) , synthetic (Xie et al., 2018) and semi-supervised corpora (Lichtarge et al., 2018) , and other task-specific techniques (Junczys-Dowmunt et al., 2018) has achieved impressive results on this task.\n[BOS] However, all prior work ignores document-wide context for GEC, and uses sentence-level models.\n[BOS] For spell checking, Flor and Futagi (2012) used document-level context to check if a candidate correction for a misspelled word had been used earlier in the document.\n[BOS] Zheng et al. (2018) proposed splitting runon sentences into separate sentences.\n[BOS] However, they did not use cross-sentence context.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 3, "token_end": 31, "char_start": 20, "char_end": 106, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Gehring et al., 2017;": "3648736", "Vaswani et al., 2017)": "13756489"}}}, {"token_start": 32, "token_end": 45, "char_start": 108, "char_end": 145, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chollampatt and Ng, 2018a)": "19236015"}}}, {"token_start": 46, "token_end": 66, "char_start": 148, "char_end": 219, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ge et al., 2018;": "49564245", "Lichtarge et al., 2018)": "53220585"}}}, {"token_start": 67, "token_end": 76, "char_start": 222, "char_end": 250, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Xie et al., 2018)": "21730715"}}}, {"token_start": 77, "token_end": 91, "char_start": 255, "char_end": 303, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lichtarge et al., 2018)": "53220585"}}}, {"token_start": 96, "token_end": 112, "char_start": 321, "char_end": 371, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 144, "token_end": 181, "char_start": 525, "char_end": 691, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Flor and Futagi (2012)": "5408779"}, "Reference": {}}}, {"token_start": 182, "token_end": 210, "char_start": 698, "char_end": 833, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zheng et al. (2018)": "52812604"}, "Reference": {}}}]}
{"id": "174800751_1", "paragraph": "[BOS] While neural models can capture some dependencies between choices through shared representations, to the best of our knowledge, inferences capturing the dependencies between answer choices or different questions have been not explicitly modeled.\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "174800751_0", "paragraph": "[BOS] Recently, several QA datasets have been proposed to test machine comprehension (Richardson, 2013; Weston et al., 2015; Rajpurkar et al., 2016; Trischler et al., 2016a; Nguyen et al., 2016) .\n[BOS] Yatskar (2018) showed that a high performance on these datasets could be achieved without necessarily achieving the capability of making commonsense inferences.\n[BOS] Trischler et al. (2016b) , Kumar et al. (2016) , Liu and Perez (2017) , Min et al. (2018) and Xiong et al. (2016) proposed successful models on those datasets.\n[BOS] To address this issue, new QA datasets which require commonsense reasoning have been proposed (Khashabi et al., 2018; Ostermann et al., 2018; .\n[BOS] Using common sense inferences in Machine Comprehension is a far from solved problem.\n[BOS] There have been several attempts in literature to use inferences to answer questions.\n[BOS] Most of the previous works either attempt to infer the answer from the given text (Sachan and Xing, 2016; or an external commonsense knowledge base (Das et al., 2017; Mihaylov and Frank, 2018; Bauer et al., 2018; Weissenborn et al., 2017) .\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Multi_summ", "Narrative_cite", "Transition", "Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 12, "token_end": 53, "char_start": 63, "char_end": 194, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Richardson, 2013;": "2100831", "Weston et al., 2015;": "3178759", "Rajpurkar et al., 2016;": "11816014", "Trischler et al., 2016a;": "1167588", "Nguyen et al., 2016)": "1289517"}}}, {"token_start": 55, "token_end": 83, "char_start": 203, "char_end": 363, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Yatskar (2018)": "52895001"}, "Reference": {}}}, {"token_start": 84, "token_end": 134, "char_start": 370, "char_end": 529, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Trischler et al. (2016b)": "12834729", "Kumar et al. (2016)": "2319779", "Liu and Perez (2017)": "14351566", "Min et al. (2018)": "29161506", "Xiong et al. (2016)": "3714278"}, "Reference": {}}}, {"token_start": 145, "token_end": 169, "char_start": 589, "char_end": 676, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Khashabi et al., 2018;": "5112038"}}}, {"token_start": 208, "token_end": 225, "char_start": 903, "char_end": 973, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 228, "token_end": 266, "char_start": 981, "char_end": 1107, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Das et al., 2017;": "228172", "Mihaylov and Frank, 2018;": "29151507", "Bauer et al., 2018;": "52290656", "Weissenborn et al., 2017)": "195345850"}}}]}
{"id": "174800989_8", "paragraph": "[BOS] The work closest to ours experimented with concatenating treebanks to train a multilingual parser (Ammar et al., 2016) .\n[BOS] The authors use an S-LSTM transition-based parser similar to ours (although we do not include recurrent representations) trained on a set of lexicalised features that include multilingual word embeddings, Brown clusters, and fine-grained POS tags, whereas we only use coarse-grained POS and morphological features in a delexicalised setting.\n[BOS] They include a one-hot language-ID vector, a set of six wordorder features from the WALS (Naseem et al., 2012) , or the whole WALS vectors.\n[BOS] We use the two former plus a set of 22 selected features from WALS.\n[BOS] They perform experiments on seven highresourced languages while we report results on a larger set of 40 languages.\n[BOS] Although Ammar et al. (2016) showed that, in a lexicalised setting, treebank concatenation could perform on par with monolingual parsers, the origins and limits of these improvements are not clear.\n[BOS] We explore directions for assessing the benefits of typological features in a delexicalised parser.\n\n", "discourse_tags": ["Narrative_cite", "Reflection", "Narrative_cite", "Reflection", "Reflection", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 18, "token_end": 30, "char_start": 84, "char_end": 124, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ammar et al., 2016)": "2868247"}}}, {"token_start": 123, "token_end": 135, "char_start": 565, "char_end": 591, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Naseem et al., 2012)": "3143538"}}}, {"token_start": 184, "token_end": 225, "char_start": 831, "char_end": 1019, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ammar et al. (2016)": "2868247"}, "Reference": {}}}]}
{"id": "174800989_7", "paragraph": "[BOS] Moreover, typological features such as those present in the WALS provide information about the structure of languages (Dryer and Haspelmath, 2013) .\n[BOS] These could be useful to guide multilingual parsers, informing them about the model parameters that can be shared among languages with similar characteristics.\n[BOS] Naseem et al. (2012) and Zhang and Barzilay (2015) use word-order features available for all their languages, while Ponti et al. (2018) used features they judged relevant in many categories (not only word order).\n[BOS] The parameters proposed in the WALS are not the only way to represent properties of languages.\n[BOS] Methods based on language embeddings (stling and Tiedemann, 2017; Bjerva et al., 2019 ) also constitute interesting language representation.\n[BOS] Tckstrm et al. (2013) use a multilingual delexicalised transfer method, showing how selective parameter sharing, based on typological features and language family membership, can be incorporated in a discriminative graph-based dependency parser.\n[BOS] They select the typological features based on those used by Naseem et al. (2012) , removing two features not considered useful.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Multi_summ", "Transition", "Narrative_cite", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 19, "token_end": 32, "char_start": 101, "char_end": 152, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dryer and Haspelmath, 2013)": null}}}, {"token_start": 63, "token_end": 116, "char_start": 327, "char_end": 539, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Naseem et al. (2012)": "3143538", "Ponti et al. (2018)": "21687876"}, "Reference": {}}}, {"token_start": 139, "token_end": 159, "char_start": 664, "char_end": 732, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Bjerva et al., 2019": null}}}, {"token_start": 167, "token_end": 245, "char_start": 794, "char_end": 1173, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"T\u00e4ckstr\u00f6m et al. (2013)": "2037646"}, "Reference": {"Naseem et al. (2012)": "3143538"}}}]}
{"id": "174800989_6", "paragraph": "[BOS] Delexicalised parsers ignore the word forms and lemmas when analysing a sentence, usually relying on more abstract features such as word classes and POS tags.\n[BOS] The use of delexicalised parsers is especially relevant when learning multilingual parsers, since languages generally share only a limited amount of lexical units.\n[BOS] The approach proposed by Zeman and Resnik (2008) consists in adapting a parser for a new related language using either parallel corpora or delexicalised parsing.\n[BOS] This method can be used to quickly construct a parser if the source and target languages are sufficiently related.\n[BOS] McDonald et al. (2011) show that delexicalised parsers can be directly transferred between languages, yielding significantly higher accuracy than unsupervised parsers.\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 67, "token_end": 120, "char_start": 345, "char_end": 623, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 121, "token_end": 152, "char_start": 630, "char_end": 797, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"McDonald et al. (2011)": "6698104"}, "Reference": {}}}]}
{"id": "174800989_5", "paragraph": "[BOS] Multilingual parsing research is also encouraged by initiatives such as the CoNLL 2017 and 2018 shared tasks, on highly multilingual dependency parsing from raw text (Zeman et al., 2017 (Zeman et al., , 2018 .\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 23, "token_end": 48, "char_start": 119, "char_end": 213, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zeman et al., 2017": null, "(Zeman et al., , 2018": null}}}]}
{"id": "174800989_4", "paragraph": "[BOS] The combination of corpora in multiple languages for parser training is facilitated by the recent advent of multilingual standards and resources, in particular in Universal Dependencies for dependency syntax (Nivre et al., 2016) .\n[BOS] This initiative enables the annotation of POS, morphology and syntactic dependencies for all languages with the same guidelines and label sets.\n[BOS] The availability of such corpora favours the development of cross-lingual methods (Tiedemann, 2015) .\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 31, "token_end": 42, "char_start": 196, "char_end": 234, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Nivre et al., 2016)": "17954486"}}}, {"token_start": 78, "token_end": 89, "char_start": 453, "char_end": 492, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Tiedemann, 2015)": "7813627"}}}]}
{"id": "174800989_3", "paragraph": "[BOS] Low-resource languages may have some (insufficient) amount of training material available.\n[BOS] One can employ bilingual parsing, concatenating training corpora in two languages, to verify if there is an improvement in the results compared to a monolingual parser (Vilares et al., 2015) .\n[BOS] Direct transfer and bilingual parsing methods are close to the present article, since we also concatenate training corpora.\n[BOS] However, in our case, we combine treebanks from many more sources (around 40 languages) and include typological features.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 46, "token_end": 58, "char_start": 252, "char_end": 293, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vilares et al., 2015)": "2206746"}}}]}
{"id": "174800989_2", "paragraph": "[BOS] Instead of assuming the availability of parallel corpora, direct transfer approaches capitalize on language similarities.\n[BOS] For instance, Lynn et al. (2014) build parser for Irish by first training a delexicalised parser on another language, and then applying it on Irish.\n[BOS] They surprisingly found out that Indonesian was the language providing the best parsing results for Irish, even if they do not belong to the same language family, because longdistance dependencies are better represented in Indonesian than in the other languages tested.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 21, "token_end": 104, "char_start": 134, "char_end": 558, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Lynn et al. (2014)": "9070183"}, "Reference": {}}}]}
{"id": "174800989_1", "paragraph": "[BOS] Transfer parsing is often a suitable solution when dealing with low-resource languages (McDonald et al., 2011) .\n[BOS] Projected transfer relies on parallel corpora in which one of the languages does not have labelled training data to learn a parser, but the other does.\n[BOS] One commonly employed solution is to use word alignments to project parsed sentences from one side onto the low-resource side of the parallel text, using heuristics (Hwa et al., 2005) or partial annotations (Lacroix et al., 2016) .\n[BOS] Agi et al. (2016) parse the resource-rich languages in a multi-parallel corpus, proposing a projection method to obtain POS tags and dependency trees for low-resource languages from multiple-language word alignments.\n[BOS] The parsing model for the target language can also be obtained in an unsupervised fashion, by optimising a function that combines the likelihood of parallel data and the likelihood of the transferred model on non-annotated data in the low-resource language (Ma and Xia, 2014) .\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Narrative_cite", "Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 12, "token_end": 24, "char_start": 70, "char_end": 116, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(McDonald et al., 2011)": "6698104"}}}, {"token_start": 84, "token_end": 94, "char_start": 437, "char_end": 466, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hwa et al., 2005)": "157167"}}}, {"token_start": 95, "token_end": 107, "char_start": 470, "char_end": 512, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lacroix et al., 2016)": "16483916"}}}, {"token_start": 109, "token_end": 153, "char_start": 521, "char_end": 737, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Agi\u0107 et al. (2016)": null}, "Reference": {}}}, {"token_start": 196, "token_end": 207, "char_start": 979, "char_end": 1019, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ma and Xia, 2014)": "15371205"}}}]}
{"id": "174800989_0", "paragraph": "[BOS] Our work is at the intersection of three trends in the multilingual dependency parsing literature.\n[BOS] The first is transfer parsing, when a parser is trained on a language (or a collection of languages) and tested on another one.\n[BOS] The second is delexicalised parsing, which aims at abstracting away from the lexicon in order to neutralise genre, domain and topic biases which are heavily marked in the treebanks' vocabulary.\n[BOS] The third trend is the use of a handcrafted typological resources, such as the WALS, in multilingual NLP methods.\n\n", "discourse_tags": ["Reflection", "Transition", "Transition", "Transition"], "span_citation_mapping": []}
{"id": "202121521_1", "paragraph": "[BOS] To solve the issues above and better exploit large-scale rich-resourced reading comprehension data, in this paper, we propose several zeroshot approaches which yield state-of-the-art performances on Japanese and French SQuAD data.\n[BOS] Moreover, we also propose a supervised approach for the condition that there are training samples available for the target language.\n[BOS] To evaluate the effectiveness of our approach, we carried out experiments on two realistic public Chinese reading comprehension data: CMRC 2018 (simplified Chinese) (Cui et al., 2019) and DRCD (traditional Chinese) (Shao et al., 2018) .\n[BOS] Experimental results demonstrate the effectiveness by modeling training samples in a bilingual environment.\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 94, "token_end": 110, "char_start": 516, "char_end": 565, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cui et al., 2019)": "52984852"}}}, {"token_start": 111, "token_end": 126, "char_start": 570, "char_end": 616, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Shao et al., 2018)": "46932369"}}}]}
{"id": "202121521_0", "paragraph": "[BOS] Machine Reading Comprehension (MRC) has been a trending research topic in recent years.\n[BOS] Among various types of MRC tasks, spanextraction reading comprehension has been enormously popular (such as SQuAD (Rajpurkar et al., 2016) ), and we have seen a great progress on related neural network approaches (Wang and Jiang, 2016; Seo et al., 2016; Xiong et al., 2016; , especially those were built on pre-trained language models, such as BERT (Devlin et al., 2019) .\n[BOS] While massive achievements have been made by the community, reading comprehension in other than English has not been well-studied mainly due to the lack of large-scale training data.\n[BOS] Asai et al. (2018) proposed to use runtime machine translation for multilingual extractive reading comprehension.\n[BOS] They first translate the data from the target language to English and then obtain an answer using an English reading comprehension model.\n[BOS] Finally, they recover the corresponding answer in the original language using soft-alignment attention scores from the NMT model.\n[BOS] However, though an interesting attempt has been made, the zero-shot results are quite low, and alignments between different languages, especially for those have different word orders, are significantly different.\n[BOS] Also, they only evaluate on a rather small dataset (hundreds of samples) that was translated from SQuAD (Rajpurkar et al., 2016) , which is not that realistic.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 40, "token_end": 52, "char_start": 208, "char_end": 238, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rajpurkar et al., 2016)": "11816014"}}}, {"token_start": 63, "token_end": 88, "char_start": 287, "char_end": 372, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wang and Jiang, 2016;": "5592690", "Seo et al., 2016;": null}}}, {"token_start": 103, "token_end": 114, "char_start": 444, "char_end": 470, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Devlin et al., 2019)": "52967399"}}}, {"token_start": 151, "token_end": 298, "char_start": 668, "char_end": 1446, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Asai et al. (2018)": "52183904"}, "Reference": {"(Rajpurkar et al., 2016)": "11816014"}}}]}
{"id": "201657196_0", "paragraph": "[BOS] Most previous domain adaptation work for NMT focus on the setting where a small amount of indomain data is available.\n[BOS] Continued training (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016 ) methods first train an NMT model on outof-domain data and then fine-tune it on the indomain data.\n[BOS] Similar to our work, Kobus et al. (2017) propose to use domain tags to control the output domain, but it still needs a in-domain parallel corpus and our architecture allows more flexible modifications than just adding additional tags.\n[BOS] Unsupervised domain adaptation techniques for NMT can be divided into data-and model-centric methods (Chu and Wang, 2018) .\n[BOS] Data-centric approaches mainly focus on selecting or generating the domain-related data using existing in-domain monolingual data.\n[BOS] Both the copy method (Currey et al., 2017) and back-translation (Sennrich et al., 2016a) are representative data-centric methods.\n[BOS] In addition, Moore and Lewis (2010); Axelrod et al. (2011); Duh et al. (2013) use LMs to score the outof-domain data, based on which they select data similar to in-domain text.\n[BOS] Model-centric methods have not been fully investigated yet.\n[BOS] Gulcehre et al. (2015) propose to fuse LMs and NMT models, but their methods require querying two models during inference and have been demonstrated to underperform the data-centric ones .\n[BOS] There are also work on adaptation via retrieving sentences or n-grams in the training data similar to the test set (Farajian et al., 2017; Bapna and Firat, 2019) .\n[BOS] However, it can be difficult to find similar parallel sentences in domain adaptation settings.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Single_summ", "Narrative_cite", "Transition", "Narrative_cite", "Multi_summ", "Transition", "Single_summ", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 27, "token_end": 77, "char_start": 130, "char_end": 303, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Luong and Manning, 2015;": null, "Freitag and Al-Onaizan, 2016": "1236010"}, "Reference": {}}}, {"token_start": 83, "token_end": 125, "char_start": 331, "char_end": 544, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kobus et al. (2017)": "7497218"}, "Reference": {}}}, {"token_start": 137, "token_end": 152, "char_start": 621, "char_end": 672, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chu and Wang, 2018)": "44157913"}}}, {"token_start": 182, "token_end": 193, "char_start": 827, "char_end": 860, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Currey et al., 2017)": "40575489"}}}, {"token_start": 194, "token_end": 208, "char_start": 865, "char_end": 906, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sennrich et al., 2016a)": "15600925"}}}, {"token_start": 220, "token_end": 269, "char_start": 967, "char_end": 1130, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Axelrod et al. (2011);": "10766958", "Duh et al. (2013)": "2030497"}, "Reference": {}}}, {"token_start": 283, "token_end": 324, "char_start": 1203, "char_end": 1389, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Gulcehre et al. (2015)": "15352384"}, "Reference": {}}}, {"token_start": 333, "token_end": 367, "char_start": 1436, "char_end": 1559, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Farajian et al., 2017;": "22006749", "Bapna and Firat, 2019)": "67855706"}}}]}
{"id": "195767626_5", "paragraph": "[BOS] There have been emerging research studies that utilize the above meta-learning algorithms to NLP tasks, including language modelling (Vinyals et al., 2016 ), text classification , machine translation (Gu et al., 2018) , and relation learning (Xiong et al., 2018; Gao et al., 2019) .\n[BOS] In this paper, we propose to formulate the OOV word representation learning as a few-shot regression problem.\n[BOS] We first show that pre-training on a given corpus can somehow solve the problem.\n[BOS] To further mitigate the semantic gap between the given corpus with a new corpus, we adopt model-agnostic meta-learning (MAML) (Finn et al., 2017) to fast adapt the pre-trained model to new corpus.\n\n", "discourse_tags": ["Narrative_cite", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 22, "token_end": 34, "char_start": 120, "char_end": 162, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vinyals et al., 2016": "8909022"}}}, {"token_start": 38, "token_end": 48, "char_start": 186, "char_end": 223, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Gu et al., 2018)": null}}}, {"token_start": 50, "token_end": 69, "char_start": 230, "char_end": 286, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Xiong et al., 2018;": "52110037", "Gao et al., 2019)": "57398573"}}}, {"token_start": 129, "token_end": 149, "char_start": 588, "char_end": 643, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Finn et al., 2017)": "6719686"}}}]}
{"id": "195767626_4", "paragraph": "[BOS] Few-shot learning The paradigm of learning new tasks from a few labelled observations, referred to as few-shot learning, has received significant attention.\n[BOS] The early studies attempt to transfer knowledge learned from tasks with sufficient training data to new tasks.\n[BOS] They mainly follow a pre-train then fine-tune paradigm (Donahue et al., 2014; Bengio, 2012; Zoph et al., 2016) .\n[BOS] Recently, meta-learning is proposed and it achieves great performance on various few-shot learning tasks.\n[BOS] The intuition of meta-learning is to learn generic knowledge on a variety of learning tasks, such that the model can be adapted to learn a new task with only a few training samples.\n[BOS] Approaches for meta-learning can be categorized by the type of knowledge they learn.\n[BOS] (1) Learn a metric function that embeds data in the same class closer to each other, including Matching Networks (Vinyals et al., 2016) , and Prototypical Networks (Snell et al., 2017) .\n[BOS] The nature of metric learning makes it specified on classification problems.\n[BOS] (2) Learn a learning policy that can fast adapt to new concepts, including a better weight initialization as MAML (Finn et al., 2017 ) and a better optimizer (Ravi and Larochelle, 2017) .\n[BOS] This line of research is more general and can be applied to different learning paradigms, including both classification and regression.\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite", "Transition", "Transition", "Transition", "Narrative_cite", "Transition", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 55, "token_end": 86, "char_start": 307, "char_end": 396, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Donahue et al., 2014;": "6161478", "Bengio, 2012;": null, "Zoph et al., 2016)": "16631020"}}}, {"token_start": 185, "token_end": 197, "char_start": 891, "char_end": 931, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vinyals et al., 2016)": "8909022"}}}, {"token_start": 199, "token_end": 212, "char_start": 938, "char_end": 980, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Snell et al., 2017)": "309759"}}}, {"token_start": 248, "token_end": 259, "char_start": 1181, "char_end": 1206, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Finn et al., 2017": "6719686"}}}, {"token_start": 261, "token_end": 274, "char_start": 1213, "char_end": 1257, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ravi and Larochelle, 2017)": "67413369"}}}]}
{"id": "195767626_3", "paragraph": "[BOS] Our approach utilizes both pieces of information for an accurate estimation of OOV embeddings.\n[BOS] To leverage limited context information, we apply a complex model in contrast to the linear transformation used in the past, and learn to embed in a few-shot setting.\n[BOS] We also show that incorporating morphological features can further enhance the model when the context is extremely limited (i.e., only two or four sentences).\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "195767626_2", "paragraph": "[BOS] The second family of approaches utilizes the morphology of words (e.g., morphemes, character n-grams and character) to construct embedding vectors of unseen words based on sub-word information.\n[BOS] For example, Luong et al. (2013) proposed a morphology-aware word embedding technique by processing a sequence of morphemes with a recurrent neural network.\n[BOS] Bojanowski et al. (2017) extended skip-gram model by assigning embedding vectors to every character n-grams and represented each word as the sum of its n-grams.\n[BOS] Pinter et al. (2017) proposed MIMICK to induce word embedding from character features with a bi-LSTM model.\n[BOS] Although these approaches demonstrate reasonable performance, they rely mainly on morphology structure and cannot handle some special type of words, such as transliteration, entity names, or technical terms.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 47, "token_end": 76, "char_start": 219, "char_end": 362, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Luong et al. (2013)": "14276764"}, "Reference": {}}}, {"token_start": 77, "token_end": 114, "char_start": 369, "char_end": 529, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Bojanowski et al. (2017)": "207556454"}, "Reference": {}}}, {"token_start": 115, "token_end": 141, "char_start": 536, "char_end": 643, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Pinter et al. (2017)": "10361075"}, "Reference": {}}}]}
{"id": "195767626_1", "paragraph": "[BOS] The first family of approaches follows the distributional hypothesis (Firth, 1957) to infer the meaning of a target word based on its context.\n[BOS] If sufficient observations are given, simply applying existing word embedding techniques (e.g., word2vec) can already learn to embed OOV words.\n[BOS] However, in a real scenario, mostly the OOV word only occur for a very limited times in the new corpus, which hinders the quality of the updated embedding (Lazaridou et al., 2017; Herbelot and Baroni, 2017) .\n[BOS] Several alternatives have been proposed in the literature.\n[BOS] Lazaridou et al. (2017) proposed additive method by using the average embeddings of context words (Lazaridou et al., 2017) as the embedding of the target word.\n[BOS] Herbelot and Baroni (2017) extended the skip-gram model to nonce2vec by initialized with additive embedding, higher learning rate and window size.\n[BOS] Khodak et al. (2018) introduced a la carte, which augments the additive method by a linear transformation of context embedding.\n\n", "discourse_tags": ["Single_summ", "Transition", "Narrative_cite", "Transition", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 7, "token_end": 31, "char_start": 37, "char_end": 148, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Firth, 1957)": null}}}, {"token_start": 85, "token_end": 117, "char_start": 397, "char_end": 511, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lazaridou et al., 2017;": "205032138", "Herbelot and Baroni, 2017)": "26628362"}}}, {"token_start": 129, "token_end": 169, "char_start": 585, "char_end": 744, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Lazaridou et al. (2017)": "205032138"}, "Reference": {"(Lazaridou et al., 2017)": "205032138"}}}, {"token_start": 170, "token_end": 203, "char_start": 751, "char_end": 897, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Herbelot and Baroni (2017)": "26628362"}, "Reference": {}}}, {"token_start": 204, "token_end": 233, "char_start": 904, "char_end": 1031, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Khodak et al. (2018)": "21669304"}, "Reference": {}}}]}
{"id": "195767626_0", "paragraph": "[BOS] OOV Word Embedding Previous studies of handling OOV words were mainly based on two types of information: 1) context information and 2) morphology features.\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "196170479_4", "paragraph": "[BOS] 3 Making a Long Form QA Dataset\n\n", "discourse_tags": ["Other"], "span_citation_mapping": []}
{"id": "196170479_3", "paragraph": "[BOS] Multi-document summarization The ELI5 task of writing a paragraph length response from multiple supporting documents can be seen as a form of query-based multi-document summarization (Tombros and Sanderson, 1998 (Kocisky et al., 2018) 9.8 656 4.7 9.8 10.7 38.0 1.7 7.5 23.4 2.2 6.8 47K CoQA (Reddy et al., 2018) 5.5 271 2.7 2 5 27 2 5 15 1 43 127K SQuAD (2.0) (Rajpurkar et al., 2018) 9.9 116.6 3.2 1.4 8.9 45.3 6.0 3.6 9.6 4.4 17.6 150K HotpotQA (Yang et al., 2018) 17 text generation to answer a question, rather than to write about a general topic.\n[BOS] In addition, ELI5 contains a diverse set of questions which can involve more than one Wikipedia concept.\n\n", "discourse_tags": ["Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 29, "token_end": 56, "char_start": 148, "char_end": 240, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 88, "token_end": 101, "char_start": 288, "char_end": 317, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 119, "token_end": 136, "char_start": 354, "char_end": 390, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 169, "token_end": 182, "char_start": 439, "char_end": 472, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "196170479_2", "paragraph": "[BOS] Abstractive QA Abstractive datasets include NarrativeQA (Kocisky et al., 2018) , a dataset of movie and book summaries and CoQA (Reddy et al., 2018) , a multi-domain dialogue dataset.\n[BOS] Both collect responses with crowdworkers and find that written answers are mostly extractive and short.\n[BOS] MS MARCO (Nguyen et al., 2016) , a dataset of crowdsourced responses to Bing queries, has written answers around 1 sentence long with short input passages.\n[BOS] TriviaQA (Joshi et al., 2017) contains longer multi-document web input, collected using Bing and Wikipedia.\n[BOS] As the dataset is built from trivia, most questions can be answered with a short extractive span.\n\n", "discourse_tags": ["Multi_summ", "Transition", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 9, "token_end": 29, "char_start": 50, "char_end": 124, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 30, "token_end": 49, "char_start": 129, "char_end": 189, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 69, "token_end": 105, "char_start": 306, "char_end": 461, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 106, "token_end": 155, "char_start": 468, "char_end": 679, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "196170479_1", "paragraph": "[BOS] Extractive QA Extractive question answering datasets such as TREC (Voorhees, 2003) , SQuAD (Rajpurkar et al., 2016 , NewsQA (Trischler et al., 2017) , SearchQA (Dunn et al., 2017) , and QuAC (Choi et al., 2018) constrain the answer to a word or short phrase from the input and evaluate using exact match or F1 with the ground truth span.\n[BOS] HotpotQA (Yang et al., 2018) extends this approach by building questions which challenge models to conduct multi-hop reasoning across multiple paragraphs, but the answer is still a short span.\n[BOS] Further, the answer must be straightforward, as it needs to be copied from the supporting evidenceprecluding most \"how\" or \"why\" type questions.\n\n", "discourse_tags": ["Multi_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 12, "token_end": 22, "char_start": 67, "char_end": 90, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 22, "token_end": 34, "char_start": 91, "char_end": 122, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 34, "token_end": 47, "char_start": 123, "char_end": 156, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 47, "token_end": 57, "char_start": 157, "char_end": 185, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dunn et al., 2017)": "11606382"}}}, {"token_start": 59, "token_end": 69, "char_start": 192, "char_end": 216, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Choi et al., 2018)": "52057510"}}}, {"token_start": 96, "token_end": 135, "char_start": 350, "char_end": 542, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "196170479_0", "paragraph": "[BOS] Various QA datasets have been proposed in roughly two categories: extractive answers and short abstractive answers (see Table 1 ).\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "201739920_0", "paragraph": "[BOS] We now describe the Junczys-Dowmunt (2018) system that was the inspiration for ours.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": [{"token_start": 6, "token_end": 17, "char_start": 26, "char_end": 55, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "184482782_2", "paragraph": "[BOS] A recently emerging technique in the field of natural language processing (NLP) is the employment of transfer learning (Howard and Ruder, 2018; Devlin et al., 2018) .\n[BOS] The main idea of these approaches is to pretrain a neural language model on large general corpora and then fine-tune this model for a task at hand by adding an additional task-specific layer on top of the language model and train it for a couple of additional epochs.\n[BOS] A recent model called Bidirectional encoder representations from transformers (BERT) (Devlin et al., 2018) was pretrained on the concatenation of BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2,500M words) and then successfully applied to a number of NLP tasks without changing its core architecture and with relatively inexpensive fine-tuning for each specific task.\n[BOS] According to our knowledge, it has not been applied on a hate speech detection task yet, however it reached state-of-the-art results in the question answering task on the SQuAD dataset (Rajpurkar et al., 2016) as well as beat the baseline models in several language inference tasks.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 21, "token_end": 39, "char_start": 107, "char_end": 170, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Howard and Ruder, 2018;": "40100965", "Devlin et al., 2018)": "52967399"}}}, {"token_start": 100, "token_end": 178, "char_start": 475, "char_end": 841, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Devlin et al., 2018)": "52967399"}, "Reference": {"(Zhu et al., 2015)": "6866988"}}}, {"token_start": 216, "token_end": 229, "char_start": 1019, "char_end": 1057, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rajpurkar et al., 2016)": "11816014"}}}]}
{"id": "184482782_1", "paragraph": "[BOS] A common difficulty that arises with supervised approaches for hate speech and aggression detection is a skewed class distribution in datasets.\n[BOS] note that in the dataset used in the study only 5% of tweets were labeled as hate speech.\n[BOS] To counteract this, datasets are often resampled with different techniques to improve on the predictive power of the systems over all classes.\n[BOS] Aroyehun and Gelbukh (2018) increased the size of the used dataset by translating examples to four different languages, namely French, Spanish, German, and Hindi, and translating them back to English.\n[BOS] Their system placed first in the Aggression Detection in Social Media Shared Task of the aforementioned TRAC workshop.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 76, "token_end": 140, "char_start": 401, "char_end": 726, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "184482782_0", "paragraph": "[BOS] A number of workshops that dealt with offensive content, hate speech and aggression were organized in the past several years, which points to the increasing interest in the field.\n[BOS] Due to important contributions of publications from TA-COS 1 , Abusive Language Online 2 , and TRAC 3 , hate speech detection became better understood and established as a hard problem.\n[BOS] The report on shared task from the TRAC workshop (Kumar et al., 2018) shows that of 45 systems trying to identify hateful content in English and Hindi Facebook posts, the best-performing ones achieved weighted macro-averaged F-scores of just over 0.6.\n[BOS] Schmidt and Wiegand (2017) note in their survey that supervised learning approaches are predominantly used for hate speech detection.\n[BOS] Among those, the most widespread are support vector machines (SVM) and recurrent neural networks, which are emerging in recent times (Pavlopoulos et al., 2017) .\n[BOS] Zhang et al. (2018) devised a neural network architecture combining convolutional and gated recurrent layers for detecting hate speech, achieving state-of-the-art performance on several Twitter datasets.\n[BOS] used SVMs with different surface-level features, such as surface n-grams, word skip-grams and word representation n-grams induced with Brown clustering.\n[BOS] They concluded that surface n-grams perform well for hate speech detection but also noted that these features might not be enough to discriminate between profanity and hate speech with high accuracy and that deeper linguistic features might be required for this scenario.\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 81, "token_end": 131, "char_start": 419, "char_end": 635, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Kumar et al., 2018)": "59336626"}, "Reference": {}}}, {"token_start": 132, "token_end": 193, "char_start": 642, "char_end": 941, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Schmidt and Wiegand (2017)": "9626793"}, "Reference": {"(Pavlopoulos et al., 2017)": "2944650"}}}, {"token_start": 195, "token_end": 233, "char_start": 950, "char_end": 1153, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhang et al. (2018)": "46939253"}, "Reference": {}}}]}
{"id": "207917437_1", "paragraph": "[BOS] Recently, research interests in MRQA have been extended to conversational-style QA, in which a series of inter-related QA turns is performed in the expectation that it would simulate more natural interactions involving a human.\n[BOS] Datasets such as CoQA (Reddy et al., 2019) and QuAC (Choi et al., 2018) have been developed to facilitate the relevant research efforts (Yatskar, 2019) .\n[BOS] Given this trend, Gao et al. (2019) was first to propose a framework for conversational question generation (CQG).\n[BOS] Their proposal has initiated the dedicated field of CQG by particularly considering coreferences and conversion flows, both may be essential elements in conversational QA.\n[BOS] Their proposal, however, remained answeraware, which may somehow restrict its application areas, in particular such as dialogue systems.\n[BOS] Thus answer-unaware conversational question generation first to offered by the present work would be a natural research direction to go.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Single_summ", "Single_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 49, "token_end": 61, "char_start": 257, "char_end": 282, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Reddy et al., 2019)": "52055325"}}}, {"token_start": 62, "token_end": 72, "char_start": 287, "char_end": 311, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Choi et al., 2018)": "52057510"}}}, {"token_start": 78, "token_end": 89, "char_start": 350, "char_end": 391, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yatskar, 2019)": "52895001"}}}, {"token_start": 95, "token_end": 174, "char_start": 418, "char_end": 835, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Gao et al. (2019)": "189927819"}, "Reference": {}}}]}
{"id": "207917437_0", "paragraph": "[BOS] Given a range of application areas, such as intelligent tutoring systems, dialogue systems and question answering systems, question generation has attracted larger research attention in NLP community.\n[BOS] The major trend in question generation has shifted from template-based generation systems to neural network-based end-to-end methods (Pan et al., 2019) , which generally employs encoder-decoder models.\n[BOS] Succeeding the pioneering work (Du et al., 2017) , several proposals Du and Cardie, 2018; Yuan et al., 2017; Tang et al., 2017) have been made to chiefly improve the quality of generated questions.\n[BOS] These methods all deal with text-based question answering, which relies on datasets, such as SQuAD (Rajpurkar et al., 2018) , which was originally developed for the machine reading for question answering (MRQA) research.\n[BOS] In the context of the present work, however, it should be noted that the majority of these methods are answer-aware, which means that a generation system requires the corresponding answer to a to-be-generated question is supplied.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 51, "token_end": 70, "char_start": 306, "char_end": 364, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Pan et al., 2019)": "162168545"}}}, {"token_start": 82, "token_end": 94, "char_start": 432, "char_end": 469, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Du et al., 2017)": "2172129"}}}, {"token_start": 95, "token_end": 118, "char_start": 472, "char_end": 548, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Du and Cardie, 2018;": "21702856", "Yuan et al., 2017;": "7562142", "Tang et al., 2017)": "37738077"}}}, {"token_start": 150, "token_end": 162, "char_start": 718, "char_end": 748, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rajpurkar et al., 2018)": "47018994"}}}]}
{"id": "207969314_2", "paragraph": "[BOS] Compared to a conventional NMT model with only a single head, multi-head is assumed to have a stronger ability to extract different features in different subspaces.\n[BOS] However, there are no explicit mechanism that make them distinct (Voita et al., 2019; Michel et al., 2019) .\n[BOS] Li et al. (2018) had shown that using a disagreement regularization to encourage different attention heads to have different behaviors can improve the performance of multi-head attention.\n[BOS] Iida et al. (2019) proposed a multi-hop attention where the second-hop serves as a head gate function to normalize the attentional context of each head.\n[BOS] Not only limited in the field of neural machine translation, Strubell et al. (2018) combined multi-head self-attention with multi-task learning, this led to a promising result for semantic role labeling.\n[BOS] Similar to the above studies, we also attempt to model diversity for multi-head attention.\n[BOS] In this work, we apply dif-ferent attention function to capture different aspects of features in multiple heads directly, which is more intuitive and explicit.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Single_summ", "Single_summ", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 36, "token_end": 63, "char_start": 186, "char_end": 283, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Voita et al., 2019;": "162183964", "Michel et al., 2019)": null}}}, {"token_start": 65, "token_end": 98, "char_start": 292, "char_end": 479, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Li et al. (2018)": "53081097"}, "Reference": {}}}, {"token_start": 99, "token_end": 134, "char_start": 486, "char_end": 638, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Iida et al. (2019)": "196181767"}, "Reference": {}}}, {"token_start": 146, "token_end": 179, "char_start": 706, "char_end": 848, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Strubell et al. (2018)": "5068376"}, "Reference": {}}}]}
{"id": "207969314_1", "paragraph": "[BOS] However, self-attention, which employs neither recurrence nor convolution, has great difficulty in incorporating position information (Vaswani et al., 2017) .\n[BOS] To tackle this problem, Shaw et al. (2018) presented an extension that can be used to incorporate relative position information for sequence.\n[BOS] And Shen et al. (2018) tried to encode the temporal order and introduced a directional self-attention which only composes of directional order.\n[BOS] On the other hand, although with a global receptive field, the ability of selfattention recently came into question (Tang et al., 2018) .\n[BOS] And modeling localness, either restricting context sizes Wu et al., 2019; Child et al., 2019) or balancing the contribution of local and global information (Xu et al., 2019) , has been shown to be able to improve the expressiveness of self-attention.\n[BOS] In contrast to these studies, we aim to improve the self-attention in a systematic and multifaceted perspective, rather than just paying attention to one specific characteristic.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ", "Narrative_cite", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 19, "token_end": 32, "char_start": 105, "char_end": 162, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vaswani et al., 2017)": "13756489"}}}, {"token_start": 39, "token_end": 61, "char_start": 195, "char_end": 312, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Shaw et al. (2018)": "3725815"}, "Reference": {}}}, {"token_start": 63, "token_end": 91, "char_start": 323, "char_end": 462, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Shen et al. (2018)": "19152001"}, "Reference": {}}}, {"token_start": 107, "token_end": 122, "char_start": 543, "char_end": 604, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Tang et al., 2018)": "52100282"}}}, {"token_start": 130, "token_end": 149, "char_start": 644, "char_end": 706, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Wu et al., 2019;": "59310641", "Child et al., 2019)": "129945531"}}}, {"token_start": 150, "token_end": 167, "char_start": 710, "char_end": 786, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Xu et al., 2019)": "196206239"}}}]}
{"id": "207969314_0", "paragraph": "[BOS] In the field of neural machine translation, the two most used attention mechanisms are additive attention (Bahdanau et al., 2015) and dot attention (Luong et al., 2015) .\n[BOS] Based on the latter, Vaswani et al. (2017) proposed a multi-head selfattention, that is not only highly parallelizable but also with better performance.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ"], "span_citation_mapping": [{"token_start": 17, "token_end": 29, "char_start": 93, "char_end": 135, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bahdanau et al., 2015)": "61556494"}}}, {"token_start": 30, "token_end": 41, "char_start": 140, "char_end": 174, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Luong et al., 2015)": "1998416"}}}, {"token_start": 48, "token_end": 79, "char_start": 204, "char_end": 335, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Vaswani et al. (2017)": "13756489"}, "Reference": {}}}]}
{"id": "174800180_3", "paragraph": "[BOS] Robustness to noise is an important aspect in machine learning.\n[BOS] It has been studied for various models (Szegedy et al., 2014; Goodfellow et al., 2015) , including NLP in general (Papernot et al., 2016; Samanta and Mehta, 2017; Liang et al., 2018; Jia and Liang, 2017; Ebrahimi et al., 2018; Gao et al., 2018) , and character-based NMT in particular (Heigold et al., 2018; Belinkov and Bisk, 2018) .\n[BOS] Unlike this work, we compare robustness to noise for units of different granularity.\n[BOS] Moreover, we focus on representation learning rather than on the quality of the translation output.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 19, "token_end": 40, "char_start": 100, "char_end": 162, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Szegedy et al., 2014;": "604334", "Goodfellow et al., 2015)": "6706414"}}}, {"token_start": 42, "token_end": 93, "char_start": 175, "char_end": 320, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Papernot et al., 2016;": "12390290", "Samanta and Mehta, 2017;": "38134825", "Liang et al., 2018;": "10642653", "Jia and Liang, 2017;": "7228830", "Ebrahimi et al., 2018;": "21698802", "Gao et al., 2018)": "4858173"}}}, {"token_start": 95, "token_end": 121, "char_start": 327, "char_end": 408, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Heigold et al., 2018;": "19009822", "Belinkov and Bisk, 2018)": "3513372"}}}]}
{"id": "174800180_2", "paragraph": "[BOS] Subword translation units aim at reducing the vocabulary size and the out-of-vocabulary (OOV) rate.\n[BOS] Researchers have used BPE units (Sennrich et al., 2016) , morphological segmentation (Bradbury and Socher, 2016), characters (Durrani et al., 2014; , and hybrid units (Ling et al., 2015; Costa-juss and Fonollosa, 2016) to address the OOV word problem in MT.\n[BOS] The choice of translation unit impacts what the network learns.\n[BOS] Sennrich (2017) carried a systematic error analysis by comparing subword versus character units and found the latter to be better at handling OOV and transliterations, whereas BPEbased subword units were better at capturing syntactic dependencies.\n[BOS] In contrast, here we focus on representation learning, not translation quality.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 29, "token_end": 42, "char_start": 134, "char_end": 167, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sennrich et al., 2016)": "1114678"}}}, {"token_start": 43, "token_end": 54, "char_start": 170, "char_end": 224, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 55, "token_end": 65, "char_start": 226, "char_end": 258, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 68, "token_end": 89, "char_start": 266, "char_end": 330, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ling et al., 2015;": "5799549", "Costa-juss\u00e0 and Fonollosa, 2016)": "1712853"}}}, {"token_start": 112, "token_end": 160, "char_start": 446, "char_end": 693, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "174800180_1", "paragraph": "[BOS] Other studies carried a more fine-grained neuronlevel analysis for NMT and LM Lakretz et al., 2019) .\n[BOS] While previous work focused on words, here we compare units of different granularities.\n\n", "discourse_tags": ["Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 15, "token_end": 29, "char_start": 73, "char_end": 105, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Lakretz et al., 2019)": "81978369"}}}]}
{"id": "174800180_0", "paragraph": "[BOS] Representation analysis aims at demystifying what is learned inside the neural network blackbox.\n[BOS] This includes analyzing word and sentence embeddings (Adi et al., 2017; Qian et al., 2016b; Ganesh et al., 2017; Conneau et al., 2018, among others) , RNN states (Qian et al., 2016a; Shi et al., 2016; Wu and King, 2016; Wang et al., 2017) , and NMT representations (Shi et al., 2016; Belinkov et al., 2017a) , as applied to morphological (Vylomova et al., 2017; , semantic (Qian et al., 2016b; Belinkov et al., 2017b) and syntactic (Linzen et al., 2016; Tran et al., 2018; Conneau et al., 2018) tasks.\n[BOS] See for a recent survey.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 22, "token_end": 61, "char_start": 123, "char_end": 242, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Adi et al., 2017;": "6771196", "Qian et al., 2016b;": "3104544", "Ganesh et al., 2017;": "19153261"}}}, {"token_start": 66, "token_end": 99, "char_start": 260, "char_end": 347, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Shi et al., 2016;": "7197724", "Wu and King, 2016;": "1328299", "Wang et al., 2017)": "14589038"}}}, {"token_start": 101, "token_end": 122, "char_start": 354, "char_end": 416, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Shi et al., 2016;": "7197724", "Belinkov et al., 2017a)": "7100502"}}}, {"token_start": 126, "token_end": 137, "char_start": 433, "char_end": 469, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 139, "token_end": 160, "char_start": 473, "char_end": 526, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Qian et al., 2016b;": "3104544", "Belinkov et al., 2017b)": "24544277"}}}, {"token_start": 161, "token_end": 187, "char_start": 531, "char_end": 603, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Linzen et al., 2016;": "14091946", "Tran et al., 2018;": "3785155", "Conneau et al., 2018)": "24461982"}}}]}
{"id": "184483107_0", "paragraph": "[BOS] Hate Speech Detection research attracts researchers from diverse background like computational Linguistic, computer science, and social science.\n[BOS] The actual term hate speech was coined by (Warner and Hirschberg, 2012) .\n[BOS] Various Authors used different notion like offensive language (Razavi et al., 2010) , cyberbullying (Xu et al., 2012) , aggression (Kumar et al., 2018) .\n[BOS] (Davidson et al., 2017 ) studied tweet classification of hate speech and offensive language and defined hate speech as following: language that is used to express hatred towards a targeted group or is intended to be derogatory, to humiliate, or to insult the members of the group.\n[BOS] Authors observed that offensive language is often miss-classified as hate speech.\n[BOS] They have trained a multi-class classifier on N-gram features weighted by its TF-IDF weights and PoS tags.\n[BOS] In addition to these, features like sentiment score of each tweet, number of hashtags, and URLS, mentions are considered.\n[BOS] Authors concluded that Logistic Regression and Linear SVM are better than NB, Decision Tree, and Random Forests.\n[BOS] (Schmidt and Wiegand, 2017) perform comprehensive survey on hate speech.\n[BOS] They have identified features like surface features, sentiment, word generalization, lexical, linguistics etc.\n[BOS] can be used by classifier.\n[BOS] tried to address the problem of discriminating profanity from hate speech in the social media posts.\n[BOS] N-grams, skipgram and clustering based word representation features are considered for the 3-class classification.\n[BOS] The author uses SVM and advance ensemble based classifier for this task and achieved 80% accuracy.\n[BOS] (Gambck and Sikdar, 2017 ) performed 4-class classification on Twitter messages using CNN with word embedding generated through Word2vec and character n-grams.\n[BOS] Authors claim that word embedding generated through Word2vec outperformed random vector and n-gram characters.\n[BOS] (Zhang et al., 2018) proposed a new method based on CNN and LSTM with drop out and pooling for hate speech detection.\n[BOS] Authors concluded that their method achieved improvement on F1 score of most of the hate speech datasets.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 28, "token_end": 44, "char_start": 173, "char_end": 228, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Warner and Hirschberg, 2012)": "12477446"}}}, {"token_start": 52, "token_end": 65, "char_start": 280, "char_end": 320, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Razavi et al., 2010)": "13966254"}}}, {"token_start": 66, "token_end": 77, "char_start": 323, "char_end": 354, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Xu et al., 2012)": "9912528"}}}, {"token_start": 78, "token_end": 87, "char_start": 357, "char_end": 388, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kumar et al., 2018)": "59336626"}}}, {"token_start": 89, "token_end": 247, "char_start": 397, "char_end": 1125, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Davidson et al., 2017": "1733167"}, "Reference": {}}}, {"token_start": 248, "token_end": 284, "char_start": 1132, "char_end": 1321, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Schmidt and Wiegand, 2017)": "9626793"}, "Reference": {}}}, {"token_start": 354, "token_end": 410, "char_start": 1694, "char_end": 1970, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Gamb\u00e4ck and Sikdar, 2017": null}, "Reference": {}}}, {"token_start": 411, "token_end": 461, "char_start": 1977, "char_end": 2206, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Zhang et al., 2018)": "46939253"}, "Reference": {}}}]}
{"id": "202541632_2", "paragraph": "[BOS] Recent work improves upon adversarial approaches by additionally leveraging the idea of back translation (dos Santos et al., 2018; Logeswaran et al., 2018; Lample et al., 2019; Prabhumoye et al., 2018) .\n[BOS] It was previously used for unsupervised Statistical Machine Translation (SMT) (Fung and Yee, 1998; Munteanu et al., 2004; Smith et al., 2010) and Neural Machine Translation (NMT) (Conneau et al., 2017b; Lample et al., 2017; Artetxe et al., 2017) , where it iteratively takes the pseudo pairs to train a translation model and then use the refined model to generate new pseudo-parallel pairs with enhanced quality.\n[BOS] However, the success of this method relies on good quality of the pseudo-parallel pairs.\n[BOS] Our approach proposes using retrieved sentences from the corpus based on semantic similarity as a decent starting point and then refining them using the trained translation models iteratively.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 16, "token_end": 57, "char_start": 94, "char_end": 207, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(dos Santos et al., 2018;": "29165442", "Prabhumoye et al., 2018)": "13959787"}}}, {"token_start": 64, "token_end": 97, "char_start": 243, "char_end": 357, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Fung and Yee, 1998;": "2357627", "Munteanu et al., 2004;": null, "Smith et al., 2010)": "2468787"}}}, {"token_start": 98, "token_end": 133, "char_start": 362, "char_end": 461, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Conneau et al., 2017b;": "3470398", "Artetxe et al., 2017)": "3515219"}}}]}
{"id": "202541632_1", "paragraph": "[BOS] In the past few years, attribute-controlled NLG has witnessed renewed interest by researchers working on neural approaches to generation (Hu et al., 2017; Jhamtani et al., 2017; Melnyk et al., 2017; Mueller et al., 2017; Zhang et al., 2018; Prabhumoye et al., 2018; Niu and Bansal, 2018) .\n[BOS] Among them, many attribute-controlled text rewriting methods similarly employ GANbased models to disentangle the content and style of text in a shared latent space (Shen et al., 2017; Fu et al., 2018) .\n[BOS] However, existing work that applies these ideas to text suffers from both training difficulty (Salimans et al., 2016; Arjovsky and Bottou, 2017; Bousmalis et al., 2017) , and ineffective manipulation of the latent space which leads to content loss (Li et al., 2018) and generation of grammatically-incorrect sentences.\n[BOS] Other lines of research avoid adversarial training altogether.\n[BOS] Li et al. (2018) proposed a much simpler approach: identify style-carrying n-grams, replace them with phrases of the opposite style, and train a neural language model to combine them in a natural way.\n[BOS] Despite outperforming the adversarial approaches, its performance is dependent on the availability of an accurate word identifier, a precise word replacement selector and a perfect language model to fix the grammatical errors introduced by the crude swap.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Narrative_cite", "Transition", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 22, "token_end": 85, "char_start": 111, "char_end": 293, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hu et al., 2017;": "20981275", "Jhamtani et al., 2017;": "9737200", "Melnyk et al., 2017;": "1215120", "Mueller et al., 2017;": "2148537", "Zhang et al., 2018;": "52091536", "Prabhumoye et al., 2018;": "13959787", "Niu and Bansal, 2018)": "13690180"}}}, {"token_start": 97, "token_end": 132, "char_start": 363, "char_end": 502, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Shen et al., 2017;": "7296803", "Fu et al., 2018)": "6484065"}}}, {"token_start": 147, "token_end": 178, "char_start": 585, "char_end": 679, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Salimans et al., 2016;": "1687220", "Arjovsky and Bottou, 2017;": "18828233", "Bousmalis et al., 2017)": "206595056"}}}, {"token_start": 182, "token_end": 199, "char_start": 711, "char_end": 776, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Li et al., 2018)": "52091536"}}}, {"token_start": 220, "token_end": 308, "char_start": 905, "char_end": 1367, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "202541632_0", "paragraph": "[BOS] Attribute-controlled text rewriting remains a longstanding problem in NLG, where most work has focused on studying the stylistic variation in text (Gatt and Krahmer, 2018) .\n[BOS] Early contributions in this area defined stylistic features using rules to vary generation (Brooke et al., 2010) .\n[BOS] For instance, Sheikha and Inkpen (2011) proposed an adaptation of the SimpleNLG realiser (Gatt et al., 2009) to handle formal versus informal language via constructing lexicons of formality (e.g., are not vs. aren't).\n[BOS] More contemporary approaches have tended to eschew rules in favour of data-driven methods to identify relevant linguistic features to stylistic attributes (Ballesteros et al., 2015; Di Fabbrizio et al., 2008; Krahmer and van Deemter, 2012) .\n[BOS] For example, Mairesse and Walker's PER-SONAGE system (Mairesse and Walker, 2011) uses machine-learning models to take as inputs a list of real-valued style parameters and generate sentences to project different personality traits.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Single_summ", "Narrative_cite", "Single_summ"], "span_citation_mapping": [{"token_start": 24, "token_end": 39, "char_start": 125, "char_end": 177, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Gatt and Krahmer, 2018)": "16946362"}}}, {"token_start": 51, "token_end": 64, "char_start": 252, "char_end": 298, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Brooke et al., 2010)": "7948393"}}}, {"token_start": 69, "token_end": 126, "char_start": 321, "char_end": 524, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Gatt et al., 2009)": "3939596"}}}, {"token_start": 140, "token_end": 185, "char_start": 601, "char_end": 770, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ballesteros et al., 2015;": "18499217", "Di Fabbrizio et al., 2008;": "12225323"}}}, {"token_start": 190, "token_end": 235, "char_start": 792, "char_end": 1009, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "184483887_5", "paragraph": "[BOS] The problem of distinguishing general profanity from hate speech is not a trivial task and requires features that capture a deeper understanding of the text not always possible with surface grams.\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "184483887_4", "paragraph": "[BOS] Hope at SemEval-2019 Task 6: Mining social media language to discover offensive language machine learning methods are used in the literature, from Logistic regression, Nave Bayes, Decision Trees, Random forests, SMVs to neural networks.\n[BOS] Previous analysis of hate speech modeling (Schmidt and Wiegand, 2017) shows that there is a too wide range of features used, and a more advanced feature relevance analysis was needed (Waseem et al., 2017) .\n[BOS] A first shared task on aggression identification aiming to classify aggressive speech into overt, covert or no aggression was held at the TRAC Workshop collocated with COLING 2018 (Kumar et al., 2018) .\n[BOS] 130 teams registered to participate in the task, 30 teams submitted their test runs and 20 teams sent their system description paper, which are included in the TRAC workshop proceedings.\n\n", "discourse_tags": ["Transition", "Single_summ", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 53, "token_end": 99, "char_start": 258, "char_end": 453, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Schmidt and Wiegand, 2017)": "9626793"}, "Reference": {"(Waseem et al., 2017)": "8821211"}}}, {"token_start": 131, "token_end": 142, "char_start": 630, "char_end": 662, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kumar et al., 2018)": "59336626"}}}]}
{"id": "184483887_3", "paragraph": "[BOS] Lexical detection methods for the offensive language tend to have low precision because they fail to classify messages not containing listed offensive terms.\n[BOS] On the other hand, various\n\n", "discourse_tags": ["Transition", "Transition"], "span_citation_mapping": []}
{"id": "184483887_2", "paragraph": "[BOS] Based on work on hate speech, cyberbullying and online abuse, Waseem et al., 2017 proposses a typology that captures central similarities and differences between subtasks and discuss its implications for data annotation and feature construction.\n[BOS] Additionally, Waseem et al. (2017) emphasize the practical actions that can be taken by researchers to best approach their abusive language detection subtask of interest.\n\n", "discourse_tags": ["Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 4, "token_end": 50, "char_start": 15, "char_end": 251, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Waseem et al., 2017": "8821211"}, "Reference": {}}}, {"token_start": 53, "token_end": 85, "char_start": 272, "char_end": 428, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Waseem et al. (2017)": "8821211"}, "Reference": {}}}]}
{"id": "184483887_1", "paragraph": "[BOS] Offensive language is often subdivided into various intercalated categories, since different subtasks have been grouped under this label.\n[BOS] One of the most analyzed such language is \"hate speech\", i.e. discriminative remarks, such as the racist or sexist ones (Norbata et al., 2016) .\n\n", "discourse_tags": ["Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 50, "token_end": 66, "char_start": 248, "char_end": 292, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "184483887_0", "paragraph": "[BOS] This topic has attracted significant attention in recent years, evidenced by increasing number of recent publications and a several scientific events such as ALW and TRAC workshops.\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "208092413_0", "paragraph": "[BOS] Paraphrase generation has attracted a number of different NLP approaches.\n[BOS] These have included rule-based approaches (McKeown, 1979; Meteer and Shaked, 1988 ) and data-driven methods (Madnani and Dorr, 2010) , with recently the most common approach being that the task is treated as a language translation task (Bannard and Callison-Burch, 2005; Barzilay and McKeown, 2001; Pang et al., 2003) -often performed using a bilingual corpus pivoting back and forth (Madnani and Dorr, 2010; Prakash et al., 2016; Mallinson et al., 2017) .\n[BOS] Other methods proposed include more recently the use of Deep Reinforcement Learning (Li et al., 2018) , supervised learning using sequence-to-sequence models (Gupta et al., 2018; Prakash et al., 2016) and unsupervised approaches (Bowman et al., 2016; Roy and Grangier, 2019) .\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 20, "token_end": 38, "char_start": 106, "char_end": 167, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(McKeown, 1979;": "462954", "Meteer and Shaked, 1988": "5076418"}}}, {"token_start": 40, "token_end": 54, "char_start": 174, "char_end": 218, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Madnani and Dorr, 2010)": "17652653"}}}, {"token_start": 69, "token_end": 102, "char_start": 296, "char_end": 403, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bannard and Callison-Burch, 2005;": "15728911", "Barzilay and McKeown, 2001;": "9842595", "Pang et al., 2003)": "11728052"}}}, {"token_start": 107, "token_end": 140, "char_start": 429, "char_end": 540, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Madnani and Dorr, 2010;": "17652653", "Prakash et al., 2016;": "9385494", "Mallinson et al., 2017)": "17246494"}}}, {"token_start": 151, "token_end": 162, "char_start": 605, "char_end": 650, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Li et al., 2018)": "21646317"}}}, {"token_start": 163, "token_end": 188, "char_start": 653, "char_end": 749, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Gupta et al., 2018;": "12737290", "Prakash et al., 2016)": "9385494"}}}, {"token_start": 189, "token_end": 209, "char_start": 754, "char_end": 823, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bowman et al., 2016;": "748227", "Roy and Grangier, 2019)": "170078852"}}}]}
{"id": "203594044_3", "paragraph": "[BOS] To preserve semantics more explicitly, Fu et al. (2018) use a multi-decoder model to learn content representations that do not reflect styles.\n[BOS] Shetty et al. (2017) use a cycle constraint that penalizes L 1 distance between input and round-trip transfer reconstruction.\n[BOS] Our cycle consistency loss is inspired by Shetty et al. (2017) , together with the idea of back translation in unsupervised neural machine translation (Artetxe et al., 2017; Lample et al., 2017) , and the idea of cycle constraints in image generation by Zhu et al. (2017) .\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 8, "token_end": 31, "char_start": 45, "char_end": 148, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Fu et al. (2018)": "6484065"}, "Reference": {}}}, {"token_start": 32, "token_end": 59, "char_start": 155, "char_end": 280, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Shetty et al. (2017)": "39764280"}, "Reference": {}}}, {"token_start": 61, "token_end": 75, "char_start": 291, "char_end": 349, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Shetty et al. (2017)": "39764280"}}}, {"token_start": 81, "token_end": 106, "char_start": 378, "char_end": 481, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Artetxe et al., 2017;": "3515219", "Lample et al., 2017)": "3518190"}}}, {"token_start": 114, "token_end": 124, "char_start": 521, "char_end": 558, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Zhu et al. (2017)": "206770979"}}}]}
{"id": "203594044_2", "paragraph": "[BOS] Textual Transfer Models In terms of generating the transferred sentences, to address the lack of parallel data, Hu et al. (2017) used variational autoencoders to generate content representations devoid of style, which can be converted to sentences with a specific style.\n[BOS] Ficler and Goldberg (2017) used conditional language models to generate sentences where the desired content and style are conditioning contexts.\n[BOS] used a feature-based approach that deletes characteristic words from the original sentence, retrieves similar sentences in the target corpus, and generates based on the original sentence and the characteristic words from the retrieved sentences.\n[BOS] integrated reinforcement learning into the textual transfer problem.\n[BOS] Another way to address the lack of parallel data is to use learning frameworks based on adversarial objectives (Goodfellow et al., 2014) ; several have done so for textual transfer (Yu et al., 2017; Li et al., 2017; Yang et al., 2018a; Shen et al., 2017; Fu et al., 2018) .\n[BOS] Recent work uses target-domain language models as discriminators to provide more stable feedback in learning (Yang et al., 2018b) .\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 21, "token_end": 52, "char_start": 118, "char_end": 276, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hu et al. (2017)": "20981275"}, "Reference": {}}}, {"token_start": 53, "token_end": 77, "char_start": 283, "char_end": 427, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ficler and Goldberg (2017)": "11054023"}, "Reference": {}}}, {"token_start": 146, "token_end": 159, "char_start": 849, "char_end": 897, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Goodfellow et al., 2014)": "1033682"}}}, {"token_start": 165, "token_end": 204, "char_start": 925, "char_end": 1032, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yu et al., 2017;": "3439214", "Li et al., 2017;": "98180", "Yang et al., 2018a;": "4702087", "Shen et al., 2017;": "7296803", "Fu et al., 2018)": "6484065"}}}, {"token_start": 220, "token_end": 233, "char_start": 1122, "char_end": 1170, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yang et al., 2018b)": "44061800"}}}]}
{"id": "203594044_1", "paragraph": "[BOS] In contemporaneous work, Mir et al. (2019) similarly proposed three types of metrics for style transfer tasks.\n[BOS] There are two main differences compared to our work: (1) They use a stylekeyword masking procedure before evaluating semantic similarity, which works on the Yelp dataset (the only dataset Mir et al. (2019) test on) but does not work on our Literature dataset or similarly complicated tasks, because the masking procedure goes against preserving content-specific nonstyle-related words.\n[BOS] (2) They do not provide a way of aggregating three metrics for the purpose of model selection and overall comparison.\n[BOS] We address these two problems, and we also propose metrics that are simple in addition to being effective, which is beneficial for ease of use and widespread adoption.\n\n", "discourse_tags": ["Single_summ", "Reflection", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 7, "token_end": 26, "char_start": 31, "char_end": 116, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Mir et al. (2019)": "102354384"}, "Reference": {}}}, {"token_start": 57, "token_end": 72, "char_start": 280, "char_end": 328, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Mir et al. (2019)": "102354384"}}}]}
{"id": "203594044_0", "paragraph": "[BOS] Textual Transfer Evaluation Recent work has included human evaluation of the three categories (post-transfer style accuracy, semantic preservation, fluency), but does not propose automatic evaluation metrics for all three Prabhumoye et al., 2018; Chen et al., 2018; .\n[BOS] There have been recent proposals for supervised evaluation metrics , but these require annotation and are therefore unavailable for new textual transfer tasks.\n[BOS] There is a great deal of recent work in textual transfer (Yang et al., 2018b; Santos et al., 2018; Logeswaran et al., 2018; Nikolov and Hahnloser, 2018) , but all either lack certain categories of unsupervised metric or lack human validation of them, which we contribute.\n[BOS] Moreover, the textual transfer community lacks discussion of early stopping criteria and methods of holistic model comparison.\n[BOS] We propose a one-number summary for transfer quality, which can be used to select and compare models.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Narrative_cite", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 55, "char_start": 6, "char_end": 270, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Prabhumoye et al., 2018;": "13959787"}}}, {"token_start": 92, "token_end": 131, "char_start": 486, "char_end": 598, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yang et al., 2018b;": "44061800", "Santos et al., 2018;": "29165442", "Logeswaran et al., 2018;": "53217784", "Nikolov and Hahnloser, 2018)": "53041597"}}}]}
{"id": "202541578_5", "paragraph": "[BOS] In Grammatical Error Correction (Ng et al., 2013 (Ng et al., , 2014 a system is presented with input texts written usually by a language learner, and is tasked with detecting and fixing grammatical (and other) mistakes.\n[BOS] Approaches to this task often incorporate task-specific knowledge, e.g., by designing classifiers for specific error types (Knight and Chander, 1994; Rozovskaya et al., 2014 ) that can be trained without manually labeled data, or by adapting statistical machine-translation methods (JunczysDowmunt and Grundkiewicz, 2014) .\n[BOS] Methods for the sub-problem of error detection are similar in spirit to sentence compression systems, in that they are implemented as word-based neural sequence labelers (Rei, 2017; .\n[BOS] Neural encoderdecoder methods are also commonly applied to the error correction task (Ge et al., 2018; Chollampatt and Ng, 2018; Zhao et al., 2019) , but suffer from a lack of training data, which is why taskspecific tricks need to be applied (Kasewa et al., 2018; Junczys-Dowmunt et al., 2018) .\n\n", "discourse_tags": ["Multi_summ", "Narrative_cite", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 2, "token_end": 50, "char_start": 6, "char_end": 225, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Ng et al., 2013": "660745", "(Ng et al., , 2014": null}, "Reference": {}}}, {"token_start": 69, "token_end": 94, "char_start": 318, "char_end": 407, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Rozovskaya et al., 2014": "1547333"}}}, {"token_start": 105, "token_end": 126, "char_start": 465, "char_end": 553, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 152, "token_end": 164, "char_start": 696, "char_end": 742, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 178, "token_end": 206, "char_start": 815, "char_end": 899, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Zhao et al., 2019)": "67856013"}}}, {"token_start": 219, "token_end": 250, "char_start": 956, "char_end": 1046, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "202541578_4", "paragraph": "[BOS] Out of available summarization datasets (Dernoncourt et al., 2018) , we find the one by Toutanova et al. (2016) particularly interesting because (1) it specifically targets abstractive summarization systems, (2) the lengths of texts in this dataset (short paragraphs) seem well-suited for text editing, and (3) an analysis showed that the dataset covers many different summarization operations.\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 5, "token_end": 19, "char_start": 23, "char_end": 72, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 24, "token_end": 35, "char_start": 91, "char_end": 117, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Toutanova et al. (2016)": "8241566"}}}]}
{"id": "202541578_3", "paragraph": "[BOS] Single-document summarization is a task that requires systems to shorten texts in a meaningpreserving way.\n[BOS] It has been approached with deletion-based methods on the token level (Filippova et al., 2015) and the sentence level (Narayan et al., 2018; Liu, 2019) .\n[BOS] Other papers have used neural encoder-decoder methods (Tan et al., 2017; Rush et al., 2015; Paulus et al., 2017) to do abstractive summarization, which allows edits beyond mere deletion.\n[BOS] This can be motivated by the work of Jing and McKeown (2000) , who identified a small number of fundamental high-level editing operations that are useful for producing summaries (reduction, combination, syntactic transformation, lexical paraphrasing, generalization/specification, and reordering).\n[BOS] See et al. (2017) extended a neural encoder-decoder model with a copy mechanism to allow the model to more easily reproduce input tokens during generation.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 34, "token_end": 46, "char_start": 177, "char_end": 213, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 48, "token_end": 64, "char_start": 222, "char_end": 270, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Narayan et al., 2018;": "3510042", "Liu, 2019)": "85500417"}}}, {"token_start": 70, "token_end": 99, "char_start": 302, "char_end": 391, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Tan et al., 2017;": "26698484", "Rush et al., 2015;": "1918428", "Paulus et al., 2017)": "21850704"}}}, {"token_start": 121, "token_end": 172, "char_start": 501, "char_end": 769, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 173, "token_end": 204, "char_start": 776, "char_end": 931, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"See et al. (2017)": null}, "Reference": {}}}]}
{"id": "202541578_2", "paragraph": "[BOS] Independent of this work, Dong et al. (2019) recently proposed a text-editing model, similar to ours, for text simplification.\n[BOS] The main differences to our work are: (i) They introduce an interpreter module which acts as a language model for the so-far-realized text, and (ii) they generate added tokens one-by-one from a full vocabulary rather than from an optimized set of frequently added phrases.\n[BOS] The latter allows their model to generate more diverse output, but it may negatively effect the inference time, precision, and the data efficiency of their model.\n[BOS] Another recent model similar to ours is called Levenshtein Transformer Gu et al. (2019) , which does text editing by performing a sequence of deletion and insertion actions.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 7, "token_end": 122, "char_start": 32, "char_end": 580, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 123, "token_end": 159, "char_start": 587, "char_end": 760, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "202541578_1", "paragraph": "[BOS] Text Simplification is a paraphrasing task that is known to benefit from modeling edit operations.\n[BOS] A simple instance of this type are sentence compression systems that apply a drop operation at the token/phrase level (Filippova and Strube, 2008; Filippova et al., 2015) , while more intricate systems also apply splitting, reordering, and lexical substitution (Zhu et al., 2010) .\n[BOS] Simplification has also been attempted with systems developed for phrasebased MT (Xu et al., 2016a) , as well as with neural encoder-decoder models (Zhang and Lapata, 2017) .\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 33, "token_end": 61, "char_start": 186, "char_end": 281, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 69, "token_end": 85, "char_start": 324, "char_end": 390, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhu et al., 2010)": "15636533"}}}, {"token_start": 96, "token_end": 108, "char_start": 465, "char_end": 498, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Xu et al., 2016a)": "2177849"}}}, {"token_start": 113, "token_end": 126, "char_start": 517, "char_end": 571, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhang and Lapata, 2017)": "7473831"}}}]}
{"id": "202541578_0", "paragraph": "[BOS] Recent work discusses some of the difficulties of learning neural decoders for text generation (Wiseman et al., 2018; Prabhakaran et al., 2018) .\n[BOS] Conventional seq2seq approaches require large amounts of training data, are hard to control and to constrain to desirable outputs.\n[BOS] At the same time, many NLP tasks that appear to be full-fledged text generation tasks are natural testbeds for simpler methods.\n[BOS] In this section we briefly review some of these tasks.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 15, "token_end": 36, "char_start": 85, "char_end": 149, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wiseman et al., 2018;": "52135124", "Prabhakaran et al., 2018)": "51856759"}}}]}
{"id": "204915809_2", "paragraph": "[BOS] Distant supervision (Fang and Cohn, 2016; Plank and Agic, 2018) has been used to learn POS taggers for low-resource languages using crosslingual corpora.\n[BOS] The goal of these works is to learn word-level POS tags, rather than sentence-level syntactic embeddings.\n[BOS] Furthermore, our method does not require explicit POS sequences for the low-resource language, which results in a simpler training process than distant supervision.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 19, "char_start": 6, "char_end": 69, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Fang and Cohn, 2016;": "84842", "Plank and Agic, 2018)": "52125788"}}}]}
{"id": "204915809_1", "paragraph": "[BOS] The syntactic content of language models was studied by examining syntax trees (Hewitt and Manning, 2019) , subject-object agreement (Goldberg, 2019) , and evaluation on syntactically altered datasets (Linzen et al., 2016; Marvin and Linzen, 2018) .\n[BOS] These works did not examine multilingual models.\n\n", "discourse_tags": ["Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 12, "token_end": 25, "char_start": 72, "char_end": 111, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hewitt and Manning, 2019)": "106402715"}}}, {"token_start": 26, "token_end": 36, "char_start": 114, "char_end": 155, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Goldberg, 2019)": "58007068"}}}, {"token_start": 40, "token_end": 61, "char_start": 176, "char_end": 253, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Linzen et al., 2016;": "14091946", "Marvin and Linzen, 2018)": "198327899"}}}]}
{"id": "204915809_0", "paragraph": "[BOS] Training semantic embeddings based on multilingual data was studied by MUSE (Conneau et al., 2017) and LASER (Artetxe and Schwenk, 2018) at the word and sentence levels respectively.\n[BOS] Multitask training for disentangling semantic and syntactic information was studied in (Chen et al., 2019) .\n[BOS] This work also used a nearest neighbour method to evaluate the syntactic properties of models, though their focus was on disentanglement rather than embedding quality.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 13, "token_end": 24, "char_start": 77, "char_end": 104, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Conneau et al., 2017)": "3470398"}}}, {"token_start": 25, "token_end": 36, "char_start": 109, "char_end": 142, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Artetxe and Schwenk, 2018)": "56895585"}}}, {"token_start": 45, "token_end": 101, "char_start": 195, "char_end": 477, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Chen et al., 2019)": "91183938"}, "Reference": {}}}]}
{"id": "174799580_3", "paragraph": "[BOS] Hierarchical Reinforcement Learning In this work, we adopt the Options Framework (Sutton et al., 1999) in HRL, in which a high-level agent learns to determine more abstract options and a low-level agent learns to take less abstract actions given the option.\n[BOS] Recent work has shown that HRL is effective in various tasks, e.g., Atari games (Kulkarni et al., 2016) , relation classification (Feng et al., 2018) , relation extraction (Takanobu et al., 2018) , and video captioning .\n\n", "discourse_tags": ["Reflection", "Narrative_cite"], "span_citation_mapping": [{"token_start": 12, "token_end": 23, "char_start": 69, "char_end": 108, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sutton et al., 1999)": "76564"}}}, {"token_start": 75, "token_end": 88, "char_start": 338, "char_end": 373, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kulkarni et al., 2016)": "4669377"}}}, {"token_start": 89, "token_end": 99, "char_start": 376, "char_end": 419, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Feng et al., 2018)": "19100390"}}}, {"token_start": 100, "token_end": 112, "char_start": 422, "char_end": 465, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Takanobu et al., 2018)": "53250562"}}}]}
{"id": "174799580_2", "paragraph": "[BOS] Sequence Operation Methods Our work is also closely related to sequence operation methods, which are widely used in SMT (Durrani et al., 2011 (Durrani et al., , 2015 Pal et al., 2016) and starts to attract attention in NMT (Stahlberg et al., 2018) .\n[BOS] Compared with methods based on seq2seq models, sequence operation methods are inherently more interpretable (Stahlberg et al., 2018) .\n[BOS] Notably, our method is revision-based, i.e., it operates directly on the input sentence and does not generate from scratch as in machine translation systems.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 21, "token_end": 49, "char_start": 122, "char_end": 189, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Durrani et al., 2011": "453090", "(Durrani et al., , 2015": "10103601", "Pal et al., 2016)": "4660990"}}}, {"token_start": 55, "token_end": 67, "char_start": 225, "char_end": 253, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Stahlberg et al., 2018)": "52123353"}}}, {"token_start": 79, "token_end": 97, "char_start": 309, "char_end": 394, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Stahlberg et al., 2018)": "52123353"}}}]}
{"id": "174799580_1", "paragraph": "[BOS] As far as we know, there are two work that avoid disentangled representations.\n[BOS] Zhang et al. (2018b) construct a pseudo-aligned dataset with an SMT model and then learn two NMT models jointly and iteratively.\n[BOS] A concurrent work, Luo et al. (2019) , propose to learn two dual seq2seq models between two styles via reinforcement learning, without disentangling style and content.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 20, "token_end": 50, "char_start": 91, "char_end": 219, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhang et al. (2018b)": "52091536"}, "Reference": {}}}, {"token_start": 55, "token_end": 89, "char_start": 245, "char_end": 393, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Luo et al. (2019)": "165163728"}, "Reference": {}}}]}
{"id": "174799580_0", "paragraph": "[BOS] Text Style Transfer Most work on text style transfer learns disentangled representations of style and content.\n[BOS] We categorize them based on how they represent content.\n[BOS] Hidden vector approaches represent content as hidden vectors, e.g., Hu et al. (2017) adversarially incorporate a VAE and a style classifier; Shen et al. (2017) propose a cross-aligned AE that adversarially aligns the hidden states of the decoder; Fu et al. (2018) design a multi-decoder model and a style-embedding model for better style representations; use language models as style discriminators; John et al. (2018) utilize bagof-words prediction for better disentanglement of style and content.\n[BOS] Deletion approaches represent content as the input sentence with stylized words deleted, e.g., delete stylized ngrams based on corpus-level statistics and stylize it based on similar, retrieved sentences; jointly train a neutralization module and a stylization module the with reinforcement learning; Zhang et al. (2018a) facilitate the stylization step with a learned sentiment memory.\n\n", "discourse_tags": ["Transition", "Reflection", "Multi_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 48, "token_end": 66, "char_start": 253, "char_end": 324, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hu et al. (2017)": "20981275"}, "Reference": {}}}, {"token_start": 67, "token_end": 92, "char_start": 326, "char_end": 430, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Shen et al. (2017)": "7296803"}, "Reference": {}}}, {"token_start": 93, "token_end": 124, "char_start": 432, "char_end": 583, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Fu et al. (2018)": "6484065"}, "Reference": {}}}, {"token_start": 125, "token_end": 149, "char_start": 585, "char_end": 683, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"John et al. (2018)": "195348003"}, "Reference": {}}}, {"token_start": 207, "token_end": 226, "char_start": 991, "char_end": 1076, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhang et al. (2018a)": "52068673"}, "Reference": {}}}]}
{"id": "202784139_1", "paragraph": "[BOS] On the other hand, a few works including, , Xu et al. (2018) adopt an eraseand-replace approach and design their methods to erase the style-related words first and then fill in words of different style attributes.\n[BOS] Nonparallel text style transfer is also relevant to unsupervised machine translation.\n[BOS] Prabhumoye et al. (2018) , Subramanian et al. (2018) , Logeswaran et al. (2018) and dos Santos et al. (2018) apply back-translation technique from unsupervised machine translation for style transfer task.\n[BOS] Our work follows the framework of CAAE, and we propose several adjustments to improve the performance.\n\n", "discourse_tags": ["Single_summ", "Transition", "Multi_summ", "Reflection"], "span_citation_mapping": [{"token_start": 13, "token_end": 51, "char_start": 50, "char_end": 219, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Xu et al. (2018)": "46889991"}, "Reference": {}}}, {"token_start": 66, "token_end": 121, "char_start": 318, "char_end": 522, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(2018)": null, "Logeswaran et al. (2018)": "53217784"}, "Reference": {}}}]}
{"id": "202784139_0", "paragraph": "[BOS] Text style transfer without parallel data is an active research topic.\n[BOS] Mueller et al. (2017) designed a variational auto-encoder (VAE) framework; Hu et al. (2017) used VAE with controllable attributes; Shen et al. (2017) proposed to adversarially train a Cross-Aligned Auto-Encoder (CAAE) to align two different styles.\n[BOS] To improve performances, several works including, (Fu et al., 2017; Yang et al., 2018; dos Santos et al., 2018; Logeswaran et al., 2018) were proposed.\n[BOS] Fu et al. (2017) suggested a multi-head decoder to generate sentences with different styles; Yang et al. (2018) utilized language models as discriminators to stabilize training; dos Santos et al. (2018) used a classifier to aid style transfer; Logeswaran et al. (2018) also made use of a conditional discriminator for multiple style transfer.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Narrative_cite", "Multi_summ"], "span_citation_mapping": [{"token_start": 15, "token_end": 33, "char_start": 83, "char_end": 156, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Mueller et al. (2017)": "2148537"}, "Reference": {}}}, {"token_start": 34, "token_end": 47, "char_start": 158, "char_end": 212, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hu et al. (2017)": "20981275"}, "Reference": {}}}, {"token_start": 48, "token_end": 78, "char_start": 214, "char_end": 331, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Shen et al. (2017)": "7296803"}, "Reference": {}}}, {"token_start": 83, "token_end": 120, "char_start": 363, "char_end": 474, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Fu et al., 2017;": "6484065", "Yang et al., 2018;": "44061800", "dos Santos et al., 2018;": "29165442", "Logeswaran et al., 2018)": "53217784"}}}, {"token_start": 124, "token_end": 143, "char_start": 496, "char_end": 587, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Fu et al. (2017)": "6484065"}, "Reference": {}}}, {"token_start": 144, "token_end": 160, "char_start": 589, "char_end": 672, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Yang et al. (2018)": "44061800"}, "Reference": {}}}, {"token_start": 162, "token_end": 176, "char_start": 678, "char_end": 738, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(2018)": null}, "Reference": {}}}, {"token_start": 177, "token_end": 200, "char_start": 740, "char_end": 838, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Logeswaran et al. (2018)": "53217784"}, "Reference": {}}}]}
{"id": "208001870_4", "paragraph": "[BOS] In summary, two aspects of the past literature on UGC normalization are striking.\n[BOS] First, all the past work is based on UGC-specific resources such as lexicons or large UGC corpora.\n[BOS] Second, most successful models are modular in the sense that they combine several independent modules that capture different aspects of the problem.\n\n", "discourse_tags": ["Transition", "Transition", "Transition"], "span_citation_mapping": []}
{"id": "208001870_3", "paragraph": "[BOS] In 2016, the MoNoise model (van der Goot and van Noord, 2017) significantly improved the Stateof-the-art with a feature-based Random Forest.\n[BOS] The model ranks candidates provided by modules such as a spelling checker (aspell), a n-gram based language model and word embeddings trained on millions of tweets.\n\n", "discourse_tags": ["Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 6, "token_end": 74, "char_start": 19, "char_end": 317, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "208001870_2", "paragraph": "[BOS] In 2015, on the occasion of the Workshop on Noisy User-Generated Text, a shared task on lexical normalization of English tweets was organized (Baldwin et al., 2015) for which a collection of annotated tweets for training and evaluation was released.\n[BOS] We will refer it as the lexnorm15 dataset.\n[BOS] A wide range of approaches competed.\n[BOS] The best approach (Supranovich and Patsepnia, 2015) used a UGC feature-based CRF model for detection and normalization.\n\n", "discourse_tags": ["Narrative_cite", "Reflection", "Transition", "Single_summ"], "span_citation_mapping": [{"token_start": 19, "token_end": 38, "char_start": 79, "char_end": 170, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Baldwin et al., 2015)": "14500933"}}}, {"token_start": 77, "token_end": 102, "char_start": 372, "char_end": 473, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Supranovich and Patsepnia, 2015)": "494297"}, "Reference": {}}}]}
{"id": "208001870_1", "paragraph": "[BOS] The first systematic attempt was Han and Baldwin (2011) .\n[BOS] They released 549 tweets with their normalized word-aligned counterparts and the first result for a normalization system on tweets.\n[BOS] Their model was a Support-Vector-Machine for detecting noisy words.\n[BOS] Then a lookup and ngram based system would pick the best candidate among the closest ones in terms of edit and phonetic distances.\n[BOS] Following this work, the literature explored different modelling framework to tackle the task, whether it is Statistical Machine Translation (Li and Liu, 2012) , purely unsupervised approach (Yang and Eisenstein, 2013) , or syllables level model .\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 7, "token_end": 80, "char_start": 39, "char_end": 412, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Han and Baldwin (2011)": "2577850"}, "Reference": {}}}, {"token_start": 99, "token_end": 109, "char_start": 528, "char_end": 578, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Li and Liu, 2012)": "63392431"}}}, {"token_start": 111, "token_end": 121, "char_start": 588, "char_end": 637, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yang and Eisenstein, 2013)": "13890994"}}}]}
{"id": "208001870_0", "paragraph": "[BOS] There is an extensive literature on normalizing text from UGC.\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "182953033_3", "paragraph": "[BOS] Furthermore, we also propose Bag Loss, which can train ARNs in an end-to-end manner without any anchor word annotation.\n[BOS] The design of Bag Loss is partially inspired by multi-instance learning (MIL) (Zhou and Zhang, 2007; Zhou et al., 2009; Surdeanu et al., 2012) , but with a different target.\n[BOS] MIL aims to predict a unified label of a bag of instances, while Bag Loss is proposed to train ARNs whose anchor detector is required to predict the label of each instance.\n[BOS] Therefore previous MIL methods are not suitable for training ARNs.\n\n", "discourse_tags": ["Reflection", "Narrative_cite", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 39, "token_end": 70, "char_start": 180, "char_end": 274, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhou and Zhang, 2007;": null, "Zhou et al., 2009;": null, "Surdeanu et al., 2012)": "5869747"}}}]}
{"id": "182953033_2", "paragraph": "[BOS] Different from previous methods, this paper proposes a new architecture to address nested mention detection.\n[BOS] Compared with region-based approaches, our ARNs detect mentions by exploiting head-driven phrase structures, rather than exhaustive classifying over subsequences.\n[BOS] Therefore ARNs can significantly reduce the size of candidate mentions and lead to much lower time complexity.\n[BOS] Compared with schema-based approaches, ARNs can naturally address nested mentions since different mentions will have different anchor words.\n[BOS] There is no need to design complex tagging schemas, no spurious structures and no structural ambiguity.\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "182953033_1", "paragraph": "[BOS] Schema-based approaches address nested mentions by designing more expressive tagging schemas, rather than changing tagging units.\n[BOS] One representative direction is hypergraph-based methods (Lu and Roth, 2015; Katiyar and Cardie, 2018; , where hypergraphbased tags are used to ensure nested mentions can be recovered from word-level tags.\n[BOS] Besides, Muis and Lu (2017) developed a gap-based tagging schema to capture nested structures.\n[BOS] However, these schemas should be designed very carefully to prevent spurious structures and structural ambiguity .\n[BOS] But more expressive, unambiguous schemas will inevitably lead to higher time complexity during both training and decoding.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Single_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 28, "token_end": 48, "char_start": 174, "char_end": 243, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lu and Roth, 2015;": "5818711"}}}, {"token_start": 74, "token_end": 110, "char_start": 363, "char_end": 567, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "182953033_0", "paragraph": "[BOS] Nested mention detection requires to identify all entity mentions in texts, rather than only outmost mentions in conventional NER.\n[BOS] This raises a critical issue to traditional sequential labeling models because they can only assign one label to each token.\n[BOS] To address this issue, mainly two kinds of methods have been proposed.\n[BOS] Region-based approaches detect mentions by identifying over subsequences of a sentence respectively, and nested mentions can be detected because they correspond to different subsequences.\n[BOS] For this, Finkel and Manning (2009) regarded nodes of parsing trees as candidate subsequences.\n[BOS] Recently, Xu et al. (2017) and Sohrab and Miwa (2018) tried to directly classify over all subsequences of a sentence.\n[BOS] Besides, proposed a transition-based method to construct nested mentions via a sequence of specially designed actions.\n[BOS] Generally, these approaches are straightforward for nested mention detection, but mostly with high computational cost as they need to classify over almost all sentence subsequences.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Transition", "Single_summ", "Multi_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 100, "token_end": 118, "char_start": 555, "char_end": 639, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Finkel and Manning (2009)": "10573012"}, "Reference": {}}}, {"token_start": 121, "token_end": 172, "char_start": 656, "char_end": 888, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Xu et al. (2017)": "7204774", "Sohrab and Miwa (2018)": "53080784"}, "Reference": {}}}]}
{"id": "204958538_7", "paragraph": "[BOS] In this dataset, we have 4,761 unique character pairs annotated with a relatedness score.\n[BOS] Figure 2 shows the statistics over the relatedness scores.\n[BOS] As shown in the table, only a small number of character pairs are closely related, while the majority of the characters have either interacted very few times or did not interact at all.\n[BOS] However, it is important to include these unrelated pairs while evaluating the quality of the character embeddings, as unrelated pairs might be closer than related ones especially for minor characters that do not speak much during the dialogue.\n\n", "discourse_tags": ["Other", "Other", "Other", "Other"], "span_citation_mapping": []}
{"id": "204958538_6", "paragraph": "[BOS] For three movies, the Pearson correlation between the two annotators is 0.8394, which reflects a very good agreement.\n[BOS] We then average the scores assigned by the annotators and use the result as the human relatedness ground-truth score for each pair of characters.\n\n", "discourse_tags": ["Other", "Other"], "span_citation_mapping": []}
{"id": "204958538_5", "paragraph": "[BOS] For each movie in that dataset, two human annotators watched the movies and annotated a dense relatedness matrix of characters on a 1-5 scale.\n[BOS] Table 2 shows the meaning of each score.\n[BOS] These scores reflect the level of interaction or how closely related the characters are over the course of the movie.\n[BOS] For example, given two characters X and Y, a high score for X and Y is assigned if e.g., X is the father of Y, regardless of the amount of interaction between the two characters.\n[BOS] We also give a high score for the cases where X and Y are closely interacted, even if they are unrelated in terms of kinship.\n[BOS] Due to the sparseness of the number of closely related characters, we asked the annotators to select the higher score when hesitating between two scores.\n\n", "discourse_tags": ["Other", "Other", "Other", "Other", "Other", "Other"], "span_citation_mapping": []}
{"id": "204958538_4", "paragraph": "[BOS] To measure the relatedness between characters in movies, we construct a new annotated dataset based on a publicly available dataset (Azab et al., 2018) .\n[BOS] That dataset includes 28K turns spoken by 396 different speakers in eighteen movies covering different genres, with the subtitles of each movie labeled with the character name of their corresponding speakers.\n[BOS] On average, each character uttered 452 words.\n\n", "discourse_tags": ["Other", "Other", "Other"], "span_citation_mapping": [{"token_start": 20, "token_end": 32, "char_start": 111, "char_end": 157, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Azab et al., 2018)": "44084674"}}}]}
{"id": "204958538_3", "paragraph": "[BOS] Measures of semantic relatedness between words indicate the degree to which words are associated with any kind of semantic relationship such as synonymy, antonymy, and so on.\n[BOS] Semantic relatedness is commonly used as an absolute intrinsic evaluation task to assess and compare the quality of different word embeddings (Schnabel et al., 2015; Yih and Qazvinian, 2012; Upadhyay et al., 2016) and phrase embeddings (Wilson and Mihalcea, 2017) .\n[BOS] Similarly, we define character relatedness as the degree to which a pair of characters in a given story are related to each other based on the story plot and their level of interaction throughout the dialogue.\n[BOS] Given a pair of characters, we would like the relatedness score between their embedding representations to have a high correlation with their corresponding human-based relatedness score.\n[BOS] Thus, the distance of the embeddings between closely related characters should be smaller than the distance between less related ones.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Reflection", "Reflection", "Transition"], "span_citation_mapping": [{"token_start": 56, "token_end": 87, "char_start": 313, "char_end": 400, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Schnabel et al., 2015;": "6197592", "Yih and Qazvinian, 2012;": "16393512", "Upadhyay et al., 2016)": "5357629"}}}, {"token_start": 88, "token_end": 100, "char_start": 405, "char_end": 450, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wilson and Mihalcea, 2017)": "30742488"}}}]}
{"id": "204958538_2", "paragraph": "[BOS] Identifying and analyzing character relations in literary texts is a well studied problem (Agarwal et al., 2013; Makazhanov et al., 2014; Elson et al., 2010; Iyyer et al., 2016) .\n[BOS] Most of these models depend on analyzing the co-occurrence of the char-acters and stylistic features used while characters address each other.\n[BOS] These models are really important to summarize, understand, and generate stories (Elson et al., 2010) .\n[BOS] In this work, we use the task of character relation classification as an extrinsic evaluation task to evaluate the impact of character embeddings on this task.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 50, "char_start": 6, "char_end": 183, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Agarwal et al., 2013;": "6829505", "Makazhanov et al., 2014;": "14195296", "Elson et al., 2010;": "1974676", "Iyyer et al., 2016)": "1139597"}}}, {"token_start": 87, "token_end": 103, "char_start": 378, "char_end": 442, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Elson et al., 2010)": "1974676"}}}]}
{"id": "204958538_1", "paragraph": "[BOS] Recently, several approaches have been proposed to build dynamic representations for entities (Henaff et al., 2016; Ji et al., 2017; Kobayashi et al., 2016 Kobayashi et al., , 2017 .\n[BOS] One common approach is to rely on neural language models to encode the local context of an entity and use the resulting context vectors as the embedding for subsequent occurrences of that entity (Kobayashi et al., 2016 (Kobayashi et al., , 2017 .\n[BOS] Another approach is to learn a generative model that generates the representation of an entity mention (Ji et al., 2017) .\n[BOS] Henaff et al. (2016) proposed an explicit entity tracking model by relying on an external memory to store information about entities as they appear in a given sentence.\n[BOS] While these rich representations improve the performance on several tasks such as coreference and reading comprehension, they rely on explicit mentions of entities in text as available in toy datasets such as bAbi (Weston et al., 2015) .\n[BOS] Thus, it is difficult to apply these representations in a dialogue setting due to the sparseness of name mentions in dialogue, as well as the lack of explicit conversation connections between characters (as available in movies) (Azab et al., 2018) .\n[BOS] Most of the existing story understanding work feeds the model with the vector representations of names based on a global model such as Word2Vec or Glove, which hinders the ability of these models to understand dialogue (Tapaswi et al., 2016; Na et al., 2017; Lei et al., 2018) .\n[BOS] Recently, Li et al. (2016) relied on TV series scripts in order to learn speaker persona representations and used these representations to improve the performance of neural conversation models.\n[BOS] Unlike (Ji et al., 2017; Li et al., 2016) , we focus on representing character names in dialogue settings and learning different embeddings for characters from different story dialogues in a way that reflects the relatedness of story characters; more specifically, we propose the use of speaker prediction as an auxiliary supervision to improve the character representation.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Narrative_cite", "Single_summ", "Narrative_cite", "Narrative_cite", "Narrative_cite", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 11, "token_end": 44, "char_start": 63, "char_end": 186, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Henaff et al., 2016;": "11243593", "Ji et al., 2017;": "5564363", "Kobayashi et al., 2016": "10239453", "Kobayashi et al., , 2017": "855563"}}}, {"token_start": 51, "token_end": 94, "char_start": 221, "char_end": 439, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kobayashi et al., 2016": "10239453", "(Kobayashi et al., , 2017": "855563"}}}, {"token_start": 107, "token_end": 120, "char_start": 515, "char_end": 568, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ji et al., 2017)": "5564363"}}}, {"token_start": 122, "token_end": 155, "char_start": 577, "char_end": 745, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Henaff et al. (2016)": "11243593"}, "Reference": {}}}, {"token_start": 191, "token_end": 202, "char_start": 961, "char_end": 987, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Weston et al., 2015)": "3178759"}}}, {"token_start": 233, "token_end": 255, "char_start": 1138, "char_end": 1243, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Azab et al., 2018)": "44084674"}}}, {"token_start": 257, "token_end": 322, "char_start": 1252, "char_end": 1528, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Tapaswi et al., 2016;": "1017389", "Na et al., 2017;": "3957104", "Lei et al., 2018)": "52171684"}}}, {"token_start": 326, "token_end": 359, "char_start": 1547, "char_end": 1730, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Li et al. (2016)": "2955580"}, "Reference": {}}}, {"token_start": 361, "token_end": 368, "char_start": 1744, "char_end": 1760, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 369, "token_end": 376, "char_start": 1762, "char_end": 1778, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Li et al., 2016)": "2955580"}}}]}
{"id": "204958538_0", "paragraph": "[BOS] Learning distributional representation of words plays an increasingly important role in representing text in many tasks (Bengio et al., 2013; Chen and Manning, 2014) .\n[BOS] The existence of huge datasets allowed learning high quality word embeddings in an unsupervised way by training a neural network on fake objectives (Mikolov et al., 2013a,b; Turney and Pantel, 2010) .\n[BOS] A major strength of these learned word embeddings is that they are able to capture useful semantic information that can be easily used in other tasks of interest such as semantic similarity and relatedness between pair of words (Mikolov et al., 2013a; Pennington et al., 2014; Wilson and Mihalcea, 2017) and dependency parsing (Chen and Manning, 2014; Dyer et al., 2015) .\n[BOS] However, these models treat names and entities no more than the tokens used to mention them.\n[BOS] As a result, these models are unable to well represent names in nar-rative understanding task because the word \"John\" in a given story can be very different from the word \"John\" in another narrative.\n[BOS] In this work, we only focus on representing character names and not the whole embedding space (Ji et al., 2017) .\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Narrative_cite", "Transition", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 13, "token_end": 34, "char_start": 94, "char_end": 171, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bengio et al., 2013;": "393948", "Chen and Manning, 2014)": "11616343"}}}, {"token_start": 52, "token_end": 81, "char_start": 283, "char_end": 378, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Turney and Pantel, 2010)": "1500900"}}}, {"token_start": 116, "token_end": 149, "char_start": 581, "char_end": 690, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mikolov et al., 2013a;": "5959482", "Pennington et al., 2014;": "1957433", "Wilson and Mihalcea, 2017)": "30742488"}}}, {"token_start": 150, "token_end": 168, "char_start": 695, "char_end": 757, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chen and Manning, 2014;": "11616343", "Dyer et al., 2015)": null}}}, {"token_start": 247, "token_end": 258, "char_start": 1143, "char_end": 1182, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ji et al., 2017)": "5564363"}}}]}
{"id": "174797750_3", "paragraph": "[BOS] Our work is also related to the tasks on textual relationship inference, such as textual entailment (Bowman et al., 2015) , paraphrase detection (Yin and Schtze, 2015) , and question answering (Wang et al., 2016) .\n[BOS] Unlike the textual relationships addressed in those tasks, the relationships between utterances expressing stances do not necessarily contain any rephrasing or entailing semantics, but they do carry discourse signals (e.g., reasons) related to stance expressing.\n\n", "discourse_tags": ["Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 11, "token_end": 29, "char_start": 47, "char_end": 127, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bowman et al., 2015)": "14604520"}}}, {"token_start": 30, "token_end": 43, "char_start": 130, "char_end": 173, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yin and Sch\u00fctze, 2015)": "17578970"}}}, {"token_start": 45, "token_end": 55, "char_start": 180, "char_end": 218, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wang et al., 2016)": "17464854"}}}]}
{"id": "174797750_2", "paragraph": "[BOS] Reason information has been found useful in argumentation mining (Lippi and Torroni, 2016) , where studies leverage stance and reason signals for various argumentation tasks (Hasan and Ng, 2014; Boltuzic and Snajder, 2014; Sobhani et al., 2015) .\n[BOS] We study how to exploit the reason information to better understand the stance, thus addressing a different task.\n\n", "discourse_tags": ["Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 9, "token_end": 20, "char_start": 50, "char_end": 96, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lippi and Torroni, 2016)": "9561587"}}}, {"token_start": 30, "token_end": 60, "char_start": 160, "char_end": 250, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hasan and Ng, 2014;": "6441832", "Boltuzic and Snajder, 2014;": "15983978", "Sobhani et al., 2015)": "11538163"}}}]}
{"id": "174797750_1", "paragraph": "[BOS] Stance classification has recently received much attention in the opinion mining community.\n[BOS] Different approaches have been proposed to classify stances of individual utterances in ideological forums (Murakami and Raymond, 2010; Somasundaran and Wiebe, 2010; Gottopati et al., 2013; Qiu et al., 2015) and social media (Augenstein et al., 2016; Du et al., 2017; Mohammad et al., 2017) .\n[BOS] In our work, we classify (dis)agreement relationships between a pair of stance-bearing utterances.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 22, "token_end": 98, "char_start": 147, "char_end": 394, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Murakami and Raymond, 2010;": "18151048", "Somasundaran and Wiebe, 2010;": "927208", "Gottopati et al., 2013;": "4981807", "Qiu et al., 2015)": "1011342", "(Augenstein et al., 2016;": "744471", "Du et al., 2017;": null, "Mohammad et al., 2017)": "8632380"}}}]}
{"id": "174797750_0", "paragraph": "[BOS] Our work is mostly related to the task of detecting agreement and disagreement in online discussions.\n[BOS] Recent studies have mainly focused on classifying (dis)agreement in dialogues (Abbott et al., 2011; Wang and Cardie, 2014; Misra and Walker, 2013; Allen et al., 2014) .\n[BOS] In these studies, various features (e.g., structural, linguistic) and/or specialised lexicons are proposed to recognise (dis)agreement in different dialogic scenarios.\n[BOS] In contrast, we detect stance (dis)agreement between independent utterances where dialogic features are absent.\n\n", "discourse_tags": ["Reflection", "Narrative_cite", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 26, "token_end": 63, "char_start": 152, "char_end": 280, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Abbott et al., 2011;": "6819967", "Wang and Cardie, 2014;": "2786154", "Misra and Walker, 2013;": "9784700", "Allen et al., 2014)": "1022407"}}}]}
{"id": "195218574_0", "paragraph": "[BOS] Relation Extraction Initial work in RE uses statistical classifiers or kernel based methods in combination with discrete syntactic features, such as part-of-speech and named entities tags, morphological features, and WordNet hypernyms (Mintz et al., 2009; Hendrickx et al., 2010) .\n[BOS] These methods have been superseded by sequence based methods, including recurrent (Socher et al., 2012; Zhang and Wang, 2015) and convolutional neural networks (Zeng et al., 2014 (Zeng et al., , 2015 .\n[BOS] Consequently, discrete features have been replaced by distributed representations of words and syntactic features (Turian et al., 2010; Pennington et al., 2014) .\n[BOS] Xu et al. (2015a,b) integrated shortest dependency path (SDP) information into a LSTMbased relation classification model.\n[BOS] Considering the SDP is useful for relation classification, because it focuses on the action and agents in a sentence (Bunescu and Mooney, 2005; .\n[BOS] established a new state-of-the-art for relation extraction on the TA-CRED dataset by applying a combination of pruning and graph convolutions to the dependency tree.\n[BOS] Recently, Verga et al. (2018) extended the Transformer architecture by a custom architecture for supervised biomedical named entity and relation extraction.\n[BOS] In comparison, we fine-tune pretrained language representations and only require distantly supervised annotation labels.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Narrative_cite", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 38, "token_end": 61, "char_start": 223, "char_end": 285, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mintz et al., 2009;": "10910955", "Hendrickx et al., 2010)": "436023"}}}, {"token_start": 76, "token_end": 92, "char_start": 366, "char_end": 419, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Socher et al., 2012;": "806709", "Zhang and Wang, 2015)": "11717703"}}}, {"token_start": 93, "token_end": 111, "char_start": 424, "char_end": 493, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zeng et al., 2014": "12873739", "(Zeng et al., , 2015": "2778800"}}}, {"token_start": 121, "token_end": 145, "char_start": 556, "char_end": 662, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Turian et al., 2010;": "629094", "Pennington et al., 2014)": "1957433"}}}, {"token_start": 147, "token_end": 206, "char_start": 671, "char_end": 941, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 246, "token_end": 271, "char_start": 1133, "char_end": 1279, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Verga et al. (2018)": "3576631"}, "Reference": {}}}]}
{"id": "184483272_4", "paragraph": "[BOS] Additional related work is presented in workshops such as TA-COS 1 , Abusive Language Online 2 , and TRAC 3 and related shared tasks such as GermEval (Wiegand et al., 2018) and TRAC (Kumar et al., 2018) .\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 31, "token_end": 44, "char_start": 147, "char_end": 178, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 45, "token_end": 54, "char_start": 183, "char_end": 208, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kumar et al., 2018)": "59336626"}}}]}
{"id": "184483272_3", "paragraph": "[BOS] Approaches to detecting hate speech on Twitter using convolutional neural networks and convolution-GRU based deep neural network are discussed in (Gambck and Sikdar, 2017) and (Zhang et al., 2018) respectively.\n\n", "discourse_tags": ["Multi_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 48, "char_start": 6, "char_end": 216, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Gamb\u00e4ck and Sikdar, 2017)": null, "(Zhang et al., 2018)": "46939253"}, "Reference": {}}}]}
{"id": "184483272_2", "paragraph": "[BOS] In order to classify correctly abusive language it is important to analyze its types.\n[BOS] A proposal of typology of abusive language sub-tasks is presented in (Waseem et al., 2017) and (ElSherief et al., 2018) examines the target of the speech: either directed towards a specific person or entity, or generalized towards a group of people sharing a common protected characteristic.\n[BOS] (Fier et al., 2017) proposes a legal framework, dataset and annotation schema of socially unacceptable discourse practices on social networking platforms in Slovenia.\n[BOS] Finally, a recent discussion on identifying profanity vs. hate speech is presented in .\n[BOS] This work highlighted the challenges of distinguishing between profanity, and threatening language which may not actually contain profane language.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 20, "token_end": 84, "char_start": 100, "char_end": 389, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Waseem et al., 2017)": "8821211", "(ElSherief et al., 2018)": "4809781"}, "Reference": {}}}, {"token_start": 85, "token_end": 117, "char_start": 396, "char_end": 562, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Fi\u0161er et al., 2017)": "28550367"}, "Reference": {}}}]}
{"id": "184483272_1", "paragraph": "[BOS] Survey papers describing key areas that have been explored for this task include (Schmidt and Wiegand, 2017) , (Fortuna and Nunes, 2018) and (Malmasi and Zampieri, 2017) .\n[BOS] The dataset for this competition is explained in (Zampieri et al., 2019a) and different approaches to the same problem are reported in (Zampieri et al., 2019b) .\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 2, "token_end": 46, "char_start": 6, "char_end": 175, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Schmidt and Wiegand, 2017)": "9626793", "(Fortuna and Nunes, 2018)": "52184457", "(Malmasi and Zampieri, 2017)": "19182892"}}}, {"token_start": 49, "token_end": 68, "char_start": 188, "char_end": 257, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zampieri et al., 2019a)": "67856299"}}}, {"token_start": 69, "token_end": 90, "char_start": 262, "char_end": 343, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zampieri et al., 2019b)": "84843035"}}}]}
{"id": "184483272_0", "paragraph": "[BOS] One of the most effective strategies for tackling this problem is to use computational methods to identify offense, aggression, and hate speech in user-generated content (e.g. posts, comments, microblogs, etc.)\n[BOS] .\n[BOS] This topic has attracted significant attention recently as evidenced in publications from the last two years.\n\n", "discourse_tags": ["Transition", "Transition", "Transition"], "span_citation_mapping": []}
{"id": "173990523_0", "paragraph": "[BOS] Exploring Properties of SAN SAN has yielded strong empirical performance in a variety of NLP tasks (Vaswani et al., 2017; Tan et al., 2018; Li et al., 2018; Devlin et al., 2019) .\n[BOS] In response to these impressive results, several studies have emerged with the goal of understanding SAN on many properties.\n[BOS] For example, Tran et al. (2018) compared SAN and RNN on language inference tasks, and pointed out that SAN is weak at learning hierarchical structure than its RNN counterpart.\n[BOS] Moreover, Tang et al. (2018) conducted experiments on subject-verb agreement and word sense disambiguation tasks.\n[BOS] They found that SAN is good at extracting semantic properties, while underperforms RNN on capturing long-distance dependencies.\n[BOS] This is in contrast to our intuition that SAN is good at capturing long-distance dependencies.\n[BOS] In this work, we focus on exploring the ability of SAN on modeling word order information.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Single_summ", "Single_summ", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 52, "char_start": 6, "char_end": 183, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vaswani et al., 2017;": "13756489", "Tan et al., 2018;": "12842009", "Li et al., 2018;": "53081097", "Devlin et al., 2019)": "52967399"}}}, {"token_start": 79, "token_end": 113, "char_start": 336, "char_end": 498, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Tran et al. (2018)": "3785155"}, "Reference": {}}}, {"token_start": 116, "token_end": 162, "char_start": 515, "char_end": 752, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Tang et al. (2018)": "52100282"}, "Reference": {}}}]}
{"id": "67769427_5", "paragraph": "[BOS] We take the attention-based sequence-to-sequence model of Bahdanau et al. (2015) as the baseline, but use LSTM cells (Hochreiter and Schmidhuber, 1997) instead of GRU cells (Cho et al., 2014) .\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 5, "token_end": 24, "char_start": 18, "char_end": 86, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Bahdanau et al. (2015)": "11212020"}}}, {"token_start": 30, "token_end": 45, "char_start": 112, "char_end": 157, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hochreiter and Schmidhuber, 1997)": "1915014"}}}, {"token_start": 47, "token_end": 58, "char_start": 169, "char_end": 197, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cho et al., 2014)": "5590763"}}}]}
{"id": "67769427_4", "paragraph": "[BOS] 3 Baseline: attention-based BiLSTM\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "67769427_3", "paragraph": "[BOS] In addition to GRNs and GCNs, there have been other graph neural networks, such as graph gated neural network (GGNN) (Li et al., 2015b; Beck et al., 2018) .\n[BOS] Since our main concern is to empirically investigate the effectiveness of AMR for NMT, we leave it to future work to compare GCN, GGNN, and GRN for our task.\n\n", "discourse_tags": ["Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 5, "token_end": 45, "char_start": 21, "char_end": 160, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Li et al., 2015b;": "8393918", "Beck et al., 2018)": "49430686"}}}]}
{"id": "67769427_2", "paragraph": "[BOS] GRNs have recently been used to model graph structures in NLP tasks.\n[BOS] In particular, use a GRN model to represent raw sentences by building a graph structure of neighboring words and a sentence-level node, showing that the encoder outperforms BiLSTMs and Transformer (Vaswani et al., 2017) on classification and sequence labeling tasks; build a GRN for encoding AMR graphs for text generation, showing that the representation is superior compared to BiLSTM on serialized AMR.\n[BOS] We extend by investigating the usefulness of AMR for neural machine translation.\n[BOS] To our knowledge, we are the first to use GRN for machine translation.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 54, "token_end": 65, "char_start": 266, "char_end": 300, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vaswani et al., 2017)": "13756489"}}}]}
{"id": "67769427_1", "paragraph": "[BOS] Recently, Marcheggiani et al. (2018) investigate semantic role labeling (SRL) on neural machine translation (NMT).\n[BOS] The predicate-argument structures are encoded via graph convolutional network (GCN) layers (Kipf and Welling, 2017) , which are laid on top of regular BiRNN or CNN layers.\n[BOS] Our work is in line with exploring semantic information, but different in exploiting AMR rather than SRL for NMT.\n[BOS] In addition, we leverage a graph recurrent network (GRN) for modeling AMRs rather than GCN, which is formally consistent with the RNN sentence encoder.\n[BOS] Since there is no oneto-one correspondence between AMR nodes and source words, we adopt a doubly-attentive LSTM decoder, which is another major difference from Marcheggiani et al. (2018) .\n\n", "discourse_tags": ["Single_summ", "Narrative_cite", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 4, "token_end": 31, "char_start": 16, "char_end": 120, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Marcheggiani et al. (2018)": "5063437"}, "Reference": {}}}, {"token_start": 40, "token_end": 57, "char_start": 177, "char_end": 242, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kipf and Welling, 2017)": null}}}, {"token_start": 151, "token_end": 175, "char_start": 673, "char_end": 769, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Marcheggiani et al. (2018)": "5063437"}}}]}
{"id": "67769427_0", "paragraph": "[BOS] Most previous work on exploring semantics for statistical machine translation (SMT) studies the usefulness of predicate-argument structure from semantic role labeling (Wong and Mooney, 2006; Wu and Fung, 2009; Liu and Gildea, 2010; Baker et al., 2012) .\n[BOS] Jones et al. (2012) first convert Prolog expressions into graphical meaning representations, leveraging synchronous hyperedge replacement grammar to parse the input graphs while generating the outputs.\n[BOS] Their graphical meaning representation is different from AMR under a strict definition, and their experimental data are limited to 880 sentences.\n[BOS] We are the first to investigate AMR on a large-scale machine translation task.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 18, "token_end": 57, "char_start": 102, "char_end": 257, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wong and Mooney, 2006;": "7785983", "Wu and Fung, 2009;": "554381", "Liu and Gildea, 2010;": "15212670", "Baker et al., 2012)": "2315102"}}}, {"token_start": 59, "token_end": 119, "char_start": 266, "char_end": 619, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Jones et al. (2012)": "5634542"}, "Reference": {}}}]}
{"id": "174798258_1", "paragraph": "[BOS] In the context of NMT, several researchers leverage document-level context for NMT (Wang et al., 2017a; Choi et al., 2017; , while we opt for sentential context.\n[BOS] In addition, contextual information are used to improve the encoder representations Lin et al., 2018) .\n[BOS] Our approach is complementary to theirs by better exploiting the encoder representations for the subsequent decoder.\n[BOS] Concerning guiding the NMT generation with source-side context, Zheng et al. (2018) split the source content into translated and untranslated parts, while we focus on exploiting global sentence-level context.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Reflection", "Single_summ"], "span_citation_mapping": [{"token_start": 12, "token_end": 34, "char_start": 58, "char_end": 127, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wang et al., 2017a;": "9768369"}}}, {"token_start": 48, "token_end": 64, "char_start": 187, "char_end": 275, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Lin et al., 2018)": "47018327"}}}, {"token_start": 85, "token_end": 127, "char_start": 407, "char_end": 615, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zheng et al. (2018)": "4812047"}, "Reference": {}}}]}
{"id": "174798258_0", "paragraph": "[BOS] Sentential context has been successfully applied in SMT (Meng et al., 2015; .\n[BOS] In these works, sentential context representation which is generated by the CNNs is exploited to guided the target sentence generation.\n[BOS] In broad terms, sentential context can be viewed as a sentence abstraction from a specific aspect.\n[BOS] From this point of view, domain information (Foster and Kuhn, 2007; Hasler et al., 2014; Wang et al., 2017b) and topic information (Xiao et al., 2012; Xiong et al., 2015; Zhang et al., 2016) can also be treated as the sentential context, the exploitation of which we leave for future work.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 10, "token_end": 19, "char_start": 58, "char_end": 80, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 73, "token_end": 99, "char_start": 362, "char_end": 445, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Foster and Kuhn, 2007;": "1421053", "Hasler et al., 2014;": "17647958", "Wang et al., 2017b)": "9990193"}}}, {"token_start": 100, "token_end": 125, "char_start": 450, "char_end": 527, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Xiao et al., 2012;": "6710961", "Xiong et al., 2015;": "2483345", "Zhang et al., 2016)": "17067853"}}}]}
{"id": "202539954_0", "paragraph": "[BOS] Interpreting Seq2Seq Models Interpretability of Seq2Seq models has recently been explored mainly from two perspectives: interpreting internal representations and understanding inputoutput behaviors.\n[BOS] Most of the existing work focus on the former thread, which analyzes the linguistic information embeded in the learned representations (Shi et al., 2016; Belinkov et al., 2017; Yang et al., 2019) or the hidden units (Ding et al., 2017; Bau et al., 2019) .\n[BOS] Several researchers turn to expose systematic differences between human and NMT translations (Lubli et al., 2018; Schwarzenberg et al., 2019) , indicating the linguistic properties worthy of investigating.\n[BOS] However, the learned representations may depend on the model implementation, which potentially limit the applicability of these methods to a broader range of model architectures.\n[BOS] Accordingly, we focus on understanding the input-output behaviors, and validate on different architectures to demonstrate the universality of our findings.\n[BOS] Concerning interpreting the input-output behavior, previous work generally treats Seq2Seq models as black-boxes (Li et al., 2016; Alvarez-Melis and Jaakkola, 2017) .\n[BOS] For example, Alvarez-Melis and Jaakkola (2017) measure the relevance between two input-output tokens by perturbing the input sequence.\n[BOS] However, they do not exploit any intermediate information such as gradients, and the relevance score only resembles attention scores.\n[BOS] Recently, Jain and Wallace (2019) show that attention scores are in weak correlation with the feature importance.\n[BOS] Starting from this observation, we exploit the intermediate gradients to better estimate word importance, which consistently outperforms its attention counterpart across model architectures and language pairs.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite", "Transition", "Reflection", "Narrative_cite", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 47, "token_end": 80, "char_start": 284, "char_end": 406, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Shi et al., 2016;": "7197724", "Belinkov et al., 2017;": "7100502", "Yang et al., 2019)": "173990523"}}}, {"token_start": 82, "token_end": 101, "char_start": 414, "char_end": 464, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ding et al., 2017;": "27930067", "Bau et al., 2019)": "53215110"}}}, {"token_start": 111, "token_end": 134, "char_start": 539, "char_end": 614, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(L\u00e4ubli et al., 2018;": "52058047", "Schwarzenberg et al., 2019)": "85543217"}}}, {"token_start": 212, "token_end": 241, "char_start": 1114, "char_end": 1195, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Li et al., 2016;": "13017314", "Alvarez-Melis and Jaakkola, 2017)": "5509327"}}}, {"token_start": 246, "token_end": 298, "char_start": 1217, "char_end": 1478, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 301, "token_end": 321, "char_start": 1495, "char_end": 1598, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Jain and Wallace (2019)": "67855860"}, "Reference": {}}}]}
{"id": "52834274_5", "paragraph": "[BOS] The key novelties in our work compared to the prior work mentioned above are the proposed sequential variational labelers and the investigation of latent variable hierarchies within these models.\n[BOS] The empirical effectiveness of latent hierarchical structure in variational modeling is a key contribution of this paper and may be applicable to the other applications discussed above.\n[BOS] Recent work, contemporaneous with this submission, similarly showed the advantages of combining hierarchical latent variables and variational learning for conversational modeling, in the context of a non-sequential model (Park et al., 2018) .\n\n", "discourse_tags": ["Reflection", "Reflection", "Narrative_cite"], "span_citation_mapping": [{"token_start": 95, "token_end": 107, "char_start": 600, "char_end": 640, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Park et al., 2018)": "4829361"}}}]}
{"id": "52834274_4", "paragraph": "[BOS] There have been several efforts in combining variational autoencoders and recurrent networks (Gregor et al., 2015; Chung et al., 2015; Fraccaro et al., 2016) .\n[BOS] While the details vary, these models typically contain latent variables at each time step in a sequence.\n[BOS] This prior work mainly focused on ways of parameterizing the time dependence between the latent variables, which gives them more power in modeling distributions over observation sequences.\n[BOS] In this paper, we similarly use latent variables at each time step, but we adopt stronger independence assumptions which leads to simpler models and inference procedures.\n[BOS] Also, the models cited above were developed for modeling data distributions, rather than for supervised or semi-supervised learning, which is our focus here.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 8, "token_end": 41, "char_start": 41, "char_end": 163, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Gregor et al., 2015;": "1930231", "Chung et al., 2015;": "1594370", "Fraccaro et al., 2016)": "14166286"}}}]}
{"id": "52834274_3", "paragraph": "[BOS] Another related thread of work is learning interpretable latent representations.\n[BOS] Zhou and Neubig (2017) factorize an inflected word into lemma and morphology labels, using continuous and categorical latent variables.\n[BOS] Hu et al. (2017) interpret a sentence as a combination of an unstructured latent code and a structured latent code, which can represent attributes of the sentence.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 15, "token_end": 41, "char_start": 93, "char_end": 228, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhou and Neubig (2017)": null}, "Reference": {}}}, {"token_start": 42, "token_end": 74, "char_start": 235, "char_end": 398, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hu et al. (2017)": "20981275"}, "Reference": {}}}]}
{"id": "52834274_2", "paragraph": "[BOS] Our work involves multi-task losses and is therefore also related to the rich literature on multi-task learning for sequence labeling (Plank et al., 2016; Augenstein and Sgaard, 2017; Bingel and Sgaard, 2017; Rei, 2017, inter alia) .\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 19, "token_end": 62, "char_start": 98, "char_end": 237, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Plank et al., 2016;": "5632184", "Augenstein and S\u00f8gaard, 2017;": null, "Bingel and S\u00f8gaard, 2017;": "3127682"}}}]}
{"id": "52834274_1", "paragraph": "[BOS] There has been a great deal of work on using variational autoencoders in semi-supervised settings Maale et al., 2016; Zhou and Neubig, 2017; Yang et al., 2017b) .\n[BOS] Semi-supervised sequence labeling has a rich history (Altun et al., 2006; Jiao et al., 2006; Mann and McCallum, 2008; Subramanya et al., 2010; Sgaard, 2011) .\n[BOS] The simplest methods, which are also popular currently, use representations learned from large amounts of unlabeled data (Miller et al., 2004; Owoputi et al., 2013; Peters et al., 2017) .\n[BOS] Recently, Zhang et al. (2017) proposed a structured neural autoencoder that can be jointly trained on both labeled and unlabeled data.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Narrative_cite", "Single_summ"], "span_citation_mapping": [{"token_start": 17, "token_end": 44, "char_start": 79, "char_end": 166, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Maale et al., 2016;": "1972072", "Zhou and Neubig, 2017;": null, "Yang et al., 2017b)": "1696516"}}}, {"token_start": 46, "token_end": 96, "char_start": 175, "char_end": 331, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Altun et al., 2006;": "17137268", "Jiao et al., 2006;": "89684", "Mann and McCallum, 2008;": "9976345", "Subramanya et al., 2010;": "14000702", "S\u00f8gaard, 2011)": "28873816"}}}, {"token_start": 109, "token_end": 141, "char_start": 400, "char_end": 525, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Miller et al., 2004;": "15548439", "Owoputi et al., 2013;": "1528374", "Peters et al., 2017)": "7197241"}}}, {"token_start": 145, "token_end": 171, "char_start": 544, "char_end": 668, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhang et al. (2017)": "8326965"}, "Reference": {}}}]}
{"id": "52834274_0", "paragraph": "[BOS] There is a growing amount of work applying neural variational methods to NLP tasks, including document modeling (Mnih and Gregor, 2014; Miao et al., 2016; Serban et al., 2017) , machine translation (Zhang et al., 2016 ), text generation (Bowman et al., 2016; Serban et al., 2017; Hu et al., 2017) , language modeling (Bowman et al., 2016; Yang et al., 2017b) , and sequence transduction (Zhou and Neubig, 2017 ), but we are not aware of any such work for sequence labeling.\n[BOS] Before the advent of neural variational methods, there were several efforts in latent variable modeling for sequence labeling (Quattoni et al., 2007; Sun and Tsujii, 2009) .\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 19, "token_end": 46, "char_start": 100, "char_end": 181, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mnih and Gregor, 2014;": "1981188", "Miao et al., 2016;": "10796", "Serban et al., 2017)": "20995314"}}}, {"token_start": 47, "token_end": 56, "char_start": 184, "char_end": 223, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhang et al., 2016": "9134916"}}}, {"token_start": 58, "token_end": 84, "char_start": 227, "char_end": 302, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bowman et al., 2016;": "748227", "Serban et al., 2017;": "20995314", "Hu et al., 2017)": "20981275"}}}, {"token_start": 85, "token_end": 104, "char_start": 305, "char_end": 364, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bowman et al., 2016;": "748227", "Yang et al., 2017b)": "1696516"}}}, {"token_start": 106, "token_end": 115, "char_start": 371, "char_end": 415, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhou and Neubig, 2017": null}}}, {"token_start": 144, "token_end": 168, "char_start": 565, "char_end": 657, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Quattoni et al., 2007;": null, "Sun and Tsujii, 2009)": "18770235"}}}]}
{"id": "196180835_1", "paragraph": "[BOS] In parallel to our work, Wang et al. (2019) also investigated the effectiveness of the adversarial regularization technique in neural language modeling and NMT.\n[BOS] They also demonstrated the impacts of the adversarial regularization technique in NMT models.\n[BOS] We investigate the effectiveness of the several practical configurations that have not been examined in their paper, such as the combinations with VAT and back-translation.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 8, "char_start": 6, "char_end": 30, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 8, "token_end": 51, "char_start": 31, "char_end": 266, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wang et al. (2019)": "174800362"}, "Reference": {}}}]}
{"id": "196180835_0", "paragraph": "[BOS] Several studies have recently applied adversarial training to NLP tasks, e.g., (Jia and Liang, 2017; Belinkov and Bisk, 2018; Hosseini et al., 2017; Samanta and Mehta, 2017; Miyato et al., 2017; Sato et al., 2018) .\n[BOS] For example, Belinkov and Bisk (2018) ; Hosseini et al. (2017) proposed methods that generate input sentences with random character swaps.\n[BOS] They utilized the generated (input) sentences as additional training data.\n[BOS] However, the main focus of these methods is the incorporation of adversarial examples in the training phase, which is orthogonal to our attention, adversarial regularization, as described in Section 1.\n[BOS] Clark et al. (2018) used virtual adversarial training (VAT), which is a semi-supervised extension of the adversarial regularization technique originally proposed in Miyato et al. (2016) , in their experiments to compare the results with those of their proposed method.\n[BOS] Therefore, the focus of the neural models differs from this paper.\n[BOS] Namely, they focused on sequential labeling, whereas we discuss NMT models.\n\n", "discourse_tags": ["Narrative_cite", "Multi_summ", "Single_summ", "Reflection", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 7, "token_end": 68, "char_start": 44, "char_end": 219, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Jia and Liang, 2017;": "7228830", "Belinkov and Bisk, 2018;": "3513372", "Hosseini et al., 2017;": "15418780", "Miyato et al., 2017;": "12167053", "Sato et al., 2018)": "13684516"}}}, {"token_start": 73, "token_end": 118, "char_start": 241, "char_end": 447, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Belinkov and Bisk (2018)": "3513372", "Hosseini et al. (2017)": "15418780"}, "Reference": {}}}, {"token_start": 157, "token_end": 243, "char_start": 662, "char_end": 1085, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Clark et al. (2018)": "52811641"}, "Reference": {"Miyato et al. (2016)": "9398766"}}}]}
{"id": "199379475_2", "paragraph": "[BOS] GEC with NMT/SMT Several studies that introduce sequence-to-sequence models in GEC heavily rely on large amounts of training data.\n[BOS] Ge et al. (2018) , who presented state-of-the-art results in GEC, proposed a supervised NMT method trained on corpora of a total 5.4 M sentence pairs.\n[BOS] We mainly use the monolingual corpus because the low resource track does not permit the use of the learner corpora.\n[BOS] Despite the success of NMT, many studies on GEC traditionally use SMT (Susanto et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014) .\n[BOS] These studies apply an offthe-shelf SMT toolkit, Moses, to GEC.\n[BOS] JunczysDowmunt and Grundkiewicz (2014) claimed that the SMT system optimized for BLEU learns to not change the source sentence.\n[BOS] Instead of BLEU, they proposed tuning an SMT system using the M 2 score with annotated development data.\n[BOS] In this study, we also tune the weights with an F 0.5 score measured by the M 2 scorer because the official score is an F 0.5 score.\n\n", "discourse_tags": ["Transition", "Single_summ", "Reflection", "Narrative_cite", "Transition", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 33, "token_end": 74, "char_start": 143, "char_end": 293, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ge et al. (2018)": "49564245"}, "Reference": {}}}, {"token_start": 112, "token_end": 138, "char_start": 488, "char_end": 554, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Susanto et al., 2014;": "884251", "Junczys-Dowmunt and Grundkiewicz, 2014)": "18318073"}}}, {"token_start": 160, "token_end": 215, "char_start": 633, "char_end": 871, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "199379475_1", "paragraph": "[BOS] this study, we apply the USMT method of Artetxe et al. (2018b) and Marie and Fujita (2018) to GEC.\n[BOS] The UNMT method (Lample et al., 2018) was ineffective under the GEC setting in our preliminary experiments.\n\n", "discourse_tags": ["Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 8, "token_end": 30, "char_start": 31, "char_end": 96, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Artetxe et al. (2018b)": "52166727", "Marie and Fujita (2018)": "53113302"}}}, {"token_start": 36, "token_end": 48, "char_start": 115, "char_end": 148, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lample et al., 2018)": "5033497"}}}]}
{"id": "199379475_0", "paragraph": "[BOS] Unsupervised Machine Translation Studies on unsupervised methods have been conducted for both NMT (Lample et al., 2018; Marie and Fujita, 2018) and SMT (Artetxe et al., 2018b Table 4 : Error types for which our best system corrected errors well or mostly did not correct on the dev data.\n[BOS] Top2 denotes the top two errors, and Bottom2 denotes the lowest two errors in terms of the F 0.5 10 .\n\n", "discourse_tags": ["Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 14, "token_end": 32, "char_start": 100, "char_end": 149, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lample et al., 2018;": "5033497", "Marie and Fujita, 2018)": "53113302"}}}, {"token_start": 33, "token_end": 45, "char_start": 154, "char_end": 180, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Artetxe et al., 2018b": "52166727"}}}]}
{"id": "186206925_4", "paragraph": "[BOS] There also exist a few other works in this direction that do not accompany a publicly available tool or demo.\n[BOS] For instance, Hasan and Ng (2014) ; Levy et al. (2018) attempt to identify relevant arguments within web text in the context of a given topic.\n\n", "discourse_tags": ["Transition", "Multi_summ"], "span_citation_mapping": [{"token_start": 29, "token_end": 60, "char_start": 136, "char_end": 264, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hasan and Ng (2014)": "6441832", "Levy et al. (2018)": "52011877"}, "Reference": {}}}]}
{"id": "186206925_3", "paragraph": "[BOS] There exist a number of online debate platforms that provide similar functionalities as our system: kialo.com, procon.org, idebate.org , among others.\n[BOS] Such websites usually provide a wide range of debate topics and various arguments in response to each topic.\n[BOS] These resources have been proven useful in a line of works in argumentation (Hua and Wang, 2017; Stab et al., 2018b; Wachsmuth et al., 2018) , among many others.\n[BOS] While they provide rich sources of information, their content is fairly limited in terms of either their topical coverage or data availability for academic research purposes.\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 70, "token_end": 98, "char_start": 340, "char_end": 418, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hua and Wang, 2017;": "21306064", "Stab et al., 2018b;": "53083232", "Wachsmuth et al., 2018)": "51880268"}}}]}
{"id": "186206925_2", "paragraph": "[BOS] Beyond published works, there are websites that employ similar technologies.\n[BOS] For instance, bing.com has recently started a service that provides two different responses to a given argument (screenshot in Figure 4 ).\n[BOS] Since there is no published work on this system, it is not clear what the underlying mechanism is.\n\n", "discourse_tags": ["Transition", "Transition", "Transition"], "span_citation_mapping": []}
{"id": "186206925_1", "paragraph": "[BOS] There is a rich line of work on using Wikipedia as source for argument mining or to assess the veracity of a claim (Thorne et al., 2018) .\n[BOS] For instance, FAKTA is a system that extracts relevant documents from Wikipedia, among other sources, to predict the factuality of an input claim (Nadeem et al., 2019) .\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 10, "token_end": 35, "char_start": 38, "char_end": 142, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Thorne et al., 2018)": "4711425"}}}, {"token_start": 40, "token_end": 77, "char_start": 165, "char_end": 318, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Nadeem et al., 2019)": "182389308"}}}]}
{"id": "186206925_0", "paragraph": "[BOS] There are few related tools to this work.\n[BOS] args.me is a platform that accepts natural language queries and returns links to the pages that contain relevant topics (Wachsmuth et al., 2017) , which are split into supporting & opposing categories (screenshot in Figure 4) .\n[BOS] Similarly, ArgumentText (Stab et al., 2018a ) takes a topic as input and returns pro/con arguments retrieved from the web.\n[BOS] This work takes the effort one step further by employing language understanding techniques.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 20, "token_end": 45, "char_start": 81, "char_end": 198, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wachsmuth et al., 2017)": "19259545"}}}, {"token_start": 65, "token_end": 108, "char_start": 299, "char_end": 508, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Stab et al., 2018a": "46920928"}, "Reference": {}}}]}
{"id": "196182171_1", "paragraph": "[BOS] Sentimental Text Generation Generating sentimental and emotional texts is a key step towards building intelligent and controllable natural language generation systems.\n[BOS] To date several works of dialogue generation Zhou and Wang, 2018 ) and text sentiment transfer task Luo et al., 2019) have studied on generating emotional or sentimental text.\n[BOS] They always pre-define a binary sentiment label (positive/negative) or a small limited set of emotions, such as \"anger\", \"love\".\n[BOS] Different from them, controlling the fine-grained sentiment (a numeric value) for story ending generation is not limited to several emotional labels, thus we can not embed each sentiment label into a separate vector as usual.\n[BOS] Therefore, we propose to introduce the numeric sentiment value via a Gaussian Kernel Layer.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Multi_summ", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 27, "token_end": 94, "char_start": 180, "char_end": 488, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhou and Wang, 2018": "3033303", "Luo et al., 2019)": "165163728"}, "Reference": {}}}]}
{"id": "196182171_0", "paragraph": "[BOS] Story generation Automatic story generation has attracted interest over the past few years.\n[BOS] Recently, many approaches are proposed to generate a better story in terms of coherence (Jain et al., 2017; , rationality , topic-consistence (Yao et al., 2018a) .\n[BOS] However, most of story generation methods lack the ability to receive guidance from users to achieve a specific goal.\n[BOS] There are only a few works focus on the controllability of story generation, especially on sentiment.\n[BOS] Tambwekar et al. (2018) introduces a policy gradient learning approach to ensure that the model ends with a specific type of event given in advance.\n[BOS] Yao et al. (2018b) uses manually annotated story data to control the ending valence and storyline of story generation.\n[BOS] Different from them, our proposed framework can acquire distant sentiment labels without the dependence on the human annotations.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Transition", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 24, "token_end": 40, "char_start": 146, "char_end": 211, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Jain et al., 2017;": "643611"}}}, {"token_start": 43, "token_end": 56, "char_start": 228, "char_end": 265, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yao et al., 2018a)": null}}}, {"token_start": 100, "token_end": 132, "char_start": 506, "char_end": 654, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Tambwekar et al. (2018)": "195347347"}, "Reference": {}}}, {"token_start": 133, "token_end": 158, "char_start": 661, "char_end": 779, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Yao et al. (2018b)": null}, "Reference": {}}}]}
{"id": "189928537_2", "paragraph": "[BOS] We expect that the derived vocabulary mapping will have sufficient coverage for lost language cognates.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "189928537_1", "paragraph": "[BOS] Nonparallel Machine Translation Advancements in distributed representations kindled exciting developments in this field, including translations at both the lexical and the sentence level.\n[BOS] Lexical translation is primarily formulated as alignment of monolingual embedding spaces into a crosslingual representation using adversarial training (Conneau et al., 2017) , VAE (Dou et al., 2018) , CCA (Haghighi et al., 2008; Faruqui and Dyer, 2014) or mutual information (Mukherjee et al., 2018) .\n[BOS] The constructed monolingual embedding spaces are usually of high quality due to the large amount of monolingual data available.\n[BOS] The improved quality of distributed representations has similarly strong impact on non-parallel translation systems that operate at the sentence level (Pourdamghani and Knight, 2017) .\n[BOS] In that case, access to a powerful language model can partially compensate for the lack of explicit parallel supervision.\n[BOS] Unfortunately, these methods cannot be applied to ancient texts due to the scarcity of available data.\n[BOS] (Snyder et al., 2010) were the first to demonstrate the feasibility of automatic decipherment of a dead language using non-parallel data.\n[BOS] The success of their approach can be attributed to cleverly designed Bayesian model that structurally incorporated powerful linguistic constraints.\n[BOS] This includes customized priors for alphabet matching, incorporation of morphological structure, etc.\n[BOS] (Berg-Kirkpatrick and Klein, 2011) proposed an alternative decipherment approach based on a relatively simple model paired with sophisticated inference algorithm.\n[BOS] While their model performed well in a noise-free scenario when matching vocabularies only contain cognates, it has not been shown successful in a full decipherment scenario.\n[BOS] Our approach outperforms these models in both scenarios.\n[BOS] Moreover, we have demonstrated that the same architecture deciphers two distinct ancient languages Ugaritic and Linear B.\n[BOS] The latter result is particularly important given that Linear B is a syllabic language.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Single_summ", "Transition", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 50, "token_end": 63, "char_start": 330, "char_end": 373, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Conneau et al., 2017)": "3470398"}}}, {"token_start": 64, "token_end": 75, "char_start": 376, "char_end": 398, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dou et al., 2018)": "53081344"}}}, {"token_start": 76, "token_end": 97, "char_start": 401, "char_end": 452, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Haghighi et al., 2008;": "7185434", "Faruqui and Dyer, 2014)": "3792324"}}}, {"token_start": 98, "token_end": 110, "char_start": 456, "char_end": 499, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mukherjee et al., 2018)": "53081058"}}}, {"token_start": 146, "token_end": 169, "char_start": 725, "char_end": 826, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Pourdamghani and Knight, 2017)": "3329081"}}}, {"token_start": 211, "token_end": 281, "char_start": 1070, "char_end": 1469, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Snyder et al., 2010)": "586636"}, "Reference": {}}}, {"token_start": 282, "token_end": 349, "char_start": 1476, "char_end": 1818, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Berg-Kirkpatrick and Klein, 2011)": "9701597"}, "Reference": {}}}]}
{"id": "189928537_0", "paragraph": "[BOS] Decoding of Ciphered Texts Early work on decipherment was primarily focused on man-made ciphers, such as substitution ciphers.\n[BOS] Most of these approaches are based on EM algorithms which are further adjusted for target decipherment scenarios.\n[BOS] These adjustments are informed by assumptions about ciphers used to produce the data (Knight and Yamada, 1999; Knight et al., 2006; Ravi and Knight, 2011; Pourdamghani and Knight, 2017) .\n[BOS] Besides the commonly used EM algorithm, (Nuhn et al., 2013; Hauer et al., 2014; Kambhatla et al., 2018 ) also tackles substitution decipherment and formulate this problem as a heuristic search procedure, with guidance provided by an external language model (LM) for candidate rescoring.\n[BOS] So far, techniques developed for man-made ciphers have not been shown successful in deciphering archaeological data.\n[BOS] This can be attributed to the inherent complexity associated with processes behind language evolution of related languages.\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite", "Multi_summ", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 56, "token_end": 100, "char_start": 293, "char_end": 444, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Knight and Yamada, 1999;": "12106333", "Knight et al., 2006;": "5715338", "Ravi and Knight, 2011;": "6060648", "Pourdamghani and Knight, 2017)": "3329081"}}}, {"token_start": 102, "token_end": 169, "char_start": 453, "char_end": 739, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Nuhn et al., 2013;": "6955865", "Hauer et al., 2014;": "16870294", "Kambhatla et al., 2018": "53083091"}, "Reference": {}}}]}
{"id": "202772457_3", "paragraph": "[BOS] For the datasets and annotations of the SRL task, most of the previous research focuses on 1) PropBank (Palmer et al., 2005) and Nom-Bank (Meyers et al., 2004 ) annotations, i.e., the CoNLL 2005 (Carreras and Mrquez, 2005 and CoNLL 2009 (Haji et al., 2009) shared tasks; 2) OntoNotes annotations (Weischedel et al., 2011) , i.e., the CoNLL 2005 and CoNLL 2012 datasets and more; 3) and FrameNet (Baker et al., 1998) annotations.\n[BOS] For the non-English languages, not all of them are widely available.\n[BOS] Apart from these, in the broad range of semantic processing, other formalisms non-exhaustively include abstract meaning representation (Banarescu et al., 2013) , universal decompositional semantics (White et al., 2016) , and semantic dependency parsing (Oepen et al., 2015) .\n[BOS] Abend and Rappoport (2017) give a better overview of various semantic representations.\n[BOS] In this paper, we primarily work on the Chinese and English datasets from the CoNLL-2009 shared task and focus on the effectiveness of incorporating syntax into the Chinese SRL task.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Narrative_cite", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 22, "token_end": 32, "char_start": 100, "char_end": 130, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Palmer et al., 2005)": "2486369"}}}, {"token_start": 33, "token_end": 45, "char_start": 135, "char_end": 166, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Meyers et al., 2004": "16273722"}}}, {"token_start": 53, "token_end": 65, "char_start": 190, "char_end": 227, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Carreras and M\u00e0rquez, 2005": "16509032"}}}, {"token_start": 66, "token_end": 78, "char_start": 232, "char_end": 262, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 83, "token_end": 97, "char_start": 280, "char_end": 327, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 118, "token_end": 128, "char_start": 392, "char_end": 421, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 167, "token_end": 180, "char_start": 619, "char_end": 675, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 181, "token_end": 193, "char_start": 678, "char_end": 734, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 195, "token_end": 207, "char_start": 741, "char_end": 789, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 209, "token_end": 227, "char_start": 798, "char_end": 884, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Abend and Rappoport (2017)": "7741748"}, "Reference": {}}}]}
{"id": "202772457_2", "paragraph": "[BOS] In another line of research, Tan et al. (2017) utilize the Transformer network for the encoder instead of the BiLSTM.\n[BOS] Strubell et al. (2018) present a novel and effective multi-head self-attention model to incorporate syntax, which is called LISA (Linguistically-Informed Self-Attention).\n[BOS] We follow their approach of replacing one attention head with the dependency head information, but use a softer way to capture the pairwise relationship between input elements (Shaw et al., 2018) .\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 6, "token_end": 28, "char_start": 25, "char_end": 123, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Tan et al. (2017)": "12842009"}, "Reference": {}}}, {"token_start": 29, "token_end": 69, "char_start": 130, "char_end": 300, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Strubell et al. (2018)": "5068376"}, "Reference": {}}}, {"token_start": 87, "token_end": 107, "char_start": 410, "char_end": 502, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Shaw et al., 2018)": "3725815"}}}]}
{"id": "202772457_1", "paragraph": "[BOS] Recent neural-network-based approaches can be roughly categorized into two classes: 1) making use of the syntactic information (FitzGerald et al., 2015; Roth and Lapata, 2016; Qian et al., 2017; , and 2) pure endto-end learning from tokens to semantic labels, e.g., Zhou and Xu (2015) ; .\n[BOS] Roth and Lapata (2016) utilize an LSTM model to obtain embeddings from the syntactic dependency paths; while construct Graph Convolutional Networks to encode the dependency structure.\n[BOS] Although He et al. (2017) 's approach is a pure end-to-end learning, they have included an analysis of adding syntactic dependency information into English SRL in the discussion section.\n[BOS] have compared syntax-agnostic and syntax-aware approaches and Xia et al. (2019) have compared different ways to represent and encode the syntactic knowledge.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ", "Multi_summ"], "span_citation_mapping": [{"token_start": 23, "token_end": 50, "char_start": 111, "char_end": 199, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(FitzGerald et al., 2015;": "15048880", "Roth and Lapata, 2016;": "5779419"}}}, {"token_start": 55, "token_end": 78, "char_start": 210, "char_end": 290, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Zhou and Xu (2015)": "12688069"}}}, {"token_start": 81, "token_end": 113, "char_start": 301, "char_end": 484, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Roth and Lapata (2016)": "5779419"}, "Reference": {}}}, {"token_start": 115, "token_end": 154, "char_start": 500, "char_end": 677, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"He et al. (2017)": "33626727"}, "Reference": {}}}, {"token_start": 167, "token_end": 187, "char_start": 746, "char_end": 841, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Xia et al. (2019)": "59561763"}, "Reference": {}}}]}
{"id": "202772457_0", "paragraph": "[BOS] Traditional semantic role labeling task (Gildea and Jurafsky, 2002) presumes that the syntactic structure of the sentence is given, either being a constituent tree or a dependency tree, like in the CoNLL shared tasks (Carreras and Mrquez, 2005; Surdeanu et al., 2008; Haji et al., 2009) .\n\n", "discourse_tags": ["Multi_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 43, "char_start": 6, "char_end": 203, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Gildea and Jurafsky, 2002)": "207747200"}, "Reference": {}}}, {"token_start": 43, "token_end": 75, "char_start": 204, "char_end": 292, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Carreras and M\u00e0rquez, 2005;": "16509032", "Surdeanu et al., 2008;": "6534839"}}}]}
{"id": "189898035_1", "paragraph": "[BOS] Recently, controlling specific aspects in text generation is drawing increasing attention (Hu et al., 2017; Logeswaran et al., 2018) .\n[BOS] In the context of dialogue generation, Wang et al. (2017) propose steering response style and topic with human provided topic hints and fine-tuning on small scenting data; Zhang et al. (2018a) propose learning to control specificity of responses; and very recently, See et al. (2019) investigate how controllable attributes of responses affect human engagement with methods of conditional training and weighted decoding.\n[BOS] Our work is different in that (1) rather than playing with a single variable like specificity or topics, our model simultaneously controls multiple variables and can take controlling with specificity or topics as special cases; and (2) we manage attribute expression in response generation with a principled approach rather than simple heuristics like in (See et al., 2019) , and thus, our model can achieve better accuracy in terms of attribute expression in generated responses.\n\n", "discourse_tags": ["Narrative_cite", "Multi_summ", "Reflection"], "span_citation_mapping": [{"token_start": 8, "token_end": 32, "char_start": 48, "char_end": 138, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hu et al., 2017;": "20981275", "Logeswaran et al., 2018)": "53217784"}}}, {"token_start": 38, "token_end": 68, "char_start": 165, "char_end": 317, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wang et al. (2017)": "13226696"}, "Reference": {}}}, {"token_start": 69, "token_end": 84, "char_start": 319, "char_end": 392, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhang et al. (2018a)": "51869205"}, "Reference": {}}}, {"token_start": 89, "token_end": 115, "char_start": 413, "char_end": 567, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"See et al. (2019)": "67855999"}, "Reference": {}}}, {"token_start": 173, "token_end": 186, "char_start": 903, "char_end": 947, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(See et al., 2019)": "67855999"}}}]}
{"id": "189898035_0", "paragraph": "[BOS] Neural response generation models are built upon the encoder-decoder framework (Sutskever et al., 2014) .\n[BOS] Starting from the basic sequence-tosequence with attention architecture (Vinyals and Le, 2015; Shang et al., 2015) , extensions under the framework have been made to combat the \"safe response\" problem (Mou et al., 2016; Tao et al., 2018) ; to model the hierarchy of conversation history (Serban et al., 2016 (Serban et al., , 2017 Xing et al., 2018) ; to generate responses with specific personas or emotions ; and to speed up response decoding .\n[BOS] In this work, we also aim to tackle the \"safe response\" problem, but in an explainable, controllable, and general way.\n[BOS] Rather than learning with a different objective (e.g., ), generation from latent variables (e.g., ), or introducing extra content (e.g., ), we explicitly describe relationship between message-response pairs by defining metawords and express the meta-words in responses through a goal tracking memory network.\n[BOS] Our method allows developers to manipulate the generation process by playing with the meta-words and provides a general solution to response generation with specific attributes such as dialogue acts.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 10, "token_end": 24, "char_start": 59, "char_end": 109, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sutskever et al., 2014)": "7961699"}}}, {"token_start": 30, "token_end": 54, "char_start": 142, "char_end": 232, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vinyals and Le, 2015;": "12300158", "Shang et al., 2015)": "7356547"}}}, {"token_start": 65, "token_end": 86, "char_start": 295, "char_end": 355, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mou et al., 2016;": "5165773", "Tao et al., 2018)": "51609155"}}}, {"token_start": 89, "token_end": 119, "char_start": 367, "char_end": 467, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Serban et al., 2016": null, "(Serban et al., , 2017": "14857825", "Xing et al., 2018)": "14247119"}}}]}
{"id": "202660912_2", "paragraph": "[BOS] Residual Adapters were first introduced for adapting vision models in Rebuffi et al. (2017) , but their formulation used a single projection layer, without any tunable hyper-parameters that could be used to adjust capacity based on the target domain.\n[BOS] Houlsby et al. (2019) utilized a new formulation of adapters to adapt BERT (Devlin et al., 2018) to multiple tasks simultaneously.\n[BOS] Our formulation of adapters is motivated by theirs, but differs in a few respects.\n[BOS] Houlsby et al. (2019) introduce adapters after every sub-layer (self-attention, feed-forward) within a transformer layer, and re-train existing layer normalization parameters for every new domain.\n[BOS] We simplify this formulation by leaving the parameters frozen, and introducing new layer normalization parameters for every task, essentially mimic-ing the structure of the transformer feed-forward layer.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Reflection", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 51, "char_start": 6, "char_end": 256, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Rebuffi et al. (2017)": "6028387"}, "Reference": {}}}, {"token_start": 52, "token_end": 86, "char_start": 263, "char_end": 393, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Houlsby et al. (2019)": "59599816"}, "Reference": {"(Devlin et al., 2018)": "52967399"}}}, {"token_start": 106, "token_end": 151, "char_start": 489, "char_end": 685, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Houlsby et al. (2019)": "59599816"}, "Reference": {}}}]}
{"id": "202660912_1", "paragraph": "[BOS] They introduce domain specific gates that control the contribution of hidden units feeding into the next layer.\n[BOS] However, they introduce a limited amount of per-domain capacity which doesn't scale well when a lot of domain specific data is available.\n\n", "discourse_tags": ["Single_summ", "Single_summ"], "span_citation_mapping": []}
{"id": "202660912_0", "paragraph": "[BOS] Several approaches have been proposed in recent literature that try to address the shortcomings of full fine-tuning when applied to domain adaptation (Chu and Wang, 2018) .\n[BOS] Michel and Neubig (2018) proposed a space efficient approach to adaptation that introduces domain-specific biases to the output vocabulary, enabling extreme personalization in settings where small amounts of data are available for a lot of different domains.\n[BOS] Thompson et al. (2018) fine-tune selected components of the base model architecture, in order to determine how much fine-tuning each component contributes to the final adaptation performance.\n[BOS] Wuebker et al. (2018) propose introducing sparse offsets from the base model parameters for every domain, reducing the memory complexity of loading and unloading domain specific parameters in real world settings.\n[BOS] train the base model to utilize neighboring samples from the training set, enabling the model to adapt to new domains without the need for additional parameter updates.\n[BOS] Learning Hidden Unit Contribution (LHUC) (Vilar, 2018 ) is perhaps closest to our work in spirit.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ", "Single_summ", "Transition", "Single_summ"], "span_citation_mapping": [{"token_start": 24, "token_end": 33, "char_start": 138, "char_end": 176, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chu and Wang, 2018)": "44157913"}}}, {"token_start": 35, "token_end": 80, "char_start": 185, "char_end": 443, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Michel and Neubig (2018)": "19247366"}, "Reference": {}}}, {"token_start": 81, "token_end": 117, "char_start": 450, "char_end": 641, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 118, "token_end": 157, "char_start": 648, "char_end": 860, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wuebker et al. (2018)": "53082542"}, "Reference": {}}}, {"token_start": 188, "token_end": 211, "char_start": 1042, "char_end": 1139, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Vilar, 2018": "44065016"}, "Reference": {}}}]}
{"id": "202565622_3", "paragraph": "[BOS] In practice, we need to extract both events and those temporal relations among them from raw text.\n[BOS] All the works above treat this as two subtasks that are solved in a pipeline.\n[BOS] To the best of our knowledge, there has been no existing work on joint event-temporal relation extraction.\n[BOS] However, the idea of \"joint\" has been studied for entityrelation extraction in many works.\n[BOS] Miwa and Sasaki (2014) frame their joint model as table filling tasks, map tabular representation into sequential predictions with heuristic rules, and construct global loss to compute the best joint predictions.\n[BOS] Li and Ji (2014) define a global structure for joint entity and relation extraction, encode local and global features based on domain and linguistic knowledge.\n[BOS] and leverage beam-search to find global optimal assignments for entities and relations.\n[BOS] Miwa and Bansal (2016) leverage LSTM architectures to jointly predict both entity and relations, but fall short on ensuring prediction consistency.\n[BOS] Zhang et al. (2017) combine the benefits of both neural net and global optimization with beam search.\n[BOS] Motivated by these works, we propose an end-to-end trainable neural structured support vector machine (neural SSVM) model to simultaneously extract events and their relations from text and ensure the global structure via ILP constraints.\n[BOS] Next, we will describe in detail our proposed method.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Transition", "Single_summ", "Single_summ", "Transition", "Single_summ", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 82, "token_end": 121, "char_start": 405, "char_end": 617, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Miwa and Sasaki (2014)": "955518"}, "Reference": {}}}, {"token_start": 122, "token_end": 151, "char_start": 624, "char_end": 783, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Li and Ji (2014)": "20744"}, "Reference": {}}}, {"token_start": 168, "token_end": 196, "char_start": 884, "char_end": 1031, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Miwa and Bansal (2016)": "2476229"}, "Reference": {}}}, {"token_start": 197, "token_end": 218, "char_start": 1038, "char_end": 1139, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhang et al. (2017)": "2204967"}, "Reference": {}}}]}
{"id": "202565622_2", "paragraph": "[BOS] Early attempts on temporal relation extraction use local pair-wise classification with handengineered features (Mani et al., 2006; Verhagen et al., 2007; Chambers et al., 2007; Verhagen and Pustejovsky, 2008) .\n[BOS] Later efforts, such as ClearTK (Bethard, 2013) , UTTime (Laokulrat et al., 2013) , NavyTime (Chambers, 2013) , and CAEVO improve earlier work with better linguistic and syntactic rules.\n[BOS] Yoshikawa et al. (2009); Ning et al. (2017) ; Leeuwenberg and Moens (2017) explore structured learning for this task, and more recently, neural methods have also been shown effective (Tourille et al., 2017; Cheng and Miyao, 2017; Meng et al., 2017; Meng and Rumshisky, 2018) .\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Multi_summ"], "span_citation_mapping": [{"token_start": 14, "token_end": 54, "char_start": 88, "char_end": 214, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mani et al., 2006;": "18281724", "Verhagen et al., 2007;": "39011", "Chambers et al., 2007;": "2745647", "Verhagen and Pustejovsky, 2008)": "10348932"}}}, {"token_start": 61, "token_end": 69, "char_start": 246, "char_end": 269, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bethard, 2013)": "41372474"}}}, {"token_start": 70, "token_end": 83, "char_start": 272, "char_end": 303, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Laokulrat et al., 2013)": "10972786"}}}, {"token_start": 84, "token_end": 92, "char_start": 306, "char_end": 331, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chambers, 2013)": "1809075"}}}, {"token_start": 108, "token_end": 140, "char_start": 415, "char_end": 531, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Yoshikawa et al. (2009);": "6945139"}, "Reference": {"Ning et al. (2017)": "28982109", "Leeuwenberg and Moens (2017)": "17894632"}}}, {"token_start": 145, "token_end": 183, "char_start": 552, "char_end": 689, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Tourille et al., 2017;": "35796099", "Cheng and Miyao, 2017;": "32821791", "Meng et al., 2017;": "6987624", "Meng and Rumshisky, 2018)": null}}}]}
{"id": "202565622_1", "paragraph": "[BOS] Existing event extraction methods in the temporal relation domain, as in the TempEval3 workshop (UzZaman et al., 2013) , all use conventional machine learning models (logistic regression, SVM, or Max-entropy) with hand-engineered features (e.g., ClearTK (Bethard, 2013) and NavyTime (Chambers, 2013) ).\n[BOS] While other domains have shown progress on event extraction using neural methods (Nguyen and Grishman, 2015; Nguyen et al., 2016; Feng et al., 2016) , recent progress in the temporal relation domain is focused more on the setting where gold events are provided.\n[BOS] Therefore, we first show the performance of a neural event extractor on this task, although it is not our main contribution.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 15, "token_end": 30, "char_start": 83, "char_end": 124, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(UzZaman et al., 2013)": "640783"}}}, {"token_start": 59, "token_end": 67, "char_start": 252, "char_end": 275, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bethard, 2013)": "41372474"}}}, {"token_start": 68, "token_end": 76, "char_start": 280, "char_end": 305, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chambers, 2013)": "1809075"}}}, {"token_start": 86, "token_end": 114, "char_start": 358, "char_end": 463, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Nguyen and Grishman, 2015;": "10913456", "Nguyen et al., 2016;": "6452487", "Feng et al., 2016)": null}}}]}
{"id": "202565622_0", "paragraph": "[BOS] In this section we briefly summarize the existing work on event extraction and temporal relation extraction.\n[BOS] To the best of our knowledge, there is no prior work on joint event and relation extraction, so we will review joint entity and relation extraction works instead.\n\n", "discourse_tags": ["Transition", "Reflection"], "span_citation_mapping": []}
{"id": "202766392_0", "paragraph": "[BOS] Our work is inspired by a large number of successful applications using neural encoder-decoder frameworks on NLP tasks such as machine translation (Cho et al., 2014a) and dialog generation (Vinyals and Le, 2015) .\n[BOS] Our work is also inspired by the recent work for KBQG based on encoderdecoder frameworks.\n[BOS] Serban et al. (2016) first proposed a neural network for mapping KB facts into natural language questions.\n[BOS] To improve the generalization, Elsahar et al. (2018) introduced extra contexts for the input fact, which achieved significant performances.\n[BOS] However, these contexts may make it difficult to generate questions that express the given predicate and associate with a definitive answer.\n[BOS] Therefore, we focus on the two research issues: expressing the given predicate and referring to a definitive answer for generated questions.\n[BOS] Moreover, our work also borrows the idea from copy mechanisms.\n[BOS] Point network predicted the output sequence directly from the input, and it can not generate new words while CopyNet (Gu et al., 2016) combined copying and generating.\n[BOS] Bao et al. (2018) proposed to copy elements in the table (KB).\n[BOS] Elsahar et al. (2018) exploited POS copy action to better capture textual contexts.\n[BOS] To incorporate advantages from above copy mechanisms, we introduce KB copy and context copy which can copy KB element and textual context, and they do not rely on POS tagging.\n\n", "discourse_tags": ["Narrative_cite", "Reflection", "Single_summ", "Single_summ", "Transition", "Reflection", "Reflection", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 25, "token_end": 36, "char_start": 133, "char_end": 172, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cho et al., 2014a)": null}}}, {"token_start": 37, "token_end": 48, "char_start": 177, "char_end": 217, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vinyals and Le, 2015)": null}}}, {"token_start": 71, "token_end": 93, "char_start": 322, "char_end": 428, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 99, "token_end": 121, "char_start": 466, "char_end": 574, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Elsahar et al. (2018)": "3444808"}, "Reference": {}}}, {"token_start": 204, "token_end": 219, "char_start": 1053, "char_end": 1111, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Gu et al., 2016)": "8174613"}, "Reference": {}}}, {"token_start": 220, "token_end": 239, "char_start": 1118, "char_end": 1180, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Bao et al. (2018)": "19099243"}, "Reference": {}}}, {"token_start": 240, "token_end": 259, "char_start": 1187, "char_end": 1270, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Elsahar et al. (2018)": "3444808"}, "Reference": {}}}]}
{"id": "195833318_0", "paragraph": "[BOS] The work is connected to the following threads of work of NLP research.\n\n", "discourse_tags": ["Other"], "span_citation_mapping": []}
{"id": "67855846_5", "paragraph": "[BOS] We present one such model in Section 6.\n[BOS] Other related work along these lines has been done by Reed and de Freitas (2015), Neelakantan et al. (2015) , and Liang et al. (2017) .\n\n", "discourse_tags": ["Reflection", "Narrative_cite"], "span_citation_mapping": [{"token_start": 22, "token_end": 30, "char_start": 106, "char_end": 132, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 31, "token_end": 41, "char_start": 134, "char_end": 159, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Neelakantan et al. (2015)": "6715185"}}}, {"token_start": 43, "token_end": 50, "char_start": 166, "char_end": 185, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Liang et al. (2017)": "2742513"}}}]}
{"id": "67855846_4", "paragraph": "[BOS] Neural symbolic reasoning: DROP is designed to encourage research on methods that combine neural methods with discrete, symbolic reasoning.\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "67855846_3", "paragraph": "[BOS] Adversarial dataset construction: We continue a recent trend in creating datasets with adversarial baselines in the loop (Paperno et al., 2016; Minervini and Riedel, 2018; Zellers et al., 2018b; Zellers et al., 2018a) .\n[BOS] In our case, instead of using an adversarial baseline to filter automatically generated examples, we use it in a crowdsourcing task, to raise the difficulty level of the questions provided by crowd workers.\n\n", "discourse_tags": ["Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 13, "token_end": 59, "char_start": 70, "char_end": 223, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Paperno et al., 2016;": "2381275", "Minervini and Riedel, 2018;": "52099643", "Zellers et al., 2018b;": "52019251", "Zellers et al., 2018a)": "53734356"}}}]}
{"id": "67855846_2", "paragraph": "[BOS] Semantic parsing: The semantic parsing literature has a long history of trying to understand complex, compositional question semantics in terms of some grounded knowledge base or other environment (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Berant et al., 2013a, inter alia) .\n[BOS] It is this literature that we modeled our questions on, particularly looking at the questions in the Wik-iTableQuestions dataset (Pasupat and Liang, 2015) .\n[BOS] If we had a structured, tabular representation of the content of our paragraphs, DROP would be largely the same as WikiTableQuestions, with similar (possibly even simpler) question semantics.\n[BOS] Our novelty is that we are the first to combine these complex questions with paragraph understanding, with the aim of encouraging systems that can produce comprehensive structural analyses of paragraphs, either explicitly or implicitly.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 19, "token_end": 64, "char_start": 108, "char_end": 292, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zelle and Mooney, 1996;": "263135", "Zettlemoyer and Collins, 2005;": "449252"}}}, {"token_start": 84, "token_end": 100, "char_start": 402, "char_end": 455, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Pasupat and Liang, 2015)": "9027681"}}}]}
{"id": "67855846_1", "paragraph": "[BOS] Many existing algebra word problem datasets also contain similar phenomena to what is in DROP (Koncel-Kedziorski et al., 2015; Ling et al., 2017) .\n[BOS] Our dataset is different in that it typically has much longer contexts, is more open domain, and requires deeper paragraph understanding.\n\n", "discourse_tags": ["Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 9, "token_end": 38, "char_start": 55, "char_end": 151, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Koncel-Kedziorski et al., 2015;": "4894130", "Ling et al., 2017)": "12777818"}}}]}
{"id": "67855846_0", "paragraph": "[BOS] Question answering datasets: With systems reaching human performance on the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) , many follow-on tasks are currently being proposed.\n[BOS] All of these datasets throw in additional complexities to the reading comprehension challenge, around tracking conversational state (Reddy et al., 2018; Choi et al., 2018) , requiring passage retrieval (Joshi et al., 2017; Yang et al., 2018) , mismatched passages and questions (Saha et al., 2018; Kocisk et al., 2018; Rajpurkar et al., 2018) , in-tegrating knowledge from external sources (Mihaylov et al., 2018; , or a particular kind of \"multi-step\" reasoning over multiple documents (Welbl et al., 2018; Khashabi et al., 2018) .\n[BOS] We applaud these efforts, which offer good avenues to study these additional phenomena.\n[BOS] However, we are concerned with paragraph understanding, which on its own is far from solved, so DROP has none of these additional complexities.\n[BOS] It consists of single passages of text paired with independent questions, with only linguistic facility required to answer the questions.\n[BOS] 1 One could argue that we are adding numerical reasoning as an \"additional complexity\", and this is true; however, it is only simple reasoning that is relatively well-understood in the semantic parsing literature, and we use it as a necessary means to force more comprehensive passage understanding.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Reflection", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 13, "token_end": 31, "char_start": 82, "char_end": 150, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rajpurkar et al., 2016)": "11816014"}}}, {"token_start": 57, "token_end": 78, "char_start": 305, "char_end": 381, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Reddy et al., 2018;": "52055325", "Choi et al., 2018)": "52057510"}}}, {"token_start": 79, "token_end": 98, "char_start": 384, "char_end": 451, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Joshi et al., 2017;": "26501419", "Yang et al., 2018)": "52822214"}}}, {"token_start": 99, "token_end": 131, "char_start": 454, "char_end": 552, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Saha et al., 2018;": "5071138", "Kocisk\u00fd et al., 2018;": "2593903", "Rajpurkar et al., 2018)": "47018994"}}}, {"token_start": 132, "token_end": 151, "char_start": 555, "char_end": 622, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 155, "token_end": 185, "char_start": 631, "char_end": 740, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Welbl et al., 2018;": "9192723", "Khashabi et al., 2018)": "5112038"}}}]}
{"id": "158046817_1", "paragraph": "[BOS] GNN for NLP: Recently, there is considerable amount of interest in applying GNN to NLP tasks and great success has been achieved.\n[BOS] For exam-ple, in neural machine translation, GNN has been employed to integrate syntactic and semantic information into encoders (Bastings et al., 2017; Marcheggiani et al., 2018) ; applied GNN to relation extraction over pruned dependency trees; the study by Yao et al. (2018) employed GNN over a heterogeneous graph to do text classification, which inspires our idea of the HDE graph; Liu et al. (2018) proposed a new contextualized neural network for sequence learning by leveraging various types of non-local contextual information in the form of information passing over GNN.\n[BOS] These studies are related to our work in the sense that we both use GNN to improve the information interaction over long context or across documents.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Reflection"], "span_citation_mapping": [{"token_start": 42, "token_end": 76, "char_start": 187, "char_end": 321, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Bastings et al., 2017;": "6206777", "Marcheggiani et al., 2018)": "5063437"}, "Reference": {}}}, {"token_start": 77, "token_end": 121, "char_start": 324, "char_end": 527, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Yao et al. (2018)": "52284222"}, "Reference": {}}}, {"token_start": 122, "token_end": 160, "char_start": 529, "char_end": 722, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Liu et al. (2018)": "53734344"}, "Reference": {}}}]}
{"id": "158046817_0", "paragraph": "[BOS] The study presented in this paper is directly related to existing research on multi-hop reading comprehension across multiple documents (Dhingra et al., 2018; Song et al., 2018; De Cao et al., 2018; Zhong et al., 2019; Kundu et al., 2018) .\n[BOS] The method presented in this paper is similar to previous studies using GNN for multi-hop reasoning (Song et al., 2018; De Cao et al., 2018) .\n[BOS] Our novelty is that we propose to use a heterogeneous graph instead of a graph with single type of nodes to incorporate different granularity levels of information.\n[BOS] The co-attention and self-attention based encoding of multi-level information presented in each input is also inspired by the CFC model (Zhong et al., 2019) because they show the effectiveness of attention mechanisms.\n[BOS] Our model is very different from the other two studies (Dhingra et al., 2018; Kundu et al., 2018) : these two studies both explicitly score the possible reasoning paths with extra NER or coreference resolution systems while our method does not require these modules and we do multi-hop reasoning over graphs.\n[BOS] Besides these studies, our work is also related to the following research directions.\n[BOS] Multi-hop RC: There exist several different data sets that require reasoning in multiple steps in literature, for example bAbI (Weston et al., 2015) , MultiRC (Khashabi et al., 2018) and OpenBookQA (Mihaylov et al., 2018) .\n[BOS] A lot of systems have been proposed to solve the multi-hop RC problem with these data sets (Sun et al., 2018; Wu et al., 2019) .\n[BOS] However, these data sets require multi-hop reasoning over multiple sentences or multiple common knowledge while the problem we want to solve in this paper requires collecting evidences across multiple documents.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Reflection", "Narrative_cite", "Multi_summ", "Reflection", "Narrative_cite", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 15, "token_end": 64, "char_start": 84, "char_end": 244, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dhingra et al., 2018;": "4957155", "Song et al., 2018;": "52172080", "De Cao et al., 2018;": "52116920", "Zhong et al., 2019;": "57375723", "Kundu et al., 2018)": null}}}, {"token_start": 78, "token_end": 101, "char_start": 325, "char_end": 393, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Song et al., 2018;": "52172080", "De Cao et al., 2018)": "52116920"}}}, {"token_start": 156, "token_end": 177, "char_start": 699, "char_end": 790, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhong et al., 2019)": "57375723"}}}, {"token_start": 188, "token_end": 243, "char_start": 852, "char_end": 1105, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Dhingra et al., 2018;": "4957155", "Kundu et al., 2018)": null}, "Reference": {}}}, {"token_start": 272, "token_end": 321, "char_start": 1263, "char_end": 1425, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Weston et al., 2015)": "3178759", "(Khashabi et al., 2018)": "5112038", "(Mihaylov et al., 2018)": "52183757"}}}, {"token_start": 333, "token_end": 358, "char_start": 1483, "char_end": 1560, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sun et al., 2018;": "53109787", "Wu et al., 2019)": "58006691"}}}]}
{"id": "196187034_1", "paragraph": "[BOS] In addition to that, a large body of work has focused on addressing the hubness problem that arises when directly inducing bilingual dictionaries from cross-lingual embeddings, either through the retrieval method Smith et al., 2017; or the mapping itself Shigeto et al., 2015; .\n[BOS] While all these previous methods directly induce bilingual dictionaries from cross-lingually mapped embeddings, our proposed method combines them with unsupervised machine translation techniques, outperforming them all by a substantial margin.\n\n", "discourse_tags": ["Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 38, "token_end": 46, "char_start": 202, "char_end": 237, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 49, "token_end": 59, "char_start": 246, "char_end": 281, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "196187034_0", "paragraph": "[BOS] While BLI has been previously tackled using count-based vector space models (Vuli and Moens, 2013) and statistical decipherment (Ravi and Knight, 2011; Dou and Knight, 2012) , these methods have recently been superseded by crosslingual embedding mappings, which work by aligning independently trained word embeddings in different languages.\n[BOS] For that purpose, early methods required a training dictionary, which was used to learn a linear transformation that mapped these embeddings into a shared crosslingual space (Mikolov et al., 2013; Artetxe et al., 2018a) .\n[BOS] The resulting cross-lingual embeddings are then used to induce the translations of words that were missing in the training dictionary by taking their nearest neighbor in the target language.\n[BOS] The amount of required supervision was later reduced through self-learning methods (Artetxe et al., 2017) , and then completely eliminated through adversarial training (Zhang et al., 2017a; or more robust iterative approaches combined with initialization heuristics (Artetxe et al., 2018b; Hoshen and Wolf, 2018) .\n[BOS] At the same time, several recent methods have formulated embedding mappings as an optimal transport problem (Zhang et al., 2017b; Alvarez-Melis and Jaakkola, 2018) .\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Transition", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 11, "token_end": 26, "char_start": 50, "char_end": 104, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vuli\u0107 and Moens, 2013)": "10068440"}}}, {"token_start": 27, "token_end": 48, "char_start": 109, "char_end": 179, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ravi and Knight, 2011;": "6060648", "Dou and Knight, 2012)": "5727182"}}}, {"token_start": 86, "token_end": 128, "char_start": 396, "char_end": 572, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mikolov et al., 2013;": "1966640"}}}, {"token_start": 172, "token_end": 187, "char_start": 831, "char_end": 883, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 190, "token_end": 204, "char_start": 895, "char_end": 966, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 207, "token_end": 232, "char_start": 976, "char_end": 1090, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Hoshen and Wolf, 2018)": "51978342"}}}, {"token_start": 243, "token_end": 273, "char_start": 1145, "char_end": 1262, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhang et al., 2017b;": "32923127"}}}]}
{"id": "202764963_0", "paragraph": "[BOS] Utilizing pair occurrences for embedding models have been considered before, both as explicit model choices and as negative sampling strategies.\n[BOS] Chang et al. (2014) and use pair occurrences to constrain the set of triples to be used in the optimization procedure.\n[BOS] For methods that rely on SGD with contrastive training, this translates to a special case of our biased sampling method where p = 1. present TATEC, a model that combines bigram and trigram interactions.\n[BOS] The trigram model uses a full matrix representation for relations, and hence has many more parameters compared to our model.\n[BOS] present JointDM and JointComplex, which could be viewed as a simplification of TATEC.\n[BOS] Unlike our model, both of these methods use the bigram terms both in training and evaluation, do not share any of the embeddings between two models, and do not provide supervision based on pair occurrences in the data.\n[BOS] Other methods that have been considered for improving the negative sampling procedure includes adversarial (Cai and Wang, 2018) and self-adversarial training.\n[BOS] None of these methods focus on improving the models to scale to large KGs.\n\n", "discourse_tags": ["Transition", "Single_summ", "Reflection", "Reflection", "Transition", "Reflection", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 25, "token_end": 50, "char_start": 157, "char_end": 275, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Chang et al. (2014)": "1077128"}, "Reference": {}}}, {"token_start": 195, "token_end": 204, "char_start": 1034, "char_end": 1066, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cai and Wang, 2018)": "3401524"}}}]}
{"id": "174800569_2", "paragraph": "[BOS] Style transfer has been an emerging field in natural language processing (NLP).\n[BOS] A couple of works have been done in changing the style of an input text and designing the output text according to some particular styles.\n[BOS] In (Rao and Tetreault, 2018) , a dataset has been introduced for formality style transfer.\n[BOS] Unsupervised text style transfer has encouraged in transforming a given text without parallel data (Shen et al., 2017; Carlson et al., 2017; Fu et al., 2018; Li et al., 2018; Niu and Bansal, 2018) .\n[BOS] Overall our system is novel as it is motivated by the need for inducing specific behavior and style in an existing NLG systems (neural, or template-based) as a means of post editing, by simultaneously being emotionally and contextually consistent.\n[BOS] We have successfully demonstrated this behavior through empirical analysis for a specific application of customer care.\n\n", "discourse_tags": ["Transition", "Transition", "Multi_summ", "Narrative_cite", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 47, "token_end": 68, "char_start": 240, "char_end": 327, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Rao and Tetreault, 2018)": "4859003"}, "Reference": {}}}, {"token_start": 76, "token_end": 121, "char_start": 385, "char_end": 530, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Shen et al., 2017;": "7296803", "Carlson et al., 2017;": "21186239", "Fu et al., 2018;": "6484065", "Li et al., 2018;": "4937880", "Niu and Bansal, 2018)": "13690180"}}}]}
{"id": "174800569_1", "paragraph": "[BOS] Emotion classification and analysis (Herzig et al., 2016) in customer support dialogue is important for better understanding of the customer and to provide better customer support.\n[BOS] Lately, a number of works have been done on controlled text generation (Hu et al., 2017; Li et al., 2017; Subramanian et al., 2017; Fedus et al., 2018; in order to generate responses with desired attributes.\n[BOS] Emotion aware text generation (Zhou and Wang, 2018; have gained popularity as it generates responses depending on a specific emotion.\n[BOS] Previous works in conditioned text generation have worked on inducing specific biases and behaviors (Herzig et al., 2017) while generation (like emotion, style, and personality trait).\n[BOS] Our work is different in the sense that it can encompass different emotional states (like joy, excitement, sad-ness, disappointment) and traits (like friendliness, apologetic, thankfulness, empathy), as is the demand of the situation.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Single_summ", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 15, "char_start": 6, "char_end": 63, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Herzig et al., 2016)": "17958293"}}}, {"token_start": 46, "token_end": 80, "char_start": 237, "char_end": 343, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hu et al., 2017;": "20981275", "Li et al., 2017;": "98180", "Subramanian et al., 2017;": "397556"}}}, {"token_start": 91, "token_end": 115, "char_start": 407, "char_end": 540, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Zhou and Wang, 2018;": "3033303"}, "Reference": {}}}, {"token_start": 125, "token_end": 139, "char_start": 608, "char_end": 668, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Herzig et al., 2017)": "34405847"}}}]}
{"id": "174800569_0", "paragraph": "[BOS] Natural language generation (NLG) module has been gaining importance in wide applications such as dialogue systems (Vinyals and Le, 2015; Shen et al., 2018; Wu et al., 2018; Serban et al., 2017a; Raghu et al., 2018; Li et al., 2016) , question answering systems (Reddy et al., 2017; Duan et al., 2017) , and many other natural language interfaces.\n[BOS] To help the users achieve their desired goals, response generation provides the medium through which a conversational agent is able to communicate with its user.\n[BOS] In (Serban et al., 2017b) , the authors have proposed an hierarchical encoder-decoder model for capturing the dependencies in the utterances of a dialogue.\n[BOS] Conditional auto-encoders have been employed in (Zhao et al., 2017) , that generates diverse replies by capturing discourse-level information in the encoder.\n[BOS] Our work differentiates from these previous works in dialogue generation in a way that we embellish the appropriate response content with courteous phrases and sentences, according to the conversation.\n[BOS] Hence, our system is an accompaniment to any standalone natural language generation system to enhance its acceptability, usefulness and user-friendliness.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Single_summ", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 19, "token_end": 68, "char_start": 104, "char_end": 238, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vinyals and Le, 2015;": "12300158", "Shen et al., 2018;": "3620643", "Wu et al., 2018;": "44113253", "Serban et al., 2017a;": "7562717", "Raghu et al., 2018;": "26072484", "Li et al., 2016)": "3147007"}}}, {"token_start": 69, "token_end": 89, "char_start": 241, "char_end": 307, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Reddy et al., 2017;": "8946168", "Duan et al., 2017)": "427742"}}}, {"token_start": 128, "token_end": 160, "char_start": 531, "char_end": 683, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Serban et al., 2017b)": "14857825"}, "Reference": {}}}, {"token_start": 161, "token_end": 194, "char_start": 690, "char_end": 847, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Zhao et al., 2017)": "14688760"}, "Reference": {}}}]}
{"id": "196188501_1", "paragraph": "[BOS] Mention-entity mappings have been used in the context of optimizing coreference performance measures (Le and Titov, 2017; Clark and Manning, 2016 ).\n[BOS] Here we show that these mappings can also be used for the resolution model itself.\n[BOS] We note that we did not try to optimize for coreference measures as in Le and Titov (2017) , and this is likely to further improve results.\n\n", "discourse_tags": ["Narrative_cite", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 14, "token_end": 34, "char_start": 74, "char_end": 154, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Le and Titov, 2017;": "15971789", "Clark and Manning, 2016": "2012188"}}}, {"token_start": 62, "token_end": 74, "char_start": 294, "char_end": 340, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Le and Titov (2017)": "15971789"}}}]}
{"id": "196188501_0", "paragraph": "[BOS] Several works have addressed the issue of entitylevel representation (Culotta et al., 2007; Wick et al., 2009; Singh et al., 2011) .\n[BOS] In Wiseman et al. (2016) an RNN is used to model each entity.\n[BOS] While this allows complex entity representations, the assignment of a mention to an RNN is a hard decision, and as such cannot be optimized in an end-to-end manner.\n[BOS] Clark and Manning (2015) use whole-entity representations as obtained from agglomerative clustering.\n[BOS] But again the clustering operation in non-differentiable, requiring the use of imitation learning.\n[BOS] In , entity refinement is more restricted, as it is only obtained from the attention vector at each step.\n[BOS] Thus, we believe that our model is the first to use entity-level representations that correspond directly to the inferred clusters, and are end-to-end differentiable.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 9, "token_end": 36, "char_start": 48, "char_end": 136, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Culotta et al., 2007;": "6618210", "Wick et al., 2009;": "9412543", "Singh et al., 2011)": "683705"}}}, {"token_start": 39, "token_end": 94, "char_start": 148, "char_end": 377, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wiseman et al. (2016)": "9163756"}, "Reference": {}}}, {"token_start": 95, "token_end": 133, "char_start": 384, "char_end": 589, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Clark and Manning (2015)": null}, "Reference": {}}}]}
{"id": "196189186_1", "paragraph": "[BOS] 4.1 Table- to-text Generation Table- to-text generation is widely applied in many domains.\n[BOS] Dubou and McKeown (2002) proposed to generate the biography by matching the text with a knowledge base.\n[BOS] Barzilay and Lapata (2005) presented an efficient method for automatically learning content selection rules from a corpus and its related database in the sports domain.\n[BOS] Liang et al. (2009) introduced a system with a sequence of local decisions for the sportscasting and the weather forecast.\n[BOS] Recently, thanks to the success of the neural network models, more work focused on the neural generative models in an endto-end style (Wiseman et al., 2017; Puduppully et al., 2018; Gehrmann et al., 2018; Bao et al., 2018; Qin et al., 2018) .\n[BOS] Lebret et al. (2016) constructed a dataset of biographies from Wikipedia, and built a neural model based on the conditional neural language models.\n[BOS] introduced a structure-aware sequence-tosequence architecture to model the inner structure of the tables and the interaction between the tables and the text.\n[BOS] Wiseman et al. (2018) focused on the interpretable and controllable generation process, and proposed a neural model using a hidden semi-markov model decoder to address these issues.\n[BOS] Nie et al. (2018) attempted to improve the fidelity of neural table-to-text generation by utilizing pre-executed symbolic operations in a sequence-to-sequence model.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Narrative_cite", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 25, "token_end": 49, "char_start": 103, "char_end": 206, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Dubou\u00e9 and McKeown (2002)": "10138069"}, "Reference": {}}}, {"token_start": 50, "token_end": 81, "char_start": 213, "char_end": 381, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Barzilay and Lapata (2005)": "1589010"}, "Reference": {}}}, {"token_start": 82, "token_end": 108, "char_start": 388, "char_end": 510, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Liang et al. (2009)": "238873"}, "Reference": {}}}, {"token_start": 126, "token_end": 180, "char_start": 604, "char_end": 757, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wiseman et al., 2017;": "23892230", "Puduppully et al., 2018;": "52153976", "Gehrmann et al., 2018;": null, "Bao et al., 2018;": "19099243", "Qin et al., 2018)": "53082776"}}}, {"token_start": 182, "token_end": 213, "char_start": 766, "char_end": 913, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Lebret et al. (2016)": "1238927"}, "Reference": {}}}, {"token_start": 243, "token_end": 279, "char_start": 1084, "char_end": 1265, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wiseman et al. (2018)": "52135124"}, "Reference": {}}}, {"token_start": 280, "token_end": 316, "char_start": 1272, "char_end": 1437, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Nie et al. (2018)": "52186560"}, "Reference": {}}}]}
{"id": "196189186_0", "paragraph": "[BOS] This work is mostly related to both table-to-text generation and low resource natural language generation.\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "75134948_2", "paragraph": "[BOS] Our analysis assumes a characterization of unit roles, where each hidden unit is observed to have some specific function.\n[BOS] Findings from Linzen et al. (2016) and others suggest that a single hidden unit can learn to track complex syntactic rules.\n[BOS] Radford et al. (2017) find that a character-level language model can implicitly assign a single unit to track sentiment, without being directly supervised.\n[BOS] (Kementchedjhieva and Lopez, 2018 ) also examine individual units in a character model and find complex behavior by inspecting activation patterns by hand.\n[BOS] In contrast, our metrics are motivated by discovering these units automatically, and capturing unit-level contributions quantitatively.\n\n", "discourse_tags": ["Reflection", "Multi_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 27, "token_end": 51, "char_start": 148, "char_end": 257, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Linzen et al. (2016)": "14091946"}, "Reference": {}}}, {"token_start": 52, "token_end": 83, "char_start": 264, "char_end": 419, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Radford et al. (2017)": "14838925"}, "Reference": {}}}, {"token_start": 84, "token_end": 116, "char_start": 426, "char_end": 581, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Kementchedjhieva and Lopez, 2018": "52157882"}, "Reference": {}}}]}
{"id": "75134948_1", "paragraph": "[BOS] Substantial prior work exists on the character level as well (Karpathy et al., 2015; Vania and Lopez, 2017; Kementchedjhieva and Lopez, 2018; Gerz et al., 2018) .\n[BOS] Smith et al. (2018) examined the character component in multilingual parsing models empirically, comparing it to the contribution of POS embeddings and pre-trained embeddings.\n[BOS] Chaudhary et al. (2018) leveraged crosslingual character-level correspondence to train NER models for low-resource languages.\n[BOS] Most related to our work is Godin et al. (2018) , who compared CNN and LSTM character models on a type-level prediction task on three languages, using the post-network softmax values to see which models identify useful character sequences.\n[BOS] Unlike their analysis, we examine a more applied token-level task (POS tagging), and focus on the hidden states within the LSTM model in order to analyze its raw view of word composition.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 8, "token_end": 48, "char_start": 43, "char_end": 166, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Karpathy et al., 2015;": "988348", "Vania and Lopez, 2017;": "2078255", "Kementchedjhieva and Lopez, 2018;": "52157882", "Gerz et al., 2018)": "49587517"}}}, {"token_start": 50, "token_end": 82, "char_start": 175, "char_end": 350, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Smith et al. (2018)": null}, "Reference": {}}}, {"token_start": 83, "token_end": 111, "char_start": 357, "char_end": 482, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Chaudhary et al. (2018)": "52124918"}, "Reference": {}}}, {"token_start": 118, "token_end": 163, "char_start": 517, "char_end": 728, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Godin et al. (2018)": "52129828"}, "Reference": {}}}, {"token_start": 168, "token_end": 204, "char_start": 758, "char_end": 922, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "75134948_0", "paragraph": "[BOS] Several recent papers attempt to explain neural network performance by investigating hidden state activation patterns on auxiliary or downstream tasks.\n[BOS] On the word level, Linzen et al. (2016) trained LSTM language models, evaluated their performance on grammatical agreement detection, and analyzed activation patterns within specific hidden units.\n[BOS] We build on this analysis strategy as we aggregate (character-) sequence activation patterns across all hidden units in a model into quantitative measures.\n\n", "discourse_tags": ["Transition", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 24, "token_end": 60, "char_start": 164, "char_end": 360, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Linzen et al. (2016)": "14091946"}, "Reference": {}}}]}
{"id": "202578048_1", "paragraph": "[BOS] Several approaches have been proposed recently to connect the rich expressiveness of contextualized word embeddings with cross-lingual transfer.\n[BOS] Mulcaire et al. (2019) based their model on ELMo (Peters et al., 2018) and proposed a polyglot contextual representation model by capturing character-level information from multilingual data.\n[BOS] Lample and Conneau (2019) adapted the objectives of BERT (Devlin et al., 2018) to incorporate cross-lingual supervision from parallel data to learn cross-lingual language models (XLMs), which have obtained state-of-the-art results on several cross-lingual tasks.\n[BOS] Similar to our approach, Schuster et al. (2019) also aligned pretrained contextualized word embeddings through linear transformation in an off-line fashion.\n[BOS] They used the averaged contextualized embeddings as an anchor for each word type, and learn a transformation in the anchor space.\n[BOS] Our approach, however, learns this transformation directly in the contextual space, and hence is explicitly designed to be word sense-preserving.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 27, "token_end": 70, "char_start": 157, "char_end": 348, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Mulcaire et al. (2019)": "67855733"}, "Reference": {"(Peters et al., 2018)": "3626819"}}}, {"token_start": 71, "token_end": 137, "char_start": 355, "char_end": 617, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Lample and Conneau (2019)": null}, "Reference": {"(Devlin et al., 2018)": "52967399"}}}, {"token_start": 143, "token_end": 195, "char_start": 649, "char_end": 916, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Schuster et al. (2019)": "67856005"}, "Reference": {}}}]}
{"id": "202578048_0", "paragraph": "[BOS] Static cross-lingual embedding learning methods can be roughly categorized as on-line and off-line methods.\n[BOS] Typically, on-line approaches integrate monolingual and cross-lingual objectives to learn cross-lingual word embeddings in a joint manner (Klementiev et al., 2012; Koisk et al., 2014; Guo et al., 2016) , while off-line approaches take pretrained monolingual word embeddings of different languages as input and retrofit them into a shared semantic space (Xing et al., 2015; Lample et al., 2018; Chen and Cardie, 2018) .\n\n", "discourse_tags": ["Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 46, "token_end": 78, "char_start": 224, "char_end": 321, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Klementiev et al., 2012;": "6758088", "Ko\u010disk\u00fd et al., 2014;": "5809776"}}}, {"token_start": 102, "token_end": 129, "char_start": 451, "char_end": 536, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Xing et al., 2015;": "3144258", "Lample et al., 2018;": "3470398"}}}]}
{"id": "174800145_4", "paragraph": "[BOS] There are also approaches to learn sentimental embeddings in the bilingual space without any sentiment resources in the target language.\n[BOS] Barnes et al. (2018) jointly minimized an alignment objective based on a seed dictionary, and a classification objective based on the sentiment corpus.\n[BOS] Its performance is compared to our method in Section 4.\n[BOS] Xu and Wan (2017) learned multilingual sentimental embeddings by extending the BiSkip model (Luong et al., 2015) .\n[BOS] However, their method does not apply to pretrained embeddings and requires large-scale parallel corpora thus is not included in our experiments.\n\n", "discourse_tags": ["Transition", "Single_summ", "Reflection", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 25, "token_end": 53, "char_start": 149, "char_end": 300, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Barnes et al. (2018)": "43943649"}, "Reference": {}}}, {"token_start": 66, "token_end": 122, "char_start": 369, "char_end": 634, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Xu and Wan (2017)": "593447"}, "Reference": {"(Luong et al., 2015)": "13603998"}}}]}
{"id": "174800145_3", "paragraph": "[BOS] Sentimental Embeddings Continuous word representations encode the syntactic context of a word but often ignore the information of sentiment polarity.\n[BOS] This drawback makes them hard to distinguish words with similar syntactic context but opposite sentiment polarity (e.g. good and bad), resulting in unsatisfactory performance on sentiment analysis.\n[BOS] Tang et al. (2014) learned word representations that encode both syntactic context and sentiment polarity by adding an objective to classify the polarity of an n-gram.\n[BOS] This method can be generalized to the cross-lingual setting by training monolingual sentimental embeddings on both languages then aligning them in a common space.\n[BOS] However, it requires sentiment resources in the target language thus is impractical for low-resource languages.\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 61, "token_end": 145, "char_start": 366, "char_end": 820, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Tang et al. (2014)": "886027"}, "Reference": {}}}]}
{"id": "174800145_2", "paragraph": "[BOS] Multilingual Word Embeddings BWE methods can be extended to the case of multiple languages by simply mapping all the languages to the vector space of a selected language.\n[BOS] However, directly learning multilingual word embeddings (MWE) in a shared space has been shown to improve performance (Ammar et al., 2016; Duong et al., 2017; Chen and Cardie, 2018; Alaux et al., 2018 ).\n[BOS] Yet, all these approaches are mainly evaluated on word translation and their effectiveness on cross-lingual sentiment analysis have not been empirically compared.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 36, "token_end": 87, "char_start": 192, "char_end": 383, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ammar et al., 2016;": "1227830", "Duong et al., 2017;": "17908320", "Chen and Cardie, 2018;": "52099904", "Alaux et al., 2018": "53216389"}}}]}
{"id": "174800145_1", "paragraph": "[BOS] Bilingual Word Embeddings Word embeddings trained separately on two languages can be aligned in a shared space to produce Bilingual Word Embeddings (BWE), which support many NLP tasks including machine translation , cross-lingual sentiment analysis (Barnes et al., 2018; Zhou et al., 2015) and crosslingual dependency parsing (Guo et al., 2015) .\n[BOS] BWE can be obtained in a supervised way using a seed dictionary Artetxe et al., 2016) , or in an unsupervised way without any bilingual data.\n[BOS] Adversarial training was the first successful attempt to learn unsupervised BWE (Zhang et al., 2017; .\n[BOS] Selflearning was proposed by (Artetxe et al., 2017) to learn BWE with minimum bilingual resources, which was later extended into a fully unsupervised framework by adding an unsupervised dictionary initialization step (Artetxe et al., 2018) .\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Narrative_cite", "Single_summ"], "span_citation_mapping": [{"token_start": 39, "token_end": 60, "char_start": 222, "char_end": 295, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Barnes et al., 2018;": "43943649", "Zhou et al., 2015)": "17864960"}}}, {"token_start": 61, "token_end": 74, "char_start": 300, "char_end": 350, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Guo et al., 2015)": "18634877"}}}, {"token_start": 87, "token_end": 98, "char_start": 407, "char_end": 444, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Artetxe et al., 2016)": "1040556"}}}, {"token_start": 120, "token_end": 130, "char_start": 570, "char_end": 606, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 133, "token_end": 183, "char_start": 616, "char_end": 855, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Artetxe et al., 2017)": "13335042"}, "Reference": {"(Artetxe et al., 2018)": "21728524"}}}]}
{"id": "174800145_0", "paragraph": "[BOS] Cross-Lingual Sentiment Analysis Existing approaches for cross-lingual sentiment analysis can be mainly divided into two categories: (i) approaches that rely on machines translation (MT) systems (ii) approaches that rely on cross-lingual word embeddings.\n[BOS] Standard MT-based approaches perform crosslingual sentiment analysis by translating the sentiment data into a selected language (e.g. English).\n[BOS] More sophisticated algorithms including co-training (Wan, 2009; Demirtas and Pechenizkiy, 2013 ) and multi-view learning (Xiao and Guo, 2012) have been shown to improve performance.\n[BOS] Zhou et al. (2015 Zhou et al. ( , 2016b performed crosslingual sentiment analysis by learning bilingual document representations.\n[BOS] These methods translate each document into the other language and enforce a bilingual constraint between the original document and the translated version.\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite", "Multi_summ", "Multi_summ"], "span_citation_mapping": [{"token_start": 86, "token_end": 105, "char_start": 457, "char_end": 511, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wan, 2009;": "3135458", "Demirtas and Pechenizkiy, 2013": null}}}, {"token_start": 107, "token_end": 118, "char_start": 518, "char_end": 558, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Xiao and Guo, 2012)": null}}}, {"token_start": 126, "token_end": 176, "char_start": 605, "char_end": 895, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhou et al. (2015": "17864960", "Zhou et al. ( , 2016b": "14043561"}, "Reference": {}}}]}
{"id": "184483883_1", "paragraph": "[BOS] The disambiguation step has been tackled previously using both supervised models and unsupervised heuristic based approaches.\n[BOS] For example, Turton (2008) presented a rule based system for disambiguating locations from PubMed abstracts.\n[BOS] Weissenbacher et al. (2015) presented results from P opulation and Distance heuristics (discussed in Section 4.3) for the disambiguation task on PubMed articles.\n[BOS] The authors also presented an SVM model with population, distance and set of meta-data as input which achieved higher performance than both the individual heuristics.\n[BOS] Gritta et al. (2018a) used a feedforward neural network approach for the disambiguation of geolocations.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 25, "token_end": 43, "char_start": 151, "char_end": 246, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Turton (2008)": "7759359"}, "Reference": {}}}, {"token_start": 44, "token_end": 110, "char_start": 253, "char_end": 587, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Weissenbacher et al. (2015)": "8231797"}, "Reference": {}}}, {"token_start": 111, "token_end": 134, "char_start": 594, "char_end": 698, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Gritta et al. (2018a)": "51870078"}, "Reference": {}}}]}
{"id": "184483883_0", "paragraph": "[BOS] Toponym detection and resolution has been widely studied, and various systems (Gritta et al., 2018b) have been proposed for these tasks.\n[BOS] Toponym detection has been implemented on texts from various sources like social media (Karagoz et al., 2016) , PubMed articles (Magge et al., 2018) etc.\n[BOS] Various named entity recognition (NER) systems including rule-based (Gritta et al., 2018b) , machine learningbased (Karagoz et al., 2016) , and deep learningbased (Magge et al., 2018) have been implemented for detecting toponyms.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 2, "token_end": 25, "char_start": 6, "char_end": 106, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Gritta et al., 2018b)": null}}}, {"token_start": 45, "token_end": 57, "char_start": 223, "char_end": 258, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Karagoz et al., 2016)": null}}}, {"token_start": 58, "token_end": 69, "char_start": 261, "char_end": 297, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Magge et al., 2018)": "49483045"}}}, {"token_start": 81, "token_end": 94, "char_start": 366, "char_end": 399, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Gritta et al., 2018b)": null}}}, {"token_start": 95, "token_end": 108, "char_start": 402, "char_end": 446, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Karagoz et al., 2016)": null}}}, {"token_start": 110, "token_end": 122, "char_start": 453, "char_end": 492, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Magge et al., 2018)": "49483045"}}}]}
{"id": "201645145_2", "paragraph": "[BOS] Another research direction that is relevant to our work is neural network pruning.\n[BOS] Frankle and Carbin (2018) showed that widely used complex architectures suffer from overparameterization, and can be significantly reduced in size without a loss in performance.\n[BOS] Goldberg (2019) observed that the smaller version of BERT achieves better scores on a number of syntax-testing experiments than the larger one.\n[BOS] Adhikari et al. (2019) questioned the necessity of computation-heavy neural networks, proving that a simple yet carefully tuned BiLSTM without attention achieves the best or at least competitive results compared to more complex architectures on the document classification task.\n[BOS] Wu et al. (2019) presented more evidence of unnecessary complexity of the self-attention mechanism, and proposed a more lightweight and scalable dynamic convolution-based architecture that outperforms the self-attention baseline.\n[BOS] These studies suggest a potential direction for future research, and are in good accordance with our observations.\n\n", "discourse_tags": ["Reflection", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 17, "token_end": 50, "char_start": 95, "char_end": 272, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Frankle and Carbin (2018)": "53388625"}, "Reference": {}}}, {"token_start": 51, "token_end": 79, "char_start": 279, "char_end": 422, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Goldberg (2019)": "58007068"}, "Reference": {}}}, {"token_start": 80, "token_end": 130, "char_start": 429, "char_end": 707, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Adhikari et al. (2019)": "133597585"}, "Reference": {}}}, {"token_start": 131, "token_end": 172, "char_start": 714, "char_end": 943, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wu et al. (2019)": "59310641"}, "Reference": {}}}]}
{"id": "201645145_1", "paragraph": "[BOS] Our work contributes to the above discussion, but rather than examining representations extracted from different layers, we focus on the understanding of the self-attention mechanism itself, since it is the key feature of Transformer-based models.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "201645145_0", "paragraph": "[BOS] There have been several recent attempts to assess BERT's ability to capture structural properties of language.\n[BOS] Goldberg (2019) demonstrated that BERT consistently assigns higher scores to the correct verb forms as opposed to the incorrect one in a masked language modeling task, suggesting some ability to model subject-verb agreement.\n[BOS] Jawahar et al. (2019) extended this work to using multiple layers and tasks, supporting the claim that BERT's intermediate layers capture rich linguistic information.\n[BOS] On the other hand, Tran et al. (2018) concluded that LSTMs generalize to longer sequences better, and are more robust with respect to agreement distractors, compared to Transformers.\n[BOS] Liu et al. (2019) investigated the transferability of contextualized word representations to a number of probing tasks requiring linguistic knowledge.\n[BOS] Their findings suggest that (a) the middle layers of Transformer-based architectures are the most transferable to other tasks, and (b) higher layers of Transformers are not as task specific as the ones of RNNs.\n[BOS] Tang et al. (2018) argued that models using self-attention outperform CNN-and RNN-based models on a word sense disambiguation task due to their ability to extract semantic features from text.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 22, "token_end": 62, "char_start": 123, "char_end": 347, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Goldberg (2019)": "58007068"}, "Reference": {}}}, {"token_start": 63, "token_end": 97, "char_start": 354, "char_end": 520, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Jawahar et al. (2019)": "195477534"}, "Reference": {}}}, {"token_start": 103, "token_end": 136, "char_start": 546, "char_end": 709, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Tran et al. (2018)": "3785155"}, "Reference": {}}}, {"token_start": 137, "token_end": 209, "char_start": 716, "char_end": 1083, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 210, "token_end": 251, "char_start": 1090, "char_end": 1281, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Tang et al. (2018)": "52100282"}, "Reference": {}}}]}
{"id": "102353583_1", "paragraph": "[BOS] Comparable Corpora Mining Comparable corpora mining aims at extracting parallel sentences from comparable monolingual corpora such as news stories written on the same topic in different languages.\n[BOS] Most of the previous methods align the documents based on metadata and then extract parallel sentences using humandefined features Marcu, 2002, 2006; Hewavitharana and Vogel, 2011) .\n[BOS] Recent neural-based methods (Chu et al., 2016; Grover and Mitra, 2017; Grgoire and Langlais, 2018) learn to identify parallel sentences in the semantic spaces.\n[BOS] However, these methods require large amounts of parallel sentence pairs to train the systems first and then test the performance on raw comparable corpora, which does not apply to languages with limited resources.\n[BOS] Instead, we explore the corpora mining in an unsupervised fashion and propose a joint training framework with machine translation.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Multi_summ", "Multi_summ", "Reflection"], "span_citation_mapping": [{"token_start": 49, "token_end": 69, "char_start": 318, "char_end": 389, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Hewavitharana and Vogel, 2011)": "9733134"}}}, {"token_start": 72, "token_end": 149, "char_start": 405, "char_end": 777, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Chu et al., 2016;": "31263144", "Grover and Mitra, 2017;": "27891001", "Gr\u00e9goire and Langlais, 2018)": "49207635"}, "Reference": {}}}]}
{"id": "102353583_0", "paragraph": "[BOS] Unsupervised NMT The current NMT systems (Sutskever et al., 2014; Cho et al., 2014a; Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017 ) are known to easily overfit and result in an inferior performance when the training data is limited (Koehn and Knowles, 2017; Isabelle et al., 2017; Sennrich, 2017) .\n[BOS] Many research efforts have been spent on how to utilize the monolingual data to improve the NMT system when only limited supervision is available (Gulcehre et al., 2015; Sennrich et al., 2016a; He et al., 2016; Zhang and Zong, 2016; .\n[BOS] Recently, Lample et al. (2018a) ; Artetxe et al. (2018) ; Lample et al. (2018b) make encouraging progress on unsupervised NMT structure mainly based on initialization, denoising language modeling, and back-translation.\n[BOS] However, all these unsupervised models are based on the back-translation learning framework to generate pseudo language pairs for training.\n[BOS] Our work leverages the information from real target language sentences.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Multi_summ", "Multi_summ", "Reflection"], "span_citation_mapping": [{"token_start": 7, "token_end": 54, "char_start": 35, "char_end": 156, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sutskever et al., 2014;": "7961699", "Cho et al., 2014a;": null, "Bahdanau et al., 2015;": "11212020", "Gehring et al., 2017;": "3648736", "Vaswani et al., 2017": "13756489"}}}, {"token_start": 69, "token_end": 98, "char_start": 234, "char_end": 323, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Koehn and Knowles, 2017;": "8822680", "Isabelle et al., 2017;": "5147501", "Sennrich, 2017)": "3085700"}}}, {"token_start": 109, "token_end": 160, "char_start": 380, "char_end": 563, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Gulcehre et al., 2015;": "15352384", "Sennrich et al., 2016a;": "15600925", "He et al., 2016;": "5758868"}}}, {"token_start": 165, "token_end": 240, "char_start": 583, "char_end": 937, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Lample et al. (2018a)": "3518190", "Artetxe et al. (2018)": "3515219", "Lample et al. (2018b)": "5033497"}, "Reference": {}}}]}
{"id": "173990530_1", "paragraph": "[BOS] The technique of reinforcement learning has been applied to multi-turn dialogue systems in several scenarios.\n[BOS] In RL-DG (Li et al., 2016b) , three rewards are defined and combined together to boost diverse response generation.\n[BOS] Due to a lack of effective control on knowledge utilization, RL-DG is unable to express extensive information during conversations.\n[BOS] As RL-DG relies on the reinforcement signal to update all components in the dialogue system, including decoder, it suffers from poor linguistic quality.\n[BOS] In Yao et al. (2018) , reinforcement learning is employed to plan a cue word (topic) path for a dialogue, where the cue word at t-th turn will assist the corresponding response generation.\n[BOS] Different from these chitchat approaches, our dialogue generation is conducted under the objective of facilitating effective information exchange and letting both participates know more about each.\n[BOS] With judiciously design of evaluation metrics, our compound reward is aligned well with human beings and provides meaningful reinforcement signal to evolve the dialogue strategy.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Transition", "Transition", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 22, "token_end": 48, "char_start": 125, "char_end": 237, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Li et al., 2016b)": "3147007"}, "Reference": {}}}, {"token_start": 103, "token_end": 144, "char_start": 544, "char_end": 729, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Yao et al. (2018)": "53722393"}, "Reference": {}}}]}
{"id": "173990530_0", "paragraph": "[BOS] Our work is related with knowledge grounded response generation and multi-turn conversation with reinforcement learning.\n[BOS] As conventional Seq2Seq (Vinyals and Le, 2015) tends to generate general and dull re-sponses, some knowledge grounded approaches have been introduced to increase the informativeness with extra knowledge.\n[BOS] MemNet (Ghazvininejad et al., 2018) encodes factual texts into memory and decodes via attention mechanism for informative generation.\n[BOS] CCM (Zhou et al., 2018) relies on structured knowledge to generate rich-information response.\n[BOS] In Lian et al. (2019) , the posterior distribution is estimated and accurate knowledge is selected to boost informative generation.\n[BOS] However, without thorough consideration and control on the knowledge utilization in multi-turn conversations, the above approaches are prone to produce repetitive and incoherent utterances.\n\n", "discourse_tags": ["Reflection", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 23, "token_end": 63, "char_start": 149, "char_end": 336, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Vinyals and Le, 2015)": "12300158"}, "Reference": {}}}, {"token_start": 64, "token_end": 95, "char_start": 343, "char_end": 476, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Ghazvininejad et al., 2018)": "15442925"}, "Reference": {}}}, {"token_start": 96, "token_end": 116, "char_start": 483, "char_end": 576, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Zhou et al., 2018)": "51608183"}, "Reference": {}}}, {"token_start": 118, "token_end": 143, "char_start": 586, "char_end": 714, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Lian et al. (2019)": "61153667"}, "Reference": {}}}]}
{"id": "202541185_5", "paragraph": "[BOS] Sequence Generation Models.\n[BOS] Recently, sequence generation models have been successfully applied in the realm of multi-label classification (MLC) (Yang et al., 2018) .\n[BOS] Different from traditional binary relevance methods, they proposed a sequence generation model for MLC tasks which takes into consideration the correlations between labels.\n[BOS] Specifically, the model follows the encoder-decoder structure with an attention mechanism (Cho et al., 2014) , where the decoder generates a sequence of labels.\n[BOS] Similar to language modeling tasks, the decoder output at each time step will be conditioned on the previous predictions during generation.\n[BOS] Therefore the correlation between generated labels is captured by the decoder.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 9, "token_end": 134, "char_start": 50, "char_end": 755, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Yang et al., 2018)": "49191384"}, "Reference": {"(Cho et al., 2014)": "5590763"}}}]}
{"id": "202541185_4", "paragraph": "[BOS] Contextualized Word Embedding (CWE) was first proposed by Peters et al. (2018) .\n[BOS] Based on the intuition that the meaning of a word is highly correlated with its context, CWE takes the complete context (sentences, passages, etc.)\n[BOS] as the input, and outputs the corresponding word vectors that are unique under the given context.\n[BOS] Recently, with the success of language models (e.g. Devlin et al. (2018) ) that are trained on large scale data, contextualizeds word embedding have been further improved and can achieve the same performance compared to (less flexible) finely-tuned pipelines.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 2, "token_end": 73, "char_start": 6, "char_end": 344, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Peters et al. (2018)": null}, "Reference": {}}}, {"token_start": 80, "token_end": 103, "char_start": 381, "char_end": 462, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Devlin et al. (2018)": "52967399"}}}]}
{"id": "202541185_3", "paragraph": "[BOS] DSTRead (Gao et al., 2019) formulate the dialogue state tracking task as a reading comprehension problem by asking slot specified questions to the BERT model and find the answer span in the dialogue history for each of the pre-defined combined slot.\n[BOS] Thus its inference time complexity is still O(n).\n[BOS] This method suffers from the fact that its generation vocabulary is limited to the words occurred in the dialogue history, and it has to do a manual combination strategy with another joint state tracking model on the development set to achieve better performance.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 112, "char_start": 6, "char_end": 581, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "202541185_2", "paragraph": "[BOS] TRADE (Wu et al., 2019) achieves state-of-theart performance on the MultiWoZ dataset by applying the copy mechanism for the value sequence generation.\n[BOS] Since TRADE takes n combinations of the domains and slots as the input, the inference time complexity of TRADE is O(n).\n[BOS] The performance improvement achieved by TRADE is mainly due to the fact that it incorporates the copy mechanism that can boost the accuracy on the name slot, which mainly needs the ability in copying names from the dialogue history.\n[BOS] However, TRADE does not report its performance on the WoZ2.0 dataset which does not have the name slot.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 2, "token_end": 64, "char_start": 6, "char_end": 282, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Wu et al., 2019)": "160009896"}, "Reference": {}}}]}
{"id": "202541185_1", "paragraph": "[BOS] StateNet (Ren et al., 2018) achieves state-ofthe-art performance with the property that its parameters are independent of the number of slot values in the candidate set, and it also supports online training or inference with dynamically changing slots and values.\n[BOS] Given a slot that needs tracking, it only needs to perform inference once to make the prediction for a turn, but this also means that its inference time complexity is proportional to the number of slots.\n\n", "discourse_tags": ["Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 93, "char_start": 6, "char_end": 479, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Ren et al., 2018)": "53035038"}, "Reference": {}}}]}
{"id": "202541185_0", "paragraph": "[BOS] Semi-scalable Belief Tracker Rastogi et al. (2017) proposed an approach that can generate fixed-length candidate sets for each of the slots from the dialogue history.\n[BOS] Although they only need to perform inference for a fixed number of values, they still need to iterate over all slots defined in the ontology to make a prediction for a given dialogue turn.\n[BOS] In addition, their method needs an external language understanding module to extract the exact entities from a dialogue to form candidates, which will not work if the label value is an abstraction and does not have the exact match with the words in the dialogue.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 124, "char_start": 6, "char_end": 636, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Rastogi et al. (2017)": "13780"}, "Reference": {}}}]}
{"id": "207999578_1", "paragraph": "[BOS] Pseudo-parallel data can be used to augment existing parallel corpora for training, and previous work has reported that such data generated by so-called back-translation can substantially improve the quality of NMT (Sennrich et al., 2016) .\n[BOS] However, this approach requires base MT systems that can generate somewhat accurate translations (Imankulova et al., 2017) .\n[BOS] Therefore, instead of creating noisy pseudo-parallel corpora, we take advantage of other in-domain pivot parallel corpora.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 23, "token_end": 51, "char_start": 126, "char_end": 244, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sennrich et al., 2016)": "845121"}}}, {"token_start": 58, "token_end": 78, "char_start": 285, "char_end": 375, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Imankulova et al., 2017)": "5240984"}}}]}
{"id": "207999578_0", "paragraph": "[BOS] The existing state-of-the-art NMT model known as the Transformer (Vaswani et al., 2017) works well on different scenarios (Lakew et al., 2018; Imankulova et al., 2019) .\n[BOS] MultiNMT using the artificial token approach (Johnson et al., 2017) is known to help the language pairs with relatively lesser data (Lakew et al., 2018; Rikters et al., 2018) and outperform bi-directional and uni-directional translation approaches (Imankulova et al., 2019) .\n[BOS] Similarly, we exploit MultiNMT approach with Transformer architecture.\n[BOS] Our work is heavily based on Imankulova et al. (2019) .\n[BOS] They proposed a multi-stage fine-tuning approach that combines multilingual modeling and domain adaptation.\n[BOS] They utilize out-of-domain pivot parallel corpora to perform domain adaptation on in-domain pivot parallel corpora and then perform multilingual transfer for a language pair of interest.\n[BOS] However, instead of utilizing out-ofdomain pivot parallel corpora, we investigate the impact of other in-domain pivot parallel corpora.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Reflection", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 17, "token_end": 28, "char_start": 59, "char_end": 93, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vaswani et al., 2017)": "13756489"}}}, {"token_start": 31, "token_end": 53, "char_start": 108, "char_end": 173, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lakew et al., 2018;": null, "Imankulova et al., 2019)": "195833172"}}}, {"token_start": 55, "token_end": 70, "char_start": 182, "char_end": 249, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Johnson et al., 2017)": "6053988"}}}, {"token_start": 75, "token_end": 99, "char_start": 271, "char_end": 356, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lakew et al., 2018;": null, "Rikters et al., 2018)": "21732361"}}}, {"token_start": 101, "token_end": 122, "char_start": 372, "char_end": 455, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Imankulova et al., 2019)": "195833172"}}}, {"token_start": 142, "token_end": 211, "char_start": 570, "char_end": 903, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Imankulova et al. (2019)": "195833172"}, "Reference": {}}}]}
{"id": "202778569_1", "paragraph": "[BOS] On the other hand, many attempts have also been made to improve the architecture of Seq2Seq models by changing the training methods.\n[BOS] Li et al. (2016a) attributed safe response problems to the use of MLE objective.\n[BOS] Some works separately attempted to replace the MLE method with maximum mutual information (Li et al., 2016a) , reinforcement learning Li et al., 2016c) and adversarial learning (Xu et al., 2017; Li et al., 2017a) .\n[BOS] Serban et al. (2017b) viewed the dialog context as prior knowledge and combined HRED model into the CVAE framework.\n[BOS] Zhao et al. (2017) further introduced dialog acts to guide the learning of CVAE.\n[BOS] In our paper, we use CVAE to learn the hierarchy generation model.\n\n", "discourse_tags": ["Transition", "Single_summ", "Narrative_cite", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 29, "token_end": 49, "char_start": 145, "char_end": 225, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Li et al. (2016a)": "7287895"}, "Reference": {}}}, {"token_start": 61, "token_end": 73, "char_start": 295, "char_end": 340, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Li et al., 2016a)": "7287895"}}}, {"token_start": 74, "token_end": 84, "char_start": 343, "char_end": 383, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Li et al., 2016c)": "3147007"}}}, {"token_start": 85, "token_end": 104, "char_start": 388, "char_end": 444, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Xu et al., 2017;": "11030403", "Li et al., 2017a)": "98180"}}}, {"token_start": 106, "token_end": 133, "char_start": 453, "char_end": 568, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Serban et al. (2017b)": "14857825"}, "Reference": {}}}, {"token_start": 134, "token_end": 153, "char_start": 575, "char_end": 655, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhao et al. (2017)": "14688760"}, "Reference": {}}}]}
{"id": "202778569_0", "paragraph": "[BOS] Vanilla Seq2Seq model usually ends up with generic and dull responses.\n[BOS] To tackle this problem, one line of research has focused on forcing the model to imitate some human's skills by augmenting the input with rich meta information.\n[BOS] For example, some works separately gave chatbots the ability of emotions (Zhou et al., 2018) , persona (Li et al., 2016b) , vision (Huber et al., 2018; Wu et al., 2018) and thinking over the knowledge base Zhu et al., 2017) .\n[BOS] In this work, we consider open domain dialogue generation with dialog acts.\n[BOS] But, only a little works (Zhao et al., 2017; Serban et al., 2017a) on open domain endto-end modeling take dialog acts into account.\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite", "Reflection", "Multi_summ"], "span_citation_mapping": [{"token_start": 66, "token_end": 75, "char_start": 314, "char_end": 342, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhou et al., 2018)": "2024574"}}}, {"token_start": 76, "token_end": 87, "char_start": 345, "char_end": 371, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Li et al., 2016b)": "2955580"}}}, {"token_start": 88, "token_end": 105, "char_start": 374, "char_end": 418, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Huber et al., 2018;": null, "Wu et al., 2018)": "24537813"}}}, {"token_start": 106, "token_end": 118, "char_start": 423, "char_end": 473, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Zhu et al., 2017)": "35440547"}}}, {"token_start": 135, "token_end": 172, "char_start": 564, "char_end": 695, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Zhao et al., 2017;": "14688760"}, "Reference": {}}}]}
{"id": "199379304_2", "paragraph": "[BOS] However, in AM, seldom works directly combine lexicon with models.\n[BOS] By using discourse feature, Levy et al. (2018) generates weak labels and use weak supervision.\n[BOS] Shnarch et al. (2018) also present a methodology to blend such weak labeled data with high quality but scarce labeled data for AM.\n[BOS] Al-Khatib et al. (2016) consider the distant supervision method.\n[BOS] Most of these works use the end-to-end training paradigm with the outside resources only for generating the weak label, which may not fully leverage the information of the lexicons.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 17, "token_end": 36, "char_start": 82, "char_end": 173, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Levy et al. (2018)": "52011877"}, "Reference": {}}}, {"token_start": 37, "token_end": 66, "char_start": 180, "char_end": 310, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Shnarch et al. (2018)": "51873028"}, "Reference": {}}}, {"token_start": 67, "token_end": 84, "char_start": 317, "char_end": 381, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Al-Khatib et al. (2016)": null}, "Reference": {}}}]}
{"id": "199379304_1", "paragraph": "[BOS] Besides syntactic and positional information, lexical information is also reported as one of the most used features in argument mining task (Cabrio and Villata, 2018) .\n[BOS] In some similar research fields such as sentiment analysis and emotion mining, a number of works have been proposed to combine lexical information with the NN models.\n[BOS] Teng et al. (2016) use lexical scores as the weights and do the weighted sum over the outputs of the LSTM model, in order to derive the sentence scores.\n[BOS] Zou et al. (2018) determines attention weights using lexicon labels, which lead the model to focus on the lexicon words.\n[BOS] Bar-Haim et al. (2017) proposes an idea of expanding lexicons to improve stance classifying task.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 8, "token_end": 33, "char_start": 52, "char_end": 172, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cabrio and Villata, 2018)": "51609977"}}}, {"token_start": 65, "token_end": 101, "char_start": 354, "char_end": 506, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Teng et al. (2016)": "7356424"}, "Reference": {}}}, {"token_start": 102, "token_end": 128, "char_start": 513, "char_end": 633, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zou et al. (2018)": "51989536"}, "Reference": {}}}, {"token_start": 129, "token_end": 152, "char_start": 640, "char_end": 737, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Bar-Haim et al. (2017)": "3204162"}, "Reference": {}}}]}
{"id": "199379304_0", "paragraph": "[BOS] Neural networks have been used in varieties of AM tasks.\n[BOS] To improve the vanilla LSTM model, Stab et al. (2018a) use attention mechanism to fuse topic and sentence information together.\n[BOS] In the work of Laha and Raykar (2016) , they present several bi-sequence classification models on different datasets.\n[BOS] However, rather than using some sophisticated architecture such as attention, it considers only different concatenation or condition method on the output of LSTM.\n[BOS] Eger et al. (2017) propose an end-to-end training model to mining argument structure, identifying argument components.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 15, "token_end": 43, "char_start": 72, "char_end": 196, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Stab et al. (2018a)": "3339724"}, "Reference": {}}}, {"token_start": 48, "token_end": 97, "char_start": 218, "char_end": 489, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Laha and Raykar (2016)": "8507616"}, "Reference": {}}}, {"token_start": 98, "token_end": 124, "char_start": 496, "char_end": 614, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Eger et al. (2017)": "3221856"}, "Reference": {}}}]}
{"id": "203610517_1", "paragraph": "[BOS] Our work was inspired by .\n[BOS] However, in this work we have focused on similar issues in neural machine translation which is has different challenges compared to text classification in terms of objective and architecture.\n[BOS] Moreover, our paper studies the effect of different counterfactual attention methods.\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "203610517_0", "paragraph": "[BOS] Relevance-based interpretation is a common technique in analyzing predictions in neural models.\n[BOS] In this method, inputs of a predictor are assigned a scalar value quantifying the importance of that particular input on the final decision.\n[BOS] Saliency methods use the gradient of the inputs to define importance (Li et al., 2016a; Ghaeini et al., 2018; Ding et al., 2019) .\n[BOS] Layer-wise relevance propagation that assigns relevance to neurons based on their contribution to activation of higher-layer neurons is also investigated in NLP (Arras et al., 2016; Ding et al., 2017; Arras et al., 2017) .\n[BOS] Another method to measure relevance is by removing the input, and tracking the difference in in network's output (Li et al., 2016b) .\n[BOS] While these methods focus on explaining a model's decision, Shi et al. (2016) ; Kdr et al. (2017) ; Calvillo and Crocker (2018) investigate how a particular concept is represented in the network.\n[BOS] Analyzing and interpreting the attention mechanism in NLP (Koehn and Knowles, 2017; Ghader and Monz, 2017; Tang and Nivre, 2018; Clark et al., 2019; Vig and Belinkov, 2019) is another direction that has drawn major interest.\n[BOS] Although attention weights have been implicitly or explicitly used to explain a model's decisions, the reliability of this approach is not proven.\n[BOS] Several attempts have been made to investigate the reliability of this approach for explaining a models' decision in NLP (Serrano and Smith, 2019; Baan et al., 2019; , and also in information retrieval (Jain and Madhyastha, 2019) .\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite", "Narrative_cite", "Narrative_cite", "Multi_summ", "Narrative_cite", "Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 44, "token_end": 81, "char_start": 255, "char_end": 383, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Li et al., 2016a;": "14099741", "Ghaeini et al., 2018;": "51979567", "Ding et al., 2019)": "195584234"}}}, {"token_start": 83, "token_end": 134, "char_start": 392, "char_end": 612, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Arras et al., 2016;": "5247929", "Ding et al., 2017;": "27930067", "Arras et al., 2017)": "19624082"}}}, {"token_start": 143, "token_end": 166, "char_start": 663, "char_end": 752, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Li et al., 2016b)": "13017314"}}}, {"token_start": 180, "token_end": 219, "char_start": 821, "char_end": 956, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Shi et al. (2016)": "34975990", "K\u00e1d\u00e1r et al. (2017)": "611341", "Calvillo and Crocker (2018)": "51999256"}, "Reference": {}}}, {"token_start": 224, "token_end": 271, "char_start": 994, "char_end": 1135, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Koehn and Knowles, 2017;": "8822680", "Ghader and Monz, 2017;": "2389139", "Tang and Nivre, 2018;": null, "Clark et al., 2019;": "184486746"}}}, {"token_start": 327, "token_end": 347, "char_start": 1464, "char_end": 1511, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Serrano and Smith, 2019;": null}}}, {"token_start": 352, "token_end": 365, "char_start": 1527, "char_end": 1576, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "174802397_2", "paragraph": "[BOS] In fact, incorporating monolingual corpora into NMT has been an important topic (Sennrich et al., 2016b; Cheng et al., 2016; Edunov et al., 2018) .\n[BOS] There are also papers augmenting a standard dataset based on the parallel corpora by dropping words (Sennrich et al., 2016a) , replacing words (Wang et al., 2018) , editing rare words (Fadaee et al., 2017) , etc.\n[BOS] Different from these about data-augmentation techniques, our approach is only trained on parallel corpora and outperforms a representative data-augmentation work (Sennrich et al., 2016b) trained with extra monolingual data.\n[BOS] When monolingual data is included, our approach yields further improvements.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 5, "token_end": 43, "char_start": 15, "char_end": 151, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sennrich et al., 2016b;": null, "Cheng et al., 2016;": null, "Edunov et al., 2018)": "52113461"}}}, {"token_start": 60, "token_end": 73, "char_start": 245, "char_end": 284, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sennrich et al., 2016a)": "14919987"}}}, {"token_start": 74, "token_end": 84, "char_start": 287, "char_end": 322, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wang et al., 2018)": "52078335"}}}, {"token_start": 85, "token_end": 98, "char_start": 325, "char_end": 365, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Fadaee et al., 2017)": "3291104"}}}, {"token_start": 123, "token_end": 138, "char_start": 518, "char_end": 565, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sennrich et al., 2016b)": null}}}]}
{"id": "174802397_1", "paragraph": "[BOS] We noticed that Michel and Neubig (2018) proposed a dataset for testing the machine translation on noisy text.\n[BOS] Meanwhile they adopt a domain adaptation method to first train a NMT model on a clean dataset and then finetune it on noisy data.\n[BOS] This is different from our setting in which no noisy training data is available.\n[BOS] Another difference is that one of our primary goals is to improve NMT models on the standard clean test data.\n[BOS] This differs from Michel and Neubig (2018) whose goal is to improve models on noisy test data.\n[BOS] We leave the extension to their setting for future work.\n[BOS] Adversarial Examples Generation Our work is inspired by adversarial examples generation, a popular research area in computer vision, e.g. in (Szegedy et al., 2014; Goodfellow et al., 2015; Moosavi-Dezfooli et al., 2016) .\n[BOS] In NLP, many authors endeavored to apply similar ideas to a variety of NLP tasks, such as text classification (Miyato et al., 2017; Ebrahimi et al., 2018b) , machine comprehension (Jia and Liang, 2017) , dialogue generation (Li et al., 2017) , machine translation (Belinkov and Bisk, 2018) , etc.\n[BOS] Closely related to (Miyato et al., 2017) which attacked the text classification models in the embedding space, ours generates adversarial examples based on discrete word replacements.\n[BOS] The experiments show that ours achieve better performance on both clean and noisy data.\n[BOS] Data Augmentation Our approach can be viewed as a data-augmentation technique using adversarial examples.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Reflection", "Reflection", "Single_summ", "Reflection", "Narrative_cite", "Narrative_cite", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 5, "token_end": 53, "char_start": 22, "char_end": 252, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Michel and Neubig (2018)": "52155427"}, "Reference": {}}}, {"token_start": 96, "token_end": 114, "char_start": 480, "char_end": 556, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Michel and Neubig (2018)": "52155427"}, "Reference": {}}}, {"token_start": 136, "token_end": 187, "char_start": 682, "char_end": 845, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Szegedy et al., 2014;": null, "Goodfellow et al., 2015;": "6706414", "Moosavi-Dezfooli et al., 2016)": "12387176"}}}, {"token_start": 212, "token_end": 234, "char_start": 944, "char_end": 1009, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Miyato et al., 2017;": "12167053", "Ebrahimi et al., 2018b)": "21698802"}}}, {"token_start": 235, "token_end": 244, "char_start": 1012, "char_end": 1055, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Jia and Liang, 2017)": "7228830"}}}, {"token_start": 245, "token_end": 255, "char_start": 1058, "char_end": 1095, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Li et al., 2017)": "98180"}}}, {"token_start": 256, "token_end": 268, "char_start": 1098, "char_end": 1143, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Belinkov and Bisk, 2018)": "3513372"}}}, {"token_start": 275, "token_end": 324, "char_start": 1176, "char_end": 1434, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Miyato et al., 2017)": "12167053"}, "Reference": {}}}]}
{"id": "174802397_0", "paragraph": "[BOS] Robust Neural Machine Translation Improving robustness has been receiving increasing attention in NMT.\n[BOS] For example, Belinkov and Bisk (2018) ; ; Karpukhin et al. (2019) ; Sperber et al. (2017) focused on designing effective synthetic and/or natural noise for NMT using black-box methods.\n[BOS] Cheng et al. (2018) proposed adversarial stability training to improve the robustness on arbitrary noise type.\n[BOS] Ebrahimi et al. (2018a) used white-box methods to generate adversarial examples on character-level NMT.\n[BOS] Different from prior work, our work uses a white-box method for the word-level NMT model and introduces a new method using doubly adversarial inputs to both attach and defend the model.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 21, "token_end": 72, "char_start": 128, "char_end": 299, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Belinkov and Bisk (2018)": "3513372", "Karpukhin et al. (2019)": "59604474", "Sperber et al. (2017)": "21657379"}, "Reference": {}}}, {"token_start": 73, "token_end": 94, "char_start": 306, "char_end": 416, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Cheng et al. (2018)": "21715690"}, "Reference": {}}}, {"token_start": 95, "token_end": 123, "char_start": 423, "char_end": 526, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ebrahimi et al. (2018a)": "49413369"}, "Reference": {}}}]}
{"id": "208050387_4", "paragraph": "[BOS] Parallel Sentence Mining.\n[BOS] Automatic mining parallel sentences from comparable documents is an important and useful task to improve Statistical Machine Translation.\n[BOS] Early efforts mainly exploited bilingual word dictionaries for bootstrapping (Fung and Cheung, 2004) .\n[BOS] Recent approaches are mainly based on bilingual word embeddings (Marie and Fujita, 2017) and sentence embeddings (Schwenk, 2018) to detect sentence pairs or continuous parallel segments (Hangya and Fraser, 2019) .\n[BOS] To the best of our knowledge, this is the first work to incorporate joint entity and word embedding into parallel sentence mining.\n[BOS] As a result the sentence pairs we include reliable alignment between entity mentions which are often out-of-vocabulary and ambiguous and thus receive poor alignment quality from previous methods.\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite", "Narrative_cite", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 31, "token_end": 46, "char_start": 213, "char_end": 282, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Fung and Cheung, 2004)": "11740443"}}}, {"token_start": 54, "token_end": 65, "char_start": 329, "char_end": 379, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Marie and Fujita, 2017)": "27330583"}}}, {"token_start": 66, "token_end": 74, "char_start": 384, "char_end": 419, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Schwenk, 2018)": "44087711"}}}, {"token_start": 75, "token_end": 92, "char_start": 423, "char_end": 502, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hangya and Fraser, 2019)": "196202115"}}}]}
{"id": "208050387_3", "paragraph": "[BOS] Cross-lingual Joint Entity and Word Embedding Learning.\n[BOS] Previous work on cross-lingual joint entity and word embedding methods largely neglect unlinkable entities (Tsai and Roth, 2016) and heavily rely on parallel or comparable sentences (Cao et al., 2018) .\n[BOS] Tsai and Roth (2016) apply a similar approach to generate code-switched data from Wikipedia, but their framework does not keep entities in the source language.\n[BOS] Using all aligned entities as a dictionary, they adopt canonical correlation analysis to project two embedding spaces into one.\n[BOS] In contrast, we only choose salient entities as anchors to learn a linear mapping.\n[BOS] Cao et al. (2018) generate comparable data via distant supervision over multilingual knowledge bases, and use an entity regularizer and a sentence regularizer to align cross-lingual words and entities.\n[BOS] Further, they design knowledge attention and cross-lingual attention to refine the alignment.\n[BOS] Essentially, they train cross-lingual embedding jointly, while we align two embedding spaces that trained independently.\n[BOS] Moreover, compared to their approach that relies on comparable data, aligned entities are easier to acquire.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Single_summ", "Single_summ", "Reflection", "Single_summ", "Single_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 28, "token_end": 40, "char_start": 147, "char_end": 196, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Tsai and Roth, 2016)": "15156124"}}}, {"token_start": 44, "token_end": 56, "char_start": 217, "char_end": 268, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cao et al., 2018)": "53079158"}}}, {"token_start": 58, "token_end": 111, "char_start": 277, "char_end": 570, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Tsai and Roth (2016)": "15156124"}, "Reference": {}}}, {"token_start": 130, "token_end": 199, "char_start": 666, "char_end": 1029, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Cao et al. (2018)": "53079158"}, "Reference": {}}}]}
{"id": "208050387_2", "paragraph": "[BOS] Our work is largely inspired from (Conneau et al., 2017) .\n[BOS] However, our work focuses on better representing entities, which are fundamentally different from common words or phrases in many aspects as described in Section 1.\n[BOS] Previous multilingual word embedding efforts including (Conneau et al., 2017) do not explicitly handle entity representations.\n[BOS] Moreover, we perform comprehensive extrinsic evaluations based on down-stream NLP applications including cross-lingual entity linking and machine translation, while previous work on cross-lingual embedding only focused on intrinsic evaluations.\n\n", "discourse_tags": ["Reflection", "Reflection", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 8, "token_end": 18, "char_start": 40, "char_end": 62, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Conneau et al., 2017)": "3470398"}}}, {"token_start": 50, "token_end": 66, "char_start": 251, "char_end": 319, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Conneau et al., 2017)": "3470398"}}}]}
{"id": "208050387_1", "paragraph": "[BOS] Another strategy for cross-lingual word embedding learning is to combine monolingual and cross-lingual training objectives (Zou et al., 2013; Klementiev et al., 2012; Luong et al., 2015; Ammar et al., 2016; Vuli et al., 2017) .\n[BOS] Compared to our direct mapping approach, these methods generally require large size of parallel data.\n\n", "discourse_tags": ["Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 14, "token_end": 67, "char_start": 71, "char_end": 231, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zou et al., 2013;": "931054", "Klementiev et al., 2012;": "6758088", "Luong et al., 2015;": "13603998", "Ammar et al., 2016;": "1227830", "Vuli\u0107 et al., 2017)": "19718228"}}}]}
{"id": "208050387_0", "paragraph": "[BOS] Cross-lingual Word Embedding Learning.\n[BOS] Mikolov et al. (2013b) first notice that word embedding spaces have similar geometric arrangements across languages.\n[BOS] They use this property to learn a linear mapping between two spaces.\n[BOS] After that, several methods attempt to improve the mapping (Faruqui and Dyer, 2014; Xing et al., 2015; Lazaridou et al., 2015; Ammar et al., 2016; Artetxe et al., 2017; Smith et al., 2017) .\n[BOS] The measures used to compute similarity between a foreign word and an English word often include distributed monolingual representations on character-level (Costa-juss and Fonollosa, 2016; Luong and Manning, 2016) , subwordlevel (Anwarus Salam et al., 2012; Rei et al., 2016; Sennrich et al., 2016; , and bi-lingual word embedding (Madhyastha and Espaa-Bonet, 2017) .\n[BOS] Recent attempts have shown that it is possible to derive cross-lingual word embedding from unaligned corpora in an unsupervised fashion (Zhang et al., 2017; Conneau et al., 2017; Artetxe et al., 2018) .\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Narrative_cite", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 11, "token_end": 48, "char_start": 51, "char_end": 242, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Mikolov et al. (2013b)": "1966640"}, "Reference": {}}}, {"token_start": 58, "token_end": 112, "char_start": 300, "char_end": 437, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Faruqui and Dyer, 2014;": "3792324", "Xing et al., 2015;": "3144258", "Lazaridou et al., 2015;": "12187767", "Ammar et al., 2016;": "1227830", "Artetxe et al., 2017;": "13335042", "Smith et al., 2017)": "11591887"}}}, {"token_start": 135, "token_end": 158, "char_start": 586, "char_end": 659, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Costa-juss\u00e0 and Fonollosa, 2016;": "1712853", "Luong and Manning, 2016)": "13972671"}}}, {"token_start": 159, "token_end": 190, "char_start": 662, "char_end": 743, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Anwarus Salam et al., 2012;": null, "Rei et al., 2016;": "5075704"}}}, {"token_start": 193, "token_end": 213, "char_start": 751, "char_end": 811, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 238, "token_end": 266, "char_start": 935, "char_end": 1020, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhang et al., 2017;": "26873455", "Conneau et al., 2017;": "3470398"}}}]}
{"id": "174799974_0", "paragraph": "[BOS] Previous work already established that identity terms (e.g. gay, Jew or woman) have a bias to cooccur with abusive language Park et al., 2018) .\n[BOS] In this work, we showed that this problem is not restricted to the small set of identity terms.\n[BOS] Most biases are introduced by the sampling method used on a dataset and they have a huge impact on classification performance.\n\n", "discourse_tags": ["Narrative_cite", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 27, "token_end": 37, "char_start": 113, "char_end": 148, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Park et al., 2018)": "52070035"}}}]}
{"id": "102353391_2", "paragraph": "[BOS] A setting harder than zero-shot is that of fully unsupervised translation (Ravi and Knight, 2011; Artetxe et al., 2017; Lample et al., 2017 Lample et al., , 2018 in which no parallel data is available for training.\n[BOS] The ideas proposed in these works (e.g., bilingual dictionaries (Conneau et al., 2017) , backtranslation (Sennrich et al., 2015a ) and language models (He et al., 2016) ) are complementary to our approach, which encourages agreement among different translation directions in the zero-shot multilingual setting.\n\n", "discourse_tags": ["Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 12, "token_end": 48, "char_start": 49, "char_end": 167, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ravi and Knight, 2011;": "6060648"}}}, {"token_start": 71, "token_end": 84, "char_start": 268, "char_end": 313, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 85, "token_end": 98, "char_start": 316, "char_end": 355, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sennrich et al., 2015a": "15600925"}}}, {"token_start": 100, "token_end": 110, "char_start": 362, "char_end": 395, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(He et al., 2016)": "5758868"}}}]}
{"id": "102353391_1", "paragraph": "[BOS] Another family of approaches is based on distillation (Hinton et al., 2014; Kim and Rush, 2016) .\n[BOS] Along these lines, Firat et al. (2016b) proposed to fine tune a multilingual model to a specified zeroshot-direction with pseudo-parallel data and Chen et al. (2017) proposed a teacher-student framework.\n[BOS] While this can yield solid performance improvements, it also adds multi-staging overhead and often does not preserve performance of a single model on the supervised directions.\n[BOS] We note that our approach (and agreement-based learning in general) is somewhat similar to distillation at training time, which has been explored for large-scale single-task prediction problems (Anil et al., 2018) .\n\n", "discourse_tags": ["Narrative_cite", "Multi_summ", "Multi_summ", "Reflection"], "span_citation_mapping": [{"token_start": 9, "token_end": 28, "char_start": 47, "char_end": 101, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 34, "token_end": 63, "char_start": 129, "char_end": 252, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Firat et al. (2016b)": null}, "Reference": {}}}, {"token_start": 64, "token_end": 109, "char_start": 257, "char_end": 496, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 148, "token_end": 157, "char_start": 697, "char_end": 716, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "102353391_0", "paragraph": "[BOS] A simple (and yet effective) baseline for zero-shot translation is pivoting that chain-translates, first to a pivot language, then to a target (Cohn and Lapata, 2007; Wu and Wang, 2007; Utiyama and Isahara, 2007) .\n[BOS] Despite being a pipeline, pivoting gets better as the supervised models improve, which makes it a strong baseline in the zero-shot setting.\n[BOS] Cheng et al. (2017) proposed a joint pivoting learning strategy that leads to further improvements.\n[BOS] Lu et al. (2018) and Arivazhagan et al. (2018) proposed different techniques to obtain \"neural interlingual\" representations that are passed to the decoder.\n[BOS] Sestorain et al. (2018) proposed another fine-tuning technique that uses dual learning (He et al., 2016) , where a language model is used to provide a signal for fine-tuning zero-shot directions.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Single_summ", "Multi_summ", "Multi_summ"], "span_citation_mapping": [{"token_start": 19, "token_end": 59, "char_start": 87, "char_end": 218, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Wu and Wang, 2007;": "3681367", "Utiyama and Isahara, 2007)": "8030425"}}}, {"token_start": 90, "token_end": 110, "char_start": 373, "char_end": 472, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 111, "token_end": 148, "char_start": 479, "char_end": 635, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 149, "token_end": 196, "char_start": 642, "char_end": 837, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Sestorain et al. (2018)": null, "(He et al., 2016)": "5758868"}, "Reference": {}}}]}
{"id": "197465409_2", "paragraph": "[BOS] There are several more structured (and more complex) models proposed for relation extraction, e.g., tree-based recurrent neural networks (Socher et al., 2010) and tree LSTMs (Tai et al., 2015) .\n[BOS] Our semi-supervised framework is an orthogonal improvement, and it is flexible enough to potentially incorporate any of these more complex models.\n\n", "discourse_tags": ["Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 23, "token_end": 38, "char_start": 106, "char_end": 164, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Socher et al., 2010)": "9923502"}}}, {"token_start": 39, "token_end": 51, "char_start": 169, "char_end": 198, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Tai et al., 2015)": "3033526"}}}]}
{"id": "197465409_1", "paragraph": "[BOS] Both and Su et al. (2018) use neural networks to encode the words and dependencies along the shortest path between the two entities, and Liu et al. additionally encode the dependency subtrees of the words for additional context.\n[BOS] We include this representation (words and dependencies) in our experiments.\n[BOS] While the inclusion of the subtrees gives Liu et al. a slight performance boost, here we opt to focus only on the varying representations of the dependency path between the entities, without the additional context.\n[BOS] Su et al. (2018) use an LSTM to model the shortest path between the entities, but keep their lexical and syntactic sequences in separate channels (and they have other channels for additional information such as part of speech (POS)).\n[BOS] Rather than maintaining distinct channels for the different representations, here we elect to keep both surface and syntactic forms in the same sequence and instead experiment with different degrees of syntactic representation.\n[BOS] We also do not include other types of information (e.g., POS) here, as it is beyond the scope of the current work.\n\n", "discourse_tags": ["Single_summ", "Reflection", "Reflection", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 4, "token_end": 28, "char_start": 15, "char_end": 137, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Su et al. (2018)": "182616"}, "Reference": {}}}, {"token_start": 30, "token_end": 47, "char_start": 143, "char_end": 234, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 64, "token_end": 74, "char_start": 333, "char_end": 375, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 103, "token_end": 152, "char_start": 544, "char_end": 777, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Su et al. (2018)": "182616"}, "Reference": {}}}]}
{"id": "197465409_0", "paragraph": "[BOS] The exploration of teacher-student models for semi-supervised learning has produced impressive results for image classification (Tarvainen and Valpola, 2017; Laine and Aila, 2016; Rasmus et al., 2015) .\n[BOS] However, they have not yet been well-studied in the context of natural language processing.\n[BOS] Hu et al. (2016) propose a teacherstudent model for the task of sentiment classification and named entity recognition, where the teacher is derived from a manually specified set of rule-templates that regularizes a neural student, thereby allowing one to combine neural and symbolic systems.\n[BOS] Our MT system is different in that the teacher is a simple running average of the students across different epochs of training, which removes the need of human supervision through rules.\n[BOS] More recently, Nagesh and Surdeanu (2018) applied the MT architecture for the task of semisupervised Named Entity Classification, which is a simpler task compared to our RE task.\n[BOS] Recent works Xu et al., 2015; Su et al., 2018) use neural networks to learn syntactic features for relation extraction via traversing the shortest dependency path.\n[BOS] Following this trend, we adapt such syntax-based neural models to both of our student and teacher classifiers in the MT architecture.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Single_summ", "Reflection", "Single_summ", "Multi_summ", "Reflection"], "span_citation_mapping": [{"token_start": 19, "token_end": 48, "char_start": 113, "char_end": 206, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Tarvainen and Valpola, 2017;": null, "Laine and Aila, 2016;": "13123084", "Rasmus et al., 2015)": null}}}, {"token_start": 69, "token_end": 125, "char_start": 313, "char_end": 604, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hu et al. (2016)": "7663461"}, "Reference": {}}}, {"token_start": 163, "token_end": 198, "char_start": 819, "char_end": 982, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Nagesh and Surdeanu (2018)": "52009536"}, "Reference": {}}}, {"token_start": 201, "token_end": 233, "char_start": 1002, "char_end": 1152, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Xu et al., 2015;": "5403702", "Su et al., 2018)": "182616"}, "Reference": {}}}]}
{"id": "196182403_1", "paragraph": "[BOS] Various automatically generated diagnostic datasets have been proposed (Weston et al., 2015; Johnson et al., 2017) .\n[BOS] While these recognize the need to evaluate multiple capabilities, evaluation is still restricted to individual units and thus cannot capture inconsistencies between predictions, like predicting that an object is at the same time to the left and to the right of another object.\n[BOS] Furthermore, questions/contexts can be sufficiently artificial for models to reverse-engineer how the dataset was created.\n[BOS] An exception contemporaneous with our work is GQA (Hudson and Manning, 2019) , where real images are used, and metrics such as consistency (similar to our own) are used for a fraction of inputs.\n[BOS] Since questions are still synthetic, and \"not as natural as other VQA datasets\" (Hudson and Manning, 2019) , it remains to be seen whether models will overfit to the generation procedure or to the implications encoded (e.g. many are simple spatial rules such as \"X to the left of Y implies Y to the right of X\").\n[BOS] Their approach is complementary to ours -they provide implications for 54% of their synthetic dataset, while we generate different implications for 67% of human generated questions in VQA, and 73% of SQuAD questions.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 3, "token_end": 26, "char_start": 14, "char_end": 120, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Weston et al., 2015;": "3178759", "Johnson et al., 2017)": "15458100"}}}, {"token_start": 105, "token_end": 143, "char_start": 587, "char_end": 735, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Hudson and Manning, 2019)": "67855531"}, "Reference": {}}}, {"token_start": 148, "token_end": 171, "char_start": 768, "char_end": 848, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hudson and Manning, 2019)": "67855531"}}}]}
{"id": "196182403_0", "paragraph": "[BOS] Since QA models often exploit shortcuts to be accurate without really understanding questions and contexts, alternative evaluations have been proposed, consisting of solutions that mitigate known biases or propose separate diagnostic datasets.\n[BOS] Examples of the former include adding multiple images for which the answer to the same question is different (Goyal et al., 2017; , or questions for which an answer is not present (Rajpurkar et al., 2018) .\n[BOS] While useful, these do not take the relationship between predictions into account, and thus do not capture problems like the ones in Figure 1 .\n[BOS] Exceptions exist when trying to gauge robustness: Ribeiro et al. (2018) consider the robustness of QA models to automatically generated input rephrasings, while Shah et al. (2019) evaluate VQA models on crowdsourced rephrasings for robustness.\n[BOS] While important for evaluation, these efforts are orthogonal to our focus on consistency.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Multi_summ", "Transition"], "span_citation_mapping": [{"token_start": 52, "token_end": 66, "char_start": 331, "char_end": 384, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 69, "token_end": 87, "char_start": 391, "char_end": 460, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rajpurkar et al., 2018)": "47018994"}}}, {"token_start": 125, "token_end": 148, "char_start": 669, "char_end": 772, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ribeiro et al. (2018)": "21740766"}, "Reference": {}}}, {"token_start": 150, "token_end": 173, "char_start": 780, "char_end": 862, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Shah et al. (2019)": "62902119"}, "Reference": {}}}]}
{"id": "202777539_4", "paragraph": "[BOS] Our study extends the method in Hsu et al. (2018) to a multi-task learning model in which the models need to generate multiple outputs with consistency.\n[BOS] Hierarchical consistency loss combines two advantages.\n[BOS] This loss considers the hierarchy among tasks, and has flexibility among tasks, similar to soft-parameter sharing methods.\n[BOS] We assess the advantages of this loss in Section 4.2.\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 8, "token_end": 34, "char_start": 38, "char_end": 158, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Hsu et al. (2018)": "21723747"}}}]}
{"id": "202777539_3", "paragraph": "[BOS] Some studies have constructed a multi-task learning model using techniques that encourages information sharing among decoders.\n[BOS] Isonuma et al. (2017) proposed an extractive summarization model that the outputs of the sentence extractor are directly used for a document classifier.\n[BOS] Anastasopoulos and Chiang (2018) introduced a triangle model to transfer the decoder information of the second task to the decoder of the first task.\n[BOS] Tan et al. (2017) introduced a coarse-to-fine model to generate headlines using important sentences chosen in the extracter.\n[BOS] These methods are cascade models that additionally input the information of the first tasks directly into the second tasks.\n[BOS] They consider the hierarchy among tasks, but these models suffer from the errors of the previous tasks.\n[BOS] Guo et al. (2018) proposed a decoder sharing method with soft-parameter sharing to train the summarization and entailment tasks.\n[BOS] Softparameter sharing has a benefit in that it provides more flexibility between the layer of summarization and entailment tasks; however, this method does not consider the hierarchy among tasks.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 23, "token_end": 55, "char_start": 139, "char_end": 291, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Isonuma et al. (2017)": "35618061"}, "Reference": {}}}, {"token_start": 56, "token_end": 86, "char_start": 298, "char_end": 447, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Anastasopoulos and Chiang (2018)": "3351788"}, "Reference": {}}}, {"token_start": 87, "token_end": 156, "char_start": 454, "char_end": 818, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Tan et al. (2017)": "19374748"}, "Reference": {}}}, {"token_start": 157, "token_end": 220, "char_start": 825, "char_end": 1155, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Guo et al. (2018)": "44105751"}, "Reference": {}}}]}
{"id": "202777539_2", "paragraph": "[BOS] Multi-task learning, which trains different tasks in one unified model, has achieved success in many natural language processing tasks (Luong et al., 2016; Liu et al., 2019) .\n[BOS] Typical multi-task learning models have a structure with a shared encoder to encode the input text and multiple decoders to generate outputs of each task.\n[BOS] Multitask learning has a benefit in that the shared encoder captures common features among tasks; in addition, the encoder focuses more on relevant and beneficial features, and disregards irrelevant and noisy features (Ruder, 2017) .\n[BOS] Although a multi-task learning model is beneficial in training a shared encoder, it is still difficult to share information among task-specific decoders.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 21, "token_end": 42, "char_start": 107, "char_end": 179, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Luong et al., 2016;": "6954272", "Liu et al., 2019)": "59523594"}}}, {"token_start": 94, "token_end": 117, "char_start": 460, "char_end": 580, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ruder, 2017)": null}}}]}
{"id": "202777539_1", "paragraph": "[BOS] Multi-task learning.\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "202777539_0", "paragraph": "[BOS] Abstractive summarization.\n[BOS] Abstractive summarization is a task to generate a short summary that captures the core meaning of the original text.\n[BOS] Rush et al. (2015) used a neural attention model, and See et al. (2017) introduced a pointergenerator network to copy out-of-vocabulary (OOV) words from the input text.\n[BOS] Hsu et al. (2018) combined abstractive and extractive summarization with an inconsistency loss to encourage consistency between word-level attention weights of the abstracter and sentence-level attention weights of the extractor.\n[BOS] Abstractive summarization techniques are generally applied to a headline generation because this is a similar task (Shen et al., 2017; Tan et al., 2017) .\n\n", "discourse_tags": ["Transition", "Transition", "Multi_summ", "Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 31, "token_end": 44, "char_start": 162, "char_end": 210, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Rush et al. (2015)": "1918428"}, "Reference": {}}}, {"token_start": 46, "token_end": 76, "char_start": 216, "char_end": 330, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"See et al. (2017)": null}, "Reference": {}}}, {"token_start": 77, "token_end": 120, "char_start": 337, "char_end": 566, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hsu et al. (2018)": "21723747"}, "Reference": {}}}, {"token_start": 121, "token_end": 155, "char_start": 573, "char_end": 725, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Shen et al., 2017;": null, "Tan et al., 2017)": "19374748"}}}]}
{"id": "202540311_3", "paragraph": "[BOS] Multilingual SRL To promote NLP applications, the CoNLL-2009 shared task advocated performing SRL for multiple languages.\n[BOS] Among the participating systems, Zhao et al. (2009a) proposed an integrated approach by exploiting largescale feature set, while Bjrkelund et al. (2009) used a generic feature selection procedure.\n[BOS] Until now, only a few of work (Lei et al., 2015; Swayamdipta et al., 2016; Mulcaire et al., 2018) seriously considered multilingual SRL.\n[BOS] Among them, Mulcaire et al. (2018) built a polyglot model (training one model on multiple languages) for multilingual SRL, but their results were far from satisfactory.\n[BOS] Therefore, this work aims to complete the overall upgrade since CoNLL-2009 shared task and leaves polyglot training as our future work.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Multi_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 33, "token_end": 51, "char_start": 167, "char_end": 255, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhao et al. (2009a)": "11146492"}, "Reference": {}}}, {"token_start": 53, "token_end": 70, "char_start": 263, "char_end": 330, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Bj\u00f6rkelund et al. (2009)": "33777646"}, "Reference": {}}}, {"token_start": 75, "token_end": 114, "char_start": 353, "char_end": 473, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lei et al., 2015;": null, "Swayamdipta et al., 2016;": "926149", "Mulcaire et al., 2018)": "44073804"}}}, {"token_start": 118, "token_end": 154, "char_start": 492, "char_end": 648, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Mulcaire et al. (2018)": "44073804"}, "Reference": {}}}]}
{"id": "202540311_2", "paragraph": "[BOS] Despite the success of syntax-agnostic SRL models, more recent work attempts to further improve performance by integrating syntactic information, with the impressive success of deep neural networks in dependency parsing (Zhang et al., 2016; .\n[BOS] Marcheggiani and Titov (2017) used graph convolutional network to encode syntax into dependency SRL.\n[BOS] proposed an extended k-order argument pruning algorithm based on syntactic tree and boosted SRL performance.\n[BOS] presented a unified neural framework to provide multiple methods for syntactic integration.\n[BOS] Our method is closely related to the one of , designed to prune as many unlikely arguments as possible.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 32, "token_end": 45, "char_start": 183, "char_end": 245, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 48, "token_end": 70, "char_start": 255, "char_end": 355, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "202540311_1", "paragraph": "[BOS] However, putting syntax aside has sparked much research interest since Zhou and Xu (2015) employed deep BiLSTMs for span SRL.\n[BOS] A series of neural SRL models without syntactic inputs were proposed.\n[BOS] applied a simple LSTM model with effective word representation, achieving encouraging results on English, Chinese, Czech and Spanish.\n[BOS] built a full end-to-end SRL model with biaffine attention and provided strong performance on English and Chinese.\n[BOS] Li et al. (2019) also proposed an end-to-end model for both dependency and span SRL with a unified argument representation, obtaining favorable results on English.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 14, "token_end": 44, "char_start": 77, "char_end": 207, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhou and Xu (2015)": "12688069"}, "Reference": {}}}, {"token_start": 95, "token_end": 131, "char_start": 474, "char_end": 637, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Li et al. (2019)": "53668350"}, "Reference": {}}}]}
{"id": "202540311_0", "paragraph": "[BOS] In early work of semantic role labeling, most of researchers were dedicated to feature engineering (Pradhan et al., 2005; Punyakanok et al., 2008; Zhao et al., 2009b Zhao et al., , 2013 .\n[BOS] The first neural SRL model was proposed by Collobert et al. (2011) , which used convolutional neural network but their efforts fell short.\n[BOS] Later, Foland and Martin (2015) effectively extended their work by using syntactic features as input.\n[BOS] Roth and Lapata (2016) introduced syntactic paths to guide neural architectures for dependency SRL.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 16, "token_end": 52, "char_start": 85, "char_end": 191, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Pradhan et al., 2005;": "2440012", "Punyakanok et al., 2008;": "11162815", "Zhao et al., 2009b": "2193825", "Zhao et al., , 2013": "1239326"}}}, {"token_start": 54, "token_end": 83, "char_start": 200, "char_end": 338, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Collobert et al. (2011)": null}, "Reference": {}}}, {"token_start": 84, "token_end": 104, "char_start": 345, "char_end": 446, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Foland and Martin (2015)": "18951510"}, "Reference": {}}}, {"token_start": 105, "token_end": 124, "char_start": 453, "char_end": 552, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Roth and Lapata (2016)": "5779419"}, "Reference": {}}}]}
{"id": "202775306_0", "paragraph": "[BOS] Cross-lingual transfer learning which acts as one of the low-resource topics (Gu et al., 2018; Lee et al., 2019; Xu et al., 2018) has attracted more and more people recently, followed by the rapid development of cross-lingual word embeddings.\n[BOS] Artetxe et al. (2017) proposed a self-learning framework and utilized a small size of word dictionary to learn the mapping between source and target word embeddings.\n[BOS] Conneau et al. (2018) leveraged adversarial training to learn a linear mapping from a source to a target space without using parallel data.\n[BOS] Joulin et al. (2018) utilized Relaxed CSLS loss to optimize this mapping problem.\n[BOS] introduced a method to leverage cross-lingual meta-representations for code-switching named entity recognition by combining multiple monolingual word embeddings.\n[BOS] Chen et al. (2018) proposed a teacher-student framework leveraging bilingual data for crosslingual transfer learning in dialogue state track-ing.\n[BOS] Upadhyay et al. (2018) leveraged joint training and cross-lingual embeddings to do zero-shot and almost zero-shot transfer learning in intent prediction and slot filling.\n[BOS] Finally, Schuster et al. (2019) utilizes Multilingual CoVe embeddings obtained from training Machine Translation systems as in (McCann et al., 2017) .\n[BOS] The main difference of our work with previous work is that our model does not leverage any external bilingual data other than 11 word pairs for embeddings refinement.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 14, "token_end": 41, "char_start": 63, "char_end": 135, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Gu et al., 2018;": "3295641", "Lee et al., 2019;": "184482872", "Xu et al., 2018)": "52197809"}}}, {"token_start": 63, "token_end": 97, "char_start": 255, "char_end": 420, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Artetxe et al. (2017)": "13335042"}, "Reference": {}}}, {"token_start": 98, "token_end": 129, "char_start": 427, "char_end": 566, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Conneau et al. (2018)": "3470398"}, "Reference": {}}}, {"token_start": 130, "token_end": 149, "char_start": 573, "char_end": 654, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Joulin et al. (2018)": "52158178"}, "Reference": {}}}, {"token_start": 178, "token_end": 208, "char_start": 829, "char_end": 974, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Chen et al. (2018)": "52050424"}, "Reference": {}}}, {"token_start": 209, "token_end": 248, "char_start": 981, "char_end": 1151, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 251, "token_end": 283, "char_start": 1167, "char_end": 1306, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Schuster et al. (2019)": "53110354"}, "Reference": {"(McCann et al., 2017)": "9447219"}}}]}
{"id": "202540846_1", "paragraph": "[BOS] Deep Transition.\n[BOS] Deep transition has been proved its superiority in language modeling (Pascanu et al., 2014) and machine translation (Miceli Barone et al., 2017; Meng and Zhang, 2019) .\n[BOS] We follow the deep transition architecture in Meng and Zhang (2019) and extend it by incorporating a novel A-GRU for ABSA tasks.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 14, "token_end": 26, "char_start": 80, "char_end": 120, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Pascanu et al., 2014)": null}}}, {"token_start": 27, "token_end": 47, "char_start": 125, "char_end": 195, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Miceli Barone et al., 2017;": "1251481", "Meng and Zhang, 2019)": "56475925"}}}, {"token_start": 52, "token_end": 63, "char_start": 218, "char_end": 271, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Meng and Zhang (2019)": "56475925"}}}]}
{"id": "202540846_0", "paragraph": "[BOS] Sentiment Analysis.\n[BOS] There are kinds of sentiment analysis tasks, such as documentlevel (Thongtan and Phienthrakul, 2019) , sentence-level 4 , aspect-level (Pontiki et al., 2014; Wang et al., 2019a) and multimodal (Chen et al., 2018; Akhtar et al., 2019) sentiment analysis.\n[BOS] For the aspect-level sentiment analysis, previous work typically apply attention mechanism (Luong et al., 2015) combining with memory network (Weston et al., 2014) or gating units to solve this task (Tang et al., 2016b; He et al., 2018a; Xue and Li, 2018; Duan et al., 2018; Tang et al., 2019; Yang et al., 2019; Bao et al., 2019) , where an aspect-independent encoder is used to generate the sentence representation.\n[BOS] In addition, some work leverage the aspect-weakly associative encoder to generate aspect-specific sentence representation (Tang et al., 2016a; Wang et al., 2016; Majumder et al., 2018) .\n[BOS] All of these methods make insufficient use of the given aspect information.\n[BOS] There are also some work which jointly extract the aspect term (and opinion term) and predict its sentiment polarity (Schmitt et al., 2018; Li et al., 2018b; Ma et al., 2018; Angelidis and Lapata, 2018; He et al., 2019; Hu et al., 2019; Dai and Song, 2019; Wang et al., 2019b) .\n[BOS] In this paper, we focus on the latter problem and leave aspect extraction (Shu et al., 2017) to future work.\n[BOS] And some work He et al., 2018b; Xu and Tan, 2018; Chen and Qian, 2019; He et al., 2019) employ the well-known BERT (Devlin et al., 2018) or document-level corpora to enhance ABSA tasks, which will be considered in our future work to further improve the performance.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite", "Narrative_cite", "Transition", "Narrative_cite", "Reflection", "Narrative_cite"], "span_citation_mapping": [{"token_start": 16, "token_end": 31, "char_start": 85, "char_end": 132, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Thongtan and Phienthrakul, 2019)": "196175311"}}}, {"token_start": 32, "token_end": 59, "char_start": 135, "char_end": 209, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Pontiki et al., 2014;": "1021411", "Wang et al., 2019a)": "196181300"}}}, {"token_start": 60, "token_end": 79, "char_start": 214, "char_end": 265, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chen et al., 2018;": "3929578", "Akhtar et al., 2019)": "155092869"}}}, {"token_start": 95, "token_end": 106, "char_start": 363, "char_end": 403, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Luong et al., 2015)": "1998416"}}}, {"token_start": 108, "token_end": 119, "char_start": 419, "char_end": 455, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 120, "token_end": 182, "char_start": 459, "char_end": 622, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Tang et al., 2016b;": "359042", "He et al., 2018a;": "52013036", "Xue and Li, 2018;": "202690932", "Duan et al., 2018;": "44165235", "Tang et al., 2019;": "174798390", "Yang et al., 2019;": "86510253", "Bao et al., 2019)": null}}}, {"token_start": 205, "token_end": 242, "char_start": 752, "char_end": 900, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Tang et al., 2016a;": "10870417", "Wang et al., 2016;": "18993998", "Majumder et al., 2018)": "53080574"}}}, {"token_start": 275, "token_end": 343, "char_start": 1077, "char_end": 1267, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Schmitt et al., 2018;": "52112235", "Li et al., 2018b;": "53292699", "Ma et al., 2018;": "53080801", "Angelidis and Lapata, 2018;": "52100878", "He et al., 2019;": "189928395", "Hu et al., 2019;": "102353837", "Dai and Song, 2019;": "195847811", "Wang et al., 2019b)": "86532819"}}}, {"token_start": 357, "token_end": 367, "char_start": 1332, "char_end": 1368, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Shu et al., 2017)": "3635104"}}}, {"token_start": 375, "token_end": 430, "char_start": 1405, "char_end": 1575, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"He et al., 2018b;": "48357955", "Xu and Tan, 2018;": "53086613", "Chen and Qian, 2019;": "196176857", "He et al., 2019)": "189928395", "(Devlin et al., 2018)": "52967399"}}}]}
{"id": "198147768_0", "paragraph": "[BOS] Before the adoption of neural models, early approaches to AEG involved identifying error statistics and patterns in the corpus and applying them to grammatically correct sentences (Brockett et al., 2006; Rozovskaya and Roth, 2010) .\n[BOS] Inspired by the back-translation approach, recent AEG approaches inject errors into grammatically correct input sentences by adopting methods from neural machine translation (Felice and Yuan, 2014; Kasewa et al., 2018) .\n[BOS] Xie et al. (2018) propose an approach that adds noise to the beam-search phase of an back-translation based AEG model to generate more diverse errors.\n[BOS] They use the synthesized parallel data generated by this method to train a multi-layer convolutional GEC model and achieve a 5 point F 0.5 improvement on the CoNLL-2014 test data (Ng et al., 2014) .\n[BOS] Ge et al. (2018) propose a fluency-boosting learning method that generates less fluent sentences from correct sentences and pairs them with correct sentences to create new error-correct sentence pairs during training.\n[BOS] Their GEC model trained with artificial errors approaches human-level performance on multiple test sets.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 26, "token_end": 50, "char_start": 154, "char_end": 236, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Brockett et al., 2006;": "757808", "Rozovskaya and Roth, 2010)": "16131749"}}}, {"token_start": 73, "token_end": 95, "char_start": 379, "char_end": 463, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Felice and Yuan, 2014;": "6282257", "Kasewa et al., 2018)": "52896498"}}}, {"token_start": 97, "token_end": 177, "char_start": 472, "char_end": 825, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Xie et al. (2018)": "21730715"}, "Reference": {"(Ng et al., 2014)": null}}}, {"token_start": 179, "token_end": 238, "char_start": 834, "char_end": 1162, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ge et al. (2018)": "49564245"}, "Reference": {}}}]}
{"id": "189999088_4", "paragraph": "[BOS] An interesting approach is that of deliberation networks, which jointly train an encoder and first and second stage decoders (Xia et al., 2017) .\n[BOS] The second stage decoder has access to both left and right side context and this has been shown to improve translation (Xia et al., 2017; .\n[BOS] We follow this approach as it offers a very flexible framework to incorporate additional information in the second stage decoder.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 13, "token_end": 32, "char_start": 70, "char_end": 149, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Xia et al., 2017)": "13481571"}}}, {"token_start": 35, "token_end": 62, "char_start": 162, "char_end": 294, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "189999088_3", "paragraph": "[BOS] Translation refinement: The idea of treating machine translation as a two step approach dates back to statistical models, e.g. in order to improve a draft sentence-level translation by exploring document-wide context through hill-climbing for local refinements (Hardmeier et al., 2012) .\n[BOS] Iterative refinement approaches have also been proposed that start with a draft translation and then predict discrete substitutions based on an attention mechanism (Novak et al., 2016) , or using nonautoregressive methods with a focus on speeding up decoding (Lee et al., 2018) .\n[BOS] Translation refinement can also be done through learning a separate model for automatic post-editing (Niehues et al., 2016; Junczys-Dowmunt and Grundkiewicz, 2017; Chatterjee et al., 2018) , but this requires additional training data with draft translations and their correct version.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 43, "token_end": 59, "char_start": 231, "char_end": 291, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hardmeier et al., 2012)": "16112861"}}}, {"token_start": 82, "token_end": 93, "char_start": 444, "char_end": 484, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Novak et al., 2016)": "4863328"}}}, {"token_start": 104, "token_end": 117, "char_start": 535, "char_end": 577, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lee et al., 2018)": "3438497"}}}, {"token_start": 126, "token_end": 169, "char_start": 634, "char_end": 774, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Niehues et al., 2016;": "18017180", "Junczys-Dowmunt and Grundkiewicz, 2017;": "19973349", "Chatterjee et al., 2018)": "53235202"}}}]}
{"id": "189999088_2", "paragraph": "[BOS] An alternative way of exploring image representations is to have an attention mechanism (Bahdanau et al., 2015) on the output of the last convolutional layer of a CNN (Xu et al., 2015) .\n[BOS] The layer represents the activation of K different convolutional filters on evenly quantised N  N spatial regions of the image.\n[BOS] Caglayan et al. (2017) Helcl et al. (2018) is the closest to our work: we also use a doubly-attentive transformer architecture and explore spatial visual information.\n[BOS] However, we differ in two main aspects (Section 3): (i) our approach explores additional textual context through a second pass decoding process and uses visual information only at this stage, and (ii) in addition to convolutional filters we use objectlevel visual information.\n[BOS] The latter has only been explored to generate a single global representation (Grnroos et al., 2018) and used for example to initialise the encoder (Huang et al., 2016) .\n[BOS] We note that translation refinement is different translation re-ranking from a text-only model based on image representation (Shah et al., 2016; Hitschler et al., 2016; , since the latter assumes that the correct translation can already be produced by a text-only model.\n[BOS] Caglayan et al. (2019) investigate the importance and the contribution of multimodality for MMT.\n[BOS] They perform careful experiments by using input degradation and observe that, specially under limited textual context, multimodal models exploit the visual input to generate better translations.\n[BOS] Caglayan et al. (2019) also show that MMT systems exploit visual cues and obtain correct translations even with typographical errors in the source sentences.\n[BOS] In this paper, we build upon this idea and investigate the potential of visual cues for refining translation.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Reflection", "Reflection", "Narrative_cite", "Narrative_cite", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 13, "token_end": 25, "char_start": 74, "char_end": 117, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bahdanau et al., 2015)": "11212020"}}}, {"token_start": 27, "token_end": 44, "char_start": 125, "char_end": 190, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Xu et al., 2015)": "1055111"}}}, {"token_start": 69, "token_end": 86, "char_start": 333, "char_end": 375, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Helcl et al. (2018)": "198309699"}}}, {"token_start": 173, "token_end": 187, "char_start": 837, "char_end": 888, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 189, "token_end": 204, "char_start": 898, "char_end": 956, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 219, "token_end": 243, "char_start": 1044, "char_end": 1132, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Shah et al., 2016;": "16585731"}}}, {"token_start": 265, "token_end": 318, "char_start": 1242, "char_end": 1539, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Caglayan et al. (2019)": "84842989"}, "Reference": {}}}, {"token_start": 319, "token_end": 352, "char_start": 1546, "char_end": 1703, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Caglayan et al. (2019)": "84842989"}, "Reference": {}}}]}
{"id": "189999088_1", "paragraph": "[BOS] The image representation is integrated into the MT models by initialising the encoder or decoder (Elliott et al., 2015; Caglayan et al., 2017; Madhyastha et al., 2017) ; element-wise multiplication with the source word annotations (Caglayan et al., 2017) ; or projecting the image representation and encoder context to a common space to initialise the decoder .\n[BOS] Elliott and Kdr (2017) and Helcl et al. (2018) instead model the source sentence and reconstruct the image representation jointly via multi-task learning.\n\n", "discourse_tags": ["Narrative_cite", "Multi_summ"], "span_citation_mapping": [{"token_start": 15, "token_end": 46, "char_start": 84, "char_end": 173, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Elliott et al., 2015;": "18550051", "Madhyastha et al., 2017)": "26481271"}}}, {"token_start": 53, "token_end": 66, "char_start": 213, "char_end": 260, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 86, "token_end": 120, "char_start": 374, "char_end": 528, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Elliott and K\u00e1d\u00e1r (2017)": "20272964", "Helcl et al. (2018)": "198309699"}, "Reference": {}}}]}
{"id": "189999088_0", "paragraph": "[BOS] MMT: Approaches to MMT vary with regards to how they represent images and how they incorporate this information in the models.\n[BOS] Initial approaches use RNN-based sequence to sequence models (Bahdanau et al., 2015) enhanced with a single, global image vector, extracted as one of the layers of a CNN trained for object classification (He et al., 2016) , often the penultimate or final layer.\n\n", "discourse_tags": ["Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 31, "token_end": 49, "char_start": 162, "char_end": 223, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bahdanau et al., 2015)": "11212020"}}}, {"token_start": 69, "token_end": 79, "char_start": 321, "char_end": 360, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(He et al., 2016)": "206594692"}}}]}
{"id": "173188058_1", "paragraph": "[BOS] Interest in learning general-purpose representations for natural language through unsupervised, multi-task and transfer learning has been skyrocketing lately Radford et al., 2018; McCann et al., 2018; Chronopoulou et al., 2019; Phang et al., 2018; .\n[BOS] In parallel to our work, studies that focus on generalization have appeared on publication servers, empirically studying generalization to multiple tasks (Yogatama et al., 2019; Liu et al., 2019 natural langauge understanding, focusing on reading comprehension, which we view as an important and broad language understanding task.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 13, "token_end": 61, "char_start": 88, "char_end": 252, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Radford et al., 2018;": "49313245", "McCann et al., 2018;": "49393754", "Chronopoulou et al., 2019;": "67855637"}}}, {"token_start": 83, "token_end": 105, "char_start": 383, "char_end": 456, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yogatama et al., 2019;": "59523704", "Liu et al., 2019": "59523594"}}}]}
{"id": "173188058_0", "paragraph": "[BOS] Prior work has shown that RC performance can be improved by training on a large dataset and transferring to a smaller one, but at a small scale (Min et al., 2017; Chung et al., 2018) .\n[BOS] has recently shown this in a larger experiment for multi-choice questions, where they first fine-tuned BERT on RACE (Lai et al., 2017) and then finetuned on several smaller datasets.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 7, "token_end": 45, "char_start": 32, "char_end": 188, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Min et al., 2017;": "7928230", "Chung et al., 2018)": "3453374"}}}, {"token_start": 64, "token_end": 78, "char_start": 289, "char_end": 331, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lai et al., 2017)": "6826032"}}}]}
{"id": "201641861_1", "paragraph": "[BOS] Our dataset has significant potential to be further expanded.\n[BOS] Following the context-sensitive translation (Bawden et al., 2018; Mller et al., 2018; Zhang et al., 2018; Miculicich et al., 2018) , our dataset includes translations of multiple sentences.\n[BOS] However, the translatable XML tags are separated, so the page-level global information is missing.\n[BOS] One promising direction is thus to create page-level translation examples.\n[BOS] Finally, considering the recent focus on multilingual NMT models (Johnson et al., 2017) , multilingually aligning the text will enrich our dataset.\n\n", "discourse_tags": ["Reflection", "Narrative_cite", "Reflection", "Reflection", "Narrative_cite"], "span_citation_mapping": [{"token_start": 15, "token_end": 54, "char_start": 88, "char_end": 204, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bawden et al., 2018;": "5016370", "M\u00fcller et al., 2018;": "52921687", "Zhang et al., 2018;": "52938038", "Miculicich et al., 2018)": "52044834"}}}, {"token_start": 106, "token_end": 119, "char_start": 497, "char_end": 543, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Johnson et al., 2017)": "6053988"}}}]}
{"id": "201641861_0", "paragraph": "[BOS] Automatic extraction of parallel sentences has a long history (Varga et al., 2005) , and usually statistical methods and dictionaries are used.\n[BOS] By contrast, our data collection solely relies on the XML structure, because the original data have been well structured and aligned.\n[BOS] Recently, collecting training corpora is the most important in training NLP models, and thus it is recommended to maintain well-aligned documents and structures when building multilingual online services.\n[BOS] That will significantly contribute to the research of language technologies.\n[BOS] We followed the syntax-based NMT models (Eriguchi et al., 2016 (Eriguchi et al., , 2017 Aharoni and Goldberg, 2017) to handle the XML structures.\n[BOS] One significant difference between the syntax-based NMT and our task is that we need to output sourceconditioned structures that are able to be parsed as XML, whereas the syntax-based NMT models do not always need to follow formal rules for their output structures.\n[BOS] In that sense, it would be interesting to relate our task to source code generation (Oda et al., 2015) in future work.\n\n", "discourse_tags": ["Narrative_cite", "Reflection", "Transition", "Transition", "Reflection", "Reflection", "Narrative_cite"], "span_citation_mapping": [{"token_start": 2, "token_end": 20, "char_start": 6, "char_end": 88, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Varga et al., 2005)": "13133927"}}}, {"token_start": 108, "token_end": 141, "char_start": 606, "char_end": 705, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Eriguchi et al., 2016": "12851711", "(Eriguchi et al., , 2017": "14519034", "Aharoni and Goldberg, 2017)": "8078153"}}}, {"token_start": 216, "token_end": 228, "char_start": 1075, "char_end": 1116, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Oda et al., 2015)": "15979705"}}}]}
{"id": "195750845_1", "paragraph": "[BOS] While Koehn and Knowles (2017) mentioned domain mismatch as a challenge for neural machine translation, Khayrallah and Koehn (2018) addressed noisy training data and focus on the types of noise occurring in web-crawled corpora.\n[BOS] Michel and Neubig (2018) proposed a new dataset (MTNT) to test MT models for robustness to the types of noise encountered in the Internet and demonstrated that these challenges cannot be overcome by simple domain adaptation techniques alone.\n[BOS] Belinkov and Bisk (2018) and Heigold et al. (2017) showed that NMT systems are very sensitive to slightly perturbed input forms, and hinted at the importance of injecting noisy examples during training, also known as adversarial examples.\n[BOS] Further research proposed several methods of generating and using noisy examples as NMT input to advance the understanding and improve the translation quality.\n[BOS] Following machine vision, two major branches being explored when generating noisy examples, i) white box methods, where adversarial examples are generated with access to the model parameters (Ebrahimi et al., 2018; Cheng et al., 2018a Cheng et al., ,b, 2019 and ii) black-box attacks, where examples are generated without accessing model internals (Zhao et al., 2018; Lee et al., 2018; ?\n[BOS] ; Anastasopoulos et al., 2019; Vaibhav et al., 2019) ; see Belinkov and Glass (2019) for a categorization of such work.\n[BOS] In particular, some have focused on specific variations of naturally-occurring noise, such as grammatical errors produced by non-native speakers (Anastasopoulos et al., 2019) or errors extracted from Wikipedia edits (Belinkov and Bisk, 2018) .\n[BOS] It has also been shown that adding synthetic noise does not trivially increase robustness to natural noise (Belinkov and Bisk, 2018) and may require specific recipes (Karpukhin et al., 2019) .\n[BOS] recently emphasized the importance of meaning-preserving perturbations and along with Cheng et al. (2019) demonstrated the utility of adversarial training without significantly impairing performance on clean data and domain.\n[BOS] Durrani et al. (2019) showed that character-based representations are more robust towards noise compared to such learned using BPE-based sub-word units in the task of machine translation.\n\n", "discourse_tags": ["Multi_summ", "Single_summ", "Multi_summ", "Transition", "Narrative_cite", "Narrative_cite", "Narrative_cite", "Narrative_cite", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 3, "token_end": 22, "char_start": 12, "char_end": 108, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Koehn and Knowles (2017)": "8822680"}, "Reference": {}}}, {"token_start": 23, "token_end": 54, "char_start": 110, "char_end": 233, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Khayrallah and Koehn (2018)": "44090489"}, "Reference": {}}}, {"token_start": 55, "token_end": 100, "char_start": 240, "char_end": 481, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Michel and Neubig (2018)": "52155427"}, "Reference": {}}}, {"token_start": 101, "token_end": 154, "char_start": 488, "char_end": 726, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Belinkov and Bisk (2018)": "3513372", "Heigold et al. (2017)": "19009822"}, "Reference": {}}}, {"token_start": 202, "token_end": 241, "char_start": 1019, "char_end": 1156, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ebrahimi et al., 2018;": "49413369", "Cheng et al., 2018a": "3689056"}}}, {"token_start": 244, "token_end": 274, "char_start": 1165, "char_end": 1286, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhao et al., 2018;": "3513418", "Lee et al., 2018;": "53593076"}}}, {"token_start": 276, "token_end": 297, "char_start": 1295, "char_end": 1345, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Anastasopoulos et al., 2019;": "52053467", "Vaibhav et al., 2019)": "67856759"}}}, {"token_start": 299, "token_end": 315, "char_start": 1352, "char_end": 1412, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Belinkov and Glass (2019)": "56657817"}}}, {"token_start": 335, "token_end": 352, "char_start": 1532, "char_end": 1593, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Anastasopoulos et al., 2019)": "52053467"}}}, {"token_start": 356, "token_end": 369, "char_start": 1619, "char_end": 1660, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Belinkov and Bisk, 2018)": "3513372"}}}, {"token_start": 377, "token_end": 398, "char_start": 1697, "char_end": 1801, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Belinkov and Bisk, 2018)": "3513372"}}}, {"token_start": 400, "token_end": 416, "char_start": 1810, "char_end": 1859, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Karpukhin et al., 2019)": "59604474"}}}, {"token_start": 419, "token_end": 456, "char_start": 1877, "char_end": 2092, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Cheng et al. (2019)": "174802397"}, "Reference": {}}}, {"token_start": 457, "token_end": 498, "char_start": 2099, "char_end": 2286, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "195750845_0", "paragraph": "[BOS] The fragility of neural networks (Szegedy et al., 2013) has been shown to extend to neural machine translation models (Belinkov and Bisk, 2018; Heigold et al., 2017) and recent work focused on various aspects of the problem.\n[BOS] From the identification of the causes of this brittleness, to the induction of (adversarial) inputs that trigger the unwanted behavior (attacks) and making such models robust against various types of noisy inputs (defenses); improving robustness has been receiving increasing attention in NMT.\n\n", "discourse_tags": ["Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 2, "token_end": 18, "char_start": 6, "char_end": 61, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 24, "token_end": 47, "char_start": 90, "char_end": 171, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Belinkov and Bisk, 2018;": "3513372", "Heigold et al., 2017)": "19009822"}}}]}
{"id": "202769115_2", "paragraph": "[BOS] In this work, we design a sequence matching model based on BERT.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "202769115_1", "paragraph": "[BOS] Recently, pre-trained language models have been successfully used in NLP tasks.\n[BOS] ELMo (Peters et al., 2017) is trained as a bidirectional language model.\n[BOS] OpenAI GPT (Alec Radford, 2018) uses a basic left-to-right transformer to learn a language model.\n[BOS] BERT, used in this paper, is based on the architecture of a bidirectional Transformer and per-trained on Masked Language Model task and Next Sentence Prediction.\n[BOS] Using the pre-trained parameters, BERT (Devlin et al., 2018) achieves state-of-the-art performance on the GLUE benchmark (Wang et al., 2018) and SQuAD 1.1 (Rajpurkar et al., 2016) by fine-tuning in corresponding supervised data.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Reflection", "Multi_summ"], "span_citation_mapping": [{"token_start": 19, "token_end": 37, "char_start": 92, "char_end": 164, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 38, "token_end": 65, "char_start": 171, "char_end": 268, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 104, "token_end": 160, "char_start": 477, "char_end": 671, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Devlin et al., 2018)": "52967399"}, "Reference": {"(Wang et al., 2018)": "5034059", "(Rajpurkar et al., 2016)": "11816014"}}}]}
{"id": "202769115_0", "paragraph": "[BOS] Sequence Matching task has attracted lots of attention in the past decades.\n[BOS] There are many related works on Question Answering(QA) (Yin et al., 2015; Tay et al., 2018; Wang et al., 2017; Tymoshenko and Moschitti, 2018; Min et al., 2017) , Natural Language Inference(NLI) (Peters et al., 2018; Kim et al., 2018; and so on.\n[BOS] (Yin et al., 2015) use attention mechanism with convolutional layer to model sentence pairs.\n[BOS] In (Tymoshenko and Moschitti, 2018) , they combine the similarity features of members within the same pair and traditional sentence pair similarity and achieve state-of-the-art results on several answer selection datasets including WikiQA (Yang et al., 2015) .\n[BOS] (Peters et al., 2018) and incorporate pretrained language models to text sequence matching task and achieve performance improvement on SNLI (Bowman et al., 2015) .\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 22, "token_end": 69, "char_start": 120, "char_end": 248, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yin et al., 2015;": "5535381", "Tay et al., 2018;": "207603084", "Wang et al., 2017;": "9395040", "Tymoshenko and Moschitti, 2018;": "53082203", "Min et al., 2017)": "7928230"}}}, {"token_start": 70, "token_end": 91, "char_start": 251, "char_end": 321, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Peters et al., 2018;": "3626819"}}}, {"token_start": 97, "token_end": 116, "char_start": 340, "char_end": 432, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Yin et al., 2015)": "5535381"}, "Reference": {}}}, {"token_start": 117, "token_end": 175, "char_start": 439, "char_end": 697, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Tymoshenko and Moschitti, 2018)": "53082203"}, "Reference": {"(Yang et al., 2015)": "1373518"}}}, {"token_start": 177, "token_end": 212, "char_start": 706, "char_end": 867, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Peters et al., 2018)": "3626819"}, "Reference": {"(Bowman et al., 2015)": "14604520"}}}]}
