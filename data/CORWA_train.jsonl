{"id": "9997677_1", "paragraph": "[BOS] Finally, we believe that the main benefit of our imitation learning approach, namely that it is able to learn using a non-decomposable loss function, is orthogonal to using continuous representations such as the hidden state and memory cell in the LSTM of Wen et al. (2015) .\n[BOS] Recent work by Ranzato et al. (2016) showed how RNNs can be trained at the sequence level (as opposed to the word level) with nondecomposable loss functions in the context of machine translation using imitation learning, and such an approach would also be applicable to NLG.\n\n", "discourse_tags": ["Reflection", "Single_summ"], "span_citation_mapping": [{"token_start": 46, "token_end": 59, "char_start": 242, "char_end": 279, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Wen et al. (2015)": "739696"}}}, {"token_start": 64, "token_end": 121, "char_start": 303, "char_end": 562, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ranzato et al. (2016)": "7147309"}, "Reference": {}}}]}
{"id": "9997677_0", "paragraph": "[BOS] Apart from the works we compared against, only Konstas and Lapata (2013) and Mei et al. (2016) do not need pre-aligned data.\n[BOS] Konstas and Lapata (2013) 's approach incorporates the work of Liang et al. (2009) that learns a generative semi-Markov model to calculate the alignments.\n[BOS] We note that this alignment model is developed on the datasets considered, and does not generalize equally well to other datasets (Angeli et al., 2010) .\n[BOS] On the other hand, the naive alignments we infer are much simpler and we improve them by joint learning of word and content action prediction with respect to the sentence-level evaluation via BLEU and ROUGE.\n[BOS] Concurrently, Mei et al. (2016) introduced an encoder-aligner-decoder model to perform content selection and surface realization without pre-aligned data.\n[BOS] Their work employs bidirectional LSTM-RNN models, similarly to the work of Wen et al. (2015) , and a coarse-to-fine aligner.\n[BOS] Unfortunately, they do not report results in the datasets we performed our evaluation on, do not compare against Wen et al. (2015) , and their code was unavailable when we were preparing this article.\n[BOS] Imitation learning algorithms for structured prediction have been applied successfully to a variety of tasks, such as dependency parsing (Goldberg and Nivre, 2013) and dynamic feature selection (He et al., 2013) .\n[BOS] Vlachos and Clark (2014) applied a variant of DAGGER (Ross et al., 2011) to learning a semantic parser from unaligned training examples, which is the reverse task to NLG, i.e. predicting the MR given the NL utterance.\n[BOS] To circumvent the lack of alignment information they resorted to defining a randomized expert policy similar to the heuristic one we define, but NLG poses a greater challenge since the output space is all English sentences possible given the vocabulary considered.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Narrative_cite", "Reflection", "Single_summ", "Single_summ", "Reflection", "Narrative_cite", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 11, "token_end": 37, "char_start": 53, "char_end": 130, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Konstas and Lapata (2013)": "10577922", "Mei et al. (2016)": "1354459"}}}, {"token_start": 38, "token_end": 74, "char_start": 137, "char_end": 291, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Konstas and Lapata (2013)": "10577922"}, "Reference": {"Liang et al. (2009)": "238873"}}}, {"token_start": 91, "token_end": 106, "char_start": 386, "char_end": 449, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Angeli et al., 2010)": "13402912"}}}, {"token_start": 152, "token_end": 217, "char_start": 686, "char_end": 957, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Mei et al. (2016)": "1354459"}, "Reference": {"Wen et al. (2015)": "739696"}}}, {"token_start": 234, "token_end": 245, "char_start": 1054, "char_end": 1094, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Wen et al. (2015)": "739696"}}}, {"token_start": 277, "token_end": 287, "char_start": 1289, "char_end": 1334, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Goldberg and Nivre, 2013)": "815755"}}}, {"token_start": 288, "token_end": 299, "char_start": 1339, "char_end": 1382, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(He et al., 2013)": "105260"}}}, {"token_start": 301, "token_end": 356, "char_start": 1391, "char_end": 1608, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Vlachos and Clark (2014)": "12696810"}, "Reference": {"(Ross et al., 2011)": "103456"}}}]}
{"id": "9991841_2", "paragraph": "[BOS] CCG semantics Work on semantic parsing has mapped sentences onto semantic representations with latent CCGs (Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010; Kwiatkowski et al., 2013) for restricted domains.\n[BOS] Recent work has scaled these techniques to wide-coverage datasets (Artzi et al., 2015) .\n[BOS] Krishnamurthy and Mitchell (2014) also explore joint CCG syntactic and semantic parsing.\n[BOS] They use a smaller semantic lexicon, containing 130 predicates, rather than the 3257 PropBank verbs.\n[BOS] In contrast to our results, jointly modelling the semantics lowers their model's syntactic accuracy.\n[BOS] Other CCG-based SRL models haved used CCG dependencies as features for predicting semantic roles (Gildea and Hockenmaier, 2003; Boxwell et al., 2009 ), but performance is limited by relying on 1-best parses-a problem we resolved with a joint model.\n[BOS] A * parsing A * parsing has previously been explored for less general models than ours.\n[BOS] Klein and Manning (2003) and Auli and Lopez (2011b) use A * parsing for models with tree-structured dependencies.\n[BOS] The best reported speed improvement is parsing 1.2 times faster, whereas we improve by a factor of 5.\n[BOS] Our model also allows the more complex graph-structured dependencies required for semantic role labelling.\n[BOS] Lewis and Steedman (2014a) demonstrate an efficient A * algorithm for CCG, but cannot model dependencies.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Single_summ", "Single_summ", "Reflection", "Narrative_cite", "Transition", "Multi_summ", "Multi_summ", "Reflection", "Single_summ"], "span_citation_mapping": [{"token_start": 12, "token_end": 50, "char_start": 66, "char_end": 196, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zettlemoyer and Collins, 2009;": "1950452", "Kwiatkowski et al., 2010;": "6228816", "Kwiatkowski et al., 2013)": "14341841"}}}, {"token_start": 62, "token_end": 75, "char_start": 270, "char_end": 313, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Artzi et al., 2015)": "5499420"}}}, {"token_start": 77, "token_end": 118, "char_start": 322, "char_end": 517, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Krishnamurthy and Mitchell (2014)": "1833955"}, "Reference": {}}}, {"token_start": 155, "token_end": 177, "char_start": 702, "char_end": 779, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Gildea and Hockenmaier, 2003;": "11869911", "Boxwell et al., 2009": "5267702"}}}, {"token_start": 220, "token_end": 271, "char_start": 980, "char_end": 1201, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Klein and Manning (2003)": "6422949", "Auli and Lopez (2011b)": "2530120"}, "Reference": {}}}, {"token_start": 290, "token_end": 314, "char_start": 1321, "char_end": 1426, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Lewis and Steedman (2014a)": "11487448"}, "Reference": {}}}]}
{"id": "9991841_1", "paragraph": "[BOS] CCG parsing Our log-linear model is closely related to that of Clark and Curran (2007) , but we model SRL dependencies instead of CCG dependencies.\n[BOS] The best CCG parsing results were achieved by Auli and Lopez (2011a) , who, like us, score CCG parses based jointly on supertagging and dependency model scores.\n[BOS] Decoding their model requires dual-decomposition, to maximize agreement between the separate models.\n[BOS] We avoid the need for this technique by using a unigram supertagging model, rather than a sequence model.\n\n", "discourse_tags": ["Reflection", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 23, "char_start": 6, "char_end": 92, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Clark and Curran (2007)": null}}}, {"token_start": 37, "token_end": 93, "char_start": 160, "char_end": 427, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Auli and Lopez (2011a)": "11925428"}, "Reference": {}}}]}
{"id": "9991841_0", "paragraph": "[BOS] Joint syntactic and SRL models There have been many proposals for jointly parsing syntactic and semantic dependencies.\n[BOS] Llus et al. (2013) introduce a joint arc-factored model for parsing syntactic and semantic dependencies, using dualdecomposition to maximize agreement between the models.\n[BOS] SRL performance is slightly worse than a pipeline version.\n[BOS] Naradowsky et al. (2012) introduce a SRL model with latent syntax representations, by modelling a latent dependency tree during training, which is marginalized out at test time.\n[BOS] However, performance at English SRL is roughly 7 points beneath state of the art.\n[BOS] Other notable models include those of Johansson (2009) and Titov et al. (2009) .\n\n", "discourse_tags": ["Transition", "Single_summ", "Transition", "Single_summ", "Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 22, "token_end": 57, "char_start": 131, "char_end": 301, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Llu\u00eds et al. (2013)": "7225808"}, "Reference": {}}}, {"token_start": 70, "token_end": 108, "char_start": 373, "char_end": 550, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Naradowsky et al. (2012)": "2735247"}, "Reference": {}}}, {"token_start": 127, "token_end": 147, "char_start": 645, "char_end": 723, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Johansson (2009)": "17490576", "Titov et al. (2009)": null}}}]}
{"id": "998988_3", "paragraph": "[BOS] A second advantage of the subword-based IOB tagging over the character-based is its speed.\n[BOS] The subword-based approach is faster because fewer words than characters needed to be labeled.\n[BOS] We observed a speed increase in both training and testing.\n[BOS] In the training stage, the subword approach was almost two times faster than the character-based.\n\n", "discourse_tags": ["Transition", "Transition", "Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "998988_2", "paragraph": "[BOS] Our results were compared with the best performers' results in the Bakeoff 2005.\n[BOS] Two participants' results were chosen as bases: No.15-b, ranked the first in the AS corpus, and No.14, the best performer in CITYU, MSR and PKU.\n[BOS] .\n[BOS] The No.14 used CRF-modeled IOB tagging while No.15-b used MaxEnt-modeled IOB tagging.\n[BOS] Our results produced by the MaxEnt are denoted as \"ours(ME)\" while \"ours(CRF)\" for the CRF approaches.\n[BOS] We achieved the highest F-scores in three corpora except the AS corpus.\n[BOS] We think the proposed subwordbased approach played the important role for the achieved good results.\n\n", "discourse_tags": ["Reflection", "Other", "Other", "Other", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "998988_1", "paragraph": "[BOS] Our main contribution is to extend the IOB tagging approach from being a character-based to a subword-based one.\n[BOS] We proved that the new approach enhanced the word segmentation significantly in all the experiments, MaxEnts, CRFs and using confidence measure.\n[BOS] We tested our approach using the standard Sighan Bakeoff 2005 data set in the closed test.\n[BOS] In Table 7 we align our results with some top runners' in the Bakeoff 2005.\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "998988_0", "paragraph": "[BOS] The IOB tagging approach adopted in this work is not a new idea.\n[BOS] It was first implemented in Chinese word segmentation by (Xue and Shen, 2003) using the maximum entropy methods.\n[BOS] Later, (Peng and McCallum, 2004) implemented the idea using the CRF-based approach, which yielded better results than the maximum entropy approach because it could solve the label bias problem (Lafferty et al., 2001 ).\n[BOS] However, as we mentioned before, this approach does not take advantage of the prior knowledge of in-vocabulary words; It produced a higher R-oov but a lower R-iv.\n[BOS] This problem has been observed by some participants in the Bakeoff 2005 (Asahara et al., 2005) , where they applied the IOB tagging to recognize OOVs, and added the OOVs to the lexicon used in the HMMbased or CRF-based approaches.\n[BOS] (Nakagawa, 2004) used hybrid HMM models to integrate word level and character level information seamlessly.\n[BOS] We used confidence measure to determine a better balance between R-oov and R-iv.\n[BOS] The idea of using the confidence measure has appeared in (Peng and McCallum, 2004) , where it was used to recognize the OOVs.\n[BOS] In this work we used it more than that.\n[BOS] By way of the confidence measure we combined results from the dictionary-based and the IOBtagging-based and as a result, we could achieve the optimal performance.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 23, "token_end": 34, "char_start": 105, "char_end": 154, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Xue and Shen, 2003)": "3121045"}}}, {"token_start": 43, "token_end": 88, "char_start": 203, "char_end": 411, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Peng and McCallum, 2004)": "10649571"}, "Reference": {"(Lafferty et al., 2001": "277918"}}}, {"token_start": 136, "token_end": 186, "char_start": 624, "char_end": 820, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Asahara et al., 2005)": "6960149"}, "Reference": {}}}, {"token_start": 187, "token_end": 208, "char_start": 827, "char_end": 934, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Nakagawa, 2004)": "2988891"}, "Reference": {}}}, {"token_start": 229, "token_end": 259, "char_start": 1028, "char_end": 1153, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Peng and McCallum, 2004)": "10649571"}}}]}
{"id": "998391_0", "paragraph": "[BOS] Our contributions build on previous work in making seq2seq models more computationally efficient.\n[BOS] Luong et al. (2015) introduce various attention mechanisms that are computationally simpler and perform as well or better than the original one presented in Bahdanau et al. (2014) .\n[BOS] However, these typically still require O(D 2 ) computation complexity, or lack the flexibility to look at the full source sequence.\n[BOS] Efficient location-based attention (Xu et al., 2015) has also been explored in the image recognition domain.\n[BOS] Wu et al. (2016) presents several enhancements to the standard seq2seq architecture that allow more efficient computation on GPUs, such as only attending on the bottom layer.\n[BOS] Kalchbrenner et al. (2016) propose a linear time architecture based on stacked convolutional neural networks.\n[BOS] Gehring et al. (2016) also propose the use of convolutional encoders to speed up NMT.\n[BOS] de Brbisson and Vincent (2016) propose a linear attention mechanism based on covariance matrices applied to information retrieval.\n[BOS] Raffel et al. (2017) enable online linear time attention calculation by enforcing that the alignment between input and output sequence elements be monotonic.\n[BOS] Previously, monotonic attention was proposed for morphological inflection generation by Aharoni and Goldberg (2016) .\n\n", "discourse_tags": ["Reflection", "Single_summ", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 19, "token_end": 56, "char_start": 110, "char_end": 289, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Luong et al. (2015)": "1998416"}, "Reference": {"Bahdanau et al. (2014)": "11212020"}}}, {"token_start": 85, "token_end": 98, "char_start": 436, "char_end": 488, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Xu et al., 2015)": "1055111"}}}, {"token_start": 109, "token_end": 144, "char_start": 551, "char_end": 725, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wu et al. (2016)": "3603249"}, "Reference": {}}}, {"token_start": 145, "token_end": 168, "char_start": 732, "char_end": 841, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kalchbrenner et al. (2016)": null}, "Reference": {}}}, {"token_start": 169, "token_end": 192, "char_start": 848, "char_end": 933, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Gehring et al. (2016)": "6728280"}, "Reference": {}}}, {"token_start": 193, "token_end": 216, "char_start": 940, "char_end": 1070, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Br\u00e9bisson and Vincent (2016)": "6101861"}, "Reference": {}}}, {"token_start": 217, "token_end": 246, "char_start": 1077, "char_end": 1234, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Raffel et al. (2017)": "14345813"}, "Reference": {}}}, {"token_start": 254, "token_end": 267, "char_start": 1290, "char_end": 1356, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Aharoni and Goldberg (2016)": "122829"}}}]}
{"id": "9977802_1", "paragraph": "[BOS] Similarly to cort, research toolkits such as BART (Versley et al., 2008) or Reconcile (Stoyanov et al., 2009 ) provide a framework to implement and compare coreference resolution approaches.\n[BOS] In contrast to these toolkits, we make the latent structure underlying coreference approaches explicit, which facilitates development of new approaches and renders the development more transparent.\n[BOS] Furthermore, we provide a generic and customizable learning algorithm.\n\n", "discourse_tags": ["Narrative_cite", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 11, "token_end": 21, "char_start": 51, "char_end": 78, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Versley et al., 2008)": "115878438"}}}, {"token_start": 22, "token_end": 33, "char_start": 82, "char_end": 114, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Stoyanov et al., 2009": "11067652"}}}]}
{"id": "9977802_0", "paragraph": "[BOS] Many researchers on coreference resolution release an implementation of the coreference model described in their paper (Lee et al., 2013; Durrett and Klein, 2013; Bjrkelund and Kuhn, 2014, inter alia) .\n[BOS] However, these implementations implement only one approach following one paradigm (such as mention ranking or antecedent trees).\n\n", "discourse_tags": ["Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 9, "token_end": 49, "char_start": 57, "char_end": 206, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lee et al., 2013;": "13475584", "Durrett and Klein, 2013;": "16039645"}}}]}
{"id": "9950135_6", "paragraph": "[BOS] This work is most closely related to the graphbased parsing approaches with multiple high-order refinements (Rush and Petrov, 2012; Zhang et al., 2014) , although the neural networks were not used in their parsers.\n[BOS] Rush and Petrov (2012) proposed a multi-pass coarse-to-fine approach in which a coarse model was used to prune the search space in order to make the inference with up to thirdorder features practical.\n[BOS] They start with a lineartime vine pruning pass and build up to high-order models.\n[BOS] Zhang et al (2014) introduced a randomized greedy algorithm for dependency parsing in which they begin with a tree drawn from the uniform distribution and use hill-climbing strategy to find the optimal parse tree.\n[BOS] Although they reported that drawing the initial tree randomly results in the same performance as when initialized from a trained first-order distribution, but multiple random restarts are required to avoid getting stuck in a locally optimal solution.\n[BOS] Their greedy algorithm breaks the parsing into a sequence of local steps, which correspond to choosing the head for each modifier word (one arc at a time) in the bottom-up order relative to the current tree.\n[BOS] In contrast, we employed the global inference algorithm to change the entire tree (all at a time) in each refinement step, which makes the improvement more efficient.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 10, "token_end": 37, "char_start": 47, "char_end": 157, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rush and Petrov, 2012;": "11905341", "Zhang et al., 2014)": "1260516"}}}, {"token_start": 51, "token_end": 117, "char_start": 227, "char_end": 515, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Rush and Petrov (2012)": "11905341"}, "Reference": {}}}, {"token_start": 118, "token_end": 245, "char_start": 522, "char_end": 1206, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhang et al (2014)": "1260516"}, "Reference": {}}}]}
{"id": "9950135_5", "paragraph": "[BOS] In their many-task neural model, Hashimoto et al (2016) included a graph-based dependency parse in which the traditional MLP-based method that Kiperwasser and Goldberg (2016b) used was replaced with a bilinear one.\n[BOS] Dozat and Manning (2017) modified the neural graph-based approach of (Kiperwasser and Goldberg, 2016b ) in a few ways to improve the performance.\n[BOS] In addition to building a network that is larger and uses more regularization, they replace the traditional MLPbased attention mechanism and affine label classifier with biaffine ones.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 4, "token_end": 51, "char_start": 15, "char_end": 220, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hashimoto et al (2016)": "2213896"}, "Reference": {"Kiperwasser and Goldberg (2016b)": null}}}, {"token_start": 52, "token_end": 89, "char_start": 227, "char_end": 372, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Dozat and Manning (2017)": "7942973"}, "Reference": {"(Kiperwasser and Goldberg, 2016b": null}}}]}
{"id": "9950135_4", "paragraph": "[BOS] Kiperwasser and Goldberg (2016a) also proposed a hierarchical tree LSTM to model the dependency tree structures in which each word is represented by the concatenation of its left and right modifier (child) vectors, and the modifier vectors are generated by two (leftward or rightward) recurrent neural networks.\n[BOS] The tree representations were produced in a bottom-up recursive way with the (greedy) easy-first parsing algorithm (Goldberg and Elhadad, 2010) .\n[BOS] Similarly, Cheng et al (2016) proposed a graph-based neural dependency parser that is able to predict the scores for the next arc, conditioning on previous parsing decisions.\n[BOS] In addition to using one bi-directional recurrent network that produces a recurrent vector for each word, they also have uni-directional recurrent neural networks (left-to-right and right-toleft) that keep track of the probabilities of each previous parsing actions.\n\n", "discourse_tags": ["Single_summ", "Narrative_cite", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 100, "char_start": 6, "char_end": 467, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Goldberg and Elhadad, 2010)": "3146611"}}}, {"token_start": 102, "token_end": 190, "char_start": 476, "char_end": 923, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Cheng et al (2016)": "2180008"}, "Reference": {}}}]}
{"id": "9950135_3", "paragraph": "[BOS] Graph-based parsers use machine learning for scoring each possible edge for a given sentence, typically by factoring the graphs into their component arcs, and constructing the parse tree with the highest score from these weighted edges.\n[BOS] Kiperwasser and Goldberg (2016b) presented a neural graph-based parser in which the bi-directional L-STM's recurrent output vector for each word is concatenated with each possible head's vector (also produced by the same biLSTM), and the result is used as input to a multi-layer perceptron (MLP) for scoring this modifier-head pair.\n[BOS] Given the scores of the arcs, the highest scoring tree is constructed using Eisner's decoding algorithm (Eisner, 1996) .\n[BOS] Labels are predicted similarly, with each word's recurrent output vector and its head's vector being used in a multi-class MLP.\n\n", "discourse_tags": ["Transition", "Single_summ", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 46, "token_end": 127, "char_start": 249, "char_end": 587, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 128, "token_end": 153, "char_start": 594, "char_end": 706, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Eisner, 1996)": "3262717"}}}]}
{"id": "9950135_2", "paragraph": "[BOS] A beam search and a conditional random field loss function were incorporated into the transitionbased neural network models (Weiss et al., 2015; Zhou et al., 2015; Andor et al., 2016) , which allow the parsers to keep the top-k partial parse trees and revoke previous actions once it finds evidence that they may have been incorrect by locally greedy choices.\n[BOS] Dyer et al (2015) used three LSTMs to represent the buffer, stack, and parsing history, getting state-of-the-art results on Chinese and English dependency parsing tasks.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ"], "span_citation_mapping": [{"token_start": 16, "token_end": 44, "char_start": 92, "char_end": 189, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Weiss et al., 2015;": null, "Zhou et al., 2015;": "17887856", "Andor et al., 2016)": "2952144"}}}, {"token_start": 80, "token_end": 120, "char_start": 372, "char_end": 541, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Dyer et al (2015)": null}, "Reference": {}}}]}
{"id": "9950135_1", "paragraph": "[BOS] In transition-based parsing, we learn a model for scoring transitions from one state to the next, conditioned on the parse history, and parse a sentence by taking the highest-scoring transition out of every state until a complete dependency graph has been derived.\n[BOS] Chen and Manning (2014) made the first successful attempt at introducing deep learning into a transition-based dependency parser.\n[BOS] At each step, the feed-forward neural network assigns a probability to every action the parse can take from certain state (words on the stack and buffer).\n[BOS] Some researchers have attempted to address the limitations of (Chen and Manning, 2014) by augmenting it with additional complexity.\n\n", "discourse_tags": ["Reflection", "Single_summ", "Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 53, "token_end": 110, "char_start": 277, "char_end": 567, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Chen and Manning (2014)": null}, "Reference": {}}}, {"token_start": 118, "token_end": 128, "char_start": 621, "char_end": 660, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chen and Manning, 2014)": null}}}]}
{"id": "9950135_0", "paragraph": "[BOS] Dependency-based syntactic representations of sentences have been found to be useful for various NLP tasks, especially for those involving natural language understanding in some way.\n[BOS] We briefly review prior work both on graph-based and transition-based neural dependency parsers.\n\n", "discourse_tags": ["Transition", "Transition"], "span_citation_mapping": []}
{"id": "9928714_12", "paragraph": "[BOS] Up to this point, the zero anaphora of the following three cases cannot be resolved: (i) no subject was detected for any predicate in a group linked by the subject sharing relations in the SSPN, (ii) no subject sharing relation was recognized for a predicate in the SSPN and (iii) non-1 http://svmlight.joachims.org/ 2 Note that if a predicate appears in a relative clause and a noun modified by the clause is the semantic subject of the predicate, the noun is not regarded as subject by our subject detector.\n[BOS] 3 The subject sharing recognizer is likely to regard two predicates, each of which has its own subject, as non-subject sharing predicate pairs, but it is still logically possible that they are judged as subject sharing predicate pairs hence as a part of an SSPN.\n[BOS] To resolve zero anaphora in these cases, we apply a state-of-the-art ILP-based zero anaphora resolution method (Iida and Poesio, 2011) in Step 5.\n[BOS] This method determines zero anaphor and its antecedent by joint inference using the results of subject detection, zero anaphor detection and intraand inter-sentential antecedent identification.\n[BOS] In the original method by Iida and Poesio (2011) , the inter-sentential zero anaphora was resolved, but in this work we focus on intra-sentential zero anaphora.\n[BOS] To adapt their method for our problem setting, we simply removed the inter-sentential antecedent identification model from their method.\n\n", "discourse_tags": ["Other", "Transition", "Narrative_cite", "Reflection", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 192, "token_end": 211, "char_start": 860, "char_end": 925, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 251, "token_end": 275, "char_start": 1146, "char_end": 1241, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Iida and Poesio (2011)": "9800505"}, "Reference": {}}}]}
{"id": "9928714_11", "paragraph": "[BOS] Note that our subject detector checks whether each predicate in an SSPN has a syntactic subject among its arguments.\n[BOS] An SSPN can include more than one predicate, and each predicate may have its own subject 3 .\n[BOS] In this step, if two or more distinct subjects are detected for predicates in an SSPN, we use the most likely subject (i.e., the subject with the highest SVM score outputted by our subject detector) for subject propagation.\n[BOS] Note that subject propagation is not performed if the subject position of a predicate is already filled.\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "9928714_10", "paragraph": "[BOS] For use in Step 3, we create a subject detector, which judges whether an argument to a predicate is its subject using SVM light 1 , an implementation of Support Vector Machine (Vapnik, 1998) , with a polynomial kernel of 2nd degree.\n[BOS] The training instances of the subject detector are extracted from the predicate-argument relations 2 in the NAIST Text Corpus.\n[BOS] The numbers of positive and negative instances are 35,304 and 104,250 respectively.\n[BOS] As features, we used the morpho-syntactic information about the lemmas of the predicate and its argument and the functional words following the predicate and its argument.\n[BOS] The results of subject detection with 5-fold cross-validation demonstrate that our subject detector accurately detects subjects with performances of 0.949 in recall, 0.855 in precision, and 0.899 in F-score.\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 33, "token_end": 42, "char_start": 159, "char_end": 196, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vapnik, 1998)": "60750129"}}}]}
{"id": "9928714_9", "paragraph": "[BOS] Given the results of pairwise subject sharing recognition, we construct an SSPN in Step 2.\n[BOS] In an SSPN, every predicate in a sentence is a node and only the predicate pairs that were judged to be subject sharing are connected by a link.\n[BOS] The major advantage of explicitly constructing an SSPN is that it enables us to resolve zero anaphora even if a predicate with a subject zero anaphor does not have any direct subject sharing relation with a predicate with a subject, like predicates susumeru (advance) and hakensuru (dispatch) in Figure 1 .\n[BOS] By traversing the paths of the subject sharing relations in the SSPN, such predicates can be connected to successfully propagate the subject.\n[BOS] The effect of introducing SSPNs is empirically evaluated in Section 6.\n\n", "discourse_tags": ["Reflection", "Other", "Other", "Other", "Reflection"], "span_citation_mapping": []}
{"id": "9928714_8", "paragraph": "[BOS] We define subject sharing relations as follows.\n[BOS] Two predicates have a subject sharing relation if and only if they share the same subject that is referred to by (zero) anaphora or coreference.\n[BOS] Note that the shared subject does not need to be realized in the text; it can appear as inter-sentential zero anaphora or exophora.\n[BOS] In Step 1, the pairwise subject sharing relations between two predicates are recognized, but recognizing the relations between any two predicates in a sentence remains difficult.\n[BOS] We thus focus on some typical types of predicate pairs.\n[BOS] The details of the predicate pair types will be explained in Section 4.1.\n\n", "discourse_tags": ["Reflection", "Other", "Other", "Other", "Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "9928714_7", "paragraph": "[BOS] Step 5 For resolving the potential zero anaphora that were not resolved until Step 4, we apply the existing ILP-based method (Iida and Poesio, 2011 ).\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": [{"token_start": 23, "token_end": 37, "char_start": 114, "char_end": 153, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Iida and Poesio, 2011": "9800505"}}}]}
{"id": "9928714_6", "paragraph": "[BOS] Step 3 For each predicate in the set of the subject shared predicates in the SSPN, a subject is detected by our subject detector, if one exists.\n[BOS] Step 4 If a subject is detected, it is propagated to the empty subject position of each predicate in the subject shared predicates in the SSPN.\n\n", "discourse_tags": ["Other", "Other"], "span_citation_mapping": []}
{"id": "9928714_5", "paragraph": "[BOS] Step 2 A subject shared predicate network (SSPN) is constructed based on the results of pairwise subject sharing recognition.\n\n", "discourse_tags": ["Other"], "span_citation_mapping": []}
{"id": "9928714_4", "paragraph": "[BOS] Step 1 The pairwise subject sharing relations between two predicates in a sentence are recognized by our subject sharing recognizer.\n\n", "discourse_tags": ["Other"], "span_citation_mapping": []}
{"id": "9928714_3", "paragraph": "[BOS] In this section, we first give an overview of the procedure of our zero anaphora resolution method.\n[BOS] Intra-sentential zero anaphora resolution in our method is performed in the following five steps, as depicted in Figure 2 .\n\n", "discourse_tags": ["Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "9928714_2", "paragraph": "[BOS] 3 Zero anaphora resolution using subject shared predicate network\n\n", "discourse_tags": ["Other"], "span_citation_mapping": []}
{"id": "9928714_1", "paragraph": "[BOS] Concerning subject sharing recognition, related methods have been explored for pronominal anaphora (Yang et al., 2005) or coreference resolution (Bean and Riloff, 2004; Bansal and Klein, 2012) .\n[BOS] In these methods, the semantic compatibility between the contexts surrounding an anaphor and its antecedent (e.g., the compatibility of verbs kidnap and release given some arguments) was automatically extracted from raw texts in an unsupervised manner and used as features in a machine learning-based approach.\n[BOS] However, because the automatically acquired semantic compatibility is not always true or applicable in the context of any pair of an anaphor and its antecedent, the effectiveness of the compatibility features might be weakened.\n[BOS] In contrast, we accurately recognize the explicit subject sharing relations and directly use them for propagating the subject of some predicate to the empty subject position of other predicates instead of indirectly using the relations as features.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 13, "token_end": 26, "char_start": 85, "char_end": 124, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yang et al., 2005)": "2536406"}}}, {"token_start": 27, "token_end": 46, "char_start": 128, "char_end": 198, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bean and Riloff, 2004;": "2380594", "Bansal and Klein, 2012)": "12053891"}}}]}
{"id": "9928714_0", "paragraph": "[BOS] Traditional approaches to zero anaphora resolution are based on manually created heuristic rules (Kameyama, 1986; Walker et al., 1994; Okumura and Tamura, 1996; Nakaiwa and Shirai, 1996) , which are mainly motivated by the rules and preferences introduced in Centering Theory (Grosz et al., 1995) .\n[BOS] However, the research trend of zero anaphora resolution has shifted from such rule-based approaches to machine learningbased approaches because in machine learning we can easily integrate many different types of information, such as morpho-syntactic, semantic and discourse-related information.\n[BOS] Researchers have developed methods of zero anaphora resolution for Chinese (Zhao and Ng, 2007; Chen and Ng, 2013) , Japanese (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2007a; Taira et al., 2008; Sasano et al., 2008; Sasano et al., 2009; Imamura et al., 2009; Watanabe et al., 2010; Hayashibe et al., 2011; Iida and Poesio, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Yoshino et al., 2013) and Italian (Iida and Poesio, 2011) .\n[BOS] One critical issue in zero anaphora resolution is optimizing the outputs of sub-problems (e.g., zero anaphor detection and antecedent identification).\n[BOS] Recent works by Watanabe et al. (2010) , Iida and Poesio (2011) and Yoshikawa et al. (2011) revealed that joint inference improves the overall performance of zero anaphora resolution.\n[BOS] We employed one of these works as a baseline in Section 6.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Narrative_cite", "Transition", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 14, "token_end": 48, "char_start": 87, "char_end": 192, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kameyama, 1986;": "8493965", "Walker et al., 1994;": "1141127", "Okumura and Tamura, 1996;": "1435609", "Nakaiwa and Shirai, 1996)": "399277"}}}, {"token_start": 60, "token_end": 72, "char_start": 265, "char_end": 302, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Grosz et al., 1995)": "11660053"}}}, {"token_start": 136, "token_end": 150, "char_start": 679, "char_end": 725, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhao and Ng, 2007;": "7739917", "Chen and Ng, 2013)": "12269372"}}}, {"token_start": 151, "token_end": 260, "char_start": 728, "char_end": 1022, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Seki et al., 2002;": "1804978", "Isozaki and Hirao, 2003;": "5640129", "Iida et al., 2007a;": "11556806", "Taira et al., 2008;": "15825278", "Sasano et al., 2008;": "17278936", "Sasano et al., 2009;": "15101932", "Imamura et al., 2009;": "14712610", "Watanabe et al., 2010;": "8835501", "Hayashibe et al., 2011;": "16081495", "Iida and Poesio, 2011;": "9800505", "Yoshikawa et al., 2011;": "2266346", "Hangyo et al., 2013;": "5797690", "Yoshino et al., 2013)": "14059378"}}}, {"token_start": 261, "token_end": 272, "char_start": 1027, "char_end": 1058, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Iida and Poesio, 2011)": "9800505"}}}, {"token_start": 306, "token_end": 349, "char_start": 1224, "char_end": 1407, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Watanabe et al. (2010)": "8835501", "Iida and Poesio (2011)": "9800505", "Yoshikawa et al. (2011)": "2266346"}}}]}
{"id": "9914140_3", "paragraph": "[BOS] Unlike a lot of relevant work that requires additional resources such as the lexical ontology (Pilehvar and Collier, 2016; Rothe and Schtze, 2015; Jauhar et al., 2015; Chen et al., 2015; Iacobacci et al., 2015) or bilingual data (Guo et al., 2014; Ettinger et al., 2016; uster et al., 2016) , which may be unavailable in some language, our model can be trained using only an unlabeled corpus.\n[BOS] Also, some prior work proposed to learn topical embeddings and word embeddings jointly in order to consider the contexts (Liu et al., 2015a,b) , whereas this paper focuses on learning multi-sense word embeddings.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 15, "token_end": 62, "char_start": 83, "char_end": 216, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Pilehvar and Collier, 2016;": "16173223", "Rothe and Sch\u00fctze, 2015;": "15687295", "Jauhar et al., 2015;": "14667200", "Chen et al., 2015;": "7567901", "Iacobacci et al., 2015)": "16863934"}}}, {"token_start": 63, "token_end": 90, "char_start": 220, "char_end": 296, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Guo et al., 2014;": "17503845", "Ettinger et al., 2016;": "9724599"}}}, {"token_start": 127, "token_end": 142, "char_start": 501, "char_end": 547, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "9914140_2", "paragraph": "[BOS] Recently, Qiu et al. (2016) proposed an EM algorithm to learn purely sense-level representations, where the computational cost is high when decoding the sense identity sequence, because it takes exponential time to search all sense combination within a context window.\n[BOS] Our modular design addresses such drawback, where the sense selection module decodes a sense sequence with linear-time complexity, while the sense representation module remains representation learning in the pure sense level.\n\n", "discourse_tags": ["Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 4, "token_end": 52, "char_start": 16, "char_end": 274, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Qiu et al. (2016)": null}, "Reference": {}}}]}
{"id": "9914140_1", "paragraph": "[BOS] Instead of clustering, probabilistic modeling methods have been applied for learning multisense embeddings in order to make the sense selection more flexible, where Tian et al. (2014) and Jauhar et al. (2015) conducted probabilistic modeling with EM training.\n[BOS] Li and Jurafsky (2015) exploited Chinese Restaurant Process to infer the sense identity.\n[BOS] Furthermore, Bartunov et al. (2016) developed a non-parametric Bayesian extension on the skip-gram model (Mikolov et al., 2013b) .\n[BOS] Despite reasonable modeling on sense selection, all above methods mixed wordlevel and sense-level tokens during representation learning-unable to conduct representation learning in the pure sense level due to the complicated computation in their EM algorithms.\n\n", "discourse_tags": ["Multi_summ", "Single_summ", "Multi_summ", "Transition"], "span_citation_mapping": [{"token_start": 28, "token_end": 52, "char_start": 171, "char_end": 265, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Tian et al. (2014)": "9693038", "Jauhar et al. (2015)": "14667200"}, "Reference": {}}}, {"token_start": 53, "token_end": 72, "char_start": 272, "char_end": 360, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Li and Jurafsky (2015)": "6222768"}, "Reference": {}}}, {"token_start": 75, "token_end": 107, "char_start": 380, "char_end": 495, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Bartunov et al. (2016)": "12909464"}, "Reference": {"(Mikolov et al., 2013b)": "16447573"}}}]}
{"id": "9914140_0", "paragraph": "[BOS] There are three dominant types of approaches for learning multi-sense word representations in the literature: 1) clustering methods, 2) probabilistic modeling methods, and 3) lexical ontology based methods.\n[BOS] Our reinforcement learning based approach can be loosely connected to clustering methods and probabilistic modeling methods.\n[BOS] Reisinger and Mooney (2010) first proposed multi-sense word representations on the vector space based on clustering techniques.\n[BOS] With the power of deep learning, some work exploited neural networks to learn embeddings with sense selection based on clustering (Huang et al., 2012; Neelakantan et al., 2014) .\n[BOS] replaced the clustering procedure with a word sense disambiguation model using WordNet (Miller, 1995) .\n[BOS] Kgebck et al. (2015) and Vu and Parker (2016) further leveraged a weighting mechanism and interactive process in the clustering procedure.\n[BOS] Moreover, Guo et al. (2014) leveraged bilingual resources for clustering.\n[BOS] However, most of the above approaches separated the clustering procedure and the representation learning procedure without a joint objective, which may suffer from the error propagation issue.\n[BOS] Instead, the proposed approach, MUSE, enables joint training on sense selection and representation learning.\n\n", "discourse_tags": ["Transition", "Reflection", "Single_summ", "Narrative_cite", "Narrative_cite", "Multi_summ", "Single_summ", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 58, "token_end": 83, "char_start": 350, "char_end": 477, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Reisinger and Mooney (2010)": "2156506"}, "Reference": {}}}, {"token_start": 97, "token_end": 123, "char_start": 556, "char_end": 660, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Huang et al., 2012;": "372093", "Neelakantan et al., 2014)": "15251438"}}}, {"token_start": 137, "token_end": 144, "char_start": 748, "char_end": 770, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Miller, 1995)": "207846993"}}}, {"token_start": 146, "token_end": 176, "char_start": 779, "char_end": 917, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"K\u00e5geb\u00e4ck et al. (2015)": "18428685", "Vu and Parker (2016)": "14039235"}, "Reference": {}}}, {"token_start": 179, "token_end": 193, "char_start": 934, "char_end": 997, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Guo et al. (2014)": "17503845"}, "Reference": {}}}]}
{"id": "981974_3", "paragraph": "[BOS] Features extracted from LTAG derivations are different and provide distinct information when compared to predicate-argument features (PAF) or subcategorization features (SCF) used in (Moschitti, 2004) or even the later use of argument spanning trees (AST) in the same framework.\n[BOS] The adjunction operation of LTAG and the extended domain of locality is not captured by those features as we have explained in detail in Section 2.\n\n", "discourse_tags": ["Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 17, "token_end": 43, "char_start": 111, "char_end": 206, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Moschitti, 2004)": null}}}]}
{"id": "981974_2", "paragraph": "[BOS] Our approach shares similar motivations with the approach in (Shen and Joshi, 2005) which uses PropBank information to recover an LTAG treebank as if it were hidden data underlying the Penn Treebank.\n[BOS] However their goal was to extract an LTAG grammar using PropBank information from the Treebank, and not the SRL task.\n\n", "discourse_tags": ["Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 5, "token_end": 70, "char_start": 26, "char_end": 329, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Shen and Joshi, 2005)": null}, "Reference": {}}}]}
{"id": "981974_1", "paragraph": "[BOS] Compared to (Liu and Sarkar, 2006) , we have used a more sophisticated learning algorithm and a richer set of syntactic LTAG-based features in this task.\n[BOS] In particular, in this paper we built a strong baseline system using a standard set of features and did a thorough comparison between this strong baseline and our proposed system with LTAG-based features.\n[BOS] The experiments in (Liu and Sarkar, 2006) were conducted on gold parses and it failed to show any improvements after adding LTAG-based features.\n[BOS] Our experimental results show that LTAGbased features can help improve the performance of SRL systems.\n[BOS] While (Liu and Sarkar, 2006) propose some new features for SRL based on LTAG derivations, we propose several novel features and in addition they do not show that their features are useful for SRL.\n\n", "discourse_tags": ["Reflection", "Reflection", "Single_summ", "Reflection", "Single_summ"], "span_citation_mapping": [{"token_start": 4, "token_end": 12, "char_start": 18, "char_end": 40, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Liu and Sarkar, 2006)": "6244060"}}}, {"token_start": 76, "token_end": 108, "char_start": 377, "char_end": 521, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Liu and Sarkar, 2006)": "6244060"}, "Reference": {}}}, {"token_start": 130, "token_end": 172, "char_start": 643, "char_end": 833, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Liu and Sarkar, 2006)": "6244060"}, "Reference": {}}}]}
{"id": "981974_0", "paragraph": "[BOS] There has been some previous work in SRL that uses LTAG-based decomposition of the parse tree.\n[BOS] (Chen and Rambow, 2003) use LTAG-based decomposition of parse trees (as is typically done for statistical LTAG parsing) for SRL.\n[BOS] Instead of extracting a typical \"standard\" path feature from the derived tree, (Chen and Rambow, 2003) uses the path within the elementary tree from the predicate to the constituent argument.\n[BOS] Under this frame, they only recover semantic roles for those constituents that are localized within a single etree for the predicate, ignoring cases that occur outside the etree.\n[BOS] As stated in their paper, \"as a consequence, adjunct semantic roles (ARGM's) are basically absent from our test corpus\"; and around 13% complement semantic roles cannot be found in etrees in the gold parses.\n[BOS] In contrast, we recover all SRLs by exploiting more general paths in the LTAG derivation tree.\n[BOS] A similar drawback can be found in (Gildea and Hockenmaier, 2003) where a parse tree path was defined in terms of Combinatory Categorial Grammar (CCG) types using grammatical relations between predicate and arguments.\n[BOS] The two relations they defined can only capture 77% arguments in Propbank and they had to use a standard path feature as a replacement when the defined relations cannot be found in CCG derivation trees.\n[BOS] In our framework, we use intermediate sub-structures from LTAG derivations to capture these relations instead of bypassing this issue.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 24, "token_end": 72, "char_start": 107, "char_end": 320, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Chen and Rambow, 2003)": "6468633"}, "Reference": {}}}, {"token_start": 72, "token_end": 178, "char_start": 321, "char_end": 832, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Chen and Rambow, 2003)": "6468633"}, "Reference": {}}}, {"token_start": 200, "token_end": 287, "char_start": 940, "char_end": 1366, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Gildea and Hockenmaier, 2003)": "11869911"}, "Reference": {}}}]}
{"id": "9795001_7", "paragraph": "[BOS] Following this work, we employ an attentive Recurrent Network as described in to the task of abstractive summarization of scene descriptions.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "9795001_6", "paragraph": "[BOS] A similar model is used by (Rush et al., 2015) to generate headlines.\n[BOS] They train the model on a data set compiled from the GigaWord corpus, where longer sentences from news articles are paired with the corresponding headline of the article.\n[BOS] They compare the performance of an attention-based RNN with a collection of other systems.\n[BOS] They find that the vanilla attention-based RNN is unable to outperform a Moses system.\n[BOS] Only after additional tuning on extractive compresssions do they get better ROUGE scores.\n[BOS] This can be attributed to the fact that additional extractive features bias the system towards retaining more input words, which is beneficial for higher ROUGE scores.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 2, "token_end": 114, "char_start": 6, "char_end": 538, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Rush et al., 2015)": "1918428"}, "Reference": {}}}]}
{"id": "9795001_5", "paragraph": "[BOS] This encoder-decoder approach encodes a source sequence into a vector with fixed length, which the decoder decodes into the target sequence.\n[BOS] The model is trained as a whole to maximize the probability of a correct transduction given the source sentence.\n[BOS] While normal RNNs can have difficulties with long term dependencies, the Long ShortTerm Memory (LSTM) is an extension that can handle these dependencies well and which can avoid vanishing gradients (Hochreiter and Schmidhuber, 1997 RNN encoders create a single representation of the entire source sequence from which the target sequence is generated by the decoder.\n[BOS] claim that this fixed-length vector prevents improving the performance of encoder-decoder systems.\n[BOS] This is particularly the case when the RNN needs to deal with long sentences.\n[BOS] They propose an extension that allows a model to automatically search for parts of a source sentence that are relevant to predicting a target word.\n[BOS] So, each time a target word is generated by the decoder, the model tries to find the places in the source sentence where the most relevant information is concentrated.\n[BOS] This architecture differs from the basic encoder-decoder in that it encodes the input sentence into a sequence of vectors and chooses a subset of these vectors while decoding.\n[BOS] This means that not all information needs to be stored in one fixed-length vector, allowing for better performance on for instance longer sentences.\n[BOS] In this way the model can learn soft alignments between source and target segments.\n[BOS] This approach is called soft attention and the resulting model is an attention-based Recurrent Neural Network (aRNN).\n[BOS] For a more detailed description of the model, see .\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Transition", "Other"], "span_citation_mapping": [{"token_start": 62, "token_end": 292, "char_start": 341, "char_end": 1581, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Hochreiter and Schmidhuber, 1997": "1915014"}, "Reference": {}}}]}
{"id": "9795001_4", "paragraph": "[BOS] Recently, significant advances have been made in sequence to sequence learning.\n[BOS] The paradigm has shifted from traditional approaches that are more focused on optimizing the parameters of several subsystems, to a single model that learns mappings between sequences by learning fixed representations end to end.\n[BOS] This approach employs large recurrent neural networks (RNNs) and has been successfully applied to machine translation Sutskever et al., 2014) , image captioning (Vinyals et 42 al., 2015) and extractive summarization (Filippova et al., 2015) .\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 70, "token_end": 81, "char_start": 426, "char_end": 469, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Sutskever et al., 2014)": "7961699"}}}, {"token_start": 82, "token_end": 97, "char_start": 472, "char_end": 514, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 98, "token_end": 112, "char_start": 519, "char_end": 568, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Filippova et al., 2015)": "1992250"}}}]}
{"id": "9795001_3", "paragraph": "[BOS] There has been some work on the related task of sentence simplification.\n[BOS] (Coster and Kauchak, 2011; Zhu et al., 2010) develop models using data from Simple English Wikipedia paired with English Wikipedia.\n[BOS] Their models were able to perform rewording, reordering, insertion and deletion actions.\n[BOS] (Woodsend and Lapata, 2011) use Simple Wikipedia edit histories and an aligned WikipediaSimple Wikipedia corpus to induce a model based on quasi-synchronous grammar and integer linear programming.\n[BOS] (Wubben et al., 2012) propose a model for simplifying sentences using monolingual Phrase-Based Machine Translation obtaining state of the art results.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 16, "token_end": 65, "char_start": 85, "char_end": 311, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Coster and Kauchak, 2011;": "4896510", "Zhu et al., 2010)": "15636533"}, "Reference": {}}}, {"token_start": 66, "token_end": 102, "char_start": 318, "char_end": 514, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Woodsend and Lapata, 2011)": "9945908"}, "Reference": {}}}, {"token_start": 103, "token_end": 134, "char_start": 521, "char_end": 671, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Wubben et al., 2012)": "141120"}, "Reference": {}}}]}
{"id": "9795001_2", "paragraph": "[BOS] In contrast to the large body of work on extractive sentence compression, work on abstractive sentence compression is relatively sparse.\n[BOS] propose an abstractive sentence compression method based on a parse tree transduction grammar and Integer Linear Programming.\n[BOS] For their abstractive model, the grammar that is extracted is augmented with paraphrasing rules obtained from a pivoting approach to a bilingual corpus (Bannard and Burch, 2005) .\n[BOS] They show that the abstractive model outperforms an extractive model on their dataset.\n[BOS] (Cohn and Lapata, 2013) follow up on earlier work and describe a discriminative tree-to-tree transduction model that can handle mismatches on the structural and lexical level.\n\n", "discourse_tags": ["Transition", "Other", "Single_summ", "Other", "Single_summ"], "span_citation_mapping": [{"token_start": 73, "token_end": 84, "char_start": 416, "char_end": 458, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bannard and Burch, 2005)": "15728911"}}}, {"token_start": 103, "token_end": 139, "char_start": 560, "char_end": 735, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Cohn and Lapata, 2013)": "16894618"}, "Reference": {}}}]}
{"id": "9795001_1", "paragraph": "[BOS] (McDonald, 2006) develop a system using largemargin online learning combined with a decoding algorithm that searches the compression space to produce a compressed sentence.\n[BOS] Discriminative learning is used to combine the features and weight their contribution to a successful compression.\n[BOS] (Cohn and Lapata, 2007) cast the sentence compression problem as a tree-to-tree rewriting task.\n[BOS] For this task, they train a synchronous tree substitution grammar, which dictates the space of all possible rewrites.\n[BOS] By using discriminative training, a weight is assigned to each grammar rule.\n[BOS] These grammar rules are then used to generate compressions by a decoder.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 50, "char_start": 6, "char_end": 299, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 51, "token_end": 129, "char_start": 306, "char_end": 687, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Cohn and Lapata, 2007)": "1185365"}, "Reference": {}}}]}
{"id": "9795001_0", "paragraph": "[BOS] A large body of work is devoted to extractive sentence compression.\n[BOS] Here, we mention a few.\n[BOS] (Knight and Marcu, 2002) propose two models to generate a short sentence by deleting a subset of words: the decision tree model and the noisy channel model, both based on a synchronous context free grammar.\n[BOS] (Turner and Charniak, 2005) and (Galley and McKeown, 2007) build upon this model reporting improved results.\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ", "Multi_summ"], "span_citation_mapping": [{"token_start": 24, "token_end": 67, "char_start": 110, "char_end": 316, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Knight and Marcu, 2002)": "7793213"}, "Reference": {}}}, {"token_start": 68, "token_end": 96, "char_start": 323, "char_end": 431, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Turner and Charniak, 2005)": "15519576", "(Galley and McKeown, 2007)": "1762277"}, "Reference": {}}}]}
{"id": "9711750_0", "paragraph": "[BOS] The most closely related work to our supervised re-ranking of PPDB is work by Zhao et al. (2008) Chan et al. (2011) which reranked bilingually-extracted paraphrases using monolingual distributional similarities, but did not use a supervised model.\n[BOS] Work that is relevant to our classification of semantic entailment types to each paraphrase, includes learning directionality of inference rules (Bhagat et al., 2007; Berant et al., 2011) and learning hypernyms rather than paraphrases (Snow et al., 2004) .\n[BOS] Our style annotations are related to Xu et al. (2012) 's efforts at learning stylistic paraphrases.\n[BOS] Our word embeddings additions to the paraphrase database are related to many current projects on that topic, including projects that attempt to customize embeddings to lexical resources (Faruqui et al., 2015) .\n[BOS] However, the Rastogi et al. (2015) embeddings included here were shown to be state-of-the art in 428 predicting human judgements.\n\n", "discourse_tags": ["Multi_summ", "Narrative_cite", "Reflection", "Reflection", "Narrative_cite"], "span_citation_mapping": [{"token_start": 2, "token_end": 58, "char_start": 6, "char_end": 253, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhao et al. (2008)": "12003953", "Chan et al. (2011)": "6502219"}, "Reference": {}}}, {"token_start": 78, "token_end": 102, "char_start": 362, "char_end": 447, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bhagat et al., 2007;": "10118971", "Berant et al., 2011)": "171268"}}}, {"token_start": 103, "token_end": 120, "char_start": 452, "char_end": 514, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Snow et al., 2004)": "1854720"}}}, {"token_start": 128, "token_end": 146, "char_start": 560, "char_end": 622, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Xu et al. (2012)": "13050210"}}}, {"token_start": 176, "token_end": 189, "char_start": 797, "char_end": 837, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Faruqui et al., 2015)": "51838647"}}}, {"token_start": 194, "token_end": 224, "char_start": 859, "char_end": 975, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Rastogi et al. (2015)": "2021980"}}}]}
{"id": "9549525_3", "paragraph": "[BOS] Beam search is known to improve quality of NMT translation output.\n[BOS] However, it is also known that larger beam size does not always helps but rather hurts the quality (Tu et al., 2016a) .\n[BOS] Therefore it is important to understand how beam search affects quality.\n[BOS] (Wu et al., 2016; Freitag and AlOnaizan, 2017) proposed several penalty functions and pruning methods for beam search.\n[BOS] We directly visualize beam search result as a tree and manually explore hypotheses discarded by decoder.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Multi_summ", "Reflection"], "span_citation_mapping": [{"token_start": 27, "token_end": 45, "char_start": 132, "char_end": 196, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Tu et al., 2016a)": "15830483"}}}, {"token_start": 60, "token_end": 90, "char_start": 284, "char_end": 402, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Wu et al., 2016;": "3603249"}, "Reference": {}}}]}
{"id": "9549525_2", "paragraph": "[BOS] Attention (Bahdanau et al., 2014; Luong et al., 2015) is an important component for improving NMT quality.\n[BOS] Since the component behaves like alignment in traditional SMT, it has been proposed to utilize attention during training (Cheng et al., 2015; Tu et al., 2016b) or during decoding (Wu et al., 2016) .\n[BOS] In this work, we propose a way to manipulate attention and to understand the behavior.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 21, "char_start": 6, "char_end": 59, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bahdanau et al., 2014;": "11212020", "Luong et al., 2015)": "1998416"}}}, {"token_start": 49, "token_end": 68, "char_start": 214, "char_end": 278, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cheng et al., 2015;": "3935945", "Tu et al., 2016b)": "16113848"}}}, {"token_start": 69, "token_end": 79, "char_start": 282, "char_end": 315, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wu et al., 2016)": "3603249"}}}]}
{"id": "9549525_1", "paragraph": "[BOS] Visualization and manipulation of NMT could be grouped into three parts: RNN (of encoder and decoder), attention (of decoder), and beam search (of decoder).\n[BOS] RNN plays a central role in recognizing source sentences and generating target sentences.\n[BOS] Although we here treat RNN as a black-box, there exists various methods to understand RNNs, e.g. by observing intermediate values (Strobelt et al., 2016; Karpathy et al., 2015; or by removing some parts of them (Goh, 2016; Li et al., 2016) .\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 81, "token_end": 102, "char_start": 365, "char_end": 440, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Strobelt et al., 2016;": "5494923"}}}, {"token_start": 105, "token_end": 123, "char_start": 448, "char_end": 504, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Goh, 2016;": null, "Li et al., 2016)": "13017314"}}}]}
{"id": "9549525_0", "paragraph": "[BOS] There have been various methods proposed for visualizing and intervening neural models for NLP.\n[BOS] provides a concise literature review.\n\n", "discourse_tags": ["Transition", "Transition"], "span_citation_mapping": []}
{"id": "9508540_0", "paragraph": "[BOS] Domain adaptation has been studied for a wide range of natural language processing tasks (Blitzer et al., 2007; Florian et al., 2004; Daume III, 2007; Foster et al., 2010) .\n[BOS] However, little has been done for investigating summarization systems (Sandu et al., 2010; Wang and Cardie, 2013) .\n[BOS] To the best of our knowledge, we are the first to study the adaptation of neural summarization models for new domain.\n[BOS] Furthermore, Recent work in neural summarization mainly focuses on specfic extensions to improve system performance (Rush et al., 2015; Takase et al., 2016; Gu et al., 2016; Nallapati et al., 2016; Ranzato et al., 2015) .\n[BOS] It is unclear how to adapt the existing neural summarization systems to a new domain when the training data is limited or not available.\n[BOS] This is a question we aim to address in this work.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Reflection", "Narrative_cite", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 12, "token_end": 46, "char_start": 61, "char_end": 177, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Blitzer et al., 2007;": "14688775", "Florian et al., 2004;": null, "Daume III, 2007;": "5360764", "Foster et al., 2010)": "6996688"}}}, {"token_start": 56, "token_end": 75, "char_start": 234, "char_end": 299, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sandu et al., 2010;": "17086300", "Wang and Cardie, 2013)": "1030812"}}}, {"token_start": 118, "token_end": 162, "char_start": 529, "char_end": 651, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rush et al., 2015;": "1918428", "Takase et al., 2016;": "5450302", "Gu et al., 2016;": "8174613", "Nallapati et al., 2016;": "8928715", "Ranzato et al., 2015)": "7147309"}}}]}
{"id": "9428803_2", "paragraph": "[BOS] ICE is also the only tool that can graphically compare predictions of a system to a gold standard with a fine-grained distinction on the types of differences.\n[BOS] Kummerfeld and Klein (2013) present an algorithm that transforms a predicted coreference clustering into a gold clustering and records the necessary transformations, thereby quantifying different types of errors.\n[BOS] However, their algorithm only works on clusterings (sets of mentions), not pairwise links, and is therefore not able to pinpoint some of the mistakes that ICE can (such as the foreign antecedent described in Section 3).\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 34, "token_end": 117, "char_start": 171, "char_end": 609, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kummerfeld and Klein (2013)": "2232310"}, "Reference": {}}}]}
{"id": "9428803_1", "paragraph": "[BOS] The aforementioned tools are primarily meant as annotation tools.\n[BOS] They have a tendency of locking the user into one type of visualization (tree-or text-based), while often lacking advanced search functionality.\n[BOS] In contrast to them, ICE is not meant to be yet another annotation tool, but was designed as a dedicated coreference exploration tool, which enables the user to swiftly switch between different views.\n[BOS] Moreover, none of the existing tools provide an entity-grid view.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Transition"], "span_citation_mapping": []}
{"id": "9428803_0", "paragraph": "[BOS] Two popular annotation and visualization tools for coreference are PAlinkA (Orsan, 2003) and MMAX2 (Mller and Strube, 2006) , which focus on a (customizable) textual visualization with highlighting of clusters.\n[BOS] The TrED (Pajas an Stpnek, 2009 ) project is a very flexible multilevel annotation tool centered around tree-based annotations that can be used to annotate and visualize coreference.\n[BOS] It also features a powerful search engine.\n[BOS] Recent annotation tools include the web-based BRAT (Stenetorp et al., 2012) and its extension WebAnno (Yimam et al., 2013) .\n[BOS] A dedicated query and exploration tool for multi-level annotations is ANNIS (Zeldes et al., 2009) .\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 12, "token_end": 21, "char_start": 73, "char_end": 94, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Or\u0203san, 2003)": "13358273"}}}, {"token_start": 22, "token_end": 34, "char_start": 99, "char_end": 129, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 51, "token_end": 99, "char_start": 223, "char_end": 454, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 105, "token_end": 120, "char_start": 497, "char_end": 536, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 122, "token_end": 136, "char_start": 545, "char_end": 583, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 150, "token_end": 162, "char_start": 662, "char_end": 689, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "9375250_1", "paragraph": "[BOS] Previous studies on cut-and-paste summarization (Jing and McKeown, 2000; Saggion and Lapalme, 2002) investigate the operations that human summarizers perform on the source text in order to produce the summary text.\n[BOS] Our previous work argued that current extractive systems rely too heavily on notions of information centrality (Cheung and Penn, 2013) .\n[BOS] This paper extends this work by identifying specific linguistic factors correlated with the use of source-text-external elements.\n\n", "discourse_tags": ["Multi_summ", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 5, "token_end": 51, "char_start": 26, "char_end": 220, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Saggion and Lapalme, 2002)": null}, "Reference": {"(Jing and McKeown, 2000;": "800331"}}}, {"token_start": 67, "token_end": 77, "char_start": 315, "char_end": 361, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cheung and Penn, 2013)": "15945816"}}}]}
{"id": "9375250_0", "paragraph": "[BOS] A relatively large body of work exists in sentence compression (Knight and Marcu, 2000; McDonald, 2006; Galley and McKeown, 2007; Cohn and Lapata, 2008; Clarke and Lapata, 2008, inter alia) , and sentence fusion (Barzilay and McKeown, 2005; Marsi and Krahmer, 2005; Filippova and Strube, 2008; Filippova, 2010; Thadani and McKeown, 2013) .\n[BOS] Unlike this work, our sentence enhancement algorithm considers the entire source text and is not limited to the initial input sentences.\n[BOS] Few previous papers focus on combining the content of diverse sentences into one output sentence.\n[BOS] Wan et al. (2008) propose sentence augmentation by identifying \"seed\" words in a single original sentence, then adding information from auxiliary sentences based on word co-occurrence counts.\n[BOS] Elsner and Santhanam (2011) investigate the idea of fusing disparate sentences with a supervised algorithm, as discussed above.\n\n", "discourse_tags": ["Narrative_cite", "Reflection", "Transition", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 10, "token_end": 53, "char_start": 48, "char_end": 195, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Knight and Marcu, 2000;": null, "McDonald, 2006;": "3014198", "Galley and McKeown, 2007;": "1762277", "Cohn and Lapata, 2008;": "2411338"}}}, {"token_start": 55, "token_end": 102, "char_start": 202, "char_end": 343, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Barzilay and McKeown, 2005;": "16188305", "Marsi and Krahmer, 2005;": "2293515", "Filippova and Strube, 2008;": "14909308", "Filippova, 2010;": "14750088", "Thadani and McKeown, 2013)": "12635978"}}}, {"token_start": 145, "token_end": 181, "char_start": 599, "char_end": 790, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wan et al. (2008)": "17119732"}, "Reference": {}}}, {"token_start": 182, "token_end": 209, "char_start": 797, "char_end": 924, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Elsner and Santhanam (2011)": "781753"}, "Reference": {}}}]}
{"id": "9230380_2", "paragraph": "[BOS] Other methods of parser combinations have shown to be successful such as using one parser to generate features for another parser.\n[BOS] This was shown in (Nivre and McDonald, 2008) , in which Malt Parser was used as a feature to MST Parser.\n[BOS] The result was a successful combination of a transition-based and graph-based parser, but did not address adding other types of parsers into the framework.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 28, "token_end": 83, "char_start": 158, "char_end": 409, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Nivre and McDonald, 2008)": "9431510"}, "Reference": {}}}]}
{"id": "9230380_1", "paragraph": "[BOS] Parser combination with dependency trees has been examined in terms of accuracy (Sagae and Lavie, 2006; Sagae and Tsujii, 2007; Zeman an Zabokrtsk, 2005) .\n[BOS] However, the various techniques have generally examined similar parsers or parsers which have generated various different models.\n[BOS] To the best of our knowledge, our experiments are the first to look at the accuracy and part of speech error distribution when combining together constituent and dependency parsers that use many different techniques.\n[BOS] However, POS tags were used in parser combination in for combining a set of Malt Parser models with success.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 43, "char_start": 6, "char_end": 159, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sagae and Lavie, 2006;": "6133066", "Sagae and Tsujii, 2007;": "2768696"}}}]}
{"id": "9230380_0", "paragraph": "[BOS] Ensemble learning (Dietterich, 2000) has been used for a variety of machine learning tasks and recently has been applied to dependency parsing in various ways and with different levels of success.\n[BOS] (Surdeanu and Manning, 2010; Haffari et al., 2011) showed a successful combination of parse trees through a linear combination of trees with various weighting formulations.\n[BOS] To keep their tree constraint, they applied Eisner's algorithm for reparsing (Eisner, 1996) .\n\n", "discourse_tags": ["Single_summ", "Multi_summ", "Multi_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 39, "char_start": 6, "char_end": 202, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Dietterich, 2000)": "56776745"}, "Reference": {}}}, {"token_start": 40, "token_end": 102, "char_start": 209, "char_end": 479, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Surdeanu and Manning, 2010;": "1204756", "Haffari et al., 2011)": "10756783"}, "Reference": {"(Eisner, 1996)": "3262717"}}}]}
{"id": "9205274_2", "paragraph": "[BOS] The dependency language models described in this paper assign probabilities to full sentences.\n[BOS] Language models which require full sentences can be used in automatic speech recognition (ASR) and machine translation (MT).\n[BOS] The approach is to use a conventional ASR or MT decoder to produce an N-best list of the most likely candidate sentences and then re-score these with the language model.\n[BOS] This was done by Chelba et al. (1997) for ASR using a dependency language model and by Pauls and Klein (2011) for MT using a PSG-based syntactic language model.\n\n", "discourse_tags": ["Reflection", "Transition", "Transition", "Multi_summ"], "span_citation_mapping": [{"token_start": 83, "token_end": 99, "char_start": 431, "char_end": 493, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Chelba et al. (1997)": "14933205"}, "Reference": {}}}, {"token_start": 100, "token_end": 120, "char_start": 498, "char_end": 574, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Pauls and Klein (2011)": "10463701"}, "Reference": {}}}]}
{"id": "9205274_1", "paragraph": "[BOS] The most similar task to sentence completion is lexical substitution (McCarthy and Navigli, 2007) .\n[BOS] The main difference between them is that in the latter the word to be substituted provides a very important clue in choosing the right candidate, while in sentence completion this is not available.\n[BOS] Another related task is selectional preference modeling (Saghdha, 2010; Ritter et al., 2010) , where the aim is to assess the plausibility of possible syntactic arguments for a given word.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 10, "token_end": 22, "char_start": 54, "char_end": 103, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 65, "token_end": 84, "char_start": 340, "char_end": 408, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(S\u00e9aghdha, 2010;": "272315", "Ritter et al., 2010)": "14061182"}}}]}
{"id": "9205274_0", "paragraph": "[BOS] The best-known language model based on dependency parsing is that of Chelba et al. (1997) .\n[BOS] This model writes the probability in the familiar left-toright chain rule decomposition in the linear order of the sentence, conditioning the probability of the next word on the linear trigram context, as well as some part of the dependency graph information relating to the words on its left.\n[BOS] The language models we propose are far simpler to train and compute.\n[BOS] A somewhat similar model to our unlabelled dependency language model was proposed in Graham and van Genabith (2010) .\n[BOS] However they seem to have used different probability estimators which ignore the fact that each node in the dependency tree can have multiple children.\n[BOS] Other research on syntactic language modelling has focused on using phrase structure grammars (Pauls and Klein, 2012; Charniak, 2001; Roark, 2001; Hall and Johnson, 2003) .\n[BOS] The linear complexity of deterministic dependency parsing makes dependency language models such as ours more scalable than these approaches.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Reflection", "Single_summ", "Single_summ", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 2, "token_end": 23, "char_start": 6, "char_end": 95, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Chelba et al. (1997)": "14933205"}}}, {"token_start": 95, "token_end": 145, "char_start": 479, "char_end": 754, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Graham and van Genabith (2010)": "6098470"}, "Reference": {}}}, {"token_start": 156, "token_end": 184, "char_start": 829, "char_end": 931, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Pauls and Klein, 2012;": "17366870", "Charniak, 2001;": "457176", "Roark, 2001;": "6237722"}}}]}
{"id": "917915_0", "paragraph": "[BOS] There have been significant efforts in the both the monolingual parsing and machine translation literature to address the impact of the MAP approximation and the choice of labels in their respective models; we survey the work most closely related to our approach.\n[BOS] May and Knight (2006) extract nbest lists containing unique translations rather than unique derivations, while Kumar and Byrne (2004) use the Minimum Bayes Risk decision rule to select the lowest risk (highest BLEU score) translation rather than derivation from an n-best list.\n[BOS] Tromble et al. (2008) extend this work to lattice structures.\n[BOS] All of these approaches only marginalize over alternative candidate derivations generated by a MAPdriven decoding process.\n[BOS] More recently, work by Blunsom et al. (2007) propose a purely discriminative model whose decoding step approximates the selection of the most likely translation via beam search.\n[BOS] Matsusaki et al. (2005) and Petrov et al. (2006) propose automatically learning annotations that add information to categories to improve monolingual parsing quality.\n[BOS] Since the parsing task requires selecting the most non-annotated tree, the annotations add an additional level of structure that must be marginalized during search.\n[BOS] They demonstrate improvements in parse quality only when a variational approximation is used to select the most likely unannotated tree rather than simply stripping annotations from the MAP annotated tree.\n[BOS] In our work, we focused on approximating the selection of the most likely unlabeled derivation during search, rather than as a post-processing operation; the methods described above might improve this approximation, at some computational expense.\n\n", "discourse_tags": ["Reflection", "Multi_summ", "Single_summ", "Transition", "Single_summ", "Multi_summ", "Multi_summ", "Multi_summ", "Reflection"], "span_citation_mapping": [{"token_start": 48, "token_end": 66, "char_start": 276, "char_end": 379, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"May and Knight (2006)": "312883"}, "Reference": {}}}, {"token_start": 68, "token_end": 105, "char_start": 387, "char_end": 553, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kumar and Byrne (2004)": "11706155"}, "Reference": {}}}, {"token_start": 106, "token_end": 122, "char_start": 560, "char_end": 621, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Tromble et al. (2008)": "7657227"}, "Reference": {}}}, {"token_start": 148, "token_end": 177, "char_start": 780, "char_end": 934, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Blunsom et al. (2007)": "6677774"}, "Reference": {}}}, {"token_start": 178, "token_end": 278, "char_start": 941, "char_end": 1490, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Matsusaki et al. (2005)": "8008954", "Petrov et al. (2006)": "6684426"}, "Reference": {}}}]}
{"id": "9134916_0", "paragraph": "[BOS] There are roughly two lines of research related to our work: neural machine translation and variational neural model.\n[BOS] We describe them in succession.\n\n", "discourse_tags": ["Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "905294_0", "paragraph": "[BOS] Our work on ensembling dependency parsers is based on Sagae and Lavie (2006) and Surdeanu and Manning (2010) ; an additional contribution of this work is to show that the normalized ensemble votes correspond to MBR parsing.\n[BOS] Petrov (2010) proposed a similar model combination with random initializations for phrase-structure parsing, using products of constituent marginals.\n[BOS] The local optima in his base model's training objective arise from latent variables instead of neural networks (in our case).\n[BOS] Model distillation was proposed by Bucil et al. (2006) , who used a single neural network to simulate a large ensemble of classifiers.\n[BOS] More recently, Ba and Caruana (2014) showed that a single shallow neural network can closely replicate the performance of an ensemble of deep neural networks in phoneme recognition and object detection.\n[BOS] Our work is closer to Hinton et al. (2015) , in the sense that we do not simply compress the ensemble and hit the \"soft target,\" but also the \"hard target\" at the same time 10 .\n[BOS] These previous works only used model compression and distillation for classification; we extend the work to a structured prediction problem (dependency parsing).\n[BOS] Tckstrm et al. (2013) similarly used an ensemble of other parsers to guide the prediction of a seed model, though in a different context of \"ambiguityaware\" ensemble training to re-lexicalize a transfer model for a target language.\n[BOS] We similarly use an ensemble of models as a supervision for a sin-gle model.\n[BOS] By incorporating the ensemble uncertainty estimates in the cost function, our approach is cheaper, not requiring any marginalization during training.\n[BOS] An additional difference is that we learn from the gold labels (\"hard targets\") rather than only ensemble estimates on unlabeled data.\n[BOS] Kim and Rush (2016) proposed a distillation model at the sequence level, with application in sequence-to-sequence neural machine translation.\n[BOS] There are two primary differences with this work.\n[BOS] First, we use a global model to distill the ensemble, instead of a sequential one.\n[BOS] Second, Kim and Rush (2016) aim to distill a larger model into a smaller one, while we propose to distill an ensemble instead of a single model.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Transition", "Single_summ", "Single_summ", "Reflection", "Reflection", "Single_summ", "Reflection", "Reflection", "Reflection", "Single_summ", "Reflection", "Reflection", "Single_summ"], "span_citation_mapping": [{"token_start": 5, "token_end": 32, "char_start": 18, "char_end": 114, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Surdeanu and Manning (2010)": "1204756"}}}, {"token_start": 54, "token_end": 81, "char_start": 236, "char_end": 385, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Petrov (2010)": "1204756"}, "Reference": {}}}, {"token_start": 109, "token_end": 139, "char_start": 524, "char_end": 658, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Bucil\u01ce et al. (2006)": null}, "Reference": {}}}, {"token_start": 143, "token_end": 178, "char_start": 680, "char_end": 867, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ba and Caruana (2014)": "1500270"}, "Reference": {}}}, {"token_start": 179, "token_end": 192, "char_start": 874, "char_end": 916, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Hinton et al. (2015)": "7200347"}}}, {"token_start": 255, "token_end": 307, "char_start": 1226, "char_end": 1457, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 378, "token_end": 408, "char_start": 1844, "char_end": 1985, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kim and Rush (2016)": "8451212"}, "Reference": {}}}, {"token_start": 441, "token_end": 474, "char_start": 2145, "char_end": 2281, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kim and Rush (2016)": "8451212"}, "Reference": {}}}]}
{"id": "8996207_2", "paragraph": "[BOS] As shown in Table 1 , all the methods outperform the baseline because they have extra models to guide the hierarchical rule selection in some ways which might lead to better translation.\n[BOS] Apparently, our method also performs better than the other two approaches, indicating that our method is more effective in the hierarchical rule selection as both source-side and target-side rules are selected together.\n\n", "discourse_tags": ["Transition", "Reflection"], "span_citation_mapping": []}
{"id": "8996207_1", "paragraph": "[BOS] We compare our method with the baseline and some typical approaches listed in Table 1 where XP+ denotes the approach in (Marton and Resnik, 2008) and TOFW (topological ordering of function words) stands for the method in (Setiawan et al., 2009) .\n[BOS] As (Xiong et al., 2009 )'s work is based on phrasal SMT system with bracketing transduction grammar rules (Wu, 1997) and (Shen et al., 2009 )'s work is based on the string-to-dependency SMT model, we do not implement these two related work due to their different models from ours.\n[BOS] We also do not compare with )'s work due to its less practicability of integrating numerous sub-models.\n[BOS] Table 1 : Comparison results, our method is significantly better than the baseline, as well as the other two approaches (p < 0.01)\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 18, "token_end": 33, "char_start": 98, "char_end": 151, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Marton and Resnik, 2008)": "2442439"}}}, {"token_start": 34, "token_end": 58, "char_start": 156, "char_end": 250, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Setiawan et al., 2009)": "17024370"}}}, {"token_start": 60, "token_end": 133, "char_start": 259, "char_end": 539, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Xiong et al., 2009": "5822823"}, "Reference": {"(Wu, 1997)": "912349", "(Shen et al., 2009": "17001645"}}}]}
{"id": "8996207_0", "paragraph": "[BOS] Our baseline is the implemented Hiero-like SMT system where only the standard features are employed and the performance is state-of-the-art.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "8920526_1", "paragraph": "[BOS] Their algorithm processes candidate source phrases one by one through the whole search space and checks if the candidate phrase complies with ITG constraints.\n[BOS] Besides, their algorithm checks validity via cover vector and does not formalize ITG structure.\n[BOS] The shift-reduce decoding algorithm holds ITG structure via three stacks.\n[BOS] As a result, it can offer ITG-legal spans directly and decode faster.\n[BOS] Furthermore, with\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Transition", "Other"], "span_citation_mapping": []}
{"id": "8920526_0", "paragraph": "[BOS] Galley and Manning (2008) present a hierarchical phrase reordering model aimed at improving non-local reorderings.\n[BOS] Via the hierarchical mergence of two blocks, the orientation of long distance words can be computed.\n[BOS] Their shift-reduce algorithm does not import ITG constraints and admits the translation violating ITG constraints.\n[BOS] Zens et al. (2004) introduce a left-toright decoding algorithm with ITG constraints on the alignment template system (Och et al., 1999) .\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 70, "char_start": 6, "char_end": 348, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 71, "token_end": 104, "char_start": 355, "char_end": 490, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zens et al. (2004)": "3151217"}, "Reference": {"(Och et al., 1999)": "10335066"}}}]}
{"id": "8555434_4", "paragraph": "[BOS] Somewhat related to our work is the concept of coverage-based NMT (Tu et al., 2016) , where the model architecture is changed to explicitly account for source coverage.\n[BOS] In our work, we use a standard NMT architecture, but track coverage with accumulated attention weights.\n\n", "discourse_tags": ["Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 11, "token_end": 24, "char_start": 53, "char_end": 89, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Tu et al., 2016)": "146843"}}}]}
{"id": "8555434_3", "paragraph": "[BOS] In (Tang et al., 2016) , the NMT decoder is modified to switch between using externally defined phrases and standard NMT word hypotheses.\n[BOS] However, only one target phrase per source phrase is considered, and the reported improvements are significant only when manually selected phrase pairs (mostly for rare named entities) are used.\n\n", "discourse_tags": ["Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 67, "char_start": 6, "char_end": 344, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "8555434_2", "paragraph": "[BOS] In (Arthur et al., 2016) , a statistical word lexicon is used to influence NMT hypotheses, also based on the attention mechanism.\n[BOS] (Glehre et al., 2015) combine target n-gram LM scores with NMT scores to find the best translation.\n[BOS] (He et al., 2016 ) also use a target LM, but add further SMT features such as word penalty and word lexica to the NMT beam search.\n[BOS] To the best of our knowledge, no previous work extends the beam search with phrasal translation hypotheses of PBMT, like we propose in this paper.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 32, "char_start": 6, "char_end": 135, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Arthur et al., 2016)": "10086161"}, "Reference": {}}}, {"token_start": 33, "token_end": 60, "char_start": 142, "char_end": 241, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(G\u00fcl\u00e7ehre et al., 2015)": "15352384"}, "Reference": {}}}, {"token_start": 61, "token_end": 96, "char_start": 248, "char_end": 378, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(He et al., 2016": "38084627"}, "Reference": {}}}]}
{"id": "8555434_1", "paragraph": "[BOS] In (Stahlberg et al., 2017) , a two-step translation process is used, where in the first step a SMT translation lattice is generated, and in the second step the NMT decoder combines NMT scores with the Bayes-risk of the translations according to the lattice.\n[BOS] In contrast, we explicitly use phrasal translations and language model scores in an integrated search.\n\n", "discourse_tags": ["Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 62, "char_start": 6, "char_end": 264, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Stahlberg et al., 2017)": "1134638"}, "Reference": {}}}]}
{"id": "8555434_0", "paragraph": "[BOS] In the line of research closely related to our approach, neural models are used as additional features in vanilla phrase-based systems.\n[BOS] Examples include the work of (Devlin et al., 2014) , (JunczysDowmunt et al., 2016) , etc.\n[BOS] Such approaches have certain limitations: first, the search space of the model is still restricted by what can be produced using a phrase table extracted from parallel data based on word alignments.\n[BOS] Second, the organization of the search, in which only a limited target word history (e.g. 4 last target words) is available for each partial hypothesis, makes it difficult to integrate recurrent neural network LMs and translation models which take all previously generated target words into account.\n[BOS] That is why, for instance, the attention-based NMT models were usually applied only in rescoring (Peter et al., 2016) .\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 29, "token_end": 57, "char_start": 148, "char_end": 230, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Devlin et al., 2014)": "7417943"}}}, {"token_start": 161, "token_end": 182, "char_start": 786, "char_end": 872, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Peter et al., 2016)": null}}}]}
{"id": "8549757_3", "paragraph": "[BOS] The Edinburgh Geo-annotator has been developed in tandem with the Edinburgh Geoparser and earlier versions have been used in the GeoDigRef project to create evaluation data for historical text collections as well as in the botanical domain (Llewellyn et al., 2012; Llewellyn et al., 2011) where we adapted it to allow curators to geo-reference the textual metadata associated with herbarium specimens.\n[BOS] The current version has also been used to create gold standard data for Trading Consequences, a historical text mining project on mining location-centric trading information relevant to the nineteenth century (Klein et al., 2014) .\n[BOS] The Pelagios project, which deals with texts about the ancient world, has recently developed Recogito 5 , a geo-resolution correction tool similar to our own.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 44, "token_end": 66, "char_start": 229, "char_end": 294, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Llewellyn et al., 2012;": "12717039", "Llewellyn et al., 2011)": null}}}, {"token_start": 105, "token_end": 131, "char_start": 510, "char_end": 643, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "8549757_2", "paragraph": "[BOS] There are currently several geoparsing tools available, such as GeoLocate 3 , and CLAVIN 4 , as well as our own tool, the Edinburgh Geoparser.\n[BOS] All of these enable users to geo-reference text collections but do not address the question of how to interact with the geo-annotations in order to correct them, nor do they assist in creating evaluation materials for particular text collections.\n\n", "discourse_tags": ["Transition", "Transition"], "span_citation_mapping": []}
{"id": "8549757_1", "paragraph": "[BOS] Within the field of Information Retrieval, there is an ACM special interest group on spatially-related information, SIGSPATIAL 2 , with regular geographic IR conferences (GIR conferences) where georeferencing research is presented, see for example Purves et al. (2007) .\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 36, "token_end": 54, "char_start": 200, "char_end": 274, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Purves et al. (2007)": "7404968"}}}]}
{"id": "8549757_0", "paragraph": "[BOS] Within the field of NLP, SpatialML is probably the best known work in the area of geo-referencing.\n[BOS] SpatialML is an annotation scheme for marking up natural language references to places and grounding them to coordinates.\n[BOS] The SpatialML corpus (Mani et al., 2008) instantiates this annotation scheme and can be used as an evaluation corpus for geo-resolution .\n[BOS] Other researchers develop their own geo-annotated corpora and evaluate against these, e.g. Clough (2005) , Leidner (2007) .\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 49, "token_end": 79, "char_start": 239, "char_end": 374, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Mani et al., 2008)": "17090524"}, "Reference": {}}}, {"token_start": 86, "token_end": 111, "char_start": 419, "char_end": 504, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Clough (2005)": "17745294", "Leidner (2007)": null}}}]}
{"id": "8446444_0", "paragraph": "[BOS] Besides (Frank and Semeck, 2004 )'s work, as mentioned above, there have been several studies on the relationship between deep syntax and semantic parsing.\n[BOS] Although the studies did not focus on direct mappings between deep syntax and shallow semantics, they suggested a strong relationship between the two.\n[BOS] (Miyao and Tsujii, 2004) evaluated the accuracy of an HPSG parser against PropBank semantic annotations, and showed that the HPSG dependants correlated with semantic arguments of the PropBank, particularly with \"core\" arguments.\n[BOS] In (Gildea and Hockenmaier, 2003) and (Zhang et al., 2008) , features from deep parses were used for semantic parsing, together with features from CFG or dependency parses.\n[BOS] The deep features were reported to contribute to a performance gain.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Multi_summ", "Transition"], "span_citation_mapping": [{"token_start": 2, "token_end": 60, "char_start": 6, "char_end": 318, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Frank and Semeck\u00fd, 2004": "17525690"}, "Reference": {}}}, {"token_start": 61, "token_end": 109, "char_start": 325, "char_end": 553, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Miyao and Tsujii, 2004)": "1788449"}, "Reference": {}}}, {"token_start": 110, "token_end": 155, "char_start": 560, "char_end": 732, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Gildea and Hockenmaier, 2003)": "11869911", "(Zhang et al., 2008)": "13498230"}, "Reference": {}}}]}
{"id": "8420349_1", "paragraph": "[BOS] For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.\n[BOS] Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models.\n[BOS] This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.\n[BOS] Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.\n[BOS] The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007) .\n[BOS] Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules.\n[BOS] This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.\n[BOS] Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.\n[BOS] Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.\n[BOS] They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.\n[BOS] adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.\n[BOS] Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment.\n[BOS] Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation.\n[BOS] Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.\n[BOS] Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.\n[BOS] Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.\n[BOS] Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Reflection", "Single_summ", "Narrative_cite", "Single_summ", "Reflection", "Reflection", "Multi_summ", "Single_summ", "Other", "Single_summ", "Single_summ", "Reflection", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 37, "char_start": 6, "char_end": 185, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 39, "token_end": 66, "char_start": 196, "char_end": 323, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Zhai et al., 2012)": "1333990"}, "Reference": {}}}, {"token_start": 104, "token_end": 130, "char_start": 526, "char_end": 646, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Blunsom et al. (2008": "3079254"}, "Reference": {}}}, {"token_start": 140, "token_end": 155, "char_start": 692, "char_end": 756, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chiang, 2007)": "3505719"}}}, {"token_start": 157, "token_end": 176, "char_start": 765, "char_end": 850, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Levenberg et al. (2012)": "18893263"}, "Reference": {}}}, {"token_start": 224, "token_end": 267, "char_start": 1104, "char_end": 1298, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Burkett and Klein (2008)": "1138220", "Burkett et al. (2010)": "2707891"}, "Reference": {}}}, {"token_start": 289, "token_end": 310, "char_start": 1413, "char_end": 1501, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Liu et al. (2012)": null}, "Reference": {}}}, {"token_start": 311, "token_end": 336, "char_start": 1508, "char_end": 1644, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Burkett and Klein (2012)": "12545394"}, "Reference": {}}}, {"token_start": 368, "token_end": 395, "char_start": 1821, "char_end": 1950, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zollmann and Venugopal (2006)": "819325"}, "Reference": {}}}, {"token_start": 396, "token_end": 418, "char_start": 1957, "char_end": 2058, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zollmann and Vogel (2011)": "8666976"}, "Reference": {}}}]}
{"id": "8420349_0", "paragraph": "[BOS] In this study, we move in a new direction to build a tree-based translation model with effective unsupervised U-tree structures.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "8246817_2", "paragraph": "[BOS] Convolutional neural networks (CNNs) has been widely used in NLP fields in recent years, and yield effective results.\n[BOS] Kalchbrenner et al. (2014) introduced a convolutional architecture dubbed the Dynamic Convolutional Neural Network (DCNN) for the semantic modeling of sentences.\n[BOS] Kim (2014) used CNN for sentence classification, and achieved excellent results on multiple benchmarks.\n[BOS] Yin et al. (2015) presented a general Attention Based Convolutional Neural Network (ABCNN) for modeling a pair of sentences, which can be applied to a wide variety of tasks.\n[BOS] Hu et al. (2015) used CNN architectures for matching natural language sentences.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 27, "token_end": 60, "char_start": 130, "char_end": 291, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kalchbrenner et al. (2014)": "1306065"}, "Reference": {}}}, {"token_start": 61, "token_end": 79, "char_start": 298, "char_end": 401, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kim (2014)": null}, "Reference": {}}}, {"token_start": 80, "token_end": 117, "char_start": 408, "char_end": 581, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 118, "token_end": 134, "char_start": 588, "char_end": 668, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hu et al. (2015)": "4497054"}, "Reference": {}}}]}
{"id": "8246817_1", "paragraph": "[BOS] Question answering has attracted lots of attention in recent years.\n[BOS] Sukhbaatar et al. (2015) introduced a neural network with a recurrent attention model over a possibly large external memory, which is called end-to-end memory networks.\n[BOS] After that, Kumar et al. (2015) introduced the dynamic memory network (DMN) which processes input sequences and questions, forms episodic memories, and generates relevant answers.\n[BOS] Based on DMN, Xiong et al. (2016) proposed several improvements for memory and input modules, and introduced a novel input module for images in order to be able to answer visual questions.\n[BOS] With rapid growth of knowledge bases (KBs) on the web and the development of neural network based (NN-based) methods, NN-based KB-QA has already achieved impressive results.\n[BOS] Zhang et al. (2016) presented a neural attention-based model to represent the question with dynamic attention.\n[BOS] Wang et al. (2016) have done a lot of valuable exploration of different attention methods in recurrent neural network (RNN) models.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Transition", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 14, "token_end": 51, "char_start": 80, "char_end": 248, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Sukhbaatar et al. (2015)": "1399322"}, "Reference": {}}}, {"token_start": 55, "token_end": 87, "char_start": 267, "char_end": 434, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kumar et al. (2015)": "2319779"}, "Reference": {}}}, {"token_start": 93, "token_end": 128, "char_start": 455, "char_end": 629, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Xiong et al. (2016)": "14294589"}, "Reference": {}}}, {"token_start": 169, "token_end": 191, "char_start": 816, "char_end": 926, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhang et al. (2016)": "3925660"}, "Reference": {}}}, {"token_start": 192, "token_end": 220, "char_start": 933, "char_end": 1064, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wang et al. (2016)": null}, "Reference": {}}}]}
{"id": "8246817_0", "paragraph": "[BOS] We got a lot of inspiration from others' work, they've given many shoulders on which this paper is standing.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "8066499_0", "paragraph": "[BOS] Alignment-based Parsing Flanigan et al. (2014) (JAMR) pipeline concept and relation identification with a graph-based algorithm.\n[BOS] extend JAMR by performing the concept and relation identification tasks jointly with an incremental model.\n[BOS] Both systems rely on features based on a set of alignments produced using bi-lexical cues and hand-written rules.\n[BOS] In contrast, our models train directly on parallel corpora, and make only minimal use of alignments to anonymize named entities.\n[BOS] (CAMR) perform a series of shift-reduce transformations on the output of an externally-trained dependency parser, similar to Damonte et al. (2017) , Brandt et al. (2016) , Puzikov et al. (2016) , and Goodman et al. (2016) .\n[BOS] Artzi et al. (2015) use a grammar induction approach with Combinatory Categorical Grammar (CCG), which relies on pretrained CCGBank categories, like Bjerva et al. (2016) .\n[BOS] Pust et al. (2015) recast parsing as a string-to-tree Machine Translation problem, using unsupervised alignments (Pourdamghani et al., 2014) , and employing several external semantic resources.\n[BOS] Our neural approach is engineering lean, relying only on a large unannotated corpus of English and algorithms to find and canonicalize named entities.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Transition", "Reflection", "Narrative_cite", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 49, "char_start": 6, "char_end": 247, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Flanigan et al. (2014)": "5000956"}, "Reference": {}}}, {"token_start": 122, "token_end": 162, "char_start": 623, "char_end": 730, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Damonte et al. (2017)": null, "Brandt et al. (2016)": "18037181", "Puzikov et al. (2016)": "5459392", "Goodman et al. (2016)": "17055413"}}}, {"token_start": 164, "token_end": 207, "char_start": 739, "char_end": 908, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Artzi et al. (2015)": null}, "Reference": {"Bjerva et al. (2016)": "2986409"}}}, {"token_start": 209, "token_end": 253, "char_start": 917, "char_end": 1110, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Pust et al. (2015)": "6971327"}, "Reference": {"(Pourdamghani et al., 2014)": "217895"}}}]}
{"id": "8060447_0", "paragraph": "[BOS] Apart from the work on direct/projected transfer and annotation projection mentioned above, the proposed method can be seen as a more explicit kind of domain adaptation, similar to Titov (2011) or Blitzer et al. (2006) .\n[BOS] It is also somewhat similar in spirit to Mikolov et al. (2013b) , where a small number of word translation pairs are used to estimate a mapping between distributed representations of words in two different languages and build a word translation model.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ"], "span_citation_mapping": [{"token_start": 29, "token_end": 39, "char_start": 157, "char_end": 199, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Titov (2011)": "2270158"}}}, {"token_start": 40, "token_end": 48, "char_start": 203, "char_end": 224, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Blitzer et al. (2006)": "15978939"}}}, {"token_start": 56, "token_end": 99, "char_start": 264, "char_end": 484, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Mikolov et al. (2013b)": "1966640"}, "Reference": {}}}]}
{"id": "8028296_4", "paragraph": "[BOS] In our experiments, EM converges very fast regardless of the initial state.\n[BOS] Indeed, in the M-step, we use our new inference method for computing an estimation of entities.\n[BOS] The use of the EM algorithm in our approach is discussed in more detail in Section 3.3.\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "8028296_3", "paragraph": "[BOS] The informativeness scores of mention pair relations (Section 3.2.1) are our unknown parameters.\n[BOS] Our inference method only requires the ranking of the informativeness scores (and not their exact values).\n[BOS] Therefore, it is much easier to estimate the ranking of these parameters than parameters like P (f 1 , .\n[BOS] .\n[BOS] .\n[BOS] , f k |C ij ), and our search space for finding an optimized ranking of the informativeness scores is very small.\n[BOS] Since it is easier to have an initial guess about the ranking of informativeness scores (rather than guessing an initial entity set), we start from an E-step with a random ranking.\n\n", "discourse_tags": ["Transition", "Reflection", "Reflection", "Reflection", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "8028296_2", "paragraph": "[BOS] We use the expectation maximization algorithm for unsupervised learning.\n[BOS] EM has been previously used for coreference resolution (Cherry and Bergsma, 2005; Ng, 2008; Charniak and Elsner, 2009 ).\n[BOS] Cherry and Bergsma (2005) and Charniak and Elsner (2009) use EM for pronoun resolution, and Ng (2008) models coreference resolution as EM clustering.\n[BOS] The model parameters of Ng (2008) are of the form P (f 1 , .\n[BOS] .\n[BOS] .\n[BOS] , f k |C ij ), where f i is a feature, and C ij corresponds to the coreference decision of two mentions m i and m j .\n[BOS] These parameters along with the entity set, are two sets of unknown variables in Ng (2008) .\n[BOS] He computes the posterior probabilities of entities in the E-step, and determines the parameters from the N-best clustering (i.e. estimated entities) in the M-step.\n[BOS] Ng (2008) starts from an initial guess about the entities and determines the parameters based on this initial guess (M-step).\n[BOS] In order to compute the N-best clustering, Ng (2008) uses the Bell tree approach of Luo et al. (2004) .\n\n", "discourse_tags": ["Reflection", "Narrative_cite", "Multi_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 19, "token_end": 44, "char_start": 117, "char_end": 202, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cherry and Bergsma, 2005;": "2101297", "Ng, 2008;": "4376006", "Charniak and Elsner, 2009": "10539539"}}}, {"token_start": 47, "token_end": 71, "char_start": 212, "char_end": 298, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Cherry and Bergsma (2005)": "2101297", "Charniak and Elsner (2009)": "10539539"}, "Reference": {}}}, {"token_start": 73, "token_end": 258, "char_start": 304, "char_end": 1078, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ng (2008)": "4376006"}, "Reference": {"Luo et al. (2004)": "8810581"}}}]}
{"id": "8028296_1", "paragraph": "[BOS] The increasing importance of multilingual processing, brought the deployment of semi-supervised and unsupervised methods into attention for automatic processing of limited resource languages.\n[BOS] There are several works which treat coreference resolution as an unsupervised problem (Cardie and Wagstaff, 1999; Angheluta et al., 2004; Haghighi and Klein, 2007; Ng, 2008; Poon and Domingos, 2008; Haghighi and Klein, 2009; Haghighi and Klein, 2010; Kobdani et al., 2011) .\n[BOS] We compare our results with the unsupervised systems of Haghighi and Klein (2007) , Poon and Domingos (2008) , Klein (2009), and Kobdani et al. (2011) .\n[BOS] The Haghighi and Klein (2010) approach is an almost unsupervised approach, and we do not include this system in our comparisons.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 37, "token_end": 109, "char_start": 240, "char_end": 476, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cardie and Wagstaff, 1999;": "5534672", "Angheluta et al., 2004;": "56362275", "Haghighi and Klein, 2007;": "372666", "Ng, 2008;": "4376006", "Poon and Domingos, 2008;": "7124715", "Haghighi and Klein, 2009;": "16637008", "Haghighi and Klein, 2010;": "9203411", "Kobdani et al., 2011)": "531644"}}}, {"token_start": 117, "token_end": 154, "char_start": 517, "char_end": 635, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Haghighi and Klein (2007)": "372666", "Poon and Domingos (2008)": "7124715", "Klein (2009), and": "16637008", "Kobdani et al. (2011)": "531644"}}}, {"token_start": 156, "token_end": 183, "char_start": 644, "char_end": 772, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Haghighi and Klein (2010)": "9203411"}}}]}
{"id": "8028296_0", "paragraph": "[BOS] Early coreference resolution systems were mainly rule-based systems (Lappin and Leass, 1994; Baldwin, 1997) .\n[BOS] The success of statistical approaches in different NLP tasks together with the availability of coreference annotated corpora (like MUC-6 (Chinchor and Sundheim, 2003) and MUC-7 (Chinchor, 2001) ) facilitated a shift from deploying rule-based methods to machine learning approaches in coreference research in the 1990s.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 9, "token_end": 27, "char_start": 55, "char_end": 113, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lappin and Leass, 1994;": "11500985", "Baldwin, 1997)": "5299685"}}}, {"token_start": 50, "token_end": 63, "char_start": 253, "char_end": 288, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chinchor and Sundheim, 2003)": null}}}, {"token_start": 64, "token_end": 74, "char_start": 293, "char_end": 315, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "7995281_1", "paragraph": "[BOS] Regarding punctuation processing for dependency parsing, Li et al. (2010) proposed to utilize punctuations to segment sentences into small fragments and then parse the fragments separately.\n[BOS] A similar approach is proposed by Spitkovsky et al. (2011) which also designed a set of constraints on the fragments to improve unsupervised dependency parsing.\n\n", "discourse_tags": ["Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 10, "token_end": 36, "char_start": 63, "char_end": 195, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Li et al. (2010)": "16644782"}, "Reference": {}}}, {"token_start": 39, "token_end": 68, "char_start": 212, "char_end": 362, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Spitkovsky et al. (2011)": "7274536"}, "Reference": {}}}]}
{"id": "7995281_0", "paragraph": "[BOS] In this work, we proposed to treat punctuations as properties of context words for dependency parsing.\n[BOS] Experiments with an arc-standard parser showed that our method effectively improves parsing performance and we achieved the best accuracy for single-model transition-based parser.\n\n", "discourse_tags": ["Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "797950_1", "paragraph": "[BOS] McClosky et al. (2006) used a large corpus of parsed text to obtain improved parsing results through self-training.\n[BOS] A key difference in our general framework is that it allows for a parser with one type of syntactic representation to improve the accuracy of a different parser with a different type of formalism.\n[BOS] In this regard, our work is related to that of Sagae et al. (2007) , who used a stacking-like framework to allow a surface dependency parser to improve an HPSG parser.\n[BOS] In that work, however, as in other work that combines different parsers through stacking (Martins et al., 2008; Nivre and McDonald, 2008) or voting (Henderson and Brill, 1999) , multiple parsers need to process new text at runtime.\n[BOS] In our approach for leveraging diverse parsers, one of the parsers is used only to create a parsed corpus from which we extract clusters of words that have similar syntactic behaviors, and only one parser is needed at run-time.\n\n", "discourse_tags": ["Single_summ", "Reflection", "Single_summ", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 29, "char_start": 6, "char_end": 121, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 77, "token_end": 106, "char_start": 378, "char_end": 498, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Sagae et al. (2007)": "8393638"}, "Reference": {}}}, {"token_start": 123, "token_end": 140, "char_start": 585, "char_end": 642, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Martins et al., 2008;": "14108286", "Nivre and McDonald, 2008)": "9431510"}}}, {"token_start": 141, "token_end": 151, "char_start": 646, "char_end": 680, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Henderson and Brill, 1999)": "5666926"}}}]}
{"id": "797950_0", "paragraph": "[BOS] Many aspects of this research were inspired by the recent work of Koo et al. (2008) , who reported impressive results on improving dependency parsing accuracy using a second order edge-factored model and word clusters derived from plain text using the Brown et al. (1992) algorithm.\n[BOS] Our clustering approach is significantly different, focusing on the use of parsed data to produce strictly syntactic clusters.\n[BOS] It is possible that using both types of clusters may be beneficial.\n\n", "discourse_tags": ["Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 14, "token_end": 59, "char_start": 72, "char_end": 288, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Koo et al. (2008)": "1916754"}, "Reference": {"Brown et al. (1992)": "10986188"}}}]}
{"id": "793385_9", "paragraph": "[BOS] Ultimately, all we need to do for a new relation is define our information need in the form of a question.\n[BOS] 2 Our approach provides a naturallanguage API for application developers who are interested in incorporating a relation-extraction component in their programs; no linguistic knowledge or pre-defined schema is needed.\n[BOS] To implement our approach, we require two components: training data and a reading-comprehension model.\n[BOS] In Section 4, we construct a large relationextraction dataset and querify it using an efficient crowdsourcing procedure.\n[BOS] We then adapt an existing state-of-the-art reading-comprehension model to suit our problem formulation (Section 5).\n\n", "discourse_tags": ["Transition", "Reflection", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "793385_8", "paragraph": "[BOS] In the zero-shot scenario, we are given a new relation R N +1 (x, y) at test-time, which was neither specified nor observed beforehand.\n[BOS] For example, the deciphered(x, y) relation, as in \"Turing and colleagues came up with a method for efficiently deciphering the Enigma\", is too domainspecific to exist in common knowledge-bases.\n[BOS] We then querify R N +1 (x, y) into q x (\"Which code did x break?\")\n[BOS] or q y (\"Who cracked y?\n[BOS] \"), and run our reading-comprehension model for each sentence in the document(s) of interest, while instantiating the question template with different entities that might participate in this relation.\n[BOS] 1 Each time the model returns a non-null answer a for a given question q e , it extracts the relation R N +1 (e, a).\n\n", "discourse_tags": ["Reflection", "Other", "Reflection", "Other", "Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "793385_7", "paragraph": "[BOS] Applying schema querification to N relations from a pre-existing relation-extraction dataset converts it into a reading-comprehension dataset.\n[BOS] We then use this dataset to train a readingcomprehension model, which given a sentence s and a question q returns a set of text spans A within s that answer q (to the best of its ability).\n\n", "discourse_tags": ["Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "793385_6", "paragraph": "[BOS] The challenge now becomes one of querification: translating R(e, ?)\n[BOS] into q.\n[BOS] Rather than querify R(e, ?)\n[BOS] for every entity e, we propose a method of querifying the relation R. We treat e as a variable x, querify the parametrized query R(x, ?)\n[BOS] (e.g. occupation(x, ?))\n[BOS] as a question template q x (\"What did x do for a living?\n[BOS] \"), and then instantiate this template with the relevant entities, creating a tailored natural-language question for each entity e (\"What did Steve Jobs do for a living?\").\n[BOS] This process, schema querification, is by an order of magnitude more efficient than querifying individual instances because annotating a relation type automatically annotates all of its instances.\n\n", "discourse_tags": ["Other", "Other", "Reflection", "Reflection", "Other", "Other", "Other", "Reflection"], "span_citation_mapping": []}
{"id": "793385_5", "paragraph": "[BOS] We consider the slot-filling challenge in relation extraction, in which we are given a knowledgebase relation R, an entity e, and a sentence s. For example, consider the relation occupation, the entity \"Steve Jobs\", and the sentence \"Steve Jobs was an American businessman, inventor, and industrial designer\".\n[BOS] Our goal is to find a set of text spans A in s for which R(e, a) holds for each a  A.\n[BOS] In our example, A = {businessman, inventor, industrial designer}.\n[BOS] The empty set is also a valid answer (A = ) when s does not contain any phrase that satisfies R(e, ?).\n[BOS] We observe that given a natural-language question q that expresses R(e, ?)\n[BOS] (e.g. \"What did Steve Jobs do for a living?\n[BOS] \"), solving the reading comprehension problem of answering q from s is equivalent to solving the slot-filling challenge.\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection", "Reflection", "Reflection", "Other", "Reflection"], "span_citation_mapping": []}
{"id": "793385_4", "paragraph": "[BOS] Our zero-shot scenario, in which no manifestation of the test relation is observed during training, is substantially more challenging (see Section 6.3).\n[BOS] In universal-schema terminology, we add a new empty column (the target knowledgebase relation), plus a few new columns with a single entry each (reflecting the textual relations in the sentence).\n[BOS] These columns share no entities with existing columns, making the rest of the matrix irrelevant.\n[BOS] To fill the empty column from the others, we match their descriptions.\n[BOS] Toutanova et al. (2015) proposed a similar approach that decomposes natural-language relations and computes their similarity in a universal schema setting; however, they did not extend their method to knowledge-base relations, nor did they attempt to recover out-of-schema relations as we do.\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection", "Reflection", "Single_summ"], "span_citation_mapping": [{"token_start": 108, "token_end": 169, "char_start": 547, "char_end": 839, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Toutanova et al. (2015)": "2127100"}, "Reference": {}}}]}
{"id": "793385_3", "paragraph": "[BOS] Universal schema (Riedel et al., 2013) represents open IE extractions and knowledge-base facts in a single matrix, whose rows are entity pairs and columns are relations.\n[BOS] The redundant schema (each knowledge-base relation may overlap with multiple natural-language relations) enables knowledge-base population via matrix completion techniques.\n[BOS] Verga et al. (2017) predict facts for entity pairs that were not observed in the original matrix; this is equivalent to extracting seen relation types with unseen entities (see Section 6.1).\n[BOS] Rocktschel et al. (2015) and Demeester et al. (2016) use inference rules to predict hidden knowledge-base relations from observed naturallanguage relations.\n[BOS] This setting is akin to generalizing across different manifestations of the same relation (see Section 6.2) since a natural-language description of each target relation appears in the training data.\n[BOS] Moreover, the information about the unseen relations is a set of explicit inference rules, as opposed to implicit natural-language questions.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Multi_summ", "Multi_summ", "Transition"], "span_citation_mapping": [{"token_start": 2, "token_end": 67, "char_start": 6, "char_end": 354, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Riedel et al., 2013)": "2687019"}, "Reference": {}}}, {"token_start": 68, "token_end": 102, "char_start": 361, "char_end": 532, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Verga et al. (2017)": "14922772"}, "Reference": {}}}, {"token_start": 111, "token_end": 184, "char_start": 558, "char_end": 919, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Rockt\u00e4schel et al. (2015)": "6488690", "Demeester et al. (2016)": "6484161"}, "Reference": {}}}]}
{"id": "793385_2", "paragraph": "[BOS] Open information extraction (open IE) (Banko et al., 2007 ) is a schemaless approach for extracting facts from text.\n[BOS] While open IE systems need no relation-specific training data, they often treat different phrasings as different relations.\n[BOS] In this work, we hope to extract a canonical slot value independent of how the original text is phrased.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 53, "char_start": 6, "char_end": 252, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Banko et al., 2007": null}, "Reference": {}}}]}
{"id": "793385_1", "paragraph": "[BOS] This setting differs from prior art in relation extraction.\n[BOS] Bronstein et al. (2015) explore a similar zero-shot setting for event-trigger identification, in which R N +1 is specified by a set of trigger words at test time.\n[BOS] They generalize by measuring the similarity between potential triggers and the given seed set using unsupervised methods.\n[BOS] We focus instead on slot filling, where questions are more suitable descriptions than trigger words.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 13, "token_end": 71, "char_start": 72, "char_end": 362, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Bronstein et al. (2015)": "5561881"}, "Reference": {}}}]}
{"id": "793385_0", "paragraph": "[BOS] We are interested in a particularly harsh zero-shot learning scenario: given labeled examples for N relation types during training, extract relations of a new type R N +1 at test time.\n[BOS] The only information we have about R N +1 are parametrized questions.\n\n", "discourse_tags": ["Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "7926547_3", "paragraph": "[BOS] Our work also has a connection to the research direction that exploits resources-rich languages to construct similar tools for resource-poor languages.\n[BOS] This can be done with parallel data (Pauls et al., 2010) or without parallel data (Cohen et al., 2011) .\n[BOS] Saers et al. (2009) also propose a cubic biparsing algorithm based on beam pruning.\n[BOS] They apply this algorithm for generative model-based ITG grammar induction.\n\n", "discourse_tags": ["Reflection", "Narrative_cite", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 34, "token_end": 45, "char_start": 186, "char_end": 220, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Pauls et al., 2010)": "1494188"}}}, {"token_start": 47, "token_end": 57, "char_start": 232, "char_end": 266, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cohen et al., 2011)": "14287962"}}}, {"token_start": 59, "token_end": 95, "char_start": 275, "char_end": 440, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Saers et al. (2009)": "8981495"}, "Reference": {}}}]}
{"id": "7926547_2", "paragraph": "[BOS] Researchers also try to rescore the weights of translation rules.\n[BOS] They rescore the weights of rules extracted from the two-step pipeline, by using the similar latent log-linear model as us (Blunsom et al., 2008; Kriinen, 2009) , or incorporating various features using labeled data (Huang and Xiang, 2010) .\n[BOS] Such methods still need to run the heuristic two-step pipeline to extract the grammar, while our method can directly learn the grammar and correspondent weights.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 34, "token_end": 57, "char_start": 171, "char_end": 238, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Blunsom et al., 2008;": "6677774"}}}, {"token_start": 63, "token_end": 73, "char_start": 281, "char_end": 317, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "7926547_1", "paragraph": "[BOS] The first effort is to improve word alignment by considering phrase/syntax information (May and Knight, 2007; DeNero and Klein, 2010; Pauls et al., 2010; Burkett et al., 2010; Riesa et al., 2011) .\n[BOS] Such approaches also use discriminative framework to combine word alignment and syntactic alignment information.\n[BOS] In this way, they prefer word alignments that are consistent with syntactic structure alignments.\n[BOS] However, labeled word alignment data are required in order to learn the discriminative model.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 12, "token_end": 55, "char_start": 67, "char_end": 201, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(May and Knight, 2007;": "14328976", "DeNero and Klein, 2010;": "16749512", "Pauls et al., 2010;": "1494188", "Burkett et al., 2010;": "2707891", "Riesa et al., 2011)": "45404405"}}}]}
{"id": "7926547_0", "paragraph": "[BOS] Because of the great importance of synchronous grammars to SMT systems, researchers have proposed various methods in order to improve the quality of grammars.\n[BOS] In addition to the generative model introduced in Section 1, researchers also have made efforts on word alignment and grammar rescoring.\n\n", "discourse_tags": ["Transition", "Transition"], "span_citation_mapping": []}
{"id": "7890036_2", "paragraph": "[BOS] Finally, there is considerable work on using recurrent neural networks to represent word sequences, such as phrases or sentences (Socher et al., 2011; Kiros et al., 2015) .\n[BOS] We note that the techniques used for learning sentence representations have much in common with those we use for sentential context representations.\n[BOS] Yet, sentential context representations aim to reflect the information in the sentence only inasmuch as it is relevant to the target slot.\n[BOS] Specifically, different target positions in the same sentence can yield completely different context representations.\n[BOS] In contrast, sentence representations aim to reflect the entire contents of the sentence.\n\n", "discourse_tags": ["Narrative_cite", "Reflection", "Transition", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 20, "token_end": 40, "char_start": 114, "char_end": 176, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Socher et al., 2011;": "3116311", "Kiros et al., 2015)": "9126867"}}}]}
{"id": "7890036_1", "paragraph": "[BOS] A couple of recent works extended word2vec's CBOW by replacing its internal context representation.\n[BOS] Ling et al. (2015b) proposed a continuous window, which is a simple linear projection of the context window embeddings into a low dimensional vector.\n[BOS] Ling et al. (2015a) proposed 'CBOW with attention', which is used for finding the relevant features in a context window.\n[BOS] In contrast to our model, both approaches confine the context to a fixed-size window.\n[BOS] Furthermore, they limit their scope to using these context representations only internally to improve the learning of target words embeddings, rather than evaluate the benefit of using them directly in NLP tasks, as we do.\n[BOS] represent words in context using bidirectional LSTMs and multilingual supervision.\n[BOS] In contrast, our model is focused on representing the context alone.\n[BOS] Yet, as shown in our lexical substitution and word sense disambiguation evaluations, it can easily be used for modeling the meaning of words in context as well.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Reflection", "Reflection", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 23, "token_end": 53, "char_start": 112, "char_end": 261, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ling et al. (2015b)": "14800090"}, "Reference": {}}}, {"token_start": 54, "token_end": 83, "char_start": 268, "char_end": 388, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ling et al. (2015a)": "1144632"}, "Reference": {}}}]}
{"id": "7890036_0", "paragraph": "[BOS] Substitute vectors (Yuret, 2012) represent contexts as a probabilistic distribution over the potential gap-filler words for the target slot, pruned to its top-k most probable words.\n[BOS] While using this representation showed interesting potential (Yatbaz et al., 2012; Melamud et al., 2015a) , it can currently be generated efficiently only with n-gram language models and hence is limited to fixed-size context windows.\n[BOS] It is also high dimensional and sparse, in contrast to our proposed representations.\n[BOS] Syntactic dependency context embeddings have been proposed recently (Levy and Goldberg, 2014a; Bansal et al., 2014) .\n[BOS] They depend on the availability of a high-quality dependency parser, and can be viewed as a 'bag-of-dependencies' rather than a single representation for the entire sentential context.\n[BOS] However, we believe that incorporating such dependency-based information in our model is an interesting future direction.\n\n", "discourse_tags": ["Single_summ", "Multi_summ", "Reflection", "Narrative_cite", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 39, "char_start": 6, "char_end": 187, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Yuret, 2012)": "15610572"}, "Reference": {}}}, {"token_start": 40, "token_end": 67, "char_start": 194, "char_end": 299, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yatbaz et al., 2012;": "13398019", "Melamud et al., 2015a)": "10076790"}}}, {"token_start": 109, "token_end": 133, "char_start": 526, "char_end": 641, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Levy and Goldberg, 2014a;": null, "Bansal et al., 2014)": "7803700"}}}]}
{"id": "7888227_2", "paragraph": "[BOS] Some previous efforts utilized the techniques of soft syntactic constraints to increase the search space in hierarchical phrase-based models (Marton and Resnik 2008; Chiang et al. 2009; Huang et al. 2010) , string-to-tree models (Venugopal et al. 2009 ) or tree-to-tree (Chiang 2010) systems.\n[BOS] These methods focus on softening matching constraints on the root label of each rule regardless of its internal tree structure, and often generate many new syntactic categories 3 .\n[BOS] It makes them more difficult to satisfy syntactic constraints for the tree-to-string decoding.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 18, "token_end": 44, "char_start": 114, "char_end": 210, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Marton and Resnik 2008;": "2442439", "Chiang et al. 2009;": "3544821", "Huang et al. 2010)": "8988829"}}}, {"token_start": 45, "token_end": 59, "char_start": 213, "char_end": 257, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Venugopal et al. 2009": "917915"}}}, {"token_start": 61, "token_end": 71, "char_start": 263, "char_end": 289, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "7888227_1", "paragraph": "[BOS] The most related work is the forest-based translation method Zhang et al. 2009 ) in which rule extraction and decoding are implemented over k-best parse trees (e.g., in the form of packed forest) instead of one best tree as translation input.\n[BOS] Liu and Liu (2010) proposed a joint parsing and translation model by casting tree-based translation as parsing (Eisner 2003) , in which the decoder does not respect the source tree.\n[BOS] These methods can increase the treeto-string search space.\n[BOS] However, the decoding time complexity of their methods is high, i.e., more than ten or several dozen times slower than typical treeto-string decoding (Liu and Liu 2010) .\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 8, "token_end": 18, "char_start": 35, "char_end": 84, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Zhang et al. 2009": "42184"}}}, {"token_start": 56, "token_end": 107, "char_start": 255, "char_end": 501, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Liu and Liu (2010)": "9707947"}, "Reference": {"(Eisner 2003)": "1542925"}}}, {"token_start": 135, "token_end": 147, "char_start": 627, "char_end": 676, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Liu and Liu 2010)": "9707947"}}}]}
{"id": "7888227_0", "paragraph": "[BOS] String-to-tree SMT approaches also utilize the similarity-based matching constraint on target side to generate target translation.\n[BOS] This paper applies it on source side to reconstruct new similar source parse trees for decoding at the decoding time, which aims to increase the tree-to-string search space for decoding, and improve decoding generalization for tree-to-string translation.\n\n", "discourse_tags": ["Transition", "Reflection"], "span_citation_mapping": []}
{"id": "781753_4", "paragraph": "[BOS] Input sentences with differing content present a challenge to the models used in previous work.\n[BOS] All these models use deterministic node alignment heuristics to merge the input dependency graphs.\n[BOS] Filippova and Strube (2008) align all content words with the same lemma and part of speech; Barzilay and McKeown (2005) and Marsi and Krahmer (2005) use syntactic methods based on tree similarity.\n[BOS] Neither method is likely to work well for our data.\n[BOS] Lexical methods over-align, since there are many potential points of correspondence between our sentences, only some of which should be merged-\"the Doha trade round\" and \"U.S. trade representative\" share a word, but probably ought to remain separate regardless.\n[BOS] Syntactic methods, on the other hand, are unlikely to find any alignments since the input sentences are not paraphrases and have very different trees.\n[BOS] Our system selects the set of nodes to merge during ILP optimization, allowing it to choose correspondences that lead to a sensible overall solution.\n\n", "discourse_tags": ["Transition", "Transition", "Multi_summ", "Reflection", "Transition", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 35, "token_end": 56, "char_start": 213, "char_end": 303, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Filippova and Strube (2008)": "14909308"}, "Reference": {}}}, {"token_start": 57, "token_end": 85, "char_start": 305, "char_end": 409, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Barzilay and McKeown (2005)": "16188305"}, "Reference": {}}}]}
{"id": "781753_3", "paragraph": "[BOS] For supervised learning to be effective, it is necessary to find or produce example data.\n[BOS] Previous work does produce some examples written by humans, though these are used during evaluation, not for learning (a large corpus of fusions (McKeown et al., 2010) was recently compiled as a first step toward a supervised fusion system).\n[BOS] However, they elicit these examples by asking experimental subjects to fuse selected input sentences-the choice of which sentences to fuse is made by the system, not the subjects.\n[BOS] In contrast, our dataset consists of sentences humans actually chose to fuse as part of a practical writing task.\n[BOS] Moreover, our sentences have disparate content, while previous work focuses on sentences whose content mostly overlaps.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 42, "token_end": 57, "char_start": 223, "char_end": 269, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "781753_2", "paragraph": "[BOS] In their ILP, Filippova and Strube (2008) optimize a function based on syntactic importance scores learned from a corpus of general text.\n[BOS] While similar methods have been used for the related task of sentence compression, improvements can be obtained using supervised learning (Knight and Marcu, 2000; Turner and Charniak, 2005; Cohn and Lapata, 2009 ) if a suitable corpus of compressed sentences can be obtained.\n[BOS] This paper is the first we know of to adopt the supervised strategy for sentence fusion.\n\n", "discourse_tags": ["Single_summ", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 32, "char_start": 6, "char_end": 143, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Filippova and Strube (2008)": "14909308"}, "Reference": {}}}, {"token_start": 52, "token_end": 78, "char_start": 268, "char_end": 361, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Turner and Charniak, 2005;": "15519576", "Cohn and Lapata, 2009": "6429026"}}}]}
{"id": "781753_1", "paragraph": "[BOS] Our work most closely follows Filippova and Strube (2008) , which proposes using Integer Linear Programming (ILP) for extraction of an output dependency tree.\n[BOS] ILP allows specification of grammaticality constraints in terms of dependency relationships (Clarke and Lapata, 2008) , as opposed to previous fusion methods (Barzilay and McKeown, 2005; Marsi and Krahmer, 2005) which used language modeling to extract their output.\n\n", "discourse_tags": ["Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 7, "token_end": 35, "char_start": 36, "char_end": 164, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Filippova and Strube (2008)": "14909308"}, "Reference": {}}}, {"token_start": 47, "token_end": 57, "char_start": 238, "char_end": 288, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Clarke and Lapata, 2008)": "3004447"}}}, {"token_start": 62, "token_end": 84, "char_start": 314, "char_end": 382, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Barzilay and McKeown, 2005;": "16188305"}}}]}
{"id": "781753_0", "paragraph": "[BOS] Previous work on sentence fusion examines the task in the context of multidocument summarization, targeting groups of sentences with mostly redundant content.\n[BOS] The pioneering work on fusion is Barzilay and McKeown (2005) , which introduces the framework used by subsequent projects: they represent the inputs by dependency trees, align some words to merge the input trees into a lattice, and then extract a single, connected dependency tree as the output.\n\n", "discourse_tags": ["Transition", "Single_summ"], "span_citation_mapping": [{"token_start": 29, "token_end": 89, "char_start": 171, "char_end": 466, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Barzilay and McKeown (2005)": "16188305"}, "Reference": {}}}]}
{"id": "7752968_9", "paragraph": "[BOS] On the other hand, if the target relation co-occurs with questions related to \"tv appearance\" in training, by treating the whole relation as a token (i.e. relation id), we could better learn the correspondence between this token and phrases like \"tv show\" and \"play on\".\n[BOS] The two types of relation representation contain different levels of abstraction.\n[BOS] As shown in Table 1 , the word-level focuses more on local information (words and short phrases), and the relation-level focus more on global information (long phrases and skip-grams) but suffer from data sparsity.\n[BOS] Since both these levels of granularity have their own pros and cons, we propose a hierarchical matching approach for KB relation detection: for a candidate relation, our approach matches the input question to both word-level and relation-level representations to get the final ranking score.\n[BOS] Section 4 gives the details of our proposed approach.\n\n", "discourse_tags": ["Transition", "Other", "Other", "Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "7752968_8", "paragraph": "[BOS] (containing word \"play\") in the embedding space.\n\n", "discourse_tags": ["Other"], "span_citation_mapping": []}
{"id": "7752968_7", "paragraph": "[BOS] (2) Relation as Word Sequence (word-level).\n[BOS] In this case, the relation is treated as a sequence of words from the tokenized relation name.\n[BOS] It has better generalization, but suffers from the lack of global information from the original relation names.\n[BOS] For example in Figure 1 (b), when doing only word-level matching, it is difficult to rank the target relation \"starring roles\" higher compared to the incorrect relation \"plays produced\".\n[BOS] This is because the incorrect relation contains word \"plays\", which is more similar to the question Table 1 : An example of KB relation (episodes written) with two types of relation tokens (relation names and words), and two questions asking this relation.\n[BOS] The topic entity is replaced with token <e> which could give the position information to the deep networks.\n[BOS] The italics show the evidence phrase for each relation token in the question.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Transition", "Transition", "Transition", "Other"], "span_citation_mapping": []}
{"id": "7752968_6", "paragraph": "[BOS] (1) Relation Name as a Single Token (relationlevel).\n[BOS] In this case, each relation name is treated as a unique token.\n[BOS] The problem with this approach is that it suffers from the low relation coverage due to limited amount of training data, thus cannot generalize well to large number of opendomain relations.\n[BOS] For example, in Figure 1 , when treating relation names as single tokens, it will be difficult to match the questions to relation names \"episodes written\" and \"starring roles\" if these names do not appear in training data -their relation embeddings h r s will be random vectors thus are not comparable to question embeddings h q s.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Other"], "span_citation_mapping": []}
{"id": "7752968_5", "paragraph": "[BOS] Previous research (Yih et al., 2015; Yin et al., 2016) formulates KB relation detection as a sequence matching problem.\n[BOS] However, while the questions are natural word sequences, how to represent relations as sequences remains a challenging problem.\n[BOS] Here we give an overview of two types of relation sequence representations commonly used in previous work.\n\n", "discourse_tags": ["Multi_summ", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 4, "token_end": 31, "char_start": 24, "char_end": 125, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Yih et al., 2015;": "18309765", "Yin et al., 2016)": "557620"}, "Reference": {}}}]}
{"id": "7752968_4", "paragraph": "[BOS] 3 Background: Different Granularity in KB Relations\n\n", "discourse_tags": ["Other"], "span_citation_mapping": []}
{"id": "7752968_3", "paragraph": "[BOS] Another difference between relation detection in KBQA and general RE is that general RE research assumes that the two argument entities are both available.\n[BOS] Thus it usually benefits from features (Nguyen and Grishman, 2014; Gormley et al., 2015) or attention mechanisms based on the entity information (e.g. entity types or entity embeddings).\n[BOS] For relation detection in KBQA, such information is mostly missing because: (1) one question usually contains single argument (the topic entity) and (2) one KB entity could have multiple types (type vocabulary size larger than 1,500).\n[BOS] This makes KB entity typing itself a difficult problem so no previous used entity information in the relation detection model.\n[BOS] 3\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Transition", "Other"], "span_citation_mapping": [{"token_start": 34, "token_end": 53, "char_start": 198, "char_end": 256, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Nguyen and Grishman, 2014;": "18895336", "Gormley et al., 2015)": "247735"}}}]}
{"id": "7752968_2", "paragraph": "[BOS] Relation Detection in KBQA Systems Relation detection for KBQA also starts with featurerich approaches Bast and Haussmann, 2015) towards usages of deep networks (Yih et al., 2015; Dai et al., 2016 ) and attention models (Yin et al., 2016; Golub and He, 2016 (Bordes et al., 2013) ), like (Dai et al., 2016) ; (2) factorize the relation names to sequences and formulate relation detection as a sequence matching and ranking task.\n[BOS] Such factorization works because that the relation names usually comprise meaningful word sequences.\n[BOS] For example, Yin et al. (2016) split relations to word sequences for single-relation detection.\n[BOS] Liang et al. (2016) also achieve good performance on WebQSP with wordlevel relation representation in an end-to-end neural programmer model.\n[BOS] Yih et al. (2015) use character tri-grams as inputs on both question and relation sides.\n[BOS] Golub and He (2016) propose a generative framework for single-relation KBQA which predicts relation with a character-level sequenceto-sequence model.\n\n", "discourse_tags": ["Multi_summ", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 16, "token_end": 28, "char_start": 86, "char_end": 134, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Bast and Haussmann, 2015)": "495573"}}}, {"token_start": 28, "token_end": 49, "char_start": 135, "char_end": 202, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yih et al., 2015;": "18309765", "Dai et al., 2016": "2887257"}}}, {"token_start": 51, "token_end": 76, "char_start": 209, "char_end": 285, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yin et al., 2016;": "557620", "Golub and He, 2016": "5213476", "(Bordes et al., 2013)": null}}}, {"token_start": 78, "token_end": 87, "char_start": 289, "char_end": 312, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dai et al., 2016)": "2887257"}}}, {"token_start": 129, "token_end": 147, "char_start": 561, "char_end": 643, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Yin et al. (2016)": "557620"}, "Reference": {}}}, {"token_start": 148, "token_end": 179, "char_start": 650, "char_end": 790, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Liang et al. (2016)": "2742513"}, "Reference": {}}}, {"token_start": 180, "token_end": 202, "char_start": 797, "char_end": 885, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Yih et al. (2015)": "18309765"}, "Reference": {}}}, {"token_start": 203, "token_end": 234, "char_start": 892, "char_end": 1041, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Golub and He (2016)": "5213476"}, "Reference": {}}}]}
{"id": "7752968_1", "paragraph": "[BOS] The above research assumes there is a fixed (closed) set of relation types, thus no zero-shot learning capability is required.\n[BOS] The number of relations is usually not large: The widely used ACE2005 has 11/32 coarse/fine-grained relations; SemEval2010 Task8 has 19 relations; TAC-KBP2015 has 74 relations although it considers open-domain Wikipedia relations.\n[BOS] All are much fewer than thousands of relations in KBQA.\n[BOS] As a result, few work in this field focuses on dealing with large number of relations or unseen relations.\n[BOS] Yu et al. (2016) proposed to use relation embeddings in a low-rank tensor method.\n[BOS] However their relation embeddings are still trained in supervised way and the number of relations is not large in the experiments.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Transition", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 121, "token_end": 164, "char_start": 551, "char_end": 769, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Yu et al. (2016)": "348944"}, "Reference": {}}}]}
{"id": "7752968_0", "paragraph": "[BOS] Relation Extraction Relation extraction (RE) is an important sub-field of information extraction.\n[BOS] General research in this field usually works on a (small) pre-defined relation set, where given a text paragraph and two target entities, the goal is to determine whether the text indicates any types of relations between the entities or not.\n[BOS] As a result RE is usually formulated as a classification task.\n[BOS] Traditional RE methods rely on large amount of hand-crafted features (Zhou et al., 2005; Rink and Harabagiu, 2010; Sun et al., 2011) .\n[BOS] Recent research benefits a lot from the advancement of deep learning: from word embeddings (Nguyen and Grishman, 2014; Gormley et al., 2015) to deep networks like CNNs and LSTMs (Zeng et al., 2014; dos Santos et al., 2015; Vu et al., 2016) and attention models .\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 89, "token_end": 120, "char_start": 474, "char_end": 559, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhou et al., 2005;": "3160937", "Rink and Harabagiu, 2010;": "12390391", "Sun et al., 2011)": "15013932"}}}, {"token_start": 135, "token_end": 155, "char_start": 643, "char_end": 708, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Nguyen and Grishman, 2014;": "18895336", "Gormley et al., 2015)": "247735"}}}, {"token_start": 158, "token_end": 188, "char_start": 726, "char_end": 807, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zeng et al., 2014;": "12873739", "dos Santos et al., 2015;": "15620570", "Vu et al., 2016)": "17297069"}}}]}
{"id": "7633695_2", "paragraph": "[BOS] With the availability of large-scale error corrected data (Mizumoto et al., 2011) , the statistical machine translation (SMT) approach to GEC became popular and was employed in state-of-the-art GEC systems.\n[BOS] Comparison of the classification approach and the machine translation approach can be found in (Rozovskaya and Roth, 2016) and .\n[BOS] Recently, an end-to-end neural machine translation framework was proposed for GEC (Yuan and Briscoe, 2016) , which was shown to achieve competitive results.\n[BOS] Neural network joint models have shown to be improve SMT-based GEC systems due to their ability to model words and phrases in a continuous space, access to larger contexts from source side, and abil-ity to learn non-linear mappings from input to output.\n[BOS] In this paper, we exploit the advantages of the SMT approach and neural network joint models (NNJMs) by adapting an NNJM based on the L1 background of the writers and integrating it into the SMT framework.\n[BOS] We perform KL divergence regularized adaptation to prevent overfitting on the smaller in-domain data.\n[BOS] KL divergence regularization was previously used by Yu et al. (2013) for speaker adaptation.\n[BOS] Joty et al. (2015) proposed another NNJM adaptation method, which uses a regularized objective function that encourages a network trained on general-domain data to be closer to an indomain NNJM.\n[BOS] Other adaptation techniques used in SMT include mixture modeling (Foster and Kuhn, 2007; Moore and Lewis, 2010; Sennrich, 2012) and alternative decoding paths (Koehn and Schroeder, 2007) .\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Narrative_cite", "Transition", "Reflection", "Reflection", "Narrative_cite", "Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 6, "token_end": 22, "char_start": 31, "char_end": 87, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mizumoto et al., 2011)": "5844380"}}}, {"token_start": 57, "token_end": 78, "char_start": 252, "char_end": 341, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rozovskaya and Roth, 2016)": "18563136"}}}, {"token_start": 84, "token_end": 107, "char_start": 367, "char_end": 460, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yuan and Briscoe, 2016)": "16766006"}}}, {"token_start": 239, "token_end": 253, "char_start": 1097, "char_end": 1165, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Yu et al. (2013)": "16753181"}}}, {"token_start": 258, "token_end": 301, "char_start": 1196, "char_end": 1390, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Joty et al. (2015)": "9074757"}, "Reference": {}}}, {"token_start": 310, "token_end": 332, "char_start": 1445, "char_end": 1524, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Foster and Kuhn, 2007;": "1421053", "Moore and Lewis, 2010;": "8170227", "Sennrich, 2012)": "11102913"}}}, {"token_start": 333, "token_end": 347, "char_start": 1529, "char_end": 1583, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Koehn and Schroeder, 2007)": "9536363"}}}]}
{"id": "7633695_1", "paragraph": "[BOS] The cross-linguistic influences between L1 and L2 have been mainly used for the task of native language identification (Massung and Zhai, 2016) .\n[BOS] It has also been used in typology prediction (Berzak et al., 2014) and predicting error distributions in ESL data (Berzak et al., 2015) .\n[BOS] L1-based adaptation has previously shown to improve GEC for specific error types using the classification approach.\n[BOS] Rozovskaya and Roth (2010) used an approach to correct preposition errors by restricting the candidate corrections to those observed in L1-specific data.\n[BOS] They further added artificial training data that mimic the error frequency in L1-specific text to improve accuracy.\n[BOS] In their later work, Rozovskaya and Roth (2011) focused on L1-based adaptation for preposition and article correction, by modifying the prior probabilities in the nave Bayes classifier during decision time based on L1-specific ESL learner text.\n[BOS] Both approaches use native data for training, but rely on non-native L1-specific text to introduce artificial errors or to modify the prior probabilities.\n[BOS] Dahlmeier and Ng (2011) implemented a system to correct collocation errors, by adding paraphrases derived from L1 into the confusion set.\n[BOS] Specifically, they use a bilingual L1-L2 corpus, to obtain L2 paraphrases, which are likely to be translated to the same phrase in L1.\n[BOS] There is no prior work on L1-based adaptation for GEC using the machine translation approach, which is a one of the most popular approaches for GEC.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Transition", "Single_summ", "Single_summ", "Single_summ", "Transition", "Single_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 21, "token_end": 33, "char_start": 94, "char_end": 149, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Massung and Zhai, 2016)": "35217690"}}}, {"token_start": 41, "token_end": 54, "char_start": 183, "char_end": 224, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Berzak et al., 2014)": "1642440"}}}, {"token_start": 59, "token_end": 72, "char_start": 263, "char_end": 293, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Berzak et al., 2015)": "5544829"}}}, {"token_start": 96, "token_end": 151, "char_start": 424, "char_end": 699, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Rozovskaya and Roth (2010)": "15175549"}, "Reference": {}}}, {"token_start": 157, "token_end": 206, "char_start": 727, "char_end": 950, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Rozovskaya and Roth (2011)": "14977841"}, "Reference": {}}}, {"token_start": 238, "token_end": 304, "char_start": 1118, "char_end": 1396, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Dahlmeier and Ng (2011)": "8330264"}, "Reference": {}}}]}
{"id": "7633695_0", "paragraph": "[BOS] In the past decade, there has been increasing attention on GEC in English, mainly due to the growing number of English as second language (ESL) learners around the world.\n[BOS] The popularity of this problem grew further through Helping Our Own (HOO) (Dale and Kilgarriff, 2011; Dale et al., 2012) and CoNLL shared tasks .\n[BOS] The majority of the published work on GEC aimed at building classifiers or rule-based systems for specific error types and combined them to build hybrid systems Rozovskaya et al., 2014) .\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 47, "token_end": 73, "char_start": 235, "char_end": 303, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dale and Kilgarriff, 2011;": "18357549", "Dale et al., 2012)": "11159647"}}}, {"token_start": 106, "token_end": 120, "char_start": 475, "char_end": 520, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Rozovskaya et al., 2014)": "1547333"}}}]}
{"id": "7534444_3", "paragraph": "[BOS] More recently, inspired by the success of deep dilated CNNs for image segmentation in computer vision (Yu and Koltun, 2016; , convolutional neural networks have been employed as fast models for tagging, speech generation and machine translation.\n[BOS] use dilated CNNs to efficiently generate speech, and describes an encoder-decoder model for machine translation which uses dilated CNNs over bytes in both the encoder and decoder.\n[BOS] Strubell et al. (2017) first described the one-dimensional ID-CNN architecture which is the basis for this work, demonstrating its success as a fast and accurate NER tagger.\n[BOS] Gehring et al. (2017) report state-ofthe-art results and much faster training from using many CNN layers with gated activations as encoders and decoders for a sequence-to-sequence model.\n[BOS] While our architecture is similar to the encoder architecture of these models, ours is differentiated by (1) being tailored to smaller-data regimes such as parsing via our iterated architecture and loss, and (2) employing two-dimensional convolutions to model the adjacency matrix of the parse tree.\n[BOS] We are the first to our knowledge to use dilated convolutions for parsing, or to use twodimensional dilated convolutions for NLP.\n[BOS] Kiperwasser and Goldberg (2016) 93.9 91.9 Cheng et al. (2016) 94.10 91.49 Kuncoro et al. (2016) 94.3 92.1 Hashimoto et al. (2017) 94.67 92.90 Ma and Hovy (2017) 94.9 93.0 Dozat and Manning (2017) 95\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Single_summ", "Single_summ", "Reflection", "Reflection", "Narrative_cite"], "span_citation_mapping": [{"token_start": 15, "token_end": 27, "char_start": 70, "char_end": 128, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 82, "token_end": 121, "char_start": 444, "char_end": 617, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Strubell et al. (2017)": "1655651"}, "Reference": {}}}, {"token_start": 122, "token_end": 165, "char_start": 624, "char_end": 810, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Gehring et al. (2017)": "3648736"}, "Reference": {}}}, {"token_start": 252, "token_end": 331, "char_start": 1259, "char_end": 1454, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Cheng et al. (2016)": "2180008", "Kuncoro et al. (2016)": "905294", "Hashimoto et al. (2017)": "2213896", "Ma and Hovy (2017)": "6925721", "Dozat and Manning (2017)": "7942973"}}}]}
{"id": "7534444_2", "paragraph": "[BOS] Despite their success in the area of computer vision, in NLP convolutional neural networks have mainly been relegated to tasks such as sentence classification, where each input sequence is mapped to a single label (rather than a label for each token) Kim (2014) ; Kalchbrenner et al. (2014); Zhang et al. (2015) ; Toutanova et al. (2015) .\n[BOS] As described above, CNNs have also been used to encode token representations from embeddings of their characters, which similarly perform a pooling operation over characters.\n[BOS] Lei et al. (2015) present a CNN variant where convolutions adaptively skip neighboring words.\n[BOS] While the flexibility of this model is powerful, its adaptive behavior is not well-suited to GPU acceleration.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 33, "token_end": 84, "char_start": 183, "char_end": 343, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Kim (2014)": null, "Kalchbrenner et al. (2014);": "1306065", "Zhang et al. (2015)": "368182", "Toutanova et al. (2015)": "2127100"}}}, {"token_start": 116, "token_end": 157, "char_start": 533, "char_end": 743, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Lei et al. (2015)": "2146847"}, "Reference": {}}}]}
{"id": "7534444_1", "paragraph": "[BOS] Previously, (Chen and Manning, 2014) pioneered neural network paring with a transitionbased dependency parser which used features from a fast feed-forward neural network over word, token and label embeddings.\n[BOS] Many improved upon this work by increasing the size of the network and using a structured training objective (Weiss et al., 2015; Andor et al., 2016) .\n[BOS] (Kiperwasser and Goldberg, 2016) were the first to present a graph-based neural network parser, employing an MLP with bidirectional LSTM inputs to score arcs and labels.\n[BOS] (Cheng et al., 2016) propose a similar network, except with additional forward and backward encoders to allow for conditioning on previous predictions.\n[BOS] (Kuncoro et al., 2016) take a different approach, distilling a consensus of many LSTM-based transition-based parsers into one graph-based parser.\n[BOS] (Ma and Hovy, 2017) employ a similar model, but add a CNN over characters as an additional word representation and perform structured training using the Matrix-Tree Theorem.\n[BOS] Hashimoto et al. (2017) train a large network which performs many NLP tasks including part-of-speech tagging, chunking, graph-based parsing, and entailment, observing benefits from multitasking with these tasks.\n\n", "discourse_tags": ["Single_summ", "Narrative_cite", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 4, "token_end": 43, "char_start": 18, "char_end": 214, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Chen and Manning, 2014)": "11616343"}, "Reference": {}}}, {"token_start": 59, "token_end": 78, "char_start": 300, "char_end": 370, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Weiss et al., 2015;": "15213991", "Andor et al., 2016)": "2952144"}}}, {"token_start": 80, "token_end": 117, "char_start": 379, "char_end": 548, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Kiperwasser and Goldberg, 2016)": "1642392"}, "Reference": {}}}, {"token_start": 118, "token_end": 147, "char_start": 555, "char_end": 706, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Cheng et al., 2016)": "2180008"}, "Reference": {}}}, {"token_start": 148, "token_end": 185, "char_start": 713, "char_end": 858, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Kuncoro et al., 2016)": "905294"}, "Reference": {}}}, {"token_start": 186, "token_end": 221, "char_start": 865, "char_end": 1038, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Ma and Hovy, 2017)": "6925721"}, "Reference": {}}}, {"token_start": 222, "token_end": 270, "char_start": 1045, "char_end": 1256, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hashimoto et al. (2017)": "2213896"}, "Reference": {}}}]}
{"id": "7534444_0", "paragraph": "[BOS] Currently, the most accurate parser in terms of labeled and unlabeled attachment scores is the neural network graph-based dependency parser of Dozat and Manning (2017) .\n[BOS] Their parser builds token representations with a bidirectional LSTM over word embeddings, followed by head and dependent MLPs.\n[BOS] Compatibility between heads and dependents is then scored using a biaffine model, and the highest scoring head for each dependent is selected.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 87, "char_start": 6, "char_end": 457, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Dozat and Manning (2017)": "7942973"}, "Reference": {}}}]}
{"id": "7499734_1", "paragraph": "[BOS] The joint model most closely related to ours is that of , modeling coreference, named entity recognition, and relation extraction.\n[BOS] Their techniques differ from ours in a few notable ways: they choose a different objective function than we do and also opt to freeze the values of certain variables during the belief propagation process rather than pruning with a coarse pass.\n[BOS] Sil and Yates (2013) jointly model NER and entity linking in such a way that they maintain uncertainty over mention boundaries, allowing information from Wikipedia to inform segmentation choices.\n[BOS] We could strengthen our model by integrating this capability; however, the primary cause of errors for mention detection on OntoNotes is parsing ambiguities rather than named entity ambiguities, so we would be unlikely to see improvements in the experiments presented here.\n[BOS] Beyond maintaining uncertainty over mention boundaries, we might also consider maintaining uncertainty over the entire parse structure, as in Finkel and Manning (2009) , who consider parsing and named entity recognition together with a PCFG.\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ", "Reflection", "Single_summ"], "span_citation_mapping": [{"token_start": 71, "token_end": 105, "char_start": 393, "char_end": 588, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Sil and Yates (2013)": "14250044"}, "Reference": {}}}, {"token_start": 156, "token_end": 199, "char_start": 875, "char_end": 1116, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Finkel and Manning (2009)": "10473638"}, "Reference": {}}}]}
{"id": "7499734_0", "paragraph": "[BOS] There are two closely related threads of prior work: those that address the tasks we consider in a different way and those that propose joint models for other related sets of tasks.\n[BOS] In the first category, Hajishirzi et al. (2013) integrate entity linking into a sieve-based coreference system (Raghunathan et al., 2010) , the aim being to propagate link decisions throughout coreference chains, block corefer-ence links between different entities, and use semantic information to make additional coreference links.\n[BOS] build coreference clusters greedily left-to-right and maintain entity link information for each cluster, namely a list of possible targets in the knowledge base as well as a current best link target that is used to extract features (though that might not be the target that is chosen by the end of inference).\n[BOS] Cheng and Roth (2013) use coreference as a preprocessing step for entity linking and then solve an ILP to determine the optimal entity link assignments for each mention based on surface properties of that mention, other mentions in its cluster, and other mentions that it is related to.\n[BOS] Compared to these systems, our approach maintains greater uncertainty about all random variables throughout inference and uses features to capture cross-task interactions as opposed to rules or hard constraints, which can be less effective for incorporating semantic knowledge (Lee et al., 2011) .\n\n", "discourse_tags": ["Transition", "Single_summ", "Transition", "Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 42, "token_end": 111, "char_start": 217, "char_end": 526, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hajishirzi et al. (2013)": "5716274"}, "Reference": {"(Raghunathan et al., 2010)": null}}}, {"token_start": 176, "token_end": 233, "char_start": 849, "char_end": 1135, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Cheng and Roth (2013)": "17784265"}, "Reference": {}}}, {"token_start": 274, "token_end": 284, "char_start": 1400, "char_end": 1437, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lee et al., 2011)": "618047"}}}]}
{"id": "7427989_2", "paragraph": "[BOS] Our work differs from previous work in that we are concerned with obtaining diversified translation rules using multiple different parsers (or parsing models) instead of a single parser (or parsing model).\n[BOS] It can be regarded as an enhancement of previous studies.\n[BOS] As shown in the following parts of this paper, it works very well with the existing techniques, such as rule composing (Galley et al., 2006) , SPMT models and rule extraction with kbest parses (Venugopal et al., 2008) .\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 74, "token_end": 86, "char_start": 386, "char_end": 422, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Galley et al., 2006)": null}}}, {"token_start": 91, "token_end": 108, "char_start": 441, "char_end": 499, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Venugopal et al., 2008)": null}}}]}
{"id": "7427989_1", "paragraph": "[BOS] As another research direction, some work is focused on enlarging the scope of rule extraction to improve rule coverage.\n[BOS] For example, (Venugopal et al., 2008) and (Mi and Huang, 2008) extracted rules from the k-best parses and forest generated by a single parser to alleviate the problem of the limited scope of 1-best parse, and achieved promising results.\n\n", "discourse_tags": ["Transition", "Multi_summ"], "span_citation_mapping": [{"token_start": 28, "token_end": 81, "char_start": 145, "char_end": 368, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Venugopal et al., 2008)": null, "(Mi and Huang, 2008)": "7189219"}, "Reference": {}}}]}
{"id": "7427989_0", "paragraph": "[BOS] In machine translation, some efforts have been made to improve rule coverage and advance the performance of syntax-based systems.\n[BOS] For example, Galley et al. (2006) proposed the idea of rule composing which composes two or more rules with shared states to form a larger, composed rule.\n[BOS] Their experimental results showed that the rule composing method could significantly improve the translation accuracy of their syntax-based system.\n[BOS] Following Galley et al. (2006) 's work, proposed SPMT models to improve the coverage of phrasal rules, and demonstrated that the system performance could be further improved by using their proposed models.\n[BOS] described a binarization method that binarized parse trees to improve the rule coverage on nonsyntactic mappings.\n[BOS] DeNeefe et al. (2007) analyized the phrasal coverage problem, and compared the phrasal coverage as well as translation accuracy for various rule extraction methods (Galley et al., 2006; .\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Transition", "Single_summ"], "span_citation_mapping": [{"token_start": 29, "token_end": 86, "char_start": 155, "char_end": 450, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Galley et al. (2006)": null}, "Reference": {}}}, {"token_start": 88, "token_end": 129, "char_start": 467, "char_end": 662, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Galley et al. (2006)": null}, "Reference": {}}}, {"token_start": 154, "token_end": 195, "char_start": 789, "char_end": 973, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"DeNeefe et al. (2007)": "2646100"}, "Reference": {}}}]}
{"id": "7416323_1", "paragraph": "[BOS] The approach in this paper is reminiscent of co-training (Blum and Mitchell, 1998; Sagae and Lavie, 2006b ) and up-training (Petrov et al., 2010) .\n[BOS] Moreover, it coincides with the stacking method used for dependency parser combination (Martinset al., 2008; Nivre and McDonald, 2008) , the Pred method for domain adaptation (Daum III and Marcu, 2006) , and the method for annotation adaptation of word segmentation and POS tagging .\n[BOS] As one of the most related works, present a similar approach to conversion between dependency treebanks.\n[BOS] In contrast to , the task studied in this paper, phrase-structure treebank conversion, is relatively complicated and more efforts should be put into feature engineering.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 10, "token_end": 29, "char_start": 51, "char_end": 111, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Blum and Mitchell, 1998;": "207228399"}}}, {"token_start": 31, "token_end": 43, "char_start": 118, "char_end": 151, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Petrov et al., 2010)": "5401473"}}}, {"token_start": 55, "token_end": 73, "char_start": 217, "char_end": 294, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Nivre and McDonald, 2008)": "9431510"}}}, {"token_start": 75, "token_end": 90, "char_start": 301, "char_end": 361, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Daum\u00e9 III and Marcu, 2006)": "14154185"}}}]}
{"id": "7416323_0", "paragraph": "[BOS] For phrase-structure treebank conversion, Wang et al. (1994) suggest to use source-side bracketing structures to select conversion results from k-best lists.\n[BOS] The approach is quite generic in the sense that it can be used for conversion between treebanks of different grammar formalisms, such as from a dependency treebank to a constituency treebank (Niu et al., 2009 ).\n[BOS] However, it suffers from limited variations in k-best lists (Huang, 2008) .\n[BOS] Zhu and Zhu (2010) propose to incorporate bracketing structures as parsing constraints in the decoding phase of a CKY-style parser.\n[BOS] Their approach shows significant improvements over Wang et al. (1994) .\n[BOS] However, it suffers from binary distinctions (consistent or inconsistent), as discussed in Section 1.\n\n", "discourse_tags": ["Single_summ", "Narrative_cite", "Narrative_cite", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 36, "char_start": 6, "char_end": 163, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wang et al. (1994)": "11176598"}, "Reference": {}}}, {"token_start": 66, "token_end": 83, "char_start": 314, "char_end": 378, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Niu et al., 2009": "3029263"}}}, {"token_start": 91, "token_end": 103, "char_start": 413, "char_end": 461, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Huang, 2008)": "1131864"}}}, {"token_start": 105, "token_end": 155, "char_start": 470, "char_end": 730, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhu and Zhu (2010)": "9457590"}, "Reference": {"Wang et al. (1994)": "11176598"}}}]}
{"id": "7240730_4", "paragraph": "[BOS] In contrast to the previous CAT research, we present a writing assistant that suggests grammar constructs as well as lexical translations following users' partial translation, aiming to provide users with choice to ease mental burden and enhance performance.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "7240730_3", "paragraph": "[BOS] Recent work has been done on using fullyfledged statistical MT systems to produce target hypotheses completing user-validated translation prefix in IMT paradigm.\n[BOS] Barrachina et al. (2008) investigate the applicability of different MT kernels within IMT framework.\n[BOS] Nepveu et al. (2004) and Ortiz-Martinez et al. (2011) further exploit user feedbacks for better IMT systems and user experience.\n[BOS] Instead of triggered by user correction, our method is triggered by word delimiter and assists both translation and learning the target language.\n\n", "discourse_tags": ["Transition", "Single_summ", "Multi_summ", "Reflection"], "span_citation_mapping": [{"token_start": 32, "token_end": 53, "char_start": 174, "char_end": 274, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Barrachina et al. (2008)": "92327"}, "Reference": {}}}, {"token_start": 54, "token_end": 88, "char_start": 281, "char_end": 409, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Nepveu et al. (2004)": "943080", "Ortiz-Martinez et al. (2011)": "10521251"}, "Reference": {}}}]}
{"id": "7240730_2", "paragraph": "[BOS] More recently, interactive MT (IMT) systems have begun to shift the user's role from postediting machine output to collaborating with the machine to produce the target text.\n[BOS] Foster et al (2000) describe TransType, a pioneering system that supports next word predictions.\n[BOS] Along the similar line, Koehn (2009) develops caitra which predicts and displays phrasal translation suggestions one phrase at a time.\n[BOS] The main difference between their systems and TransAhead is that we also display grammar patterns to provide the general patterns of predicted translations so a student translator can learn and become more proficient.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 39, "token_end": 59, "char_start": 186, "char_end": 282, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 65, "token_end": 88, "char_start": 313, "char_end": 423, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Koehn (2009)": "6341545"}, "Reference": {}}}]}
{"id": "7240730_1", "paragraph": "[BOS] emphasis on language learning.\n[BOS] Specifically, our goal is to build a translation assistant to help translator (or learner-translator) with inline grammar help and translation.\n[BOS] Unlike recent research focusing on professional (e.g., Brown and Nirenburg, 1990) , we target on both professional and student translators.\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 43, "token_end": 58, "char_start": 228, "char_end": 274, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Brown and Nirenburg, 1990)": "8429165"}}}]}
{"id": "7240730_0", "paragraph": "[BOS] Computer Assisted Translation (CAT) has been an area of active research.\n[BOS] We focus on offering suggestions during the translation process with an Figure 1 .\n[BOS] Example TransAhead responses to a source text under the translation (a) \"we\" and (b) \"we play an important role\".\n[BOS] Note that the grammar/text predictions of (a) and (b) are not placed directly under the caret (current input focus) for space limit.\n[BOS] (c) and (d) depict predominant grammar constructs which follow and (e) summarizes the confident translations of the source's character-based ngrams.\n[BOS] The frequency of grammar pattern is shown in round brackets while the history (i.e., keyword) based on the user input is shown in shades.\n\n", "discourse_tags": ["Transition", "Reflection", "Other", "Other", "Other", "Other"], "span_citation_mapping": []}
{"id": "7236507_1", "paragraph": "[BOS] There has been recent work integrating continuous and discrete features for the task of POS tagging (Ma et al., 2014b; Tsuboi, 2014) .\n[BOS] Both models have essentially the same structure as our model.\n[BOS] In contrast to their work, we systematically compare various ways to integrate discrete and continuous features, for the dependency parsing task.\n[BOS] Our model is also different from Ma et al. (2014b) in the hidden layer.\n[BOS] While they use a form of restricted Boltzmann machine to pre-train the embeddings and hidden layer from large-scale ngrams, we fully rely on supervised learning to train complex feature combinations.\n[BOS] Wang and Manning (2013) consider integrating embeddings and discrete features into a neural CRF.\n[BOS] They show that combined neural and discrete features work better without a hidden layer (i.e. Turian et al. (2010) ).\n[BOS] They argue that nonlinear structures do not work well with high dimensional features.\n[BOS] We find that using a hidden layer specifically for embedding features gives better results compared with using no hidden layers.\n\n", "discourse_tags": ["Narrative_cite", "Reflection", "Reflection", "Reflection", "Reflection", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 16, "token_end": 33, "char_start": 94, "char_end": 138, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ma et al., 2014b;": "6039651", "Tsuboi, 2014)": "7113099"}}}, {"token_start": 72, "token_end": 86, "char_start": 367, "char_end": 417, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Ma et al. (2014b)": "6039651"}}}, {"token_start": 131, "token_end": 194, "char_start": 651, "char_end": 963, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wang and Manning (2013)": "14694342"}, "Reference": {"Turian et al. (2010)": "629094"}}}]}
{"id": "7236507_0", "paragraph": "[BOS] As discussed in the introduction, our work is related to previous work on integrating word embeddings into discrete models (Turian et al., 2010; Yu et al., 2013; Guo et al., 2014) .\n[BOS] Along this line, there has also been work that uses a neural network to automatically vectorize the structures of a sentence, and then taking the resulting vector as features in a linear NLP model (Socher et al., 2012; Tang et al., 2014; Yu et al., 2015) .\n[BOS] Our results show that the use of a hidden neural layer gives superior results compared with both direct integration and integration via a hard-coded transformation function (e.g binarization or clustering).\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 17, "token_end": 45, "char_start": 92, "char_end": 185, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Turian et al., 2010;": "629094", "Yu et al., 2013;": "2032073", "Guo et al., 2014)": "6540554"}}}, {"token_start": 81, "token_end": 108, "char_start": 374, "char_end": 448, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Socher et al., 2012;": "806709", "Tang et al., 2014;": "886027", "Yu et al., 2015)": "14414445"}}}]}
{"id": "7234079_0", "paragraph": "[BOS] The task of detecting missing hyphens is related to previous work on detecting punctuation errors.\n[BOS] One of the classes of errors in the Helping Our Own (HOO) 2011 shared task (Dale and Kilgarriff, 2011) was punctuation.\n[BOS] Comma errors are the most frequent kind of punctuation error made by learners.\n[BOS] Israel et al. (2012) present a model for detecting these kinds of errors in learner texts.\n[BOS] They train CRF models on sentences from unedited essays written by high-level college students and show that they performs well on detecting errors in learner text.\n[BOS] As far as we are aware, the HOO 2011 system description of Rozovskaya et al. (2011) is the only work to specifically reference hyphen errors.\n[BOS] They use rules derived from frequencies in the training corpus to determine whether a hyphen was required between two words separated by white space.\n[BOS] The task of detecting missing hyphens is related to the task of inserting punctuation into the output of unpunctuated text (for example, the output of speech recognition, automatic generation, machine translation, etc.).\n[BOS] Systems that are built on the output of speech recognition can obviously take features like prosody into account.\n[BOS] In our case, we are dealing only with written text.\n[BOS] Gravano et al. (2009) present an n-gram-based model for automatically adding punctuation and capitalization to the output of an ASR system, without taking any of the speech signal information into account.\n[BOS] They conclude that more training data, rather than wider n-gram contexts leads to a greater improvement in accuracy.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Transition", "Reflection", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 21, "token_end": 50, "char_start": 111, "char_end": 213, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dale and Kilgarriff, 2011)": "18357549"}}}, {"token_start": 72, "token_end": 125, "char_start": 322, "char_end": 583, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Israel et al. (2012)": "7894256"}, "Reference": {}}}, {"token_start": 133, "token_end": 235, "char_start": 614, "char_end": 1114, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Rozovskaya et al. (2011)": "14612916"}, "Reference": {}}}, {"token_start": 270, "token_end": 337, "char_start": 1299, "char_end": 1627, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Gravano et al. (2009)": "7801687"}, "Reference": {}}}]}
{"id": "7229283_2", "paragraph": "[BOS] Chen and Lin (2000) is the only study that used contextual information for the same task.\n[BOS] To generate candidate categories for a word, they looked up its translations in a ChineseEnglish dictionary and the synsets of the translations in WordNet, and mapped the synsets to the categories in Cilin.\n[BOS] They used a corpus-based model similar to ours to rank the candidates.\n[BOS] They reported an accuracy of 34.4%, which is close to the 37.1% accuracy of our corpus-based model, but lower than the 61.6% accuracy of our combined knowledge-based model.\n[BOS] In addition, they could only classify the unknown words listed in the Chinese-English dictionary.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 143, "char_start": 6, "char_end": 668, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "7229283_1", "paragraph": "[BOS] Tseng used a morphological analyzer in the same way, but she also derived the morphosyntactic relationship between the morphemes.\n[BOS] She retrieved examples that share a morpheme with the target word in the same position and filtered those with a different morpho-syntactic relationship.\n[BOS] Finally, she computed the similarity between two words as the similarity between their non-shared morphemes, using a similar concept of IC of the LCA of two categories.\n[BOS] She classified unknown words into the 12 major categories only, and reported accuracies 65.8% on adjectives, 71.4% on nouns, and 52.8% on verbs.\n[BOS] These results are not as good as the 83.0% overall accuracy our combined knowledge-based model achieved for classifying unknown words into major categories.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Transition", "Reflection"], "span_citation_mapping": []}
{"id": "7229283_0", "paragraph": "[BOS] The few previous studies on semantic classification of Chinese unknown word have primarily adopted knowledge-based models.\n[BOS] Chen (2004) proposed a model that retrieves the word with the greatest association with the target word.\n[BOS] This model is computationally more expensive than our character-category association model, as it entails computing associations between every character-category, category-character, character-character, and word-word pair.\n[BOS] He reported an accuracy of 61.6% on bisyllabic V-V compounds.\n[BOS] However, he included all the test words in training the model.\n[BOS] If we also include the test words in computing character-category associations, the computationally cheaper model achieves an overall accuracy of 75.6%, with an accuracy of 75.1% on verbs.\n[BOS] adopted similar exemplar-based models.\n[BOS] Chen and Chen used a morphological analyzer to identify the head of the target word and the semantic categories of its modifier.\n[BOS] They then retrieved examples with the same head as the target word.\n[BOS] Finally, they computed the similarity between two words as the similarity between their modifiers, using the concept of information load (IC) of the least common ancestor (LCA) of the modifiers' semantic categories.\n[BOS] They reported an accuracy of 81% for classifying 200 unknown nouns.\n[BOS] Given the small test set of their study, it is hard to directly compare their results with ours.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 22, "token_end": 117, "char_start": 135, "char_end": 606, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Chen (2004)": "16578107"}, "Reference": {}}}, {"token_start": 167, "token_end": 266, "char_start": 853, "char_end": 1351, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "7205805_3", "paragraph": "[BOS] Finally, we contrast our work with rationale-based classification (Zaidan et al., 2007) which seeks to reduce supervised annotations by relying on richer annotations in the form of human-provided rationales.\n[BOS] In our work, rationales are never given during training, and the goal is to learn to generate them.\n\n", "discourse_tags": ["Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 9, "token_end": 23, "char_start": 41, "char_end": 93, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zaidan et al., 2007)": "3061036"}}}]}
{"id": "7205805_2", "paragraph": "[BOS] Our work differs from past approaches in terms of what is meant by interpretable models (generating concise yet sufficient rationales) and how interpretation is derived (rationale generation).\n[BOS] We explicitly aim to identify salient portions of the input text to justify predictions.\n[BOS] Moreover, architecturally, we detach the rationale generation from how it is used (encoding) so as to be able to directly control types of rationales that are acceptable, and to facilitate broader modular use in other applications.\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "7205805_1", "paragraph": "[BOS] Attention based models offer another lens to the inner workings of neural models (Bahdanau et al., 2015; Cheng et al., 2016; Martins and Astudillo, 2016; Chen et al., 2015; Xu and Saenko, 2015; Yang et al., 2015) .\n[BOS] Such models have been successfully applied to many NLP problems, improving both prediction accuracy as well as visualization and interpretability (Rush et al., 2015; Rocktschel et al., 2016; Hermann et al., 2015) .\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 14, "token_end": 65, "char_start": 73, "char_end": 218, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bahdanau et al., 2015;": "11212020", "Cheng et al., 2016;": "6506243", "Martins and Astudillo, 2016;": "16432551", "Chen et al., 2015;": "16566944", "Xu and Saenko, 2015;": "10363459", "Yang et al., 2015)": "8849206"}}}, {"token_start": 81, "token_end": 116, "char_start": 307, "char_end": 439, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rush et al., 2015;": "1918428", "Rockt\u00e4schel et al., 2016;": null, "Hermann et al., 2015)": "6203757"}}}]}
{"id": "7205805_0", "paragraph": "[BOS] Developing sparse interpretable models holds considerably interest in the broader research community (Letham et al., 2015; Kim et al., 2015) .\n[BOS] The need for interpretability is even more pronounced with recent neural models.\n[BOS] Efforts in this area include analyzing and visualizing state activation (Hermans and Schrauwen, 2013; Karpathy et al., 2015; , learning sparse interpretable word vectors (Faruqui et al., 2015b) , and linking word vectors to semantic lexicons or word properties (Faruqui et al., 2015a; Herbelot and Vecchi, 2015) .\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 12, "token_end": 31, "char_start": 80, "char_end": 146, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Letham et al., 2015;": "17699665", "Kim et al., 2015)": "462623"}}}, {"token_start": 53, "token_end": 76, "char_start": 271, "char_end": 365, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hermans and Schrauwen, 2013;": "20071973"}}}, {"token_start": 78, "token_end": 96, "char_start": 369, "char_end": 435, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Faruqui et al., 2015b)": "9397697"}}}, {"token_start": 98, "token_end": 129, "char_start": 442, "char_end": 553, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Faruqui et al., 2015a;": "51838647", "Herbelot and Vecchi, 2015)": "15053998"}}}]}
{"id": "7094449_4", "paragraph": "[BOS] One is phrase-based statistical machine transliteration (e.g. Song et al., 2010; Finch and Sumita, 2010 ) and the other is Conditional Random Fields which treats the task as one of sequence labelling (e.g. Shishtla et al., 2009 ).\n[BOS] More recent shared tasks have shown a wider array of promising techniques (Zhang et al., 2011; Zhang et al., 2012) , although the absolute results as measured by Word Accuracy in Top-1 (ACC), Fuzziness in Top-1 (Mean Fscore), and Mean Reciprocal Rank (MRR) have not really demonstrated any remarkable boost.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 4, "token_end": 31, "char_start": 13, "char_end": 109, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Finch and Sumita, 2010": "14251200"}}}, {"token_start": 36, "token_end": 62, "char_start": 129, "char_end": 233, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Shishtla et al., 2009": null}}}, {"token_start": 75, "token_end": 92, "char_start": 296, "char_end": 357, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhang et al., 2011;": null, "Zhang et al., 2012)": "7761590"}}}]}
{"id": "7094449_3", "paragraph": "[BOS] The reports of the shared task in NEWS 2009 (Li et al., 2009 ) and NEWS 2010 (Li et al., 2010) highlighted two particularly popular approaches for transliteration generation among the participating systems.\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 9, "token_end": 18, "char_start": 40, "char_end": 66, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Li et al., 2009": "62005149"}}}, {"token_start": 20, "token_end": 30, "char_start": 73, "char_end": 100, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Li et al., 2010)": "16427389"}}}]}
{"id": "7094449_2", "paragraph": "[BOS] The baseline in current English-Chinese transliteration generation research often refers to Li et al. (2004) .\n[BOS] They used a Joint SourceChannel Model under the direct orthographic mapping (DOM) framework, which skips the middle phonemic representation in conventional phoneme-based methods, and models the segmentation and alignment preferences by means of contextual n-grams of the transliteration units.\n[BOS] Their method was shown to outperform phoneme-based methods and those based on the noisy channel model.\n[BOS] In fact, transliteration of foreign names into Chinese is often based on the surface orthographic forms, as exemplified in the transliteration of Beckham, where the supposedly silent h in \"ham\" is taken as pronounced, resulting in  (Hanyu Pinyin: han4-mu3) in Mandarin Chinese and  (Jyutping: haam4) in Cantonese.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 2, "token_end": 103, "char_start": 6, "char_end": 525, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Li et al. (2004)": null}, "Reference": {}}}]}
{"id": "7094449_1", "paragraph": "[BOS] On the one hand, various alignment models are used for acquiring transliteration lexicons from parallel corpora and other resources (e.g. Lee et al., 2006; Jin et al., 2008; Kuo and Li, 2008) .\n[BOS] On the other hand, statistical transliteration models are built for transliterating personal names and other proper names, and these models can be based on phonemes (e.g. Knight and Graehl, 1998; Virga and Khudanpur, 2003) , graphemes (e.g. Li et al., 2004) , or their combination (e.g. Oh and Choi, 2005) .\n[BOS] They may operate on characters (e.g. Shishtla et al., 2009) , syllables (e.g. Wutiwiwatchai and Thangthai, 2010) , as well as hybrid units (e.g. Oh and Choi, 2005) .\n[BOS] In addition to phonetic features, others like temporal, semantic, and tonal features have also been found useful in transliteration (e.g. Tao et al., 2006; Li et al., 2007; Yoon et al., 2007; Kwong, 2009) .\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 24, "token_end": 51, "char_start": 128, "char_end": 197, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Lee et al., 2006;": "17062981", "Jin et al., 2008;": null, "Kuo and Li, 2008)": "10833675"}}}, {"token_start": 83, "token_end": 109, "char_start": 362, "char_end": 428, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Virga and Khudanpur, 2003)": "5628888"}}}, {"token_start": 110, "token_end": 124, "char_start": 431, "char_end": 463, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Li et al., 2004)": null}}}, {"token_start": 127, "token_end": 139, "char_start": 475, "char_end": 511, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Oh and Choi, 2005)": "17161141"}}}, {"token_start": 145, "token_end": 161, "char_start": 540, "char_end": 579, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Shishtla et al., 2009)": null}}}, {"token_start": 162, "token_end": 181, "char_start": 582, "char_end": 632, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Wutiwiwatchai and Thangthai, 2010)": null}}}, {"token_start": 185, "token_end": 198, "char_start": 646, "char_end": 683, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Oh and Choi, 2005)": "17161141"}}}, {"token_start": 222, "token_end": 257, "char_start": 808, "char_end": 896, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Tao et al., 2006;": "8424232", "Li et al., 2007;": "7527801", "Yoon et al., 2007;": "10685951", "Kwong, 2009)": "14074548"}}}]}
{"id": "7094449_0", "paragraph": "[BOS] There are basically two categories of work on machine transliteration.\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "6949557_4", "paragraph": "[BOS] Finally, our work relates to supervised topic models in Blei and McAullife (2007) .\n[BOS] In this work, latent topic variables are used to generate text as well as a supervised sentiment rating for the document.\n[BOS] However, this architecture does not permit the usage of standard discriminative models which condition freely on textual features.\n\n", "discourse_tags": ["Reflection", "Reflection", "Transition"], "span_citation_mapping": [{"token_start": 8, "token_end": 21, "char_start": 35, "char_end": 87, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Blei and McAullife (2007)": "7375081"}}}]}
{"id": "6949557_3", "paragraph": "[BOS] While our method also incorporates contextual information into existing text analysis applications, our approach is markedly different from the above approaches.\n[BOS] First, our representation of context encodes more than the relevance-based binary distinction considered in the past work.\n[BOS] Our algorithm adjusts the content model dynamically for a given task rather than pre-specifying it.\n[BOS] Second, while previous work is fully supervised, in our case relevance annotations are readily available for only a few applications and are prohibitively expensive to obtain for many others.\n[BOS] To overcome this drawback, our method induces a content model in an unsupervised fashion and connects it via latent variables to the target model.\n[BOS] This design not only eliminates the need for additional annotations, but also allows the algorithm to leverage large quantities of raw data for training the content model.\n[BOS] The tight coupling of rel-evance learning with the target analysis task leads to further performance gains.\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection", "Reflection", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "6949557_2", "paragraph": "[BOS] Another approach, taken by Choi and Cardie (2008) and Somasundaran et al. (2009) uses linguistic resources to create a latent model in a taskspecific fashion to improve performance, rather than assuming sentence-level task relevancy.\n[BOS] Choi and Cardie (2008) address a sentiment analysis task by using a heuristic decision process based on wordlevel intermediate variables to represent polarity.\n[BOS] Somasundaran et al. (2009) similarly uses a bootstrapped local polarity classifier to identify sentence polarity.\n[BOS] McDonald et al. (2007) propose a model which jointly identifies global polarity as well as paragraph-and sentence-level polarity, all of which are observed in training data.\n[BOS] While our approach uses a similar hierarchy, McDonald et al. (2007) is concerned with recovering the labels at all levels, whereas in this work we are interested in using latent document content structure as a means to benefit task predictions.\n\n", "discourse_tags": ["Multi_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 53, "char_start": 6, "char_end": 239, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Choi and Cardie (2008)": "2527473", "Somasundaran et al. (2009)": "3244636"}, "Reference": {}}}, {"token_start": 54, "token_end": 82, "char_start": 246, "char_end": 405, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Choi and Cardie (2008)": "2527473"}, "Reference": {}}}, {"token_start": 83, "token_end": 106, "char_start": 412, "char_end": 525, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Somasundaran et al. (2009)": "3244636"}, "Reference": {}}}, {"token_start": 107, "token_end": 142, "char_start": 532, "char_end": 705, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"McDonald et al. (2007)": "1347118"}, "Reference": {}}}, {"token_start": 151, "token_end": 167, "char_start": 757, "char_end": 833, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"McDonald et al. (2007)": "1347118"}, "Reference": {}}}]}
{"id": "6949557_1", "paragraph": "[BOS] Several applications in information extraction and sentiment analysis are close in spirit to our work (Pang and Lee, 2004; Patwardhan and Riloff, 2007; McDonald et al., 2007) .\n[BOS] These approaches consider global contextual information when determining whether a given sentence is relevant to the underlying analysis task.\n[BOS] All assume that relevant sentences have been annotated.\n[BOS] For instance, Pang and Lee (2004) refine the accuracy of sentiment analysis by considering only the subjective sentences of a review as determined by an independent classifier.\n[BOS] Patwardhan and Riloff (2007) take a similar approach in the context of information extraction.\n[BOS] Rather than applying their extractor to all the sentences in a document, they limit it to eventrelevant sentences.\n[BOS] Since these sentences are more likely to contain information of interest, the extraction performance increases.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 5, "token_end": 42, "char_start": 30, "char_end": 180, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Pang and Lee, 2004;": "388", "Patwardhan and Riloff, 2007;": "5749336", "McDonald et al., 2007)": "1347118"}}}, {"token_start": 78, "token_end": 107, "char_start": 414, "char_end": 576, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Pang and Lee (2004)": "388"}, "Reference": {}}}, {"token_start": 108, "token_end": 170, "char_start": 583, "char_end": 916, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Patwardhan and Riloff (2007)": "5749336"}, "Reference": {}}}]}
{"id": "6949557_0", "paragraph": "[BOS] Prior research has demonstrated the usefulness of content models for discourse-level tasks.\n[BOS] Examples of such tasks include sentence ordering (Barzilay and Lee, 2004; Elsner et al., 2007) , extraction-based summarization (Haghighi and Vanderwende, 2009 ) and text segmentation .\n[BOS] Since these tasks are inherently tied to document structure, a content model is essential to performing them successfully.\n[BOS] In contrast, the applications considered in this paper are typically developed without any discourse information, focusing on capturing sentencelevel relations.\n[BOS] Our goal is to augment these models with document-level content information.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 23, "token_end": 43, "char_start": 135, "char_end": 198, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Barzilay and Lee, 2004;": "2717698", "Elsner et al., 2007)": "1906241"}}}, {"token_start": 44, "token_end": 59, "char_start": 201, "char_end": 263, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Haghighi and Vanderwende, 2009": "678258"}}}]}
{"id": "6946103_2", "paragraph": "[BOS] Low-rank Tensor Models for NLP aim to handle the conjunction among different views of features (Cao and Khudanpur, 2014; Chen and Manning, 2014) .\n[BOS] proposed a model to compose phrase embeddings from words, which has an equivalent form of our CPbased method under certain restrictions.\n[BOS] Our work applies a similar idea to exploiting the inner structure of complex features, and can handle n-gram features with different ns.\n[BOS] Our factorization ( 3) is general and easy to adapt to new tasks.\n[BOS] More importantly, it makes the model benefit from pre-trained word embeddings as shown by the PP-attachment results.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 16, "token_end": 37, "char_start": 73, "char_end": 150, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cao and Khudanpur, 2014;": "10707600", "Chen and Manning, 2014)": "247735"}}}]}
{"id": "6946103_1", "paragraph": "[BOS] Composition Models (Deep Learning) build representations for structures based on their component word embeddings (Collobert et al., 2011; Bordes et al., 2012; Socher et al., 2012; Socher et al., 2013b) .\n[BOS] When using only word embeddings, these models achieved successes on several NLP tasks, but sometimes fail to learn useful syntactic or semantic patterns beyond the strength of combinations of word embeddings, such as the dependency relation in Figure 1(a) .\n[BOS] To tackle this problem, some work designed their model structures according to a specific kind of linguistic patterns, e.g. dependency paths (Ma et al., 2015; Liu et al., 2015) , while a recent trend enhances compositional models with linguistic features.\n[BOS] For example, Belinkov et al. (2014) concatenate embeddings with linguistic features before feeding them to a neural network; Socher et al. (2013a) and Hermann and Blunsom (2013) enhanced Recursive Neural Networks by refining the transformation matrices with linguistic features (e.g. phrase types).\n[BOS] These models are similar to ours in the sense of learning representations based on linguistic features and embeddings.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Narrative_cite", "Multi_summ", "Transition"], "span_citation_mapping": [{"token_start": 16, "token_end": 52, "char_start": 103, "char_end": 207, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Collobert et al., 2011;": "247735", "Socher et al., 2012;": "8931245", "Socher et al., 2013b)": "990233"}}}, {"token_start": 127, "token_end": 144, "char_start": 604, "char_end": 656, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ma et al., 2015;": "1727568", "Liu et al., 2015)": "7475429"}}}, {"token_start": 160, "token_end": 182, "char_start": 755, "char_end": 865, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Belinkov et al. (2014)": "2621465"}, "Reference": {}}}, {"token_start": 183, "token_end": 223, "char_start": 867, "char_end": 1040, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Socher et al. (2013a)": "990233"}, "Reference": {}}}]}
{"id": "6946103_0", "paragraph": "[BOS] Dimensionality Reduction for Complex Features is a standard technique to address high-dimensional features, including PCA, alternating structural optimization (Ando and Zhang, 2005) , denoising autoencoders (Vincent et al., 2008) , and feature embeddings (Yang and Eisenstein, 2015) .\n[BOS] These methods treat features as atomic elements and ignore the inner structure of features, so they learn separate embedding for each feature without shared parameters.\n[BOS] As a result, they still suffer from large parameter spaces when the feature space is very huge.\n[BOS] 5 Another line of research studies the inner structures of lexical features: e.g. Koo et al. (2008) , Turian et al. (2010) , Sun et al. (2011) , Nguyen and Grishman (2014) , Roth and Woodsend (2014), and Hermann et al. (2014) used pre-trained word embeddings to replace the lexical parts of features ; Srikumar and Manning (2014), and propose splitting lexical features into different parts and employing tensors to perform classification.\n[BOS] The above can therefore be seen as special cases of our model that only embed a certain part (view) of the complex features.\n[BOS] This restriction also makes their model parameters form a full rank tensor, resulting in data sparsity and high computational costs when the tensors are large.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Narrative_cite", "Reflection", "Transition"], "span_citation_mapping": [{"token_start": 21, "token_end": 32, "char_start": 129, "char_end": 187, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ando and Zhang, 2005)": "2621465"}}}, {"token_start": 33, "token_end": 45, "char_start": 190, "char_end": 235, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vincent et al., 2008)": "629094"}}}, {"token_start": 47, "token_end": 57, "char_start": 242, "char_end": 288, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yang and Eisenstein, 2015)": "399358"}}}, {"token_start": 117, "token_end": 176, "char_start": 633, "char_end": 799, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Koo et al. (2008)": null, "Turian et al. (2010)": "629094", "Nguyen and Grishman (2014)": "18895336", "Roth and": "8931245", "Woodsend (2014), and": "8931245", "Hermann et al. (2014)": "247735"}}}, {"token_start": 177, "token_end": 199, "char_start": 805, "char_end": 903, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "6886633_1", "paragraph": "[BOS] The core of our systems is based on Li et al. 's (2004) Joint Source-Channel Model under the direct orthographic mapping framework, which skips the middle phonemic representation in conventional phoneme-based methods and models the segmentation and alignment preferences by means of contextual n-grams of the transliteration segment pairs (or token pairs in their terminology).\n[BOS] A bigram model under their framework is thus as follows: where E refers to the English source name and C refers to the transliterated Chinese name.\n[BOS] With K segments aligned between E and C, e k and c k refer to the kth English segment and its corresponding Chinese segment respectively.\n\n", "discourse_tags": ["Single_summ", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 10, "token_end": 78, "char_start": 42, "char_end": 383, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "6886633_0", "paragraph": "[BOS] There are basically two categories of work on machine transliteration.\n[BOS] First, various alignment models are used for acquiring transliteration lexicons from parallel corpora and other resources (e.g. Kuo and Li, 2008) .\n[BOS] Second, statistical models are built for transliteration.\n[BOS] These models could be phoneme-based (e.g. Knight and Graehl, 1998) , grapheme-based (e.g. Li et al., 2004) , hybrid (Oh and Choi, 2005) , or based on phonetic (e.g. Tao et al., 2006) and semantic (e.g. Li et al., 2007) features.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 25, "token_end": 48, "char_start": 138, "char_end": 228, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Kuo and Li, 2008)": "10833675"}}}, {"token_start": 66, "token_end": 84, "char_start": 323, "char_end": 367, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 85, "token_end": 101, "char_start": 370, "char_end": 407, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Li et al., 2004)": null}}}, {"token_start": 102, "token_end": 110, "char_start": 410, "char_end": 436, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Oh and Choi, 2005)": "17161141"}}}, {"token_start": 114, "token_end": 127, "char_start": 451, "char_end": 483, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Tao et al., 2006)": "8424232"}}}, {"token_start": 128, "token_end": 141, "char_start": 488, "char_end": 519, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Li et al., 2007)": "7527801"}}}]}
{"id": "6858625_0", "paragraph": "[BOS] Previous studies have made great efforts to incorporate statistics and linguistic heuristics or syntactic information into word alignments (Ittycheriah and Roukos 2005; Taskar et al. 2005; Moore et al. 2006; Cherry and Lin 2006; DeNero and Klein 2007; May and Knight 2007; Fossum et al. 2008; Hermjakob 2009; Liu et al. 2010) .\n[BOS] For example, Fossum et al. (2008) used a discriminatively trained model to identify and delete incorrect links from original word alignments to improve stringto-tree transformation rule extraction, which incorporates four types of features such as lexical and syntactic features.\n[BOS] This paper presents an approach to incorporating translation span alignments into word alignments to delete spurious links and add new valuable links.\n[BOS] Some previous work directly models the syntactic correspondence in the training data for syntactic rule extraction (Imamura 2001; Groves et al. 2004; Tinsley et al. 2007; Sun et al. 2010a Sun et al. , 2010b Pauls et al. 2010) .\n[BOS] Some previous methods infer syntactic correspondences between the source and the target languages through word alignments and constituent boundary based syntactic constraints.\n[BOS] Such a syntactic alignment method is sensitive to word alignment behavior.\n[BOS] To combat this, Pauls et al. (2010) presented an unsupervised ITG alignment model that directly aligns syntactic structures for string-to-tree transformation rule extraction.\n[BOS] One major problem with syntactic structure alignment is that syntactic divergence between languages can prevent accurate syntactic alignments between the source and target languages.\n[BOS] May and Knight (2007) presented a syntactic realignment model for syntax-based MT that uses syntactic constraints to re-align a parallel corpus with word alignments.\n[BOS] The motivation behind their methods is similar to ours.\n[BOS] Our work differs from (May and Knight 2007) in two major respects.\n[BOS] First, the approach proposed by May and Knight (2007) first utilizes the EM algorithm to obtain Viterbi derivation trees from derivation forests of each (tree, string) pair, and then produces Viterbi alignments based on obtained derivation trees.\n[BOS] Our forced decoding based approach searches for the best derivation to produce translation span alignments that are used to improve the extraction of translation rules.\n[BOS] Translation span alignments are optimized by the translation model.\n[BOS] Secondly, their models are only applicable for syntax-based systems while our method can be applied to both phrase-based and syntax-based translation tasks.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Reflection", "Narrative_cite", "Transition", "Transition", "Single_summ", "Transition", "Single_summ", "Reflection", "Reflection", "Single_summ", "Reflection", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 18, "token_end": 81, "char_start": 129, "char_end": 331, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ittycheriah and Roukos 2005;": "216691", "Taskar et al. 2005;": "2379886", "Moore et al. 2006;": "10077982", "Cherry and Lin 2006;": "2787289", "DeNero and Klein 2007;": "9882011", "May and Knight 2007;": null, "Fossum et al. 2008;": "2485577", "Hermjakob 2009;": "11853100", "Liu et al. 2010)": "262273"}}}, {"token_start": 86, "token_end": 133, "char_start": 353, "char_end": 619, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Fossum et al. (2008)": "2485577"}, "Reference": {}}}, {"token_start": 171, "token_end": 214, "char_start": 872, "char_end": 1008, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Imamura 2001;": null, "Groves et al. 2004;": "6917253", "Tinsley et al. 2007;": "17451271", "Sun et al. 2010a": "7371932", "Sun et al. , 2010b": "13554713", "Pauls et al. 2010)": "1494188"}}}, {"token_start": 257, "token_end": 288, "char_start": 1296, "char_end": 1454, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Pauls et al. (2010)": "1494188"}, "Reference": {}}}, {"token_start": 315, "token_end": 349, "char_start": 1650, "char_end": 1815, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"May and Knight (2007)": null}, "Reference": {}}}, {"token_start": 363, "token_end": 372, "char_start": 1893, "char_end": 1927, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(May and Knight 2007)": null}}}, {"token_start": 381, "token_end": 428, "char_start": 1968, "char_end": 2203, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"May and Knight (2007)": null}, "Reference": {}}}]}
{"id": "6788641_3", "paragraph": "[BOS] Abstract representations.\n[BOS] With the development of Abstract Meaning Representation (AMR) (Banarescu et al., 2012) , representing semantic information with graphs has been studied in such tasks as summarization (Liu et al., 2015) and event detection (Kai and Grishman, 2015) .\n[BOS] Although several techniques on parsing sentences to AMR (Flanigan et al., 2014; Wang et al., 2015) have been developed, the performance of AMR parsing is very limited at the present.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 10, "token_end": 27, "char_start": 62, "char_end": 124, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Banarescu et al., 2012)": null}}}, {"token_start": 29, "token_end": 50, "char_start": 140, "char_end": 239, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Liu et al., 2015)": "5001921"}}}, {"token_start": 51, "token_end": 63, "char_start": 244, "char_end": 284, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 69, "token_end": 91, "char_start": 324, "char_end": 391, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Flanigan et al., 2014;": "5000956", "Wang et al., 2015)": "15344879"}}}]}
{"id": "6788641_2", "paragraph": "[BOS] Event extraction.\n[BOS] Event extraction is a traditional task in Information Extraction, which aims to recognize event mentions and arguments of predefined types (such as the ACE tasks).\n[BOS] The works on event extraction either divide the task into separate subtasks, such as event-trigger extraction and argument extraction (Liao and Grishman, 2010; Hong et al., 2011) or model it jointly Li and Ji, 2014) .\n[BOS] These works mainly focus on predefined event and argument types.\n[BOS] However, we focus on open-domain and more fine-grained event information extraction for multi-document summarization.\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 53, "token_end": 77, "char_start": 285, "char_end": 378, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Liao and Grishman, 2010;": "11187670", "Hong et al., 2011)": "2867611"}}}, {"token_start": 78, "token_end": 87, "char_start": 382, "char_end": 415, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Li and Ji, 2014)": "20744"}}}]}
{"id": "6788641_1", "paragraph": "[BOS] Recently, some works studied the use of deep learning techniques for abstractive summarization tasks, which use sequence-to-sequence generation techniques on single document or sentence summarization (Rush et al., 2015; Chopra et al., 2016) .\n[BOS] A multi-dimensional summarization methodology was proposed to transform the paradigm of traditional summarization research through multi-disciplinary fundamental exploration on semantics, dimension, knowledge, computing and cyber-physical society (Zhuge, 2016) .\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 22, "token_end": 53, "char_start": 118, "char_end": 246, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rush et al., 2015;": "1918428", "Chopra et al., 2016)": "133195"}}}, {"token_start": 81, "token_end": 99, "char_start": 432, "char_end": 515, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhuge, 2016)": "64460012"}}}]}
{"id": "6788641_0", "paragraph": "[BOS] Abstractive Multi-document summarization.\n[BOS] Previous researches have shown that human write summaries through sentence aggregation and fusion (Cheung and Penn, 2013) .\n[BOS] Abstraction-based approaches that gather information across sentences boundaries have become more and more popular in recent years.\n[BOS] Different abstractive summarization methods can be summarized into four technique routes: (1) sentence fusion based methods (Barzilay and McKeown, 2005; Filippova and Strube, 2008; Banerjee et al., 2015) first cluster sentences into several themes and then generate a new sentence for each cluster by fusing the common information of all sentences in the cluster; (2) information extraction based methods (Genest and Lapalme, 2011; Li, 2015) extract information units, such as Information Items or Basic Semantic Unit, as components for generating sentences; (3) summary revision based methods (Nenkova, 2008; Siddharthan et al., 2011) try to improve quality of summary by noun phrases rewriting and co-reference resolution; (4) pattern-based sentence generation methods (Wang and Cardie, 2013; Pighin et al., 2014; generate new sentences based on a set of sentence generation patterns learned from corpus or designed templates.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Multi_summ"], "span_citation_mapping": [{"token_start": 20, "token_end": 32, "char_start": 120, "char_end": 175, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cheung and Penn, 2013)": "15945816"}}}, {"token_start": 72, "token_end": 132, "char_start": 416, "char_end": 684, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Barzilay and McKeown, 2005;": "16188305", "Filippova and Strube, 2008;": "14909308", "Banerjee et al., 2015)": "15795297"}, "Reference": {}}}, {"token_start": 136, "token_end": 172, "char_start": 690, "char_end": 879, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Genest and Lapalme, 2011;": "4942873", "Li, 2015)": "9319303"}, "Reference": {}}}, {"token_start": 176, "token_end": 212, "char_start": 885, "char_end": 1045, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Nenkova, 2008;": "680807", "Siddharthan et al., 2011)": "47525"}, "Reference": {}}}, {"token_start": 216, "token_end": 256, "char_start": 1051, "char_end": 1250, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Wang and Cardie, 2013;": "1030812", "Pighin et al., 2014;": "9270007"}, "Reference": {}}}]}
{"id": "6787682_2", "paragraph": "[BOS] The local operators lexicalize/generalize are use for greedy decoding.\n[BOS] The idea is related to \"pegging\" algorithm (Brown et al., 1993) and greedy decoding (Germann et al., 2001) .\n[BOS] Such types of local operators are also used in Gibbs sampler for synchronous grammar induction ).\n[BOS] apply our forest on other learning algorithms.\n[BOS] Finally, we hope to exploit more features such as reordering features and syntactic features so as to further improve the performance.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 22, "token_end": 34, "char_start": 107, "char_end": 146, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Brown et al., 1993)": null}}}, {"token_start": 35, "token_end": 46, "char_start": 151, "char_end": 189, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Germann et al., 2001)": "90111"}}}]}
{"id": "6787682_1", "paragraph": "[BOS] For efficiency, we only use neighboring derivations for training.\n[BOS] Such motivation is same as contrastive estimation (Smith and Eisner, 2005; Poon et al., 2009) .\n[BOS] The difference lies in that the previous work actually care about their latent variables (pos tags, segmentation, dependency trees, etc), while we are only interested in their marginal distribution.\n[BOS] Furthermore, we focus on how to fast generate translation forest for training.\n\n", "discourse_tags": ["Reflection", "Narrative_cite", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 19, "token_end": 38, "char_start": 105, "char_end": 171, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Smith and Eisner, 2005;": "259144", "Poon et al., 2009)": "9519654"}}}]}
{"id": "6787682_0", "paragraph": "[BOS] Minimum error rate training (Och, 2003) is perhaps the most popular discriminative training for SMT.\n[BOS] However, it fails to scale to large number of features.\n[BOS] Researchers have propose many learning algorithms to train many features: perceptron (Shen et al., 2004; Liang et al., 2006) , minimum risk (Smith and Eisner, 2006; , MIRA (Watanabe et al., 2007; Chiang et al., 2009) , gradient descent .\n[BOS] The complexity of n-best lists or packed forests generation hamper these algorithms to scale to a large amount of data.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 2, "token_end": 35, "char_start": 6, "char_end": 168, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Och, 2003)": "5474833"}, "Reference": {}}}, {"token_start": 47, "token_end": 64, "char_start": 249, "char_end": 299, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Shen et al., 2004;": "750809", "Liang et al., 2006)": "1391785"}}}, {"token_start": 65, "token_end": 74, "char_start": 302, "char_end": 338, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 76, "token_end": 94, "char_start": 342, "char_end": 391, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Watanabe et al., 2007;": "2665828", "Chiang et al., 2009)": "3544821"}}}]}
{"id": "6695204_1", "paragraph": "[BOS] PCFGLA parsers are built upon generative models with latent annotations.\n[BOS] The use of automatically induced latent variables may also affect Bagging.\n[BOS] Generative sequence models with latent anno-tations can also achieve good performance for Chinese POS tagging.\n[BOS] Huang et al. (2009) described and evaluated a bi-gram HMM tagger that utilizes latent annotations.\n[BOS] Different from negative results of Bagging discriminative models, our auxiliary experiment shows that Bagging Huang et al. 's tagger can help Chinese POS tagging.\n[BOS] In other words, Bagging substantially improves both HMMLA and PCFGLA models, at least for Chinese POS tagging and constituency parsing.\n[BOS] It seems that Bagging favors the use of latent variables.\n[BOS] Figure 2 clearly shows that the Bagging model taking both data-driven and PCFG-based models as basic systems outperform the Bagging model taking either model in isolation as basic systems.\n[BOS] The combination of a PCFG-based model and a data-driven model (either graph-based or transition-based) is more effective than the combination of two datadriven models, which has received the most attention in dependency parser ensemble.\n[BOS] Table 3 is the performance of reparsing on the development data.\n[BOS] From this table, we can see by utilizing more parsers, Bagging can enhance reparsing.\n[BOS] According to Surdeanu and Manning (2010) 's findings, reparsing performs as well as other combination models.\n[BOS] Our auxiliary experiments confirm this finding: Learning-based stacking cannot achieve better performance.\n[BOS] Limited to the document length, we do not give descriptions of these experiments.\n[BOS] (15) 86.37 bagging (reparse(g, t, c)) 86.09 reparse (bagging(g, t, c)) 85.86 Table 3 : UAS of reparsing and Bagging.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Single_summ", "Reflection", "Transition", "Transition", "Other", "Other", "Other", "Other", "Narrative_cite", "Reflection", "Reflection", "Other"], "span_citation_mapping": [{"token_start": 51, "token_end": 73, "char_start": 283, "char_end": 381, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Huang et al. (2009)": "2183720"}, "Reference": {}}}, {"token_start": 274, "token_end": 287, "char_start": 1377, "char_end": 1416, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Surdeanu and Manning (2010)": "1204756"}}}]}
{"id": "6695204_0", "paragraph": "[BOS] Bagging has been applied to enhance discriminative sequence models for Chinese word segmentation (Sun, 2010b) and POS tagging (Sun and Uszkoreit, 2012) .\n[BOS] For word segmentation, experiments on discriminative Markov and semi-Markov tagging models are reported.\n[BOS] Their experiments showed that Bagging can consistently enhance a semi-Markov model but not the Markov one.\n[BOS] Experiments on POS tagging indicated that Bagging Markov models hurts tagging performance.\n[BOS] It seems that the relationships among basic processing units affect Bagging.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 13, "token_end": 22, "char_start": 77, "char_end": 115, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sun, 2010b)": "7571334"}}}, {"token_start": 23, "token_end": 35, "char_start": 120, "char_end": 157, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sun and Uszkoreit, 2012)": "2082658"}}}]}
{"id": "665002_0", "paragraph": "[BOS] Using vectors to represent word meanings is the essence of vector space models (VSM).\n[BOS] The representations capture words' semantic and syntactic information which can be used to measure semantic similarities by computing distance between the vectors.\n[BOS] Although most VSMs represent one word with only one vector, they fail to capture homonymy and polysemy of word.\n[BOS] Huang et al. (2012) introduced global document context and multiple word prototypes which distinguishes and uses both local and global context via a joint training objective.\n[BOS] Much of the research focus on the task of inducing representations for single languages.\n[BOS] Recently, a lot of progress has been made at representation learning for bilingual words.\n[BOS] Bilingual word representations have been presented by Peirsman and Pad (2010) and Sumita (2000) .\n[BOS] Also unsupervised algorithms such as LDA and LSA were used by Boyd-Graber and Resnik (2010) , Tam et al. (2007) and Zhao and Xing (2006) .\n[BOS] Zou et al. (2013) learn bilingual embeddings utilizes word alignments and monolingual embeddings result, Le et al. (2012) and Gao et al. (2014) used continuous vector to represent the source language or target language of each phrase, and then computed translation probability using vector distance.\n[BOS] Vuli and Moens (2013) learned bilingual vector spaces from non-parallel data induced by using a seed lexicon.\n[BOS] However, none of these work considered the word sense disambiguation problem which Carpuat and Wu (2007) proved it is useful for SMT.\n[BOS] In this paper, we learn bilingual semantic embeddings for source content and target phrase, and incorporate it into a phrasebased SMT system to improve translation quality.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Single_summ", "Transition", "Transition", "Narrative_cite", "Narrative_cite", "Multi_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 73, "token_end": 103, "char_start": 386, "char_end": 560, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Huang et al. (2012)": "372093"}, "Reference": {}}}, {"token_start": 137, "token_end": 158, "char_start": 758, "char_end": 853, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Peirsman and Pad\u00f3 (2010)": "8055323", "Sumita (2000)": null}}}, {"token_start": 161, "token_end": 199, "char_start": 867, "char_end": 998, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Boyd-Graber and Resnik (2010)": "2969796", "Tam et al. (2007)": "5665802", "Zhao and Xing (2006)": "14079772"}}}, {"token_start": 201, "token_end": 220, "char_start": 1007, "char_end": 1110, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zou et al. (2013)": "931054"}, "Reference": {}}}, {"token_start": 221, "token_end": 260, "char_start": 1112, "char_end": 1306, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Le et al. (2012)": "14810278", "Gao et al. (2014)": "10473972"}, "Reference": {}}}, {"token_start": 261, "token_end": 285, "char_start": 1313, "char_end": 1422, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Vuli\u0107 and Moens (2013)": "16470894"}, "Reference": {}}}, {"token_start": 300, "token_end": 316, "char_start": 1512, "char_end": 1562, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Carpuat and Wu (2007)": "135295"}, "Reference": {}}}]}
{"id": "6585180_3", "paragraph": "[BOS] 3 Attribute Modeling based on LDA\n\n", "discourse_tags": ["Other"], "span_citation_mapping": []}
{"id": "6585180_2", "paragraph": "[BOS] Hartung and Frank (2011) adopt a similar approach, by embedding LDA into a VSM for adjective-noun meaning composition, with LDA topics providing latent variables for attribute meanings.\n[BOS] That is, contrary to M&L, LDA is used to convey information about interpretable semantic attributes rather than latent topics.\n[BOS] In fact, Hartung and Frank (2011) are able to show that \"injecting\" topic distributions inferred from LDA into a VSM alleviates sparsity problems that persisted with the pattern-based VSM of Hartung and Frank (2010) .\n[BOS] highlight two strengths of VSMs that incorporate interpretable dimensions of meaning: cognitive plausibility and effectiveness in concept categorization tasks.\n[BOS] In their model, concepts are characterized in terms of salient properties and relations (e.g., children have parents, grass is green).\n[BOS] However, their approach concentrates on nouns.\n[BOS] Open questions are (i) whether it can be extended to further word classes, and (ii) whether the interpreted meaning layers are interoperable across word classes, to cope with compositionality.\n[BOS] The present paper extends their work by offering a test case for an interpretable, compositional VSM, applied to adjective-noun composition with attributes as a shared meaning layer.\n[BOS] Moreover, to our knowledge, we are the first to expose such a model to a pairwise similarity judgement task.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Transition", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 65, "char_start": 6, "char_end": 324, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 69, "token_end": 179, "char_start": 340, "char_end": 908, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hartung and Frank (2011)": "3207875"}, "Reference": {"Hartung and Frank (2010)": "18843444"}}}]}
{"id": "6585180_1", "paragraph": "[BOS] Finally, Latent Dirichlet Allocation, originally designed for tasks such as text classification and document modeling (Blei et al., 2003) , found its way into lexical semantics.\n[BOS] Ritter et al. (2010) and O Saghdha (2010) , e.g., model selectional restrictions of verb arguments by inducing topic distributions that characterize mixtures of topics observed in verb argument positions.\n[BOS] Lapata (2009, 2010) were the first to use LDA-inferred topics as dimensions in VSMs.\n\n", "discourse_tags": ["Narrative_cite", "Multi_summ", "Multi_summ"], "span_citation_mapping": [{"token_start": 14, "token_end": 28, "char_start": 82, "char_end": 143, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 37, "token_end": 81, "char_start": 190, "char_end": 394, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ritter et al. (2010)": "14061182"}, "Reference": {}}}, {"token_start": 82, "token_end": 104, "char_start": 401, "char_end": 485, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "6585180_0", "paragraph": "[BOS] Recent work in distributional semantics has engendered different perspectives on how to characterize the semantics of adjectives and adjective-noun phrases.\n[BOS] Almuhareb (2006) aims at capturing the semantics of adjectives in terms of attributes they denote using lexico-syntactic patterns.\n[BOS] His approach suffers from severe sparsity problems and does not account for the compositional nature of adjective-noun phrases, as it disregards the meaning contributed by the noun.\n[BOS] It is therefore unable to perform disambiguation of adjectives in the context of a noun.\n[BOS] Baroni and Zamparelli (2010) and Guevara (2010) focus on how best to represent compositionality in adjective-noun phrases considering different types of composition operators.\n[BOS] These works adhere to a fully latent representation of meaning, whereas Hartung and Frank (2010) assign symbolic attribute meanings to adjectives, nouns and composed phrases by incorporating attributes as dimensions in a compositional VSM.\n[BOS] By holding the attribute meaning of adjectives and nouns in distinct vector representations and combining them through vector composition, their approach improves on both weaknesses of Almuhareb's work.\n[BOS] However, their account is still closely tied to Almuhareb's pattern-based approach in that counts of co-occurrence patterns linking adjectives and nouns to attributes are used to populate the vector representations.\n[BOS] These, however, are inherently sparse.\n[BOS] The resulting model therefore still suffers from sparsity of co-occurrence data.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Multi_summ", "Single_summ", "Single_summ", "Single_summ", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 30, "token_end": 111, "char_start": 169, "char_end": 582, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 112, "token_end": 162, "char_start": 589, "char_end": 834, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Baroni and Zamparelli (2010)": "8360910", "Guevara (2010)": "17414711"}, "Reference": {}}}, {"token_start": 163, "token_end": 274, "char_start": 843, "char_end": 1441, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hartung and Frank (2010)": "18843444"}, "Reference": {}}}]}
{"id": "6519154_1", "paragraph": "[BOS] Akbik et al. (2015) introduce an iterative selftraining approach using different types of linguistic heuristics and alignment filters to improve the quality of projected roles.\n[BOS] Unlike our work that does not use any external resources, Akbik et al. (2015) make use of bilingual dictionaries.\n[BOS] Our work also leverages self-training but with a different approach: first of all, ours does not apply any heuristics to filter out projections.\n[BOS] Second, it trains and relabels all projected instances, either labeled or unlabeled, at every epoch and does not gradually introduce new unlabeled data.\n[BOS] Instead, we find it more useful to let the target language SRL system rule out noisy projections via relabeling.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 34, "char_start": 6, "char_end": 182, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 46, "token_end": 62, "char_start": 247, "char_end": 302, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "6519154_0", "paragraph": "[BOS] There have been several studies on transferring SRL systems Lapata, 2005, 2009; Mukund et al., 2010; van der Plas et al., 2011 van der Plas et al., , 2014 Kozhevnikov and Titov, 2013; Akbik et al., 2015) .\n[BOS] Pad and Lapata (2005) , as one of the earliest studies on annotation projection for SRL using parallel resources, apply different heuristics and techniques to improve the quality of their model by focusing on having better word and constituent alignments.\n[BOS] van der Plas et al. (2011) improve an annotation projection model by jointly training a transfer system for parsing and SRL.\n[BOS] They solely focus on fully projected annotations and train only on verbs.\n[BOS] In this work, we train on all predicates as well as exploit partial annotation.\n[BOS] Kozhevnikov and Titov (2013) define shared feature representations between the source and target languages in annotation projection.\n[BOS] The benefit of using shared representations is complementary to our work encouraging us to use it in future work.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ", "Single_summ", "Reflection", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 8, "token_end": 64, "char_start": 41, "char_end": 209, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Mukund et al., 2010;": null, "van der Plas et al., 2011": "18915491", "van der Plas et al., , 2014": "18915491"}}}, {"token_start": 66, "token_end": 112, "char_start": 218, "char_end": 473, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Pad\u00f3 and Lapata (2005)": "12129708"}, "Reference": {}}}, {"token_start": 113, "token_end": 153, "char_start": 480, "char_end": 684, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 171, "token_end": 196, "char_start": 777, "char_end": 909, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "6431893_1", "paragraph": "[BOS] In general, up to 85% accuracy was reached with the top 15 candidates for classification at the Cilin subclass level.\n[BOS] This performance, however, should be improved for the method to be useful in practice.\n[BOS] It is observed that Cilin, as most other thesauri, does not have a mutually exclusive classification.\n[BOS] Many words appear in more than one category (at various levels).\n[BOS] Such duplication may affect the similarity comparison between a target word and words in a category.\n[BOS] The current study thus attempts to avoid this confounding factor by removing duplicated words from Cilin for the comparison of similarity, and to extend the classification to a finer level.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Transition", "Transition", "Reflection"], "span_citation_mapping": []}
{"id": "6431893_0", "paragraph": "[BOS] To build a semantic lexicon, one has to identify the relation between words within a semantic hierarchy, and to group similar words together into a class.\n[BOS] Previous work on automatic methods for building semantic lexicons could be divided into two main groups.\n[BOS] One is automatic thesaurus acquisition, that is, to identify synonyms or topically related words from corpora based on various measures of similarity (e.g. Riloff and Shepherd, 1997; Lin, 1998; Caraballo, 1999; Thelen and Riloff, 2002; You and Chen, 2006) .\n[BOS] Another line of research, which is more closely related to the current study, is to extend existing thesauri by classifying new words with respect to their given structures (e.g. Tokunaga et al., 1997; Pekar, 2004 ).\n[BOS] An early effort along this line is Hearst (1992) , who attempted to identify hyponyms from large text corpora, based on a set of lexico-syntactic patterns, to augment and critique the content of WordNet.\n[BOS] Ciaramita (2002) compared several models in classifying nouns with respect to a simplified version of WordNet and signified the gain in performance with morphological features.\n[BOS] For Chinese, Tseng (2003) proposed a method based on morphological similarity to assign a Cilin category to unknown words from the Sinica corpus which were not in the Chinese Electronic Dictionary and Cilin; but somehow the test data were taken from Cilin, and therefore could not really demonstrate the effectiveness with unknown words found in the Sinica corpus.\n[BOS] Kwong and Tsou (2007) attempted to classify words distinctly used in Beijing, Hong Kong, Singapore, and Taiwan, with respect to the Cilin classificatory structure.\n[BOS] They brought up the issue of data heterogeneity in the task.\n[BOS] In general, automatic classification of words via similarity measurement between two words, or between a word and a class of words, was often done on words from a similar data source, with the assumption that the feature vectors under comparison are directly comparable.\n[BOS] In the Pan-Chinese context, however, the words to be classified come from corpora collected from various Chinese speech communities, but the words in the thesaurus are often based on usages found in a particular community, such as Mainland China in the case of Cilin.\n[BOS] It is thus questionable whether the words in Cilin would appear in comparable contexts in texts from other places, thus affecting the similarity measurement.\n[BOS] In view of this heterogeneous nature of the data, they experimented with extracting feature vectors for the Cilin words from different datasets and found that the classification of words from Taipei was most affected in this regard.\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite", "Narrative_cite", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Transition", "Transition", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 72, "token_end": 120, "char_start": 380, "char_end": 533, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Riloff and Shepherd, 1997;": "1437", "Lin, 1998;": "15698938", "Caraballo, 1999;": "1767510", "Thelen and Riloff, 2002;": "137155", "You and Chen, 2006)": "188487"}}}, {"token_start": 141, "token_end": 174, "char_start": 642, "char_end": 758, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Tokunaga et al., 1997;": "14116523", "Pekar, 2004": "9364850"}}}, {"token_start": 182, "token_end": 221, "char_start": 800, "char_end": 968, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hearst (1992)": "15763200"}, "Reference": {}}}, {"token_start": 222, "token_end": 254, "char_start": 975, "char_end": 1151, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ciaramita (2002)": "11734261"}, "Reference": {}}}, {"token_start": 255, "token_end": 325, "char_start": 1158, "char_end": 1522, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Tseng (2003)": "701705"}, "Reference": {}}}, {"token_start": 326, "token_end": 375, "char_start": 1529, "char_end": 1759, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kwong and Tsou (2007)": "17100954"}, "Reference": {}}}]}
{"id": "6380915_1", "paragraph": "[BOS] [Japanese] + Beam search X is a restaurant and japanese food and chinese takeaway.\n[BOS] + Reranker X is a restaurant serving japanese food in the centre of the city that offers chinese takeaway.\n[BOS] Greedy into strings X is a restaurant offering italian and indian takeaway in the city centre area near X.\n[BOS] [ Mei et al. (2015) present the only seq2seq-based NLG system known to us.\n[BOS] We extend the previous works by generating deep syntax trees as well as strings and directly comparing pipeline and joint generation.\n[BOS] In addition, we experiment with an order-of-magnitude smaller dataset than other RNN-based systems.\n\n", "discourse_tags": ["Other", "Other", "Other", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 70, "token_end": 94, "char_start": 321, "char_end": 395, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Mei et al. (2015)": "1354459"}, "Reference": {}}}]}
{"id": "6380915_0", "paragraph": "[BOS] While most recent NLG systems attempt to learn generation from data, the choice of a particular approach -pipeline or joint -is often arbitrary and depends on system architecture or particular generation domain.\n[BOS] Works using the pipeline approach in SDS tend to focus on sentence planning, improving a handcrafted generator (Walker et al., 2001; Stent et al., 2004; Paiva and Evans, 2005) or using perceptron-guided A* search (Duek and Jurek, 2015) .\n[BOS] Generators taking the joint approach employ various methods, e.g., factored language models , inverted parsing (Wong and Mooney, 2007; Konstas and Lapata, 2013) , or a pipeline of discriminative classifiers (Angeli et al., 2010) .\n[BOS] Unlike most previous Input DA inform(name=X-name, type=placetoeat, eattype=restaurant, area=citycentre, near=X-near, food=\"Chinese takeaway\", food=Japanese) Reference X is a Chinese takeaway and Japanese restaurant in the city centre near X. Greedy with trees X is a restaurant offering chinese takeaway in the centre of town near X.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite", "Other"], "span_citation_mapping": [{"token_start": 56, "token_end": 82, "char_start": 313, "char_end": 399, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Walker et al., 2001;": "7608649", "Stent et al., 2004;": "1543141", "Paiva and Evans, 2005)": "373507"}}}, {"token_start": 84, "token_end": 100, "char_start": 409, "char_end": 459, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 116, "token_end": 140, "char_start": 535, "char_end": 628, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wong and Mooney, 2007;": "799077"}}}, {"token_start": 143, "token_end": 156, "char_start": 636, "char_end": 696, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "51999008_0", "paragraph": "[BOS] Two major areas related to our work are argumentation quality evaluation and attention mechanism.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "51993096_1", "paragraph": "[BOS] There are also libraries for general NLP applications [1, 35, 10] or for high conceptuallevel programming without specific task focuses [9] .\n[BOS] With the focus on text generation, we provide a more comprehensive set of well-tailored and readily-usable modules and functionalities to relevant tasks.\n[BOS] Some platforms exist for specific types of algorithms, such as OpenAI Gym [7] , DeepMind Control Suite [43] , and ELF [44] for reinforcement learning in game environments.\n[BOS] Texar has drawn inspirations from these toolkits when designing relevant specific algorithm supports.\n\n", "discourse_tags": ["Narrative_cite", "Reflection", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 7, "token_end": 18, "char_start": 35, "char_end": 71, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 20, "token_end": 31, "char_start": 79, "char_end": 145, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "51993096_0", "paragraph": "[BOS] Text generation is a broad research area with rapid advancement.\n[BOS] Figure 2 summarizes some popular and emerging models used in the diverse contexts of the field.\n[BOS] There are some existing toolkits that focus on tasks of neural machine translation and alike, such as Google Seq2seq [6] and Tensor2Tensor [46] on TensorFlow, OpenNMT [24] on (Py)Torch, XNMT [34] on DyNet, Nematus [38] on Theano, and MarianNMT [21] on C++.\n[BOS] Par-lAI [33] is a software platform specialized for dialog research.\n[BOS] Differing from these taskfocusing toolkits, Texar aims to cover as many text generation tasks as possible.\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 51, "token_end": 58, "char_start": 281, "char_end": 299, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"[6]": "2201909"}}}, {"token_start": 59, "token_end": 66, "char_start": 304, "char_end": 322, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"[46]": "3988816"}}}, {"token_start": 70, "token_end": 75, "char_start": 338, "char_end": 350, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"[24]": "16538528"}}}, {"token_start": 82, "token_end": 87, "char_start": 365, "char_end": 374, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"[34]": "3628568"}}}, {"token_start": 91, "token_end": 96, "char_start": 385, "char_end": 397, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"[38]": "905565"}}}, {"token_start": 101, "token_end": 107, "char_start": 413, "char_end": 427, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"[21]": null}}}, {"token_start": 113, "token_end": 128, "char_start": 442, "char_end": 510, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"[33]": "3677429"}, "Reference": {}}}]}
{"id": "51881520_0", "paragraph": "[BOS] Our work is related to two research areas: math word problem solving and semantic parsing.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "44090743_0", "paragraph": "[BOS] Unlike the other corpora where documents and passages are written in a similar writing style, they are multiparty dialogs and plot summaries in our corpus, which have very different writing styles.\n[BOS] This raises another level of difficulty to match contexts between documents and queries for the task of passage completion.\n\n", "discourse_tags": ["Transition", "Transition"], "span_citation_mapping": []}
{"id": "38223308_0", "paragraph": "[BOS] In this section, we describe related work in transition based neural network parsers in terms of their design decisions regarding common components.\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "30181081_0", "paragraph": "[BOS] The proposed work is grounded in the following areas: review-helpfulness analysis, review summarization and supervised topic modeling.\n[BOS] In this section, we will discuss existing work in the literature and explain how the proposed work relates to them.\n\n", "discourse_tags": ["Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "27131087_0", "paragraph": "[BOS] Recent works that utilize subword information to construct word representation could be largely divided into two families: The models that use morphemes as a component and the others taking advantage of characters.\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "256189_0", "paragraph": "[BOS] Our work is inspired by two lines of research: (1) exploiting monolingual corpora for machine translation and (2) autoencoders in unsupervised and semi-supervised learning.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "22582644_1", "paragraph": "[BOS] Similarly to the approach of Kiros et al. [1] , most image annotation and image retrieval approaches rely on the use of CNN features for image representation.\n[BOS] The current best overall performing model (considering both image annotation and image retrieval tasks) is the Fisher Vector (FV) [4] , although its performance is most competitive on the image retrieval task.\n[BOS] FV are computed with respect to the parameters of a Gaussian Mixture Model (GMM) and an Hybrid Gaussian-Laplacian Mixture Model (HGLMM).\n[BOS] For both images and text, FV are build using deep neural network features; a VGG [16] CNN for images features, and a word2vec [17] for text features.\n[BOS] For the specific problem of image annotation, the current state-of-art is obtained with the Word2VisualVec (W2VV) model [18] .\n[BOS] This approach uses as a multimodal embedding space the same visual space where images are represented, involving a deeper text processing.\n[BOS] Finally for the largest dataset we consider (MSCOCO), the best results in certain metrics are obtained by MatchCNN (m-CNN) [5] , which is based on the use of CNNs to encode both image and text.\n[BOS] the FNE, which results in the architecture shown in Figure 1 .\n[BOS] Next we describe these components in further detail.\n\n", "discourse_tags": ["Single_summ", "Narrative_cite", "Transition", "Transition", "Narrative_cite", "Transition", "Narrative_cite", "Other", "Reflection"], "span_citation_mapping": [{"token_start": 7, "token_end": 34, "char_start": 35, "char_end": 164, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"[1]": "7732372"}, "Reference": {}}}, {"token_start": 53, "token_end": 61, "char_start": 282, "char_end": 304, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"[4]": "6180274"}}}, {"token_start": 164, "token_end": 177, "char_start": 778, "char_end": 810, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"[18]": "16007710"}}}, {"token_start": 210, "token_end": 235, "char_start": 1008, "char_end": 1090, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"[5]": "6546076"}}}]}
{"id": "22582644_0", "paragraph": "[BOS] In the last few years, several solutions have been proposed to the problem of building common representations for images and text with the goal of enabling cross-domain search [1, 2, 3, 4, 5] .\n[BOS] This paper builds upon the methodology described by Kiros et al. [1] , which is in turn based on previous works in the area of Neural Machine Translation [14] .\n[BOS] In their work, Kiros et al. [1] define a vectorized representation of an input text by using GRU RNNs.\n[BOS] In this setting, each word in the text is codified into a vector using a word dictionary, vectors which are then fed one by one into the GRUs.\n[BOS] Once the last word vector has been processed, the activations of the GRUs at the last time step conveys the representation of the whole input text in the multimodal embedding space.\n[BOS] In parallel, images are processed through a Convolutional Neural Network (CNN) pre-trained on ImageNet [15] , extracting the activations of the last fully connected layer to be used as a representation of the images.\n[BOS] To solve the dimensionality matching between both representations (the output of the GRUs and the last fully-connected of the CNN) an affine transformation is applied on the image representation.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 29, "token_end": 44, "char_start": 162, "char_end": 197, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"[1,": "7732372", "2,": "12365096", "3,": "2315434", "4,": "6180274", "5]": "6546076"}}}, {"token_start": 54, "token_end": 81, "char_start": 258, "char_end": 364, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"[1]": "7732372"}, "Reference": {"[14]": "7961699"}}}, {"token_start": 83, "token_end": 261, "char_start": 373, "char_end": 1237, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"[1]": "7732372"}, "Reference": {"[15]": "2930547"}}}]}
{"id": "18875369_0", "paragraph": "[BOS] In this section, we give an overview of previous work on both scope detection and utilization of CNNs in NLP applications.\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "16307400_0", "paragraph": "[BOS] In this section, we describe the basis of this work: the attention-based encoder-decoder NMT model and the row convolution method.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "15132118_1", "paragraph": "[BOS] (1  r 12 ) 3 , where r ij is the Pearson correlation between X i and X j , n is the size of the population, and: The Williams test is more powerful than the equivalent for independent samples (Fisher r to z), as it takes the correlations between X 1 and X 2 (metric scores) into account.\n[BOS] All else being equal, the higher the correlation between the metric scores, the greater the statistical power of the test.\n\n", "discourse_tags": ["Other", "Other"], "span_citation_mapping": []}
{"id": "15132118_0", "paragraph": "[BOS] Correlations computed for two separate automatic metrics on the same data set are not independent, and for this reason in order to test the difference in correlation between them, the degree to which the pair of metrics correlate with each other should be taken into account.\n[BOS] 1959) 1 evaluates significance in a difference in dependent correlations (Steiger, 1980) .\n[BOS] It is formulated as follows, as a test of whether the population correlation between X 1 and X 3 equals the population correlation between X 2 and X 3 :\n\n", "discourse_tags": ["Transition", "Single_summ", "Other"], "span_citation_mapping": [{"token_start": 57, "token_end": 70, "char_start": 306, "char_end": 376, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Steiger, 1980)": "123434510"}}}]}
{"id": "14795778_0", "paragraph": "[BOS] Most of the previous studies on negation and speculation scope detection task can be divided into two main aspects: the heuristic rule based methods and the machine learning based methods.\n[BOS] We respectively introduce the aspects in below.\n\n", "discourse_tags": ["Transition", "Reflection"], "span_citation_mapping": []}
{"id": "202627240_0", "paragraph": "[BOS] Memory networks provide a general architecture for online updates to a set of distinct memories Sukhbaatar et al., 2015) .\n[BOS] The link between memory networks and incremental text processing was emphasized by Cheng et al. (2016) .\n[BOS] Henaff et al. (2017) used memories to track the states of multiple entities in a text, but they predefined the alignment of entities to memories, rather than learning to align entities with memories using gates.\n[BOS] The incorporation of entities into language models has also been explored in prior work (Yang et al., 2017; Kobayashi et al., 2017) ; similarly, Dhingra et al. (2018) augment the gated recurrent unit (GRU) architecture with additional edges between coreferent mentions.\n[BOS] In general, this line of prior work assumes that coreference information is available at test time (e.g., from a coreference resolution system), rather than determining coreference in an online fashion.\n[BOS] Ji et al. (2017) propose a generative entity-aware language model that incorporates coreference as a discrete latent variable.\n[BOS] For this reason, importance sampling is required for inference, and the model cannot be trained on unlabeled data.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Single_summ", "Single_summ", "Transition", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 2, "token_end": 27, "char_start": 6, "char_end": 126, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Sukhbaatar et al., 2015)": "1399322"}}}, {"token_start": 29, "token_end": 48, "char_start": 135, "char_end": 237, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Cheng et al. (2016)": "6506243"}}}, {"token_start": 50, "token_end": 92, "char_start": 246, "char_end": 457, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Henaff et al. (2017)": "11243593"}, "Reference": {}}}, {"token_start": 94, "token_end": 122, "char_start": 468, "char_end": 595, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yang et al., 2017;": "1899153", "Kobayashi et al., 2017)": "855563"}}}, {"token_start": 125, "token_end": 153, "char_start": 609, "char_end": 733, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Dhingra et al. (2018)": "4957155"}, "Reference": {}}}, {"token_start": 197, "token_end": 222, "char_start": 949, "char_end": 1075, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ji et al. (2017)": "5564363"}, "Reference": {}}}]}
{"id": "195064006_0", "paragraph": "[BOS] The end-to-end trainable, non-task-oriented conversational dialog systems built by Vinyals and Le (2015; Shang et al. (2015; Serban et al. (2015) using sequence to sequence learning (Sutskever et al., 2014) are promising chatbot systems but do not support domain specific tasks and do not interact with knowledge bases such as databases (Sukhbaatar et al., 2015; Yin et al., 2015) , and therefore cannot provide useful information through their responses.\n[BOS] Wen et al. (2016) augment the sequence to sequence architecture with dialog history modelled by a set of belief trackers, and a distributed representation of user intent with delexicalisation and weight tying strategies.\n[BOS] Their system provides relevant and appropriate responses at each turn and also interacts with a database through a slot-value pair representation of attributes.\n[BOS] They achieve a high task success rate and show that the learned model can interact efficiently and naturally with human subjects to complete an application specific task.\n[BOS] Dodge et al. (2016) use Memory Networks (Weston et al., 2015a; Sukhbaatar et al., 2015) to train non goal oriented dialog, which showed promising results.\n[BOS] Bordes and Weston (2017) train memory networks to perform tasks non-trivial tasks such as issuing API calls to knowledge bases and manipulating entities unseen in training; the bot is also able to ask questions to fill missing information.\n[BOS] They show that memory networks can outperform a dedicated slot-filling rule-based baseline, and even classical IR and supervised embeddings; they solve the task of issuing API calls.\n\n", "discourse_tags": ["Multi_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 11, "token_end": 46, "char_start": 32, "char_end": 151, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Vinyals and Le (2015;": "12300158", "Shang et al. (2015;": "7356547", "Serban et al. (2015)": "17777862"}}}, {"token_start": 47, "token_end": 61, "char_start": 158, "char_end": 212, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sutskever et al., 2014)": "7961699"}}}, {"token_start": 78, "token_end": 101, "char_start": 309, "char_end": 386, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sukhbaatar et al., 2015;": null, "Yin et al., 2015)": "6715526"}}}, {"token_start": 113, "token_end": 212, "char_start": 468, "char_end": 1032, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wen et al. (2016)": null}, "Reference": {}}}, {"token_start": 213, "token_end": 256, "char_start": 1039, "char_end": 1193, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Dodge et al. (2016)": "2239496"}, "Reference": {"(Weston et al., 2015a;": null, "Sukhbaatar et al., 2015)": null}}}, {"token_start": 257, "token_end": 341, "char_start": 1200, "char_end": 1628, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Bordes and Weston (2017)": "2129889"}, "Reference": {}}}]}
{"id": "61629_2", "paragraph": "[BOS] There is a large body of work that tried to improve word embeddings using external resources.\n[BOS] Yu and Dredze (2014) extended the CBOW model (Mikolov et al., 2013) by adding an extra term in the training objective for generating words conditioned on similar words according to a lexicon.\n[BOS] extended the skipgram model (Mikolov et al., 2013) by representing word senses as latent variables in the generation process, and used a structured prior based on the ontology.\n[BOS] Faruqui et al. (2015) used belief propagation to update pre-trained word embeddings on a graph that encodes lexical relationships in the ontology.\n[BOS] Similarly, Johansson and Pina (2015) improved word embeddings by representing each sense of the word in a way that reflects the topology of the semantic network they belong to, and then representing the words as convex combinations of their senses.\n[BOS] In contrast to previous work that was aimed at improving type level word representations, we propose an approach for obtaining context-sensitive embeddings at the token level, while jointly optimizing the model parameters for the NLP task of interest.\n[BOS] Resnik (1993) showed the applicability of semantic classes and selectional preferences to resolving syntactic ambiguity.\n[BOS] Zapirain et al. (2013) applied models of selectional preferences automatically learned from WordNet and distributional information, to the problem of semantic role labeling.\n[BOS] Resnik (1993) ; Brill and Resnik (1994) ; Agirre (2008) and others have used WordNet information towards improving prepositional phrase attachment predictions.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection", "Single_summ", "Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 20, "token_end": 102, "char_start": 106, "char_end": 480, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Yu and Dredze (2014)": "5628616"}, "Reference": {"(Mikolov et al., 2013)": "16447573"}}}, {"token_start": 103, "token_end": 134, "char_start": 487, "char_end": 633, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Faruqui et al. (2015)": "51838647"}, "Reference": {}}}, {"token_start": 137, "token_end": 182, "char_start": 651, "char_end": 888, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Johansson and Pina (2015)": "13916958"}, "Reference": {}}}, {"token_start": 228, "token_end": 248, "char_start": 1153, "char_end": 1273, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Resnik (1993)": "15165852"}, "Reference": {}}}, {"token_start": 249, "token_end": 281, "char_start": 1280, "char_end": 1453, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zapirain et al. (2013)": "2399716"}, "Reference": {}}}, {"token_start": 282, "token_end": 319, "char_start": 1460, "char_end": 1619, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Resnik (1993)": "15165852", "Brill and Resnik (1994)": "4683457", "Agirre (2008)": "9904828"}}}]}
{"id": "61629_1", "paragraph": "[BOS] Related to the idea of concept embeddings is Rothe and Schtze (2015) who estimated WordNet synset representations, given pre-trained typelevel word embeddings.\n[BOS] In contrast, our work focuses on estimating token-level word embeddings as context sensitive distributions of concept em- beddings.\n\n", "discourse_tags": ["Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 10, "token_end": 36, "char_start": 51, "char_end": 165, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Rothe and Sch\u00fctze (2015)": "15687295"}, "Reference": {}}}]}
{"id": "61629_0", "paragraph": "[BOS] This work is related to various lines of research within the NLP community: dealing with synonymy and homonymy in word representations both in the context of distributed embeddings and more traditional vector spaces; hybrid models of distributional and knowledge based semantics; and selectional preferences and their relation with syntactic and semantic relations.\n[BOS] The need for going beyond a single vector per word-type has been well established for a while, and many efforts were focused on building multi-prototype vector space models of meaning (Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2015; Arora et al., 2016, etc.)\n[BOS] .\n[BOS] However, the target of all these approaches is obtaining multisense word vector spaces, either by incorporating sense tagged information or other kinds of external context.\n[BOS] The number of vectors learned is still fixed, based on the preset number of senses.\n[BOS] In contrast, our focus is on learning a context dependent distribution over those concept representations.\n[BOS] Other work not necessarily related to multisense vectors, but still related to our work includes Belanger and Kakade (2015) 's work which proposed a Gaussian linear dynamical system for estimating token-level word embeddings, and Vilnis and McCallum (2015)'s work which proposed mapping each word type to a density instead of a point in a space to account for uncertainty in meaning.\n[BOS] These approaches do not make use of lexical ontologies and are not amenable for joint training with a downstream NLP task.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Transition", "Transition", "Reflection", "Multi_summ", "Transition"], "span_citation_mapping": [{"token_start": 92, "token_end": 134, "char_start": 515, "char_end": 655, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Reisinger and Mooney, 2010;": "2156506", "Huang et al., 2012;": "372093", "Neelakantan et al., 2015;": "15251438"}}}, {"token_start": 225, "token_end": 251, "char_start": 1156, "char_end": 1283, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Belanger and Kakade (2015)": "570528"}, "Reference": {}}}, {"token_start": 253, "token_end": 288, "char_start": 1289, "char_end": 1442, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "604705_0", "paragraph": "[BOS] Our work is based primarily on that of Jauhar et al's RETROFIT algorithm (Jauhar et al., 2015) , which is discussed at greater length in Section 3.\n[BOS] Below we discuss previous models for building sense embeddings.\n[BOS] (Reisinger and Mooney, 2010 ) learn a fixed number of sense vectors per word by clustering context vectors corresponding to individual occurrences of a word in a large corpus, then calculating the cluster centroids.\n[BOS] These centroids are the sense vectors.\n[BOS] (Huang et al., 2012 ) build a similar model using k-means clustering, but also incorporate global textual features into initial context vectors.\n[BOS] They compile the Stanford Contextual Word Similarity dataset (SCWS), which consists of over two thousand word pairs in their sentential context, along with a similarity score based on human judgments from zero to ten.\n[BOS] (Neelakantan et al., 2015) introduce an unsupervised modification of the skipgram model (Mikolov et al., 2013b) (Chen et al., 2014) first learn general word embeddings from the skip-gram model, then initialize sense embeddings based on the synsets and glosses of WN.\n[BOS] These embeddings are then used to identify relevant occurrences of each sense in a training corpus using simple-to-complex wordssense disambiguation (S2C WSD).\n[BOS] The skip-gram model is then trained directly on the disambiguated corpus.\n[BOS] (Rothe and Schtze, 2015) build a neural-network post-processing system called AutoExtend that takes word embeddings and learns embeddings for synsets and lexemes.\n[BOS] Their model is an autoencoder neural net with lexeme and synset embeddings as hidden layers, based on the intuition that a word is the sum of its lexemes and a synset is the sum of its lexemes.\n\n", "discourse_tags": ["Narrative_cite", "Reflection", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 10, "token_end": 30, "char_start": 45, "char_end": 100, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Jauhar et al., 2015)": "14667200"}}}, {"token_start": 53, "token_end": 104, "char_start": 230, "char_end": 490, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Reisinger and Mooney, 2010": "2156506"}, "Reference": {}}}, {"token_start": 105, "token_end": 176, "char_start": 497, "char_end": 865, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Huang et al., 2012": "372093"}, "Reference": {}}}, {"token_start": 177, "token_end": 294, "char_start": 872, "char_end": 1384, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Neelakantan et al., 2015)": "15251438"}, "Reference": {"(Mikolov et al., 2013b)": "16447573", "(Chen et al., 2014)": "2434362"}}}, {"token_start": 295, "token_end": 379, "char_start": 1391, "char_end": 1753, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Rothe and Sch\u00fctze, 2015)": "15687295"}, "Reference": {}}}]}
{"id": "6030778_2", "paragraph": "[BOS] Our work complements and extends the above efforts with a relatively simple EE platform that: (a) hybridizes syntactic dependency patterns with surface patterns, (b) offers support for the extraction of recursive events; (c) is coupled with a fast runtime environment; and (d) is easily customizable to new domains.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "6030778_1", "paragraph": "[BOS] In addition to the above domain-independent frameworks, multiple previous works focused on rule-based systems built around specific domains.\n[BOS] For example, in bioinformatics, several dedicated rule-based systems obtained state-of-the-art performance in the extraction of protein-protein interactions (PPI) (Hunter et al., 2008; Huang et al., 2004) .\n\n", "discourse_tags": ["Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 52, "token_end": 74, "char_start": 281, "char_end": 357, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hunter et al., 2008;": "9855684", "Huang et al., 2004)": "12244611"}}}]}
{"id": "6030778_0", "paragraph": "[BOS] Despite the dominant focus on machine learning models for IE in the literature, previous work includes several notable rule-based efforts.\n[BOS] For example, GATE (Cunningham et al., 2011) , and the Common Pattern Specification Language (Appelt and Onyshkevych, 1998 ) introduce a rule-based framework for IE, implemented as a cascade of grammars defined using surface patterns.\n[BOS] The ICE system offers an active-learning system that learns named entity and binary relation patterns built on top of syntactic dependencies (He and Grishman, 2011 while a separate tool from the same group, TokensRegex (Chang and Manning, 2014) , defines surface patterns over token sequences.\n[BOS] Chiticariu et al. (2011) demonstrated that a rule-based NER system can match or outperform results achieved with machine learning approaches, but also showed that rule-writing is a labor intensive process even with a language specifically designed for the task.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Narrative_cite", "Single_summ"], "span_citation_mapping": [{"token_start": 30, "token_end": 80, "char_start": 164, "char_end": 384, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Cunningham et al., 2011)": null, "(Appelt and Onyshkevych, 1998": "29905988"}, "Reference": {}}}, {"token_start": 102, "token_end": 112, "char_start": 509, "char_end": 554, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(He and Grishman, 2011": "12005221"}}}, {"token_start": 121, "token_end": 132, "char_start": 598, "char_end": 635, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chang and Manning, 2014)": null}}}, {"token_start": 141, "token_end": 192, "char_start": 691, "char_end": 952, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Chiticariu et al. (2011)": "3178262"}, "Reference": {}}}]}
{"id": "6316511_1", "paragraph": "[BOS] Manipuri is a relatively free word order where the grammatical role of content words is largely determined by their case markers and not just by their positions in the sentence.\n[BOS] Machine Translation systems of Manipuri and English is reported by (Singh and Bandyopadhyay, 2010b) on development of English-Manipuri SMT system using morpho-syntactic and semantic information where the target case markers are generated based on the suffixes and semantic relations of the source sentence.\n[BOS] The above mentioned system is developed using Bengali script based Manipuri text.\n[BOS] SMT systems between English and morphologically rich highly agglutinative language suffer badly if the adequate training and language resource is not available.\n[BOS] Not only this, it is important to note that the linguistic representation of the text has implications on several NLP aspects not only in machine translations systems.\n[BOS] This is our first attempt to build and compare English-Manipuri language pair SMT systems using two different scripts of Manipuri.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Transition", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 35, "token_end": 112, "char_start": 190, "char_end": 584, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "6316511_0", "paragraph": "[BOS] Several SMT systems between English and morphologically rich languages are reported.\n[BOS] (Toutonova et al., 2007) reported the improvement of an SMT by applying word form prediction models from a stem using extensive morphological and syntactic information from source and target languages.\n[BOS] Contributions using factored phrase based model and a probabilistic tree transfer model at deep syntactic layer are made by (Bojar and Haji, 2008) of English-to-Czech SMT system.\n[BOS] (Yeniterzi and Oflazer, 2010) reported syntax-to-morphology mapping in factored phrase-based Statistical Machine Translation (Koehn and Hoang, 2007) from English to Turkish relying on syntactic analysis on the source side (English) and then encodes a wide variety of local and non-local syntactic structures as complex structural tags which appear as additional factors in the training data.\n[BOS] On the target side (Turkish), they only perform morphological analysis and disambiguation but treat the complete complex morphological tag as a factor, instead of separating morphemes.\n[BOS] (Bojar et al., 2012) pointed out several pitfalls when designing factored model translation setup.\n[BOS] All the above systems have been developed using one script for each language at the source as well as target.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 16, "token_end": 55, "char_start": 97, "char_end": 298, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 56, "token_end": 95, "char_start": 305, "char_end": 483, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Bojar and Haji\u010d, 2008)": "8563648"}, "Reference": {}}}, {"token_start": 96, "token_end": 211, "char_start": 490, "char_end": 1072, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Yeniterzi and Oflazer, 2010)": "14292100"}, "Reference": {}}}, {"token_start": 212, "token_end": 234, "char_start": 1079, "char_end": 1177, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Bojar et al., 2012)": "15119437"}, "Reference": {}}}]}
{"id": "6360322_5", "paragraph": "[BOS] bAbI ) is a collection of artificial datasets, consisting of 20 different reasoning types.\n[BOS] It encourages the development of models with the ability to chain reasoning, induction/ deduction, etc., so that they can answer a question like \"The football is in the playground\" after reading a sequence of sentences \"John is in the playground; Bob is in the office; John picked up the football; Bob went to the kitchen.\"\n[BOS] Various types of memory networks (Sukhbaatar et al., 2015; Kumar et al., 2016) have been shown effective on these tasks, and Lee et al. (2016) show that vector space models based on extensive problem analysis can obtain near-perfect accuracies on all the categories.\n[BOS] Despite these promising results, this dataset is limited to a small vocabulary (only 100-200 words) and simple language variations, so there is still a huge gap from real-world datasets that we need to fill in.\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 95, "token_end": 125, "char_start": 433, "char_end": 552, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sukhbaatar et al., 2015;": null, "Kumar et al., 2016)": "2319779"}}}, {"token_start": 127, "token_end": 155, "char_start": 558, "char_end": 699, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Lee et al. (2016)": "8221720"}, "Reference": {}}}]}
{"id": "6360322_4", "paragraph": "[BOS] Children Book Test (Hill et al., 2016) was developed in a similar spirit to the CNN/Daily Mail datasets.\n[BOS] It takes any consecutive 21 sentences from a children's book -the first 20 sentences are used as the passage, and the goal is to infer a missing word in the 21st sentence (question and answer).\n[BOS] The questions are also categorized by the type of the missing word: named entity, common noun, preposition or verb.\n[BOS] According to the first study on this dataset (Hill et al., 2016 ), a language model (an n-gram model or a recurrent neural network) with local context is sufficient for predicting verbs or prepositions; however, for named entities or common nouns, it improves performance to scan through the whole paragraph to make predictions.\n[BOS] So far, the best published results are reported by window-based memory networks.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 2, "token_end": 96, "char_start": 6, "char_end": 432, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Hill et al., 2016)": "14915449"}, "Reference": {}}}, {"token_start": 97, "token_end": 164, "char_start": 439, "char_end": 767, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Hill et al., 2016": "14915449"}, "Reference": {}}}]}
{"id": "6360322_3", "paragraph": "[BOS] Up to now, the best solutions (Sachan et al., 2015; Wang et al., 2015) are still heavily relying on manually curated syntactic/semantic features, with the aid of additional knowledge (e.g., word embeddings, lexical/paragraph databases).\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 6, "token_end": 44, "char_start": 17, "char_end": 188, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sachan et al., 2015;": "14071482", "Wang et al., 2015)": "8764466"}}}]}
{"id": "6360322_2", "paragraph": "[BOS] On the one hand, this dataset has a high demand on various reasoning capacities: over 50% of the questions require multiple sentences to answer and also the questions come in assorted categories (what, why, how, whose, which, etc) .\n[BOS] On the other hand, the full dataset has only 660 paragraphs in total (each paragraph is associated with 4 questions), which renders training statistical models (especially complex ones) very difficult.\n\n", "discourse_tags": ["Transition", "Transition"], "span_citation_mapping": []}
{"id": "6360322_1", "paragraph": "[BOS] MCTest (Richardson et al., 2013 ) is an opendomain reading comprehension task, in the form of fictional short stories, accompanied by multiplechoice questions.\n[BOS] It was carefully created using crowd sourcing, and aims at a 7-year-old reading comprehension level.\n\n", "discourse_tags": ["Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 58, "char_start": 6, "char_end": 272, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Richardson et al., 2013": "2100831"}, "Reference": {}}}]}
{"id": "6360322_0", "paragraph": "[BOS] We briefly survey other tasks related to reading comprehension.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "51782450_0", "paragraph": "[BOS] Most work on adding source hierarchical information to neural machine translation has used supervised syntax.\n[BOS] Luong et al. (2016) used a multi-task setup with a shared encoder to parse and translate the source language.\n[BOS] Eriguchi et al. (2016) introduced a tree-LSTM encoder for NMT that relied on an external parser to parse the training and test data.\n[BOS] The tree-LSTM encoder was improved upon by Chen et al. (2017a) and Yang et al. (2017) , who added a top-down pass.\n[BOS] Other approaches have used convolutional networks to model source syntax.\n[BOS] Chen et al. (2017b) enriched source word representations by extracting information from the dependency tree; a convolutional encoder was then applied to the representations.\n[BOS] Bastings et al. (2017) fed source dependency trees into a graph convolutional encoder.\n[BOS] Inducing unsupervised or semi-supervised hierarchies in NMT is a relatively recent research area.\n[BOS] Gehring et al. (2017a,b) introduced a fully convolutional model for NMT, which improved over strong sequential baselines.\n[BOS] Hashimoto and Tsuruoka (2017) added a latent graph parser to the encoder, allowing it to learn dependency-like source parses in an unsupervised manner.\n[BOS] However, they found that pre-training the parser with a small amount of human annotations yielded the best results.\n[BOS] Finally, introduced structured attention networks, which extended basic attention by allowing models to attend to latent structures such as subtrees.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Multi_summ", "Transition", "Single_summ", "Single_summ", "Transition", "Single_summ", "Single_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 19, "token_end": 45, "char_start": 122, "char_end": 231, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Luong et al. (2016)": "6954272"}, "Reference": {}}}, {"token_start": 46, "token_end": 79, "char_start": 238, "char_end": 370, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Eriguchi et al. (2016)": "12851711"}, "Reference": {}}}, {"token_start": 80, "token_end": 115, "char_start": 377, "char_end": 491, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Chen et al. (2017a)": "3504277", "Yang et al. (2017)": "7177285"}, "Reference": {}}}, {"token_start": 128, "token_end": 158, "char_start": 578, "char_end": 751, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Chen et al. (2017b)": "43165539"}, "Reference": {}}}, {"token_start": 159, "token_end": 178, "char_start": 758, "char_end": 844, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Bastings et al. (2017)": "6206777"}, "Reference": {}}}, {"token_start": 197, "token_end": 226, "char_start": 955, "char_end": 1076, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 227, "token_end": 284, "char_start": 1083, "char_end": 1356, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hashimoto and Tsuruoka (2017)": "1423962"}, "Reference": {}}}]}
{"id": "6288890_1", "paragraph": "[BOS] Supervised systems do better on the task, but not perfectly.\n[BOS] Recent work (Stoyanov et al., 2009) attempts to determine the contributions of various categories of NP to coreference scores, and shows (as stated above) that common NPs which partially match an earlier mention are not well resolved by the state-of-the-art RECONCILE system, which uses pairwise classification.\n[BOS] They also show that using gold mention boundaries makes the coreference task substantially easier, and argue that this experimental setting is \"rather unrealistic\".\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 17, "token_end": 107, "char_start": 85, "char_end": 553, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Stoyanov et al., 2009)": "6557193"}, "Reference": {}}}]}
{"id": "6288890_0", "paragraph": "[BOS] Unsupervised systems specify the assumption of same-head coreference in several ways: by as-sumption (Haghighi and Klein, 2009 ), using a head-prediction clause (Poon and Domingos, 2008) , and using a sparse Dirichlet prior on word emissions (Haghighi and Klein, 2007) .\n[BOS] (These three systems, perhaps not coincidentally, use gold mentions.)\n[BOS] An exception is Ng (2008) , who points out that head identity is not an entirely reliable cue and instead uses exact string match (minus determiners) for common NPs and an alias detection system for proper NPs.\n[BOS] This work uses mentions extracted with an NP chunker.\n[BOS] No specific results are reported for same-head NPs.\n[BOS] However, while using exact string match raises precision, many non-matching phrases are still coreferent, so this approach cannot be considered a full solution to the problem.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 18, "token_end": 30, "char_start": 95, "char_end": 132, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Haghighi and Klein, 2009": "16637008"}}}, {"token_start": 34, "token_end": 48, "char_start": 144, "char_end": 192, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Poon and Domingos, 2008)": "7124715"}}}, {"token_start": 52, "token_end": 67, "char_start": 207, "char_end": 274, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Haghighi and Klein, 2007)": "372666"}}}, {"token_start": 86, "token_end": 189, "char_start": 359, "char_end": 869, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ng (2008)": "4376006"}, "Reference": {}}}]}
{"id": "49881509_3", "paragraph": "[BOS] Corpus-level fine-grained entity typing is the task of predicting all types of entities based on their mentions in a corpus (Yaghoobzadeh and Schtze, 2015; Yaghoobzadeh and Schtze, 2017; Yaghoobzadeh et al., 2018) .\n[BOS] This is similar to our task, FNT, but in FNT the goals is to find the corpus-level types of names.\n[BOS] Corpus-level entity typing has also been used for embedding evaluation (Yaghoobzadeh and Schtze, 2016) .\n[BOS] However, they need an annotated corpus with entities.\n[BOS] For FNT, however, pretrained word embeddings are sufficient for the evaluation.\n[BOS] Finally, there exists some previous work on FNT, e.g., Chesney et al. (2017) .\n[BOS] In contrast to us, they do not explicitly focus on the evaluation of embedding models, such that their dataset only contains a limited number of types.\n[BOS] In contrast, we use 50 different types, making our dataset suitable for the type of evaluation intended.\n\n", "discourse_tags": ["Narrative_cite", "Reflection", "Narrative_cite", "Transition", "Transition", "Narrative_cite", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 19, "token_end": 70, "char_start": 85, "char_end": 219, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yaghoobzadeh and Sch\u00fctze, 2015;": "5846516", "Yaghoobzadeh and Sch\u00fctze, 2017;": "18556836"}}}, {"token_start": 110, "token_end": 127, "char_start": 383, "char_end": 435, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 157, "token_end": 181, "char_start": 590, "char_end": 666, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "49881509_2", "paragraph": "[BOS] Named entity recognition (NER) consists of recognizing and classifying mentions of entities locally in a particular context (Finkel et al., 2005) .\n[BOS] Recently, there has been increased interest in finegrained typing of mentions (Ling and Weld, 2012; Yogatama et al., 2015; Ren et al., 2016; Shimaoka et al., 2016) .\n[BOS] One way of solving our task is to collect every mention of a name, use NER to predict the context-dependent types of mentions, and then take all predictions as the global types of the name.\n[BOS] However, our focus in this paper is on how embedding models perform and propose this task as a good evaluation method.\n[BOS] We leave the comparison to an NER-based approach for future work.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 31, "char_start": 6, "char_end": 151, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Finkel et al., 2005)": "10977241"}}}, {"token_start": 35, "token_end": 79, "char_start": 170, "char_end": 323, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ling and Weld, 2012;": "9345159", "Yogatama et al., 2015;": "881437", "Ren et al., 2016;": "9778036", "Shimaoka et al., 2016)": "15976919"}}}]}
{"id": "49881509_1", "paragraph": "[BOS] Related tasks and datasets.\n[BOS] Our proposed task is fine-grained name typing (FNT).\n[BOS] A related task is entity set expansion (ESE): given a set of a few seed entities of a particular class, find other entities (Thelen and Riloff, 2002; Gupta and Manning, 2014) .\n[BOS] We can formulate FNT as ESE, however, there is a difference in the training data assumption.\n[BOS] For our task, we assume to have enough instances for each type available, and, therefore, to be able to use a supervised learning approach.\n[BOS] In contrast, for ESE, mostly only 3-5 seeds are given as training seeds for a set, which makes an evaluation like ours impossible.\n\n", "discourse_tags": ["Reflection", "Reflection", "Narrative_cite", "Transition", "Reflection", "Transition"], "span_citation_mapping": [{"token_start": 28, "token_end": 69, "char_start": 117, "char_end": 273, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Thelen and Riloff, 2002;": "137155", "Gupta and Manning, 2014)": "1367004"}}}]}
{"id": "49881509_0", "paragraph": "[BOS] Embedding evaluation.\n[BOS] Baroni et al. (2014) evaluate embeddings on different intrinsic tests: similarity, analogy, synonym detection, categorization and selectional preference.\n[BOS] Schnabel et al. (2015) introduce tasks with more fine-grained datasets.\n[BOS] The concept categorization datasets used for embedding evaluation are mostly small (<500) (Baroni et al., 2014) and therefore measure the goodness of embeddings by the quality of their clustering.\n[BOS] In contrast, we test embeddings in a classification setting and different subspaces of embeddings are analyzed.\n[BOS] Extrinsic evaluations are also used (Li and Jurafsky, 2015; Khn, 2015; Lai et al., 2015) .\n[BOS] In most tasks, embeddings are used in context/sentence representations with composition involved.\n[BOS] In this work, we evaluate embeddings in isolation, on their ability to represent multiple senses.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Narrative_cite", "Reflection", "Narrative_cite", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 6, "token_end": 35, "char_start": 34, "char_end": 187, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Baroni et al. (2014)": "85205"}, "Reference": {}}}, {"token_start": 36, "token_end": 54, "char_start": 194, "char_end": 265, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Schnabel et al. (2015)": "6197592"}, "Reference": {}}}, {"token_start": 61, "token_end": 79, "char_start": 317, "char_end": 383, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Baroni et al., 2014)": "85205"}}}, {"token_start": 113, "token_end": 139, "char_start": 593, "char_end": 681, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Li and Jurafsky, 2015;": "6222768", "K\u00f6hn, 2015;": null}}}]}
{"id": "51889492_5", "paragraph": "[BOS]  Covertly aggressive (CAG): The text is containing an indirect attack against the target using polite expressions in most cases.\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "51889492_4", "paragraph": "[BOS]  Overtly aggressive (OAG): The text is containing either aggressive lexical items or certain syntactic structures.\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "51889492_3", "paragraph": "[BOS]  Non-aggressive (NAG): There is no aggression in the text.\n\n", "discourse_tags": ["Other"], "span_citation_mapping": []}
{"id": "51889492_2", "paragraph": "[BOS] Based on Malmasi and Zampieri (2018) , distinguishing hate speech from profanity is not a trivial task and requires features that capture deeper information from the comments.\n[BOS] In this paper, we try different combinations of lexical, semantic, sentiment, and lexicon-based features to identify various levels of aggression in online texts.\n[BOS] The datasets were provided by Kumar et al. (2018b) .\n[BOS] Table 1 shows the distribution of training, validation and test (Facebook and social media) data for English and Hindi corpora.\n[BOS] The data has been labeled with one out of three possible tags:\n\n", "discourse_tags": ["Single_summ", "Reflection", "Narrative_cite", "Other", "Other"], "span_citation_mapping": [{"token_start": 2, "token_end": 39, "char_start": 6, "char_end": 181, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Malmasi and Zampieri (2018)": "3936688"}, "Reference": {}}}, {"token_start": 71, "token_end": 84, "char_start": 357, "char_end": 407, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Kumar et al. (2018b)": "4354069"}}}]}
{"id": "51889492_1", "paragraph": "[BOS] Most of the approaches proposed to detect offensive language in social media make use of multiple types of hand-engineered features.\n[BOS] Nobata et al. (2016) use n-grams, linguistic, syntactic and distributional semantic features to build a hate speech detection framework over Yahoo!\n[BOS] Finance and News and get an F-score of 81% for a combination of all features.\n[BOS] Davidson et al. (2017) combine n-grams, POS-colored n-grams, and sentiment lexicon features to detect hate speech on Twitter data.\n[BOS] Van Hee et al. (2015) use word and character n-grams along with sentiment lexicon features to identify nasty posts in ask.fm.\n[BOS] Samghabadi et al. (2017) build a model based on lexical, semantic, sentiment, and stylistic features to detect nastiness in ask.fm.\n[BOS] They also show the robustness of the model by applying it to the dataset from different other sources.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 27, "token_end": 81, "char_start": 145, "char_end": 376, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Nobata et al. (2016)": "11546523"}, "Reference": {}}}, {"token_start": 82, "token_end": 114, "char_start": 383, "char_end": 513, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Davidson et al. (2017)": "1733167"}, "Reference": {}}}, {"token_start": 115, "token_end": 146, "char_start": 520, "char_end": 645, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Van Hee et al. (2015)": "4245513"}, "Reference": {}}}, {"token_start": 147, "token_end": 202, "char_start": 652, "char_end": 892, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Samghabadi et al. (2017)": "45723107"}, "Reference": {}}}]}
{"id": "51889492_0", "paragraph": "[BOS] In recent years, several studies have been done towards detecting abusive and hateful language in online texts.\n[BOS] Some of these works target different online platforms like Twitter (Waseem and Hovy, 2016) , Wikipedia (Wulczyn et al., 2016) , and ask.fm (Samghabadi et al., 2017) to encourage other research groups to contribute to aggression identification in these sources.\n\n", "discourse_tags": ["Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 34, "token_end": 45, "char_start": 183, "char_end": 214, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Waseem and Hovy, 2016)": "1721388"}}}, {"token_start": 46, "token_end": 58, "char_start": 217, "char_end": 249, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wulczyn et al., 2016)": null}}}]}
{"id": "51881192_3", "paragraph": "[BOS] Several argument mining tasks have recently been proposed.\n[BOS] For instance, Stab and Gurevych (2017b) examine the task of whether an argument is sufficiently supported.\n[BOS] Al Khatib et al. (2016) identify and annotate a news editorial corpus with fine-grained argumentative discourse units for the purpose of analyzing the argumentation strategies used to persuade readers.\n[BOS] Wachsmuth et al. (2017) focus on identifying and annotating 15 logical, rhetorical, and dialectical dimensions that would be useful for automatically accessing the quality of an argument.\n[BOS] Most recently, the Argument Reasoning Comprehension task organized as part of SemEval 2018 has focused on selecting the correct warrant that explains reasoning of an argument that consists of a claim and a reason.\n[BOS] 2\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 15, "token_end": 36, "char_start": 85, "char_end": 177, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Stab and Gurevych (2017b)": "6801402"}, "Reference": {}}}, {"token_start": 37, "token_end": 78, "char_start": 184, "char_end": 385, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Khatib et al. (2016)": "11438927"}, "Reference": {}}}, {"token_start": 79, "token_end": 119, "char_start": 392, "char_end": 579, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wachsmuth et al. (2017)": "141282"}, "Reference": {}}}]}
{"id": "51881192_2", "paragraph": "[BOS] In contrast, there are studies that focus on factors affecting argument persuasiveness in internet debates.\n[BOS] For instance, Lukin et al. (2017) examine how audience variables (e.g., personalities) interact with argument style (e.g., factual vs. emotional arguments) to affect argument persuasive- ness.\n[BOS] Persing and Ng (2017) identify factors that negatively impact persuasiveness, so their factors, unlike ours, cannot explain what makes an argument persuasive.\n[BOS] Other argument mining tasks.\n[BOS] Some of the attributes that we annotate our corpus with have been studied.\n[BOS] For instance, Hidey et al. (2017) examine the different semantic types of claims and premises, whereas Higgins and Walker (2012) investigate persuasion strategies (i.e., ethos, pathos, logos).\n[BOS] Unlike ours, these studies use data from online debate forums and social/environment reports.\n[BOS] Perhaps more importantly, they study these attributes independently of persuasiveness.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Transition", "Reflection", "Multi_summ", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 25, "token_end": 72, "char_start": 134, "char_end": 312, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Lukin et al. (2017)": "2586121"}, "Reference": {}}}, {"token_start": 73, "token_end": 105, "char_start": 319, "char_end": 477, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Persing and Ng (2017)": "27237469"}, "Reference": {}}}, {"token_start": 131, "token_end": 148, "char_start": 614, "char_end": 693, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hidey et al. (2017)": "940795"}, "Reference": {}}}, {"token_start": 150, "token_end": 177, "char_start": 703, "char_end": 792, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Higgins and Walker (2012)": null}, "Reference": {}}}]}
{"id": "51881192_1", "paragraph": "[BOS] Persuasiveness-related tasks.\n[BOS] Most related to our study is work involving argument persuasiveness.\n[BOS] For instance, Habernal and Gurevych (2016b) and Wei et al. (2016) study the persuasiveness ranking task, where the goal is to rank two internet debate arguments written for the same topic w.r.t.\n[BOS] their persuasiveness.\n[BOS] As noted by Habernal and Gurevych, ranking arguments is a relatively easier task than scoring an argument's persuasiveness: in ranking, a system simply determines whether one argument is more persuasive than the other, but not how much more persuasive one argument is than the other; in scoring, however, a system has to determine how persuasive an argument is on an absolute scale.\n[BOS] Note that ranking is not an acceptable evaluation setting for studying argument persuasiveness in the essay domain, as feedback for an essay has to be provided independently of other essays.\n\n", "discourse_tags": ["Reflection", "Reflection", "Multi_summ", "Multi_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 27, "token_end": 81, "char_start": 131, "char_end": 339, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Habernal and Gurevych (2016b)": "3083231", "Wei et al. (2016)": "5300915"}, "Reference": {}}}, {"token_start": 85, "token_end": 163, "char_start": 358, "char_end": 728, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "51881192_0", "paragraph": "[BOS] While argument mining research has traditionally focused on determining the argumentative structure of a text document (i.e., identifying its major claim, claims, and premises, as well as the relationships between these argument components) Gurevych, 2014b, 2017a; Eger et al., 2017) , researchers have recently begun to study new argument mining tasks, as described below.\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 17, "token_end": 63, "char_start": 111, "char_end": 289, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Eger et al., 2017)": "3221856"}}}]}
{"id": "52008842_3", "paragraph": "[BOS] Although some existing SMT domain adaptation techniques can be directly applied to NMT, it is challenging for applying data weighting to NMT.\n[BOS] For NMT, the data selection approach can also be used.\n[BOS] Wang et al. (2017a) employ the data selection method for domain adaptation, which uses sentence embeddings to measure the similarity of a sentence pair to the in-domain data.\n[BOS] A recent method to apply sentence weights to NMT is cost weighting (Wang et al., 2017b; Chen et al., 2017) .\n[BOS] The NMT objective function is updated by sentence weighting when computing the cost of each mini-batch during NMT training.\n[BOS] Wang et al. (2017b) exploit an in-domain language model (Axelrod et al., 2011) to score sentences.\n[BOS] Chen et al. (2017) use a classifier to assign weights for individual sentences pairs.\n[BOS] Domain control uses word-level domain features in the word embedding layer, aiming to allow a model to be built from a diverse set of training data to produce in-domain translations (Kobus et al., 2017) .\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ", "Narrative_cite", "Transition", "Single_summ", "Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 44, "token_end": 80, "char_start": 215, "char_end": 389, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wang et al. (2017a)": "9990193"}, "Reference": {}}}, {"token_start": 86, "token_end": 110, "char_start": 421, "char_end": 502, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wang et al., 2017b;": "1054586", "Chen et al., 2017)": "37405481"}}}, {"token_start": 137, "token_end": 166, "char_start": 641, "char_end": 739, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wang et al. (2017b)": "1054586"}, "Reference": {"(Axelrod et al., 2011)": "10766958"}}}, {"token_start": 167, "token_end": 185, "char_start": 746, "char_end": 831, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Chen et al. (2017)": "37405481"}, "Reference": {}}}, {"token_start": 210, "token_end": 230, "char_start": 957, "char_end": 1040, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kobus et al., 2017)": "7497218"}}}]}
{"id": "52008842_2", "paragraph": "[BOS] At the data level, traditional domain adaptation approach can be done by data selection, data weighting or data joining.\n[BOS] Data selection approaches select data similar to the in-domain data according to some criteria.\n[BOS] Normally, the out-of-domain data can be scored by a model trained on the in-domain data and out-of-domain data.\n[BOS] For example, a language model can be used for scoring sentences (Axelrod et al., 2011) .\n[BOS] Data weighting methods weight each item which can be a corpus, a sentence or a phrase, and then train SMT models on weighted items.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 78, "token_end": 96, "char_start": 368, "char_end": 439, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Axelrod et al., 2011)": "10766958"}}}]}
{"id": "52008842_1", "paragraph": "[BOS] At the model level, combining multiple translation models in a weighted manner is used for SMT domain adaptation.\n[BOS] For NMT, fine tuning, model stacking and muti-model ensemble have been explored (Sajjad et al., 2017) .\n[BOS] Luong and Manning (2015) propose a fine-tuning method, which continues to train the already trained out-of-domain system on the in-domain data.\n[BOS] Model stacking is to build an NMT model in an online fashion, training the model from the most distant domain at the beginning, fine-tuning it on the closer domain and finalizing it by fine-tuning it on the in-domain data.\n[BOS] Muti-model ensemble combines multiple models during decoding using a balanced or weighted averaging method.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Single_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 24, "token_end": 52, "char_start": 126, "char_end": 227, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sajjad et al., 2017)": "35167387"}}}, {"token_start": 54, "token_end": 140, "char_start": 236, "char_end": 608, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Luong and Manning (2015)": null}, "Reference": {}}}]}
{"id": "52008842_0", "paragraph": "[BOS] A lot of investigations have already been conducted for domain adaptation in SMT while few in neural machine translation.\n[BOS] These methods can be roughly categorized into two classes: the model-level and data-level method.\n\n", "discourse_tags": ["Transition", "Transition"], "span_citation_mapping": []}
{"id": "51984213_2", "paragraph": "[BOS] Unlike off-line mappings, joint training models simultaneously learn word embeddings and cross-lingual alignment.\n[BOS] By jointly updating the embeddings with the alignment information, such approaches generally capture more precise cross-lingual semantic transfer (Upadhyay et al. 2016 ).\n[BOS] While few of such models still maintain separated embedding spaces for each language (Huang et al. 2015) , the majority of recent ones obtain a unified embedding space for both languages.\n[BOS] The cross-lingual semantic transfer by these models is captured from parallel corpora with sentential or document-level alignment, using techniques such as bilingual bag-of-words distances (BilBOWA) (Gouws et al. 2015) , bilingual Skip-Gram (Coulmance et al. 2015) and sparse tensor factorization (Vyas and Carpuat 2016) .\n[BOS] Neural sentence modeling.\n[BOS] Neural sentence models seek to characterize the phrasal or sentential semantics from word sequences.\n[BOS] They often adopt encoding techniques such as recurrent neural encoders (RNN) (Kiros et al. 2015) , convolutional neural encoders (CNN) (Chen et al. 2018a) , and attentive neural encoders ) to represent the composed semantics of a sentence as an embedding vector.\n[BOS] Many recent works have focused on comprehending pairwise correspondence of sentential semantics by adopting multiple neural sentence models in one learning architecture.\n[BOS] Examples of such include Siamese sentence pair models for detecting discourse relations of paraphrases or text entailment (Sha et al. 2016; Rocktschel et al. 2016; Chen et al. 2018a) , and sequence-to-sequence models for tasks like style transfer (Shen et al. 2017 ) and abstractive summarization (Chopra, Auli, and Rush 2016) .\n[BOS] Specifically, our work is related to corresponding works of neural machine translation (NMT) (Bahdanau, Cho, and Bengio 2015; Wu et al. 2016) , while our setting has major differences from NMT in the following two perspectives: (i) NMT has to bridge between corpora of the same granularity, unlike Bil-DRL that captures the multi-granular correspondence of semantics across different modalities (ii) NMT relies on training an encoder-decoder architecture, while BilDRL employs joint learning of two representation models, i.e. a dictionarybased sentence encoder and a word embedding model.\n[BOS] On the other hand, fewer efforts have been put to characterizing the associations between sentential and lexical semantics.\n[BOS] Hill et al. (2016) and Xie et al. (2016) learn off-line mappings between monolingual descriptions and lexicons to capture such associations.\n[BOS] Eisner et al. (2016) adopt a similar approach to capture emojis based on descriptions.\n[BOS] At the best of our knowledge, there has been no previous approach that learn to discover the correspondence of sentential and lexical semantics in a multilingual scenario.\n[BOS] This is exactly the focus of our work, in which the proposed strategies of multi-task and joint learning are critical to the corresponding cross-lingual learning process under limited resources.\n[BOS] Utilizing the cross-lingual and multi-granular correspondence of semantics, our approach also sheds light on addressing discourse relation detection in a multilingual scenario.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite", "Narrative_cite", "Transition", "Transition", "Narrative_cite", "Transition", "Narrative_cite", "Reflection", "Transition", "Multi_summ", "Single_summ", "Transition", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 39, "token_end": 54, "char_start": 240, "char_end": 293, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Upadhyay et al. 2016": "5357629"}}}, {"token_start": 65, "token_end": 77, "char_start": 353, "char_end": 407, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Huang et al. 2015)": "155944"}}}, {"token_start": 121, "token_end": 142, "char_start": 653, "char_end": 715, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Gouws et al. 2015)": "7021865"}}}, {"token_start": 143, "token_end": 156, "char_start": 718, "char_end": 761, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Coulmance et al. 2015)": null}}}, {"token_start": 157, "token_end": 169, "char_start": 766, "char_end": 817, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vyas and Carpuat 2016)": "3603249"}}}, {"token_start": 299, "token_end": 324, "char_start": 1516, "char_end": 1592, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sha et al. 2016;": "2617281", "Rockt\u00e4schel et al. 2016;": "2135897", "Chen et al. 2018a)": null}}}, {"token_start": 326, "token_end": 343, "char_start": 1599, "char_end": 1674, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Shen et al. 2017": "7296803"}}}, {"token_start": 345, "token_end": 361, "char_start": 1681, "char_end": 1736, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chopra, Auli, and Rush 2016)": "133195"}}}, {"token_start": 373, "token_end": 398, "char_start": 1805, "char_end": 1886, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bahdanau, Cho, and Bengio 2015;": "11212020", "Wu et al. 2016)": "3603249"}}}, {"token_start": 509, "token_end": 541, "char_start": 2471, "char_end": 2611, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hill et al. (2016)": "14745914", "Xie et al. (2016)": "31606602"}, "Reference": {}}}, {"token_start": 542, "token_end": 563, "char_start": 2618, "char_end": 2704, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Eisner et al. (2016)": "7819714"}, "Reference": {}}}]}
{"id": "51984213_1", "paragraph": "[BOS] The off-line mapping-based approach fixes the structures of pre-trained monolingual word embeddings, and induces bilingual projections based on seed-lexicon alignment (Mikolov et al. 2013a) .\n[BOS] Some variants of this approach improve the quality of bilingual projections by adding constraints such as orthogonality of transforms, normalization and mean centering of embeddings (Xing et al. 2015; Artetxe et al. 2016) .\n[BOS] Others adopt canonical correlation analysis to map separated monolingual embeddings to a shared embedding space (Faruqui and Dyer 2014; Lu et al. 2015) .\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 29, "token_end": 43, "char_start": 150, "char_end": 195, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mikolov et al. 2013a)": "1966640"}}}, {"token_start": 61, "token_end": 89, "char_start": 310, "char_end": 425, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Xing et al. 2015;": "3144258", "Artetxe et al. 2016)": "1040556"}}}, {"token_start": 99, "token_end": 123, "char_start": 495, "char_end": 585, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Faruqui and Dyer 2014;": "3792324", "Lu et al. 2015)": "874413"}}}]}
{"id": "51984213_0", "paragraph": "[BOS] In this section, we discuss two lines of relevant work.\n[BOS] Bilingual word embeddings.\n[BOS] Recently, various approaches have been proposed for training bilingual word embeddings.\n[BOS] These approaches span in two families of models: off-line mappings and joint training.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Transition"], "span_citation_mapping": []}
{"id": "51996171_0", "paragraph": "[BOS] Recurrent neural network has achieved great success because of its effective capability to capture the sequential information.\n[BOS] The RNN handles the variable-length sequence by having a recurrent hidden state whose activation at each time step is dependent on that of the previous time.\n[BOS] To reduce the negative impact of gradient vanishing, a long short-term memory unit (Hochreiter and Schmidhuber, 1997) , which has a more sophisticated activation function, was proposed.\n[BOS] Bidirectional recurrent neural networks (Schuster and Paliwal, 1997) , e.g bidirectional LSTM networks (Augenstein et al., 2016) , combine forward features as well as reverse features of the text.\n[BOS] Bidirectional networks, which get the forward features and the reverse features separately, are different from our multi-glance mechanism.\n[BOS] A Gated Recurrent Unit (GRU) ) is a good extension of a LSTM unit, because GRU maintains the performance and makes the structure to be simpler.\n[BOS] Comparing to a LSTM unit, a GRU has only two gates, an update gate and a reset gate, so it will be faster to train a GRU than a LSTM unit.\n[BOS] Attention mechanism ) is used to learn weights for every input, so it can reduce the impact of information redundancy.\n[BOS] Now, attention mechanism is commonly used in various models.\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite", "Single_summ", "Transition", "Transition", "Transition", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 62, "token_end": 80, "char_start": 358, "char_end": 420, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hochreiter and Schmidhuber, 1997)": "1915014"}}}, {"token_start": 93, "token_end": 138, "char_start": 495, "char_end": 691, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Schuster and Paliwal, 1997)": "18375389"}, "Reference": {"(Augenstein et al., 2016)": "744471"}}}]}
{"id": "5986695_1", "paragraph": "[BOS] In the visual modality, the intuition that visual representations may be useful for detecting lexical entailment is inspired by Deselaers and Ferrari (2011) .\n[BOS] Using manually annotated images from ImageNet (Deng et al., 2009) , they find that concepts and categories with narrower intensions and smaller extensions tend to have less visual variability.\n[BOS] We extend this intuition to the unsupervised setting of Google image search results and apply it to the lexical entailment task.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 16, "token_end": 32, "char_start": 90, "char_end": 162, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Deselaers and Ferrari (2011)": "1198168"}}}, {"token_start": 39, "token_end": 49, "char_start": 208, "char_end": 236, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Deng et al., 2009)": "206597351"}}}]}
{"id": "5986695_0", "paragraph": "[BOS] In the linguistic modality, the most closely related work is by Herbelot and Ganesalingam (2013) and Santus et al. (2014) , who use unsupervised distributional generality measures to identify the hypernym in a hyponym-hypernym pair.\n[BOS] Herbelot and Ganesalingam (2013) use KL divergence to compare the probability distribution of context words, given a term, to the background probability distribution of context words.\n[BOS] Santus et al. (2014) use the median entropy of the probability distributions associated with a term's top-weighted con- text words as a measure of information content.\n\n", "discourse_tags": ["Multi_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 58, "char_start": 6, "char_end": 238, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Herbelot and Ganesalingam (2013)": "1432668", "Santus et al. (2014)": "14176162"}, "Reference": {}}}, {"token_start": 59, "token_end": 95, "char_start": 245, "char_end": 428, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Herbelot and Ganesalingam (2013)": "1432668"}, "Reference": {}}}, {"token_start": 96, "token_end": 132, "char_start": 435, "char_end": 602, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Santus et al. (2014)": "14176162"}, "Reference": {}}}]}
{"id": "5033484_0", "paragraph": "[BOS] The STS task was first introduced by Agirre et al. (2012) .\n[BOS] Early methods focused on lexical semantics, surface form matching and basic syntactic similarity (Br et al., 2012; Jimenez et al., 2012) .\n[BOS] More recently, deep learning based methods became competitive (Shao, 2017; Tai et al., 2015) .\n[BOS] One approach to this task is to train a general purpose sentence encoder and then calculate the cosine similarity between the encoded vectors for the pair of sentences.\n[BOS] The encoding model can be directly trained on the STS task (Shao, 2017) or it can be trained on an alternative supervised (Conneau et al., 2017) or unsupervised (Pagliardini et al., 2017 ) task that produces sentence-level embeddings.\n[BOS] The work described in our paper falls into the latter category, introducing a new unsupervised task based on conversational data that achieves good performance on predicting seman-tic similarity scores.\n[BOS] Training on conversational data has been previously shown to be effective at email response prediction (Kannan et al., 2016; Henderson et al., 2017) .\n[BOS] We extend prior work by exploring the effectiveness of representations learned from conversational data to capture more general-purpose semantic information.\n[BOS] The approach is similar to Skip-Thought vectors (Kiros et al., 2015) , which learn sentence-level representations through prior and next sentence prediction within a document, but with our prior and next sentences being pulled from turns in a conversation.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Narrative_cite", "Transition", "Narrative_cite", "Reflection", "Narrative_cite", "Reflection", "Narrative_cite"], "span_citation_mapping": [{"token_start": 2, "token_end": 18, "char_start": 6, "char_end": 63, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Agirre et al. (2012)": "12549805"}}}, {"token_start": 24, "token_end": 51, "char_start": 97, "char_end": 208, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(B\u00e4r et al., 2012;": "6964767", "Jimenez et al., 2012)": "7640960"}}}, {"token_start": 56, "token_end": 75, "char_start": 232, "char_end": 309, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Shao, 2017;": "30958369", "Tai et al., 2015)": "3033526"}}}, {"token_start": 116, "token_end": 124, "char_start": 543, "char_end": 564, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Shao, 2017)": "30958369"}}}, {"token_start": 132, "token_end": 143, "char_start": 604, "char_end": 637, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Conneau et al., 2017)": "28971531"}}}, {"token_start": 144, "token_end": 155, "char_start": 641, "char_end": 679, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Pagliardini et al., 2017": "16251657"}}}, {"token_start": 213, "token_end": 233, "char_start": 1020, "char_end": 1091, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kannan et al., 2016;": "5759934", "Henderson et al., 2017)": "2449317"}}}, {"token_start": 265, "token_end": 278, "char_start": 1291, "char_end": 1332, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kiros et al., 2015)": "9126867"}}}]}
{"id": "5071138_2", "paragraph": "[BOS] Given that there are already a few datasets for RC, a natural question to ask is \"Do we really need any more datasets?\".\n[BOS] We believe that the answer to this question is yes.\n[BOS] Each new dataset brings in new challenges and contributes towards building better QA systems.\n[BOS] It keeps researchers on their toes and prevents research from stagnating once state-of-theart results are achieved on one dataset.\n[BOS] A classic example of this is the CoNLL NER dataset (Tjong Kim Sang and De Meulder, 2003) .\n[BOS] While several NER systems (Passos et al., 2014) gave close to human performance on this dataset, NER on general web text, domain specific text, noisy social media text is still an unsolved problem (mainly due to the lack of representative datasets which cover the real-world challenges of NER).\n[BOS] In this context, DuoRC presents 4 new challenges mentioned earlier which are not exhibited in existing RC datasets and would thus enable exploring novel neural approaches in complex language understanding.\n[BOS] The hope is that all these datasets (including ours) will collectively help in addressing a wide range of challenges in QA and prevent stagnation via overfitting on a single dataset.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Transition", "Narrative_cite", "Single_summ", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 94, "token_end": 111, "char_start": 461, "char_end": 516, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Tjong Kim Sang and De Meulder, 2003)": null}}}, {"token_start": 113, "token_end": 175, "char_start": 525, "char_end": 819, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Passos et al., 2014)": "9345583"}, "Reference": {}}}]}
{"id": "5071138_1", "paragraph": "[BOS] Another notable RC Dataset is NarrativeQA(s Kocisk et al., 2018) which contains 40K QA pairs created from plot summaries of movies.\n[BOS] It poses two tasks, where the first task involves reading the plot summaries from which the QA pairs were annotated and the second task is read the entire book or movie script (which is usually 60K words long) instead of the summary to answer the question.\n[BOS] As acknowledged by the authors, while the first task is similar in scope to the previous datasets, the second task is at present, intractable for existing neural models, owing to the length of the passage.\n[BOS] Due to the kind of the challenges presented by their second task, it is not comparable to our dataset and is much more futuristic in nature.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 126, "char_start": 6, "char_end": 612, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ko\u02c7cisk\u00fd et al., 2018)": "2593903"}, "Reference": {}}}]}
{"id": "5071138_0", "paragraph": "[BOS] Over the past few years, there has been a surge in datasets for Reading Comprehension.\n[BOS] Most of these datasets differ in the manner in which questions and answers are created.\n[BOS] For example, in SQuAD (Rajpurkar et al., 2016a) , NewsQA (Trischler et al., 2016) , TriviaQA (Joshi et al., 2017) and MovieQA (Tapaswi et al., 2016 ) the answers correspond to a span in the document.\n[BOS] MS-MARCO uses web queries as questions and the answers are synthesized by workers from documents relevant to the query.\n[BOS] On the other hand, in most cloze-style datasets (Mostafazadeh et al., 2016; Onishi et al., 2016 ) the questions are created automatically by deleting a word/entity from a sentence.\n[BOS] There are also some datasets for RC with multiple choice questions (Richardson et al., 2013; Berant et al., 2014; Lai et al., 2017) where the task is to select one among k given candidate answers.\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite", "Transition", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 41, "token_end": 54, "char_start": 209, "char_end": 240, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rajpurkar et al., 2016a)": "11816014"}}}, {"token_start": 55, "token_end": 67, "char_start": 243, "char_end": 274, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Trischler et al., 2016)": "1167588"}}}, {"token_start": 68, "token_end": 80, "char_start": 277, "char_end": 306, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Joshi et al., 2017)": "26501419"}}}, {"token_start": 81, "token_end": 92, "char_start": 311, "char_end": 340, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Tapaswi et al., 2016": "1017389"}}}, {"token_start": 135, "token_end": 159, "char_start": 552, "char_end": 620, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mostafazadeh et al., 2016;": "15337246", "Onishi et al., 2016": "5761781"}}}, {"token_start": 182, "token_end": 210, "char_start": 745, "char_end": 843, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Richardson et al., 2013;": "2100831", "Berant et al., 2014;": "8471750", "Lai et al., 2017)": "6826032"}}}]}
{"id": "51974493_3", "paragraph": "[BOS] We apply GGNNs to the problem of semantic parsing.\n[BOS] Li et al. (2016) have developed the gated architecture based on the graph neural network formulation of Scarselli et al. (2009) .\n[BOS] Recently, a slightly different design of Graph Convolutional Networks was proven effective on a KB completion task (Schlichtkrull et al., 2018) .\n[BOS] Kipf and Welling (2017) introduced Graph Convolutional Networks, while Marcheggiani and Titov (2017) employed them for natural language processing for the first time and compared them to other formulations.\n[BOS] Graph Convolutional Networks have a similar gated architecture and share most of the same properties with the Gated Graph Neural Networks used.\n\n", "discourse_tags": ["Reflection", "Multi_summ", "Narrative_cite", "Multi_summ", "Transition"], "span_citation_mapping": [{"token_start": 15, "token_end": 44, "char_start": 63, "char_end": 190, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Li et al. (2016)": "8393918"}, "Reference": {"Scarselli et al. (2009)": "206756462"}}}, {"token_start": 61, "token_end": 75, "char_start": 295, "char_end": 342, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Schlichtkrull et al., 2018)": "207908316"}}}, {"token_start": 77, "token_end": 89, "char_start": 351, "char_end": 414, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kipf and Welling (2017)": "3144218"}, "Reference": {}}}, {"token_start": 91, "token_end": 118, "char_start": 422, "char_end": 557, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Marcheggiani and Titov (2017)": "16839291"}, "Reference": {}}}]}
{"id": "51974493_2", "paragraph": "[BOS] An alternative solution to semantic parsing is to build an information extraction pipeline that views the question as a query and the KB as a source of relevant information (Yao et al., 2014) .\n[BOS] Dong et al. (2015) and Jain (2016) construct a vector representation for the question and use it to directly score candidate answers from the KB.\n[BOS] However, such approaches are hard to analyze for errors or to modify with explicit constraints.\n[BOS] For example, it is not directly possible to implement the temporal sorting constraint (argmax).\n\n", "discourse_tags": ["Narrative_cite", "Multi_summ", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 24, "token_end": 39, "char_start": 140, "char_end": 197, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yao et al., 2014)": "5915651"}}}, {"token_start": 41, "token_end": 72, "char_start": 206, "char_end": 351, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Dong et al. (2015)": "12926055"}, "Reference": {}}}]}
{"id": "51974493_1", "paragraph": "[BOS] We only train on the WebQSP-WD data set and we note that more data might be necessary to effectively train the gated graph architecture.\n[BOS] Reddy et al. (2014) suggest an unsupervised learning method to learn a model from a large web corpus, while Su et al. (2016) use patterns and crowdsourcing to create new data with specific properties.\n[BOS] These techniques can be used to further improve the performance of our model.\n\n", "discourse_tags": ["Reflection", "Multi_summ", "Reflection"], "span_citation_mapping": [{"token_start": 32, "token_end": 54, "char_start": 149, "char_end": 249, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Reddy et al. (2014)": "15324422"}, "Reference": {}}}, {"token_start": 56, "token_end": 77, "char_start": 257, "char_end": 349, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Su et al. (2016)": null}, "Reference": {}}}]}
{"id": "51974493_0", "paragraph": "[BOS] We have focused on the problem of the increasing error rates for complex questions and the encoding of the semantic graph structure.\n[BOS] In this paper, we describe a semantic parsing system for KB QA and follow the approach of Yih et al. (2015) who do not rely on syntactic parsing to construct semantic parses.\n[BOS] Our semantic graphs do not cover some aspects of the first-order logic, such as negation.\n[BOS] Reddy et al. (2016) define a semantic parser that builds first-order logic representations from syntactic dependencies.\n[BOS] They further specify how it can be extended with negation in Fancellu et al. (2017) .\n\n", "discourse_tags": ["Reflection", "Narrative_cite", "Reflection", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 33, "token_end": 52, "char_start": 174, "char_end": 252, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Yih et al. (2015)": "18309765"}}}, {"token_start": 86, "token_end": 130, "char_start": 422, "char_end": 631, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Reddy et al. (2016)": "12687036"}, "Reference": {"Fancellu et al. (2017)": "5388565"}}}]}
{"id": "52008924_1", "paragraph": "[BOS] Most NMT and NRG systems generate outputs using the beam search algorithm, which unfortunately suffers from the myopic bias.\n[BOS] To solve the myopic bias, He et al. (2017) and both propose method to take the future BLEU of decoder partial outputs into account in beam search.\n[BOS] Another study indirectly related to our work is Wiseman and Rush (2016) , it treats the target sequences in training set as the gold sequences, and directly training the beam search to select word instead of probability.\n[BOS] Although these methods are proved to be effective on NMT, it might be inappropriate to directly apply them on NRG, since appropriate responses for one query are highly diverse in terms of semantics.\n[BOS] By contrast, the proposed method exploits the nature of beam search width to alleviate the myopic bias.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 34, "token_end": 61, "char_start": 163, "char_end": 283, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"He et al. (2017)": "4021462"}, "Reference": {}}}, {"token_start": 70, "token_end": 105, "char_start": 338, "char_end": 510, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wiseman and Rush (2016)": "2783746"}, "Reference": {}}}]}
{"id": "52008924_0", "paragraph": "[BOS] Inspired by the success of the Seq2Seq framework on NMT (Cho et al., 2014; Sutskever et al., 2014b; Bahdanau et al., 2015) , this framework has been adopted for response generation (Vinyals and Le, 2015; Shang et al., 2015) and is proved to be effective on generating responses based on given queries (Sordoni et al., 2015; Li et al., 2016; Xu et al., 2017) .\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 8, "token_end": 42, "char_start": 37, "char_end": 128, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cho et al., 2014;": "5590763", "Sutskever et al., 2014b;": "7961699", "Bahdanau et al., 2015)": "11212020"}}}, {"token_start": 49, "token_end": 68, "char_start": 167, "char_end": 229, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vinyals and Le, 2015;": "12300158", "Shang et al., 2015)": "7356547"}}}, {"token_start": 75, "token_end": 105, "char_start": 263, "char_end": 363, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sordoni et al., 2015;": "94285", "Li et al., 2016;": "5165773", "Xu et al., 2017)": "11030403"}}}]}
{"id": "5404235_1", "paragraph": "[BOS] In dependency parsing, Lee et al. (2011) recently proposed a discriminative graphical model that solves morphological disambiguation and dependency parsing jointly.\n[BOS] However, their main focus was to capture interaction between morphology and syntax in morphologically-rich, highlyinflected languages (such as Latin and Ancient Greek), which are unlike Chinese.\n[BOS] More recently, Li et al. (2011) proposed the first joint model for Chinese POS tagging and dependency parsing in a graph-based parsing framework, which is one of our baseline systems.\n[BOS] On the other hand, our work is the first incremental approach to this joint task.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 6, "token_end": 66, "char_start": 29, "char_end": 371, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Lee et al. (2011)": "10492123"}, "Reference": {}}}, {"token_start": 70, "token_end": 105, "char_start": 393, "char_end": 561, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Li et al. (2011)": "14410909"}, "Reference": {}}}]}
{"id": "5404235_0", "paragraph": "[BOS] In recent years, joint segmentation and tagging have been widely investigated (e.g. Zhang and Clark (2010) ; Kruengkrai et al. (2009); Zhang and Clark (2008a); Jiang et al. (2008a); Jiang et al. (2008b) ).\n[BOS] Particularly, our framework of using a single perceptron to solve the joint problem is motivated by Zhang and Clark (2008a) .\n[BOS] Also, our joint parsing framework is an extension of Huang and Sagae (2010)'s framework, which is described in detail in Section 2.2.\n[BOS] In constituency parsing, the parsing naturally involves the POS tagging since the non-terminal symbols are commonly associated with POS tags (e.g. Klein and Manning (2003) ).\n[BOS] Rush et al. (2010) proposed to use dual composition to combine a constituency parser and a trigram POS tagger, showing the effectiveness of taking advantage of these two systems.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Reflection", "Narrative_cite", "Single_summ"], "span_citation_mapping": [{"token_start": 6, "token_end": 63, "char_start": 23, "char_end": 208, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Zhang and Clark (2010)": "2712419", "Kruengkrai et al. (2009);": "769547", "Zhang and Clark (2008a);": "105219", "Jiang et al. (2008a);": "9285364", "Jiang et al. (2008b)": "12138087"}}}, {"token_start": 73, "token_end": 91, "char_start": 257, "char_end": 341, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Zhang and Clark (2008a)": "105219"}}}, {"token_start": 96, "token_end": 110, "char_start": 360, "char_end": 425, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 148, "token_end": 162, "char_start": 622, "char_end": 661, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Klein and Manning (2003)": "11495042"}}}, {"token_start": 165, "token_end": 203, "char_start": 671, "char_end": 849, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Rush et al. (2010)": "1994530"}, "Reference": {}}}]}
{"id": "6316890_0", "paragraph": "[BOS] Although character features are very important in Chinese morphology, research in character-based approach is unpopular.\n[BOS] Chooi-Ling Goh et al. [16] , Jianfeng Gao et al. [8] and Huaping Zhang [3] adopted character information to handle unknown words; X. Luo [11] , Yao Meng [12] and Shengfen Luo [17] each presented characterbased parsing models for Chinese parsing or new-word extraction.\n[BOS] T. Nakagawa used word-level information and character-level information for word segmentation [6] .\n[BOS] Hwee Tou Ng et al. [5] investigated word-based and character-based approaches and proposed a maximum entropy character-based POS analyzer.\n[BOS] Although the character tags proposed in this paper are essentially similar to some of the previous work mentioned above, here our focus is to integrate various word features with the characterbased model in such a way that the probability of the model is undistorted.\n[BOS] The proposed model is effective in acquiring word building rules.\n[BOS] To our knowledge, our work is the first character-based approach, which outperforms the word-based approaches for SIGHAN open test.\n[BOS] Also, our approach is versatile and can be easily integrated with existing morphological systems to achieve improved performance.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Single_summ", "Single_summ", "Reflection", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 24, "token_end": 61, "char_start": 133, "char_end": 261, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"[16]": "9077041", "[8]": "15121618", "[3]": null}, "Reference": {}}}, {"token_start": 64, "token_end": 97, "char_start": 266, "char_end": 401, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"[11]": "1165688", "[12]": null, "[17]": "188543"}, "Reference": {}}}, {"token_start": 98, "token_end": 115, "char_start": 408, "char_end": 501, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 120, "token_end": 151, "char_start": 514, "char_end": 652, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"[5]": "11383732"}, "Reference": {}}}]}
{"id": "4957155_2", "paragraph": "[BOS] Part of this work was described in an unpub- lished preprint (Dhingra et al., 2017b) .\n[BOS] The current paper extends that version and focuses exclusively on coreference relations.\n[BOS] We also report results on the WikiHop dataset, including the performance of the model in the low-data regime.\n\n", "discourse_tags": ["Narrative_cite", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 10, "token_end": 27, "char_start": 44, "char_end": 90, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dhingra et al., 2017b)": "15135049"}}}]}
{"id": "4957155_1", "paragraph": "[BOS] Syntactic-recency.\n[BOS] Recent work has used syntax, in the form of dependency trees, to replace the sequential recency bias in RNNs with a syntactic recency bias (Tai et al., 2015; Swayamdipta, 2017; Qian et al., 2017; .\n[BOS] However, syntax only looks at dependencies within sentence boundaries, whereas our focus here is on longer ranges.\n[BOS] Our resulting layer is structurally similar to GraphLSTMs (Peng et al., 2017) , with an additional attention mechanism over the graph edges.\n[BOS] However, while Peng et al. (2017) found that using coreference did not lead to any gains for the task of relation extraction, here we show that it has a positive impact on the reading comprehension task.\n[BOS] Self-Attention (Vaswani et al., 2017) models are becoming popular for modeling long-term dependencies, and may also benefit from coreference information to bias the learning of those dependencies.\n[BOS] Here we focus on recurrent layers and leave such an analysis to future work.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Reflection", "Reflection", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 31, "token_end": 63, "char_start": 135, "char_end": 225, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Tai et al., 2015;": "3033526", "Swayamdipta, 2017;": "108289603"}}}, {"token_start": 91, "token_end": 106, "char_start": 379, "char_end": 433, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Peng et al., 2017)": "2797612"}}}, {"token_start": 118, "token_end": 161, "char_start": 503, "char_end": 706, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Peng et al. (2017)": "2797612"}, "Reference": {}}}, {"token_start": 162, "token_end": 202, "char_start": 713, "char_end": 909, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "4957155_0", "paragraph": "[BOS] Entity-based models.\n[BOS] Ji et al. (2017) presented a generative model for jointly predicting the next word in the text and its gold-standard coreference annotation.\n[BOS] The difference in our work is that we look at the task of reading comprehension, and also work in the more practical setting of system extracted coreferences.\n[BOS] EntNets (Henaff et al., 2016 ) also maintain dynamic memory slots for entities, but do not use coreference signals and instead update all memories after reading each sentence, which leads to poor performance in the low-data regime (c.f.\n[BOS] Table 1) .\n[BOS] Yang et al. (2017) model references in text as explicit latent variables, but limit their work to text generation.\n[BOS] Kobayashi et al. (2016) used a pooling operation to aggregate entity information across multiple mentions.\n[BOS] also noted the importance of reference resolution for reading comprehension, and we compare our model to their one-hot pointer reader.\n\n", "discourse_tags": ["Transition", "Single_summ", "Reflection", "Single_summ", "Other", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 8, "token_end": 37, "char_start": 33, "char_end": 173, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ji et al. (2017)": "5564363"}, "Reference": {}}}, {"token_start": 69, "token_end": 116, "char_start": 345, "char_end": 575, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Henaff et al., 2016": "11243593"}, "Reference": {}}}, {"token_start": 127, "token_end": 151, "char_start": 605, "char_end": 719, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Yang et al. (2017)": "1899153"}, "Reference": {}}}, {"token_start": 152, "token_end": 172, "char_start": 726, "char_end": 832, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kobayashi et al. (2016)": "10239453"}, "Reference": {}}}]}
{"id": "6199004_0", "paragraph": "[BOS] Most of existing work in neural machine translation focus on integrating SMT strategies Zhou et al., 2017; Wang et al., 2017; Shen et al., 2015) , handling rare words Sennrich et al., 2016b; Luong et al., 2015b) and designing the better framework (Tu et al., 2016; Luong et al., 2015a; Meng et al., 2016 Table 4 : Effect of different data size of parallel corpus.\n[BOS] Method NMT Model means the result of conventional NMT system trained on these low-count parallel sentences.\n[BOS] Partially Aligned Model(MSE) + Para means the result of our model fine-tuned by these parallel sentences.\n[BOS] different scenarios has drawn intensive attention in recent years.\n[BOS] Actually, there have been some effective methods to deal with them.\n[BOS] We divide the related work into three categories:\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Transition", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 13, "token_end": 37, "char_start": 79, "char_end": 150, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Zhou et al., 2017;": "17848339", "Wang et al., 2017;": "5565551", "Shen et al., 2015)": "3913537"}}}, {"token_start": 38, "token_end": 60, "char_start": 153, "char_end": 217, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Sennrich et al., 2016b;": "1114678", "Luong et al., 2015b)": "1245593"}}}, {"token_start": 61, "token_end": 88, "char_start": 222, "char_end": 309, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Tu et al., 2016;": "16113848", "Luong et al., 2015a;": "1998416", "Meng et al., 2016": "416723"}}}]}
{"id": "49653963_2", "paragraph": "[BOS] More closely related to SRL is the AMR parsing and generation system proposed by Konstas et al. (2017) .\n[BOS] This work successfully constructs a two-way mapping: generation of text given AMR representations as well as AMR parsing of natural language sentences.\n[BOS] Finally, Zhang et al. (2017) went one step further by proposing a cross-lingual end-to-end system that learns to encode natural language (i.e. Chinese source sentences) and to decode them into sentences on the target side containing open semantic relations in English, using a parallel corpus for training.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 57, "char_start": 6, "char_end": 268, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Konstas et al. (2017)": "8066499"}, "Reference": {}}}, {"token_start": 58, "token_end": 123, "char_start": 275, "char_end": 581, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhang et al. (2017)": "7365249"}, "Reference": {}}}]}
{"id": "49653963_1", "paragraph": "[BOS] Sequence-to-sequence models.\n[BOS] Seq2seq models were first discovered as powerful models for Neural Machine Translation but soon proved to be useful for any kind of problem that could be represented as a mapping between source and target sequences.\n[BOS] Vinyals et al. (2015) demonstrate that constituent parsing can be formulated as a seq2seq problem by linearizing the parse tree.\n[BOS] They obtain close to state-of-the-art results by using a large automatically parsed dataset.\n[BOS] Dong and Lapata (2016) built a model for a related problem, semantic parsing, by mapping sentences to logical form.\n[BOS] Seq2seq models have also been widely used for language generation (e.g. Karpathy and Li (2015) ; Chisholm et al. (2017) ) given their ability to produce linguistic variation in the output sequences.\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ", "Single_summ", "Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 49, "token_end": 100, "char_start": 263, "char_end": 490, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Vinyals et al. (2015)": "14223"}, "Reference": {}}}, {"token_start": 101, "token_end": 126, "char_start": 497, "char_end": 612, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Dong and Lapata (2016)": "15412473"}, "Reference": {}}}, {"token_start": 127, "token_end": 163, "char_start": 619, "char_end": 738, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Karpathy and Li (2015)": null, "Chisholm et al. (2017)": "14633379"}}}]}
{"id": "49653963_0", "paragraph": "[BOS] Semantic Role Labeling.\n[BOS] Traditional approaches to SRL relied on carefully designed features and expensive techniques to achieve global consistency such as Integer Linear Programming (Punyakanok et al., 2008) or dynamic programming .\n[BOS] First neural SRL attempts tried to mix syntactic features with neural network representations.\n[BOS] For example, FitzGerald et al. (2015) created argument and role representations using a feed-forward NN, and used a graphical model to enforce global constraints.\n[BOS] Roth and Lapata (2016) , on the other hand, proposed a neural classifier using dependency path embeddings to assign semantic labels to syntactic arguments.\n[BOS] Collobert et al. (2011) proposed the first SRL neural model that did not depend on hand-crafted features and treated the task as an IOB sequence labeling problem.\n[BOS] Later, Zhou and Xu (2015) proposed a deep bi-directional LSTM model with a CRF layer on top.\n[BOS] This model takes only the original text as input and assigns a label to each individual word in the sentence.\n[BOS] He et al. (2017) also treat SRL as a IOB tagging problem, and use again a deep bi-LSTM incorporating highway connections, recurrent dropout and hard decoding constraints together with an ensemble of experts.\n[BOS] This represents the best performing system on two span-based benchmark datasets so far (namely, CoNLL-05 and CoNLL-12).\n[BOS] show that it is possible to construct a very accurate dependency-based SRL system without using any kind of explicit syntactic information.\n[BOS] In subsequent work, combine their LSTM model with a graph convolutional network to encode syntactic information at word level, which improves their LSTM classifier results on the dependency-based benchmark dataset (CoNLL-09).\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 26, "token_end": 40, "char_start": 167, "char_end": 219, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Punyakanok et al., 2008)": "11162815"}}}, {"token_start": 64, "token_end": 96, "char_start": 365, "char_end": 514, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"FitzGerald et al. (2015)": "15048880"}, "Reference": {}}}, {"token_start": 97, "token_end": 126, "char_start": 521, "char_end": 676, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Roth and Lapata (2016)": "5779419"}, "Reference": {}}}, {"token_start": 127, "token_end": 165, "char_start": 683, "char_end": 845, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Collobert et al. (2011)": "351666"}, "Reference": {}}}, {"token_start": 168, "token_end": 212, "char_start": 859, "char_end": 1060, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhou and Xu (2015)": "12688069"}, "Reference": {}}}, {"token_start": 213, "token_end": 287, "char_start": 1067, "char_end": 1400, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"He et al. (2017)": null}, "Reference": {}}}]}
{"id": "51999781_2", "paragraph": "[BOS] To the best of our knowledge, subcharacter information in Japanese has been addressed only by Nguyen et al. (2017) and Ke and Hagiwara (2017) .\n[BOS] The former consider the language modeling task and compare several kinds of kanji decomposition, evaluating on model perplexity.\n[BOS] Ke and Hagiwara (2017) propose to use subcharacter information instead of characters, showing that such a model performs on par with word and character-level models on sentiment classification, with considerably smaller vocabulary.\n[BOS] This study explores a model comparable to that proposed by Yu et al. (2017) for Chinese.\n[BOS] We jointly learn a representation of words, kanjis, and kanjis' components, and we evaluate it on similarity, analogy, and sentiment classification tasks.\n[BOS] We also contribute jBATS, the first analogy dataset for Japanese.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Single_summ", "Narrative_cite", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 9, "token_end": 36, "char_start": 36, "char_end": 147, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Nguyen et al. (2017)": "41916585", "Ke and Hagiwara (2017)": "3289431"}}}, {"token_start": 62, "token_end": 105, "char_start": 291, "char_end": 522, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ke and Hagiwara (2017)": "3289431"}, "Reference": {}}}, {"token_start": 106, "token_end": 126, "char_start": 529, "char_end": 617, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Yu et al. (2017)": "2760254"}}}]}
{"id": "51999781_1", "paragraph": "[BOS] Japanese kanjis were borrowed from Chinese, but it remains unclear whether these success stories could also carry over to Japanese.\n[BOS] Chinese is an analytic language, but Japanese is agglutinative, which complicates tokenization.\n[BOS] Also, in Japanese, words can be spelled either in kanji or in phonetic alphabets (hiragana and katakana), which further increases data sparsity.\n[BOS] Numerous homonyms make this sparse data also noisy.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Transition"], "span_citation_mapping": []}
{"id": "51999781_0", "paragraph": "[BOS] To date, most work on representing subcharacter information relies on language-specific resources that list character components 1 .\n[BOS] A growing list of papers address various combinations of wordlevel, character-level and subcharacter-level embeddings in Chinese (Sun et al., 2014; Li et al., 2015; Yu et al., 2017) .\n[BOS] They have been successful on a range of tasks, including similarity and analogy (Yu et al., 2017; Yin et al., 2016 ), text classification (Li et al., 2015) sentiment polarity classification (Benajiba et al., 2017) , segmentation, and POS-tagging (Shao et al., 2017) .\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 35, "token_end": 72, "char_start": 202, "char_end": 326, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sun et al., 2014;": "9459634", "Li et al., 2015;": "5604513", "Yu et al., 2017)": "2760254"}}}, {"token_start": 85, "token_end": 102, "char_start": 392, "char_end": 449, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yu et al., 2017;": "2760254", "Yin et al., 2016": "5756244"}}}, {"token_start": 104, "token_end": 114, "char_start": 453, "char_end": 490, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Li et al., 2015)": "5604513"}}}, {"token_start": 114, "token_end": 127, "char_start": 491, "char_end": 548, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Benajiba et al., 2017)": "3331956"}}}, {"token_start": 128, "token_end": 143, "char_start": 551, "char_end": 600, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Shao et al., 2017)": "14878319"}}}]}
{"id": "5532093_1", "paragraph": "[BOS] McDonald (2007) proposed to use Integer Linear Programming framework in multi-document sum-marization.\n[BOS] And Sauper and Barzilay (2009) use integer linear programming framework to automatically generate Wikipedia articles.\n[BOS] There is a fundamental difference between their method and ours.\n[BOS] They used trained perceptron algorithm for ranking excerpts, whereas we give an extended LexRank with integer linear programming to optimize sentence selection for our aspect-oriented multi-document summarization.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 22, "char_start": 6, "char_end": 108, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 24, "token_end": 45, "char_start": 119, "char_end": 232, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Sauper and Barzilay (2009)": "192339"}, "Reference": {}}}]}
{"id": "5532093_0", "paragraph": "[BOS] Our event-aspect model is related to a number of previous extensions of LDA models.\n[BOS] Chemudugunta et al. (2007) proposed to introduce a background topic and document-specific topics.\n[BOS] Our background and document language models are similar to theirs.\n[BOS] However, they still treat documents as bags of words rather then sets of sentences as in our models.\n[BOS] Titov and McDonald (2008) exploited the idea that a short paragraph within a document is likely to be about the same aspect.\n[BOS] The way we separate words into stop words, background words, document words and aspect words bears similarity to that used in (Daum III and Marcu, 2006; Haghighi and Vanderwende, 2009 ).\n[BOS] proposed a topic-aspect model for simultaneously finding topics and aspects.\n[BOS] The most related extension is entityaspect model proposed by Li et al. (2010) .\n[BOS] The main difference between event-aspect model and entityaspect model is our model further consider aspect granularity and add a layer to model topic-related events.\n[BOS] Filippova and Strube (2008) proposed a dependency tree based sentence compression algorithm.\n[BOS] Their approach need a large corpus to build language model for compression, whereas we prune dependency tree using grammatical rules.\n[BOS] proposed to modify LexRank algorithm using their topic-aspect model.\n[BOS] But their task is to summarize contrastive viewpoints in opinionated text.\n[BOS] Furthermore, they use a simple greedy approach for constructing summary.\n\n", "discourse_tags": ["Reflection", "Single_summ", "Reflection", "Reflection", "Single_summ", "Narrative_cite", "Transition", "Narrative_cite", "Reflection", "Single_summ", "Single_summ", "Other", "Transition", "Single_summ"], "span_citation_mapping": [{"token_start": 20, "token_end": 43, "char_start": 96, "char_end": 193, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Chemudugunta et al. (2007)": "2447517"}, "Reference": {}}}, {"token_start": 78, "token_end": 104, "char_start": 380, "char_end": 504, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Titov and McDonald (2008)": "13609860"}, "Reference": {}}}, {"token_start": 111, "token_end": 147, "char_start": 542, "char_end": 694, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Daum\u00e9 III and Marcu, 2006;": "6241932", "Haghighi and Vanderwende, 2009": "678258"}}}, {"token_start": 169, "token_end": 182, "char_start": 817, "char_end": 864, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Li et al. (2010)": "14942302"}}}, {"token_start": 216, "token_end": 258, "char_start": 1045, "char_end": 1277, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Filippova and Strube (2008)": "17477341"}, "Reference": {}}}]}
{"id": "51871017_2", "paragraph": "[BOS] In this work, we used systematic hypothesis testing over both failures and successes to identify the strategy used by machines to reach high performance on SQuAD.\n[BOS] Systematic testing based on automatically extracted features prevent us from relying on human explanation.\n[BOS] It also limits confirmation bias, which is a concern for qualitative analysis.\n[BOS] Human investigators will tend to explain errors in term of the human skills required, even when a simpler explanation is possible.\n[BOS] It is also important to confirm that the same explanation is not applicable to the models' successes.\n[BOS] Previous error analysis focused on errors, and ignored successes.\n\n", "discourse_tags": ["Reflection", "Reflection", "Transition", "Transition", "Transition", "Other"], "span_citation_mapping": []}
{"id": "51871017_1", "paragraph": "[BOS] Those related works indicates that word to word matching, similar to the reserved engineered strategy described in Section 8, is sufficient to obtain good performance on SQuAD.\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "51871017_0", "paragraph": "[BOS] FastQA (Weissenborn et al., 2017) added simple word matching features, indicating that a word was in both the passage and question, to a simple MRC model.\n[BOS] Those simple features improved performance using this simple MRC model.\n[BOS] We observed that variations of this feature were acceptable predictors of failures and successes Adversarial SQuAD (Jia and Liang, 2017) added distractor sentences at the end of SQuAD examples.\n[BOS] Model specific distractors were created by adding random words, guided by the target model's output, until it predicted a wrong answer.\n[BOS] The resulting sentences are ungrammatical and have no semantic significance, but match words present in the question.\n[BOS] Similarly, a more generic set of distractors was created using a simple set of rules to transform the question into a statement, and replacing keywords.\n[BOS] The resulting sentence is grammatical and meaningful, but is irrelevant to the question.\n[BOS] The significant number of word matches between the question and the distractor significantly reduces performance.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Transition", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 2, "token_end": 139, "char_start": 6, "char_end": 704, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Weissenborn et al., 2017)": "18769319"}, "Reference": {"(Jia and Liang, 2017)": "7228830"}}}]}
{"id": "51868236_2", "paragraph": "[BOS] Previous work have pointed out that BLEU scores of NMT systems drop as beam size increases (Britz et al., 2017; Tu et al., 2017; Koehn and Knowles, 2017) , and the existing length normalization and coverage models can alleviate this problem to some extent.\n[BOS] In this work we show that our method can do this much better.\n[BOS] Almost no BLEU drop is observed even when beam size is set to 500.\n\n", "discourse_tags": ["Narrative_cite", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 45, "char_start": 6, "char_end": 159, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Britz et al., 2017;": "2201909", "Tu et al., 2017;": "15830483", "Koehn and Knowles, 2017)": "8822680"}}}]}
{"id": "51868236_1", "paragraph": "[BOS] Perhaps the most related work to this paper is Wu et al. (2016) .\n[BOS] In their work, the coverage problem can be interpreted in a probability story.\n[BOS] However, it fails to account for the cases that one source word is translated into multiple target words and is thus of a total attention score > 1.\n[BOS] To address this issue, we remove the probability constraint and make the coverage score interpretable for different cases.\n[BOS] Another difference lies in that our coverage model is applied to every beam search step, while Wu et al. (2016) 's model affects only a small number of translation outputs.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Reflection", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 66, "char_start": 6, "char_end": 311, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wu et al. (2016)": "3603249"}, "Reference": {}}}, {"token_start": 106, "token_end": 125, "char_start": 542, "char_end": 619, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wu et al. (2016)": "3603249"}, "Reference": {}}}]}
{"id": "51868236_0", "paragraph": "[BOS] The length preference and coverage problems have been discussed for years since the rise of statistical machine translation (Koehn, 2009) .\n[BOS] In NMT, several good methods have been developed.\n[BOS] The simplest of these is length normalization which penalizes short translations in decoding (Wu et al., 2016) .\n[BOS] More sophisticated methods focus on modeling the coverage problem with extra sub-modules in NMT and require a training process (Tu et al., 2016; Mi et al., 2016) .\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 17, "token_end": 27, "char_start": 98, "char_end": 143, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Koehn, 2009)": "32015000"}}}, {"token_start": 51, "token_end": 63, "char_start": 270, "char_end": 318, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wu et al., 2016)": "3603249"}}}, {"token_start": 83, "token_end": 102, "char_start": 427, "char_end": 488, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Tu et al., 2016;": "146843", "Mi et al., 2016)": "14900221"}}}]}
{"id": "5571221_2", "paragraph": "[BOS] Most of the pivot strategies mentioned above focus on the situation of resource-poor languages where direct translation is either very poor or not available.\n[BOS] Our approach, like Dabre et al. (2014) , tries to employ pivot strategy to help improve the performance of existing SMT systems.\n[BOS] To the best of our knowledge, our work is the first attempt to integrate word segmentation with pivot-based SMT.\n\n", "discourse_tags": ["Transition", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 35, "token_end": 61, "char_start": 189, "char_end": 298, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Dabre et al. (2014)": "6167315"}, "Reference": {}}}]}
{"id": "5571221_1", "paragraph": "[BOS] -phrase translation (i.e. triangulation), transfer method and synthetic method.\n[BOS] Nakov and Ng (2012) try to exploit the similarity between resource-poor languages and resource-rich languages for the translation task.\n[BOS] Dabre et al. (2014) used multiple decoding paths (MDP) to overcome 303 the limitation of small sized corpora.\n[BOS] Paul et al. (2013) discusses criteria to be considered for selection of good pivot language.\n[BOS] Use of sourceside segmentation as pre-processing technique has been demonstrated by (Kunchukuttan et al., 2014) .\n[BOS] Goldwater and McClosky (2005) investigates several methods for incorporating morphological information to achieve better translation from Czech to English.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Narrative_cite", "Single_summ"], "span_citation_mapping": [{"token_start": 20, "token_end": 47, "char_start": 92, "char_end": 227, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Nakov and Ng (2012)": "2865563"}, "Reference": {}}}, {"token_start": 48, "token_end": 74, "char_start": 234, "char_end": 343, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Dabre et al. (2014)": "6167315"}, "Reference": {}}}, {"token_start": 75, "token_end": 95, "char_start": 350, "char_end": 442, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Paul et al. (2013)": "17222548"}, "Reference": {}}}, {"token_start": 100, "token_end": 122, "char_start": 467, "char_end": 560, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kunchukuttan et al., 2014)": null}}}, {"token_start": 124, "token_end": 149, "char_start": 569, "char_end": 724, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Goldwater and McClosky (2005)": "2557675"}, "Reference": {}}}]}
{"id": "5571221_0", "paragraph": "[BOS] There is substantial amount on pivot-based SMT.\n[BOS] De Gispert and Marino (2006) discuss translation tasks between Catalan and English with the use of Spanish as a pivot language.\n[BOS] Pivoting is done using two techniques: pipelining of source-pivot and pivot-target SMT systems and direct translation using a synthesized Catalan-English.\n[BOS] In Utiyama and Isahara (2007) , the authors propose the use of pivot language through -phrase translation (phrase table creation) and sentence translation.\n[BOS] Wu and Wang (2007) compare three pivot strategies viz.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 15, "token_end": 78, "char_start": 60, "char_end": 348, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Gispert and Marino (2006)": null}, "Reference": {}}}, {"token_start": 80, "token_end": 113, "char_start": 358, "char_end": 510, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Utiyama and Isahara (2007)": "8030425"}, "Reference": {}}}, {"token_start": 114, "token_end": 127, "char_start": 517, "char_end": 571, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wu and Wang (2007)": "3681367"}, "Reference": {}}}]}
{"id": "51878811_1", "paragraph": "[BOS] In addition to the direct application of the general seq2seq framework, researchers attempted to integrate various properties of summarization.\n[BOS] For example, Nallapati et al. (2016) enriched the encoder with hand-crafted features such as named entities and POS tags.\n[BOS] These features have played important roles in traditional feature based summarization systems.\n[BOS] Gu et al. (2016) found that a large proportion of the words in the summary were copied from the source text.\n[BOS] Therefore, they proposed CopyNet which considered the copying mechanism during generation.\n[BOS] Recently, See et al. (2017) used the coverage mechanism to discourage repetition.\n[BOS] Cao et al. (2017b) encoded facts extracted from the source sentence to enhance the summary faithfulness.\n[BOS] There were also studies to modify the loss function to fit the evaluation metrics.\n[BOS] For instance, Ayana et al. (2016) applied the Minimum Risk Training strategy to maximize the ROUGE scores of generated summaries.\n[BOS] Paulus et al. (2017) used the reinforcement learning algorithm to optimize a mixed objective function of likelihood and ROUGE scores.\n[BOS] Guu et al. (2017) also proposed to encode human-written sentences to improvement the performance of neural text generation.\n[BOS] However, they handled the task of Language Modeling and randomly picked an existing sentence in the training corpus.\n[BOS] In comparison, we develop an IR system to find proper existing summaries as soft templates.\n[BOS] Moreover, Guu et al. (2017) used a general seq2seq framework while we extend the seq2seq framework to conduct template reranking and template-aware summary generation simultaneously.\n\n", "discourse_tags": ["Transition", "Single_summ", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection", "Single_summ"], "span_citation_mapping": [{"token_start": 30, "token_end": 57, "char_start": 169, "char_end": 277, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Nallapati et al. (2016)": "8928715"}, "Reference": {}}}, {"token_start": 73, "token_end": 113, "char_start": 385, "char_end": 590, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Gu et al. (2016)": "8174613"}, "Reference": {}}}, {"token_start": 116, "token_end": 132, "char_start": 607, "char_end": 678, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"See et al. (2017)": null}, "Reference": {}}}, {"token_start": 133, "token_end": 155, "char_start": 685, "char_end": 789, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Cao et al. (2017b)": "19198109"}, "Reference": {}}}, {"token_start": 175, "token_end": 199, "char_start": 899, "char_end": 1014, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ayana et al. (2016)": "18318429"}, "Reference": {}}}, {"token_start": 200, "token_end": 226, "char_start": 1021, "char_end": 1154, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Paulus et al. (2017)": "21850704"}, "Reference": {}}}, {"token_start": 227, "token_end": 273, "char_start": 1161, "char_end": 1407, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Guu et al. (2017)": "2318481"}, "Reference": {}}}, {"token_start": 294, "token_end": 331, "char_start": 1522, "char_end": 1694, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Guu et al. (2017)": "2318481"}, "Reference": {}}}]}
{"id": "51878811_0", "paragraph": "[BOS] Abstractive sentence summarization aims to produce a shorter version of a given sentence while preserving its meaning (Chopra et al., 2016) .\n[BOS] This task is similar to text simplification (Saggion, 2017) and facilitates headline design and refine.\n[BOS] Early studies on sentence summariza-Source anny ainge said thursday he had two one-hour meetings with the new owners of the boston celtics but no deal has been completed for him to return to the franchise .\n[BOS] (Zhou and Hovy, 2004) , syntactic tree pruning (Knight and Marcu, 2002; Clarke and Lapata, 2008) and statistical machine translation techniques (Banko et al., 2000) .\n[BOS] Recently, the application of the attentional seq2seq framework has attracted growing attention and achieved state-of-the-art performance on this task (Rush et al., 2015a; Chopra et al., 2016; Nallapati et al., 2016) .\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Transition", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 2, "token_end": 30, "char_start": 6, "char_end": 145, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chopra et al., 2016)": "133195"}}}, {"token_start": 32, "token_end": 54, "char_start": 154, "char_end": 257, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 104, "token_end": 112, "char_start": 477, "char_end": 498, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhou and Hovy, 2004)": null}}}, {"token_start": 113, "token_end": 132, "char_start": 501, "char_end": 573, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Knight and Marcu, 2002;": "7793213", "Clarke and Lapata, 2008)": "3004447"}}}, {"token_start": 133, "token_end": 146, "char_start": 578, "char_end": 641, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Banko et al., 2000)": "9952653"}}}, {"token_start": 154, "token_end": 203, "char_start": 683, "char_end": 865, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Chopra et al., 2016;": "133195", "Nallapati et al., 2016)": "8928715"}}}]}
{"id": "5765428_1", "paragraph": "[BOS] Recently, there has been a significant effort to improve the accuracy of classifiers by going beyond vector representation for sentences.\n[BOS] Notably the work of Peng et al. (Peng et al., 2017) introduces graph LSTMs to encode the meaning of a sentence by using dependency graphs.\n[BOS] Similarly Dhingra et al. (Dhingra et al., 2017) employ Gated Recurrent Units (GRUs) that encode the information of acyclic graphs to achieve state-of-the-art results in co-reference resolution.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 29, "token_end": 58, "char_start": 170, "char_end": 288, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Peng et al., 2017)": "2797612"}, "Reference": {}}}, {"token_start": 60, "token_end": 107, "char_start": 305, "char_end": 488, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Dhingra et al., 2017)": "15135049"}, "Reference": {}}}]}
{"id": "5765428_0", "paragraph": "[BOS] There is a large corpus of work on named entity recognition, with few studies using explicitly non-local information for the task.\n[BOS] One early work by Finkel et al. (Finkel et al., 2005) uses Gibbs sampling to capture long distance structures that are common in language use.\n[BOS] Another article by the same authors uses a joint representation for constituency parsing and NER, improving both techniques.\n[BOS] In addition, dependency structures have also been used to boost the recognition of bio-medical events (McClosky et al., 2011) and for automatic content extraction (Li et al., 2013) .\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 32, "token_end": 61, "char_start": 161, "char_end": 285, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 96, "token_end": 112, "char_start": 491, "char_end": 548, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(McClosky et al., 2011)": "2941631"}}}, {"token_start": 114, "token_end": 125, "char_start": 557, "char_end": 603, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Li et al., 2013)": "2114517"}}}]}
{"id": "54637_0", "paragraph": "[BOS] In fact, the effectiveness of using dependency information of words has been reported in some previous NLP tasks, for example, in dependencybased word embeddings, relation classification and sentence classification tasks (Liu et al., 2015; Socher et al., 2014; Levy and Goldberg, 2014; Komnios, 2016; Ono and Hatano, 2014) .\n[BOS] It has been shown that the combination of words and their dependency information can boost performance.\n[BOS] Besides, in the work of Vinyals et al. , they also represent output as a linearized tree structure, but their work showed that generic sequence-to-sequence approaches can achieve excellent results on syntactic constituency parsing.\n[BOS] At a glance, our proposed method is a little similar to the works of Dyer et al., Aharoni et al., Eriguchi et al., Wu et al. (Dyer et al., 2016; Aharoni and Goldberg, 2017; Eriguchi et al., 2017; Wu et al., 2017) On the other hand, our proposed model's decoder directly predicts the linearized dependency tree itself in a single neural network in Depth-first preorder order so that the next-word token is generated based on syntactic relations and tree construction itself.\n[BOS] In other words, our model is able to learn and produce a tree of words and their dependency relations by itself.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Single_summ", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 27, "token_end": 74, "char_start": 136, "char_end": 328, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Liu et al., 2015;": "7475429", "Socher et al., 2014;": "2317858", "Levy and Goldberg, 2014;": null, "Komnios, 2016;": "17540759", "Ono and Hatano, 2014)": null}}}, {"token_start": 94, "token_end": 139, "char_start": 447, "char_end": 678, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 145, "token_end": 213, "char_start": 698, "char_end": 897, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Aharoni and Goldberg, 2017;": "8078153", "Eriguchi et al., 2017;": "14519034", "Wu et al., 2017)": "41550072"}}}]}
{"id": "51874381_1", "paragraph": "[BOS] Due to the continuous rise of machine learning in recent years, the field of natural language processing is increasingly turning to machine learning.\n[BOS] In the past few years, the diagnosis of Chinese grammatical errors has also been developing in machine learning.\n[BOS] Grammatical error detection is usually considered as the sequence labeling task (Zheng et al., 2016) .\n[BOS] (Huang and WANG, 2016) used Bi-LSTM to annotate the errors in the sentence.\n[BOS] (Shiue et al., 2017) combined machine learning with traditional n-gram methods, using Bi-LSTM to detect the location of errors and adding additional linguistic information, POS, ngram.\n[BOS] Table 10 : Result on false positive rate in CGED2018 the probability of each characters, and used two strategies to decide whether a character is correct or not.\n[BOS] (Liao et al., 2017 ) used the LSTM+CRF model to detect dependencies between outputs to better detect error messages.\n[BOS] (yang et al., 2017) added more linguistic information on LSTM+CRF model, such as POS, n-gram, PMI score and dependency features.\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite", "Single_summ", "Single_summ", "Transition", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 57, "token_end": 68, "char_start": 338, "char_end": 381, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zheng et al., 2016)": "14642733"}}}, {"token_start": 70, "token_end": 91, "char_start": 390, "char_end": 465, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 92, "token_end": 133, "char_start": 472, "char_end": 656, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Shiue et al., 2017)": "29721010"}, "Reference": {}}}, {"token_start": 168, "token_end": 195, "char_start": 831, "char_end": 947, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Liao et al., 2017": "818358"}, "Reference": {}}}, {"token_start": 196, "token_end": 230, "char_start": 954, "char_end": 1082, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "51874381_0", "paragraph": "[BOS] Chinese grammatical error diagnosis task has been developed for a long time.\n[BOS] From the initial statistical methods to the current machine learning, more and more attention has been paid to.\n[BOS] (Zhang et al., 2000) searched the optimal string from all possible derivation of the input sentence using operations of character substitution, insertion, and deletion with a traditional word 3-gram language model.\n[BOS] (Chen et al., 2013) still used n-gram as the main method, and added Web resources to improve detection results.\n[BOS] (Lin and Chu, 2015) used n-gram to establish a scoring system to better give correction options.\n[BOS] (Yeh et al., 2017 ) based on n-gram used the KMP algorithm to speed up the search for correct candidates.\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 37, "token_end": 77, "char_start": 207, "char_end": 421, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Zhang et al., 2000)": "29966419"}, "Reference": {}}}, {"token_start": 78, "token_end": 105, "char_start": 428, "char_end": 539, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Chen et al., 2013)": null}, "Reference": {}}}, {"token_start": 106, "token_end": 128, "char_start": 546, "char_end": 642, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Lin and Chu, 2015)": "41845669"}, "Reference": {}}}, {"token_start": 129, "token_end": 157, "char_start": 649, "char_end": 754, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Yeh et al., 2017": "20596555"}, "Reference": {}}}]}
{"id": "6047409_5", "paragraph": "[BOS] We observe that SNLI-Finetuned Skipthoughts outperform Skipthoughts on this task as well, supporting the conjecture that adding supervision through SNLI has lead to a more informative space.\n[BOS] Moreover, the performance of SNLI RNN is impressive, outperforming both Skipthoughtbased models.\n[BOS] Finally, out of the BOW models, the SNLI one performs better than the AE one.\n[BOS] These are indications that SNLI information can aid in inducing a notion of similarity which is compatible with human intuition.\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "6047409_4", "paragraph": "[BOS] As was the case for the MSRP dataset, we believe that SICK offers an unfair advantage to BOW models, therefore we do not believe that the success of BOW AE and SNLI BOW is necessarily indicative of their quality.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "6047409_3", "paragraph": "[BOS] SICK is comprised of pairs of sentences, each associated with a relatedness score in the range from 1 to 5.\n[BOS] In order to directly evaluate the merit of the embeddings in capturing semantics, we used cosine similarity to estimate the relatedness of each pair.\n[BOS] These similarity scores were then correlated with the human-annotated scores using Pearson's and Spearman's correlation coefficients and mean squared error.\n[BOS] The results are shown in Table 7 : Results on semantic relatedness (SICK) based on cosine distances.\n[BOS] PR, SR, SE: Pearson, Spearman correlation coefficient and mean squared error, resp.\n[BOS] between model scores and human scores.\n\n", "discourse_tags": ["Other", "Reflection", "Reflection", "Other", "Other", "Other"], "span_citation_mapping": []}
{"id": "6047409_2", "paragraph": "[BOS] Transfer learning is the process of exploiting knowledge from one task or domain in order to benefit from it for a different (\"target\") task or domain.\n[BOS] This method has enjoyed considerable success in computer vision applications (notably the use of features derived from neural networks trained for object classification such as (Krizhevsky et al., 2012) for other tasks) but is less successful in language applications.\n[BOS] Collobert and Weston (2008) perform mutli-task learning on various natural language processing tasks and report a very small gain for each task.\n[BOS] Mou et al. (2016) presented negative results on their effort to transfer from the task of natural language inference to paraphrase detection.\n[BOS] In this work, we show positive results on transfer from natural language inference to paraphrase detection and to the related task of paraphrase ranking.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 45, "token_end": 72, "char_start": 241, "char_end": 366, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Krizhevsky et al., 2012)": "207763512"}}}, {"token_start": 85, "token_end": 115, "char_start": 439, "char_end": 583, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Collobert and Weston (2008)": "2617020"}, "Reference": {}}}, {"token_start": 116, "token_end": 145, "char_start": 590, "char_end": 731, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Mou et al. (2016)": null}, "Reference": {}}}]}
{"id": "6047409_1", "paragraph": "[BOS] Aside from Skipthoughts, there are numerous sentence encoders.\n[BOS] (Socher et al., 2013; Yin and Schtze, 2015; Wang and Nyberg, 2015; Socher et al., 2014) create sentence encoders which are optimized for a specific task of interest.\n[BOS] On the other hand, methods which aim at constructing \"universal\" embeddings include (Le and Mikolov, 2014; Socher et al., 2011; Li et al., 2015; Pham et al., 2015) .\n[BOS] Le and Mikolov (2014) learn paragraph embeddings by predicting sentences within a paragraph when conditioned on its representation, Pham et al. (2015) predict context in all levels of a syntactic tree, whereas Socher et al. (2011) and Li et al. (2015) present autoencoder-type models.\n[BOS] Hill et al. (2016) presented an extensive evaluation of unsupervised sentence encoders.\n[BOS] They showed that Bag of Words (BOW) models on average perform on par with non-BOW models.\n[BOS] Our results agree with this and we provide a possible explanation through examining the statistics of the datasets used for evaluation.\n[BOS] An important distinction between our work and theirs is that Hill et al. (2016) focused on models that were trained in an unsupervised fashion, whereas we also present models finetuned or trained on SNLI for natural language inference.\n[BOS] Wieting et al. (2015) learned \"universal\" sentence vectors by exploiting a database of paraphrases: they optimize an objective which encourages paraphrases to lie closer to each other in space than to negative examples.\n[BOS] Similarly to their work, we also use supervised information to construct informative embeddings, but our supervision comes from the task of natural language inference.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite", "Multi_summ", "Single_summ", "Single_summ", "Reflection", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 16, "token_end": 62, "char_start": 75, "char_end": 240, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Socher et al., 2013;": "990233", "Yin and Sch\u00fctze, 2015;": "17578970", "Wang and Nyberg, 2015;": "2654931"}}}, {"token_start": 74, "token_end": 110, "char_start": 301, "char_end": 410, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Le and Mikolov, 2014;": "2407601", "Socher et al., 2011;": "6979578", "Li et al., 2015;": "207468", "Pham et al., 2015)": "10028211"}}}, {"token_start": 112, "token_end": 134, "char_start": 419, "char_end": 549, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Le and Mikolov (2014)": "2407601"}, "Reference": {}}}, {"token_start": 135, "token_end": 152, "char_start": 551, "char_end": 619, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Pham et al. (2015)": "10028211"}, "Reference": {}}}, {"token_start": 154, "token_end": 178, "char_start": 629, "char_end": 703, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Socher et al. (2011)": "6979578", "Li et al. (2015)": "207468"}, "Reference": {}}}, {"token_start": 179, "token_end": 218, "char_start": 710, "char_end": 893, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hill et al. (2016)": "2937095"}, "Reference": {}}}, {"token_start": 253, "token_end": 289, "char_start": 1103, "char_end": 1277, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hill et al. (2016)": "2937095"}, "Reference": {}}}, {"token_start": 290, "token_end": 335, "char_start": 1284, "char_end": 1503, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wieting et al. (2015)": "5882977"}, "Reference": {}}}]}
{"id": "6047409_0", "paragraph": "[BOS] The Skip-gram model for word embeddings (Mikolov et al., 2013a; Mikolov et al., 2013b) , is trained on a text corpus with the objective of predicting the vectors of the surrounding words of a given word, when conditioned on its vector representation.\n[BOS] Its success inspired to create its sentence analogue, Skipthought vectors, which are trained by predicting the surrounding sentences when conditioned on the current one.\n[BOS] Despite this simple objective, Skipthoughts perform remarkably well on various tasks: semantic relatedness, paraphrase detection, imagesentence ranking, and a number of classification benchmarks.\n[BOS] In this paper we investigate how we can improve their embedding space through injecting small amounts of supervised information.\n\n", "discourse_tags": ["Multi_summ", "Transition", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 8, "token_end": 31, "char_start": 30, "char_end": 92, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mikolov et al., 2013a;": "5959482", "Mikolov et al., 2013b)": "16447573"}}}]}
{"id": "588859_2", "paragraph": "[BOS] As with machine translation (MT), evaluation of mation track.\n[BOS] We leave for future work the adaptation of our approach to that task.\n[BOS] (Napoles et al., 2015) .\n[BOS] The system of Felice et al. (2014) ranked highest, utilizing a combination of a rule-based system and phrase-based MT, with re-ranking via a large web-scale language model.\n[BOS] Of the non-MT based approaches, the IllinoisColumbia system was a strong performer, combining several classifiers trained for specific types of errors (Rozovskaya et al., 2014) .\n\n", "discourse_tags": ["Transition", "Reflection", "Narrative_cite", "Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 32, "token_end": 41, "char_start": 150, "char_end": 172, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Napoles et al., 2015)": null}}}, {"token_start": 43, "token_end": 85, "char_start": 181, "char_end": 353, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Felice et al. (2014)": "16548363"}, "Reference": {}}}, {"token_start": 88, "token_end": 127, "char_start": 367, "char_end": 536, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rozovskaya et al., 2014)": "1547333"}}}]}
{"id": "588859_1", "paragraph": "[BOS] More recent work has emerged as a result of a series of shared tasks, starting with the Helping Our Own (HOO) Pilot Shared Task run in 2011, which focused on a diverse set of errors in a small dataset (Dale and Kilgarriff, 2011) , and the subsequent HOO 2012 Shared Task, which focused on the automated detection and correction of preposition and determiner errors (Dale et al., 2012) .\n[BOS] The CoNLL-2013 Shared Task (Ng et al., 2013 3 focused on the correction of a limited set of five error types in essays by second-language learners of English at the National University of Singapore.\n[BOS] The follow-up CoNLL-2014 Shared Task (Ng et al., 2014 4 focused on the full generation task of correcting all errors in essays by second-language learners.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 20, "token_end": 57, "char_start": 94, "char_end": 234, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dale and Kilgarriff, 2011)": "18357549"}}}, {"token_start": 59, "token_end": 91, "char_start": 241, "char_end": 390, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dale et al., 2012)": "11159647"}}}, {"token_start": 93, "token_end": 137, "char_start": 399, "char_end": 597, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 138, "token_end": 175, "char_start": 604, "char_end": 759, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "588859_0", "paragraph": "[BOS] While this is the first year for a shared task focusing on sentence-level binary error identification, previous work and shared tasks have focused on the related tasks of intra-sentence identification and correction of errors.\n[BOS] Until recently, standard handannotated grammatical error datasets were not available, complicating comparisons and limiting the choice of methods used.\n[BOS] Given the lack of a large hand-annotated corpus at the time, Park and Levy (2011) demonstrated the use of the EM algorithm for parameter learning of a noise model using error data without corrections, performing evaluation on a much smaller set of sentences hand-corrected by Amazon Mechanical Turk workers.\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ"], "span_citation_mapping": [{"token_start": 85, "token_end": 129, "char_start": 458, "char_end": 704, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Park and Levy (2011)": "7784892"}, "Reference": {}}}]}
{"id": "573977_2", "paragraph": "[BOS] Some approaches to transliteration mining are also relevant to the present work.\n[BOS] Tao et al. (2006) show improvement in transliteration mining performance using phonetic feature vectors resembling the ones we have used.\n[BOS] Jagarlamudi and Daum III (2012) use phonemic representa- We use a substring-based log-linear model in our second stage.\n[BOS] There are some parallels to this approach in the transliteration mining litereature.\n[BOS] Some transliteration mining approaches have used a log-linear classifier to incorporate features to distinguish transliterations from non-transliterations (Klementiev and Roth, 2006; Chang et al., 2009 ).\n[BOS] Sajjad et al. (2011) use a substring-based log-linear model trained on a noisy, intermediate transliteration corpus to iteratively remove bad (lowscoring) transliteration pairs found in the discovery process.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Transition", "Narrative_cite", "Single_summ"], "span_citation_mapping": [{"token_start": 18, "token_end": 44, "char_start": 93, "char_end": 230, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Tao et al. (2006)": "8424232"}, "Reference": {}}}, {"token_start": 45, "token_end": 78, "char_start": 237, "char_end": 356, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Jagarlamudi and Daum\u00e9 III (2012)": "3176110"}, "Reference": {}}}, {"token_start": 116, "token_end": 141, "char_start": 566, "char_end": 655, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Klementiev and Roth, 2006;": "669616", "Chang et al., 2009": "1133145"}}}, {"token_start": 144, "token_end": 192, "char_start": 665, "char_end": 873, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Sajjad et al. (2011)": "1540379"}, "Reference": {}}}]}
{"id": "573977_1", "paragraph": "[BOS] Our character level model approach is a further generalization of Ravi and Knight (2009) 's work since it also allows modelling of prior linguistic knowledge in the learning process.\n[BOS] This overcomes the most significant gap in their work.\n\n", "discourse_tags": ["Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 3, "token_end": 20, "char_start": 10, "char_end": 94, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Ravi and Knight (2009)": "2717337"}}}]}
{"id": "573977_0", "paragraph": "[BOS] Unsupervised transliteration has not been widely explored.\n[BOS] Chinnakotla et al. (2010) generate transliteration candidates using manually developed character mapping rules and rerank them with a character language model.\n[BOS] The major limitations are: (i) character transliteration probability is not learnt, so there is undue reliance on the language model to handle ambiguity, and (ii) significant manual effort for good coverage of mapping rules.\n[BOS] Ravi and Knight (2009) propose a decipherment framework based approach (Knight et al., 2006) to learn phoneme mappings for transliteration without parallel data.\n[BOS] In theory, it should be able to learn transliteration probabilities and is a generalization of Chinnakotla et al. (2010) 's approach.\n[BOS] But its performance is very poor due to lack of linguistic knowledge and has a reasonable performance only when a unigram word-level LM is used.\n[BOS] This signal essentially reduces the approach to a lookup for the generated transliterations in a target language word list; the method resembles transliteration mining.\n[BOS] It will perform well only if the unigram LM has a good coverage of all named entities in the source word list.\n[BOS] For morphologically rich target languages, it may be difficult to find the exact surface words in the unigram LM.\n\n", "discourse_tags": ["Transition", "Single_summ", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 13, "token_end": 44, "char_start": 71, "char_end": 230, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Chinnakotla et al. (2010)": "8996155"}, "Reference": {}}}, {"token_start": 92, "token_end": 272, "char_start": 468, "char_end": 1332, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ravi and Knight (2009)": "2717337"}, "Reference": {"(Knight et al., 2006)": "5715338", "Chinnakotla et al. (2010)": "8996155"}}}]}
{"id": "51876026_0", "paragraph": "[BOS] Many neural networks have been proposed to solve answer span QA task.\n[BOS] Ranking continuous text spans within a passage was proposed by Yu et al. (2016) and Lee et al. (2016) .\n[BOS] Wang and Jiang (2017) combine match-LSTM, originally introduced in (Wang and Jiang, 2016) and pointer networks to produce the boundary of the answer.\n[BOS] Since then, most of the models adopted pointer networks as a prediction layer and then focused on improving other layers.\n[BOS] Some methods focused on devising more accurate attention method; Seo et al. (2017); ; Xiong et al. (2017) employ attention mechanism to match the question context mutually; In addition, apply multi-layer attention and Huang et al. (2017b) expand to multi-level attention to get more enriched attention information.\n[BOS] Other approaches use contextualized word representations to further improve the performance.\n[BOS] Salant and Berant (2017); Peters et al. (2018) utilize embedding from pre-trained language model as an additional feature and Yu et al. (2018) select machine translation model instead.\n[BOS] Also, there are few attempts at augmenting memory capacity of the model (Hu et al., 2017; Pan et al., 2017) .\n[BOS] Hu et al. (2017) refine the contextual representation with multi-hops, and Pan et al. (2017) simply use the encoded query representations as a memory vector for refining the answer prediction, which are not meant to handle long-range dependency that we consider in this work.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Single_summ", "Transition", "Multi_summ", "Transition", "Multi_summ", "Narrative_cite", "Multi_summ"], "span_citation_mapping": [{"token_start": 16, "token_end": 41, "char_start": 82, "char_end": 183, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Yu et al. (2016)": "13094204", "Lee et al. (2016)": "6537520"}}}, {"token_start": 43, "token_end": 76, "char_start": 192, "char_end": 341, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wang and Jiang (2017)": "5592690"}, "Reference": {"(Wang and Jiang, 2016)": "11004224"}}}, {"token_start": 111, "token_end": 147, "char_start": 541, "char_end": 689, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Seo et al. (2017);": "8535316", "Xiong et al. (2017)": "3714278"}, "Reference": {}}}, {"token_start": 148, "token_end": 169, "char_start": 694, "char_end": 790, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Huang et al. (2017b)": "11480374"}, "Reference": {}}}, {"token_start": 184, "token_end": 212, "char_start": 896, "char_end": 1017, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Salant and Berant (2017);": "3834355", "Peters et al. (2018)": "3626819"}, "Reference": {}}}, {"token_start": 213, "token_end": 226, "char_start": 1022, "char_end": 1080, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Yu et al. (2018)": "4842909"}, "Reference": {}}}, {"token_start": 229, "token_end": 256, "char_start": 1093, "char_end": 1194, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hu et al., 2017;": "64515663", "Pan et al., 2017)": "22177955"}}}, {"token_start": 258, "token_end": 273, "char_start": 1203, "char_end": 1272, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hu et al. (2017)": "64515663"}, "Reference": {}}}, {"token_start": 275, "token_end": 315, "char_start": 1278, "char_end": 1478, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Pan et al. (2017)": "22177955"}, "Reference": {}}}]}
{"id": "51881445_1", "paragraph": "[BOS] Another research line comes from the utilizing of knowledge bases.\n[BOS] A typical application is question-answering (QA) systems.\n[BOS] The end-toend QA also resort to the encoder-decoder framework (Yin et al., 2016; He et al., 2017a) .\n[BOS] Yin et al. (2016) enquired the knowledge-base to achieve one fact and answer the simple factoid questions by referring to the fact.\n[BOS] He et al. (2017a) extended this approach by augmenting the copying mechanism and enabled the output words to copy from the original input sequence.\n[BOS] Eric et al. (2017) noticed that neural task-oriented dialogue systems often struggle to smoothly interface with a knowledge base and they addressed the problem by augmenting the end-to-end structure with a key-value retrieval mechanism where a separate attention is performed over the key of each entry in the KB.\n[BOS] Ghazvininejad et al. (2017) represented the unstructured text as bag of words representation and also performed soft attention over the facts to retrieve a facts vector.\n[BOS] Zhu et al. (2017) generated responses with any number of answer entities in the structured KB, even when these entities never appear in the training set.\n[BOS] Dhingra et al. (2017) proposed a multi-turn dialogue agent which helps users search knowledge base by soft KB lookup.\n[BOS] In our model, we perform not only facts matching to answer factoid inquiries, but also entity diffusion to infer similar entities.\n[BOS] Given previous utterances, we retrieve the relevant facts, diffuse them, and generate responses based on diversified rele-vant knowledge items.\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 27, "token_end": 57, "char_start": 143, "char_end": 241, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yin et al., 2016;": "14039866", "He et al., 2017a)": "43225062"}}}, {"token_start": 59, "token_end": 90, "char_start": 250, "char_end": 381, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Yin et al. (2016)": "14039866"}, "Reference": {}}}, {"token_start": 91, "token_end": 121, "char_start": 388, "char_end": 535, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"He et al. (2017a)": "43225062"}, "Reference": {}}}, {"token_start": 122, "token_end": 184, "char_start": 542, "char_end": 855, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Eric et al. (2017)": "5932528"}, "Reference": {}}}, {"token_start": 185, "token_end": 220, "char_start": 862, "char_end": 1031, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 221, "token_end": 252, "char_start": 1038, "char_end": 1191, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhu et al. (2017)": "35440547"}, "Reference": {}}}, {"token_start": 253, "token_end": 280, "char_start": 1198, "char_end": 1315, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Dhingra et al. (2017)": "8951658"}, "Reference": {}}}]}
{"id": "51881445_0", "paragraph": "[BOS] The successes of sequence-to-sequence architecture (Cho et al., 2014; Sutskever et al., 2014) motivated investigation in dialogue systems that can effectively learn to generate a response sequence given the previous utterance sequence (Shang et al., 2015; Sordoni et al., 2015b; Vinyals and Le, 2015) .\n[BOS] The model is trained to minimize the negative log-likelihood of the training data.\n[BOS] Despite the current progress, the lack of response diversity is a notorious problem, where the model inherently tends to generate short, general responses in spite of different inputs.\n[BOS] Li et al. (2016a) ; Serban et al. (2017); Cao and Clark (2017) suggested that theses boring responses are common in training data and shorter responses are more likely to be given a higher likelihood.\n[BOS] To tackle the problem, Li et al. (2016a) introduced a maximum mutual information training objective.\n[BOS] Serban et al. (2017) , Cao and Clark (2017) and Chen et al. (2018) used latent variables to introduce stochasticity to enhance the response diversity.\n[BOS] Vijayakumar et al. (2016) , Shao et al. (2017) and Li et al. (2016b) recognized that the greedy search decoding process, especially beam-search with a wide beam size, leads the short responses possess higher likelihoods.\n[BOS] They reserved more diverse candidates during beam-search decoding.\n[BOS] In this paper, we present that the absence of background knowledge and common sense is another source of lacking diversity.\n[BOS] We augment the knowledge base to endto-end dialogue generation.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Multi_summ", "Single_summ", "Multi_summ", "Multi_summ", "Multi_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 6, "token_end": 29, "char_start": 23, "char_end": 99, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cho et al., 2014;": "5590763", "Sutskever et al., 2014)": "7961699"}}}, {"token_start": 32, "token_end": 75, "char_start": 127, "char_end": 306, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Shang et al., 2015;": "7356547", "Sordoni et al., 2015b;": "94285", "Vinyals and Le, 2015)": "12300158"}}}, {"token_start": 129, "token_end": 178, "char_start": 595, "char_end": 795, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Li et al. (2016a)": "7287895", "Serban et al. (2017);": "14857825", "Cao and Clark (2017)": "17611516"}, "Reference": {}}}, {"token_start": 184, "token_end": 200, "char_start": 825, "char_end": 902, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Li et al. (2016a)": "7287895"}, "Reference": {}}}, {"token_start": 201, "token_end": 237, "char_start": 909, "char_end": 1059, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Serban et al. (2017)": "14857825", "Cao and Clark (2017)": "17611516", "Chen et al. (2018)": "4895052"}, "Reference": {}}}, {"token_start": 238, "token_end": 306, "char_start": 1066, "char_end": 1359, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Vijayakumar et al. (2016)": "44614", "Shao et al. (2017)": "17431796", "Li et al. (2016b)": "14563749"}, "Reference": {}}}]}
{"id": "51871198_1", "paragraph": "[BOS] Statistical method can be divided into two categories: traditional machine learning algorithm based on feature extraction engineering (Ahn, 2006) , (Ji and Grishman, 2008) , (Liao and Grishman, 2010) , (Reichart and Barzilay, 2012) and neural network algorithm based on automatic feature extraction (Chen et al., 2015) , (Nguyen et al., 2016) , .\n[BOS] The pattern method is usually used in industry because it can achieve higher accuracy, but meanwhile a lower recall.\n[BOS] In order to improve recall, there are two main research directions: build relatively complete pattern library and use a semi-automatic method to build trigger dictionary , (Gu et al., 2016) .\n[BOS] Hybrid event-extraction methods combine statistical methods and pattern-based methods together (Jungermann and Morik, 2008) , (Bjorne et al., 2010) .\n[BOS] To our best knowledge, there is no system that automatically generates labeled data, and extracts document-level events automatically from announcements in Chinese financial field.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Narrative_cite", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 17, "token_end": 58, "char_start": 109, "char_end": 237, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ahn, 2006)": null, "(Ji and Grishman, 2008)": "1320606", "(Liao and Grishman, 2010)": "11187670", "(Reichart and Barzilay, 2012)": "18462140"}}}, {"token_start": 59, "token_end": 84, "char_start": 242, "char_end": 348, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chen et al., 2015)": "14339673", "(Nguyen et al., 2016)": "6452487"}}}, {"token_start": 122, "token_end": 147, "char_start": 550, "char_end": 671, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Gu et al., 2016)": "8174613"}}}, {"token_start": 155, "token_end": 183, "char_start": 720, "char_end": 827, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Jungermann and Morik, 2008)": "1942185", "(Bjorne et al., 2010)": "17984630"}}}]}
{"id": "51871198_0", "paragraph": "[BOS] The current EE approaches can be mainly classified into statistical methods, pattern-based method and hybrid method (Hogenboom et al., 2016) .\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 11, "token_end": 32, "char_start": 62, "char_end": 146, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hogenboom et al., 2016)": "41089825"}}}]}
{"id": "5730387_2", "paragraph": "[BOS] In technical terms, our work builds on our recent tensor-based approach for dependency parsing .\n[BOS] In that work, we use a three-way tensor to score candidate dependency relations within a first-order scoring function.\n[BOS] The tensor captures the interaction between words and their syntactic (headmodifier) relations.\n[BOS] In contrast, the scoring function in SRL involves higher-order interactions between the path, argument, predicate and their associated role label.\n[BOS] Therefore, we parametrized the scoring function with a four-way low-rank tensor.\n[BOS] To help with this extension, we developed a new initialization and update strategy.\n[BOS] Our experimental results demonstrate that the new representation tailored to SRL outperforms previous approaches.\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection", "Reflection", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "5730387_1", "paragraph": "[BOS] More recent approaches explored a broader range of features.\n[BOS] Among others, Toutanova et al. (2008) , Martins and Almeida (2014) and Yang and Zong (2014) have explored high-order features involving several arguments and even pairs of sentence predicates.\n[BOS] Other approaches have focused on semantic generalizations of lexical features using selectional preferences, neural network embeddings or latent word language models (Zapirain et al., 2013; Collobert et al., 2011; Deschacht and Moens, 2009; Roth and Woodsend, 2014) .\n[BOS] To avoid the intensive feature engineering inherent in SRL, Moschitti et al. (2008) employ kernel learning.\n[BOS] Although attractive from this perspective, the kernel-based approach comes with a high computational cost.\n[BOS] In contrast to prior work, our approach effectively learns lowdimensional representation of words and their roles, eliminating the need for heavy manual feature engineering.\n[BOS] Finally, system combination approaches such as reranking typically outperform individual systems (Bjrkelund et al., 2010) .\n[BOS] Our method can be easily integrated as a component in one of those systems.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Narrative_cite", "Single_summ", "Single_summ", "Reflection", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 16, "token_end": 60, "char_start": 87, "char_end": 265, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Toutanova et al. (2008)": "2243454", "Martins and Almeida (2014)": "7345848", "Yang and Zong (2014)": null}, "Reference": {}}}, {"token_start": 66, "token_end": 118, "char_start": 305, "char_end": 537, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zapirain et al., 2013;": "2399716", "Collobert et al., 2011;": "351666", "Deschacht and Moens, 2009;": "12064136", "Roth and Woodsend, 2014)": "8931245"}}}, {"token_start": 131, "token_end": 164, "char_start": 606, "char_end": 766, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Moschitti et al. (2008)": "2340513"}, "Reference": {}}}, {"token_start": 196, "token_end": 219, "char_start": 962, "char_end": 1074, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bj\u00f6rkelund et al., 2010)": null}}}]}
{"id": "5730387_0", "paragraph": "[BOS] A great deal of SRL research has been dedicated to designing rich, expressive features.\n[BOS] The initial work by Gildea and Jurafsky (2002) already identified a compact core set of features, which were widely adopted by the SRL community.\n[BOS] These features describe the predicate, the candidate argument, and the syntactic relation between them (path).\n[BOS] Early systems primarily extended this core set by including local context lexicalized patterns (e.g., n-grams), several extended representations of the path features, and some linguistically motivated syntactic patterns, as the syntactic frame (Surdeanu et al., 2003; Xue and Palmer, 2004; Pradhan et al., 2005) .\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 20, "token_end": 74, "char_start": 100, "char_end": 362, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Gildea and Jurafsky (2002)": "207747200"}, "Reference": {}}}, {"token_start": 116, "token_end": 146, "char_start": 590, "char_end": 680, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Surdeanu et al., 2003;": "6534839", "Xue and Palmer, 2004;": "18312340", "Pradhan et al., 2005)": "12982947"}}}]}
{"id": "622026_1", "paragraph": "[BOS] Incorporating Syntactic Information in NMT Syntactic information has been widely used in SMT (Liu et al., 2006; Marton and Resnik, 2008; Shen et al., 2008) , and a lot of previous work explore to incorporate the syntactic information in NMT, which shows the effectiveness of the syntactic information (Stahlberg et al., 2016) .\n[BOS] Shi et al. (2016) give some empirical results that the deep networks of NMT are able to capture some useful syntactic information implicitly.\n[BOS] Luong et al. (2016) propose to use a multi-task framework for NMT and neural parsing, achieving promising results.\n[BOS] Eriguchi et al. (2016) propose a string-totree NMT system by end-to-end training.\n[BOS] Different to previous work, we try to incorporate the syntactic information in the target side of NMT.\n[BOS] Ishiwatari et al. (2017) concurrently propose to use chunk-based decoder to cope with the problem of free word-order languages.\n[BOS] Differently, they adopt word-level attention, and predict the end of chunk by generating end-of-chunk tokens instead of using boundary gate.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ", "Single_summ", "Reflection", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 15, "token_end": 40, "char_start": 95, "char_end": 161, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Liu et al., 2006;": "10184967", "Marton and Resnik, 2008;": "2442439", "Shen et al., 2008)": "832217"}}}, {"token_start": 63, "token_end": 75, "char_start": 285, "char_end": 331, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Stahlberg et al., 2016)": "11642690"}}}, {"token_start": 77, "token_end": 105, "char_start": 340, "char_end": 481, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Shi et al. (2016)": "7197724"}, "Reference": {}}}, {"token_start": 106, "token_end": 133, "char_start": 488, "char_end": 602, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Luong et al. (2016)": "6954272"}, "Reference": {}}}, {"token_start": 134, "token_end": 160, "char_start": 609, "char_end": 690, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Eriguchi et al. (2016)": "12851711"}, "Reference": {}}}, {"token_start": 182, "token_end": 242, "char_start": 806, "char_end": 1080, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ishiwatari et al. (2017)": "38048605"}, "Reference": {}}}]}
{"id": "622026_0", "paragraph": "[BOS] NMT with Various Granularities.\n[BOS] A line of previous work propose to utilize other granularities besides words for NMT.\n[BOS] By further exploiting the character level (Ling et al., 2015; Costajuss and Fonollosa, 2016; Chung et al., 2016; Luong et al., 2016; Lee et al., 2016) , or the sub-word level Garca-Martnez et al., 2016) information, the corresponding NMT models capture the information inside the word and alleviate the problem of unknown words.\n[BOS] While most of them focus on decomposing words into characters or sub-words, our work aims at composing words into phrases.\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 32, "token_end": 74, "char_start": 162, "char_end": 286, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ling et al., 2015;": "5799549", "Chung et al., 2016;": "13495961", "Luong et al., 2016;": "6954272", "Lee et al., 2016)": "10509498"}}}, {"token_start": 77, "token_end": 92, "char_start": 296, "char_end": 338, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "49664123_3", "paragraph": "[BOS] In addition to the studies on GEC, there is also much research on grammatical error detection (Leacock et al., 2010; Rei and Yannakoudakis, 2016; Kaneko et al., 2017) and GEC evaluation (Tetreault et al., 2010b; Madnani et al., 2011; Dahlmeier and Ng, 2012c; Napoles et al., 2015; Bryant et al., 2017; Asano et al., 2017) .\n[BOS] We do not introduce them in detail because they are not much related to this paper's contributions.\n\n", "discourse_tags": ["Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 17, "token_end": 49, "char_start": 72, "char_end": 172, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Leacock et al., 2010;": null, "Rei and Yannakoudakis, 2016;": "1521197", "Kaneko et al., 2017)": "22901719"}}}, {"token_start": 50, "token_end": 105, "char_start": 177, "char_end": 327, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Tetreault et al., 2010b;": "11372220", "Madnani et al., 2011;": "6378062", "Dahlmeier and Ng, 2012c;": "9613043", "Napoles et al., 2015;": null, "Bryant et al., 2017;": "12122749", "Asano et al., 2017)": "35168329"}}}]}
{"id": "49664123_2", "paragraph": "[BOS] To the best of our knowledge, it is the first to conduct multi-round seq2seq inference for GEC, while similar ideas have been proposed for NMT (Xia et al., 2017) .\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 15, "token_end": 43, "char_start": 63, "char_end": 167, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Xia et al., 2017)": "13481571"}}}]}
{"id": "49664123_1", "paragraph": "[BOS] or MT-based (Brockett et al., 2006; Ng, 2011, 2012a; Yoshimoto et al., 2013; Yuan and Felice, 2013; Behera and Bhattacharyya, 2013) .\n[BOS] For example, top-performing systems Rozovskaya et al., 2014; JunczysDowmunt and Grundkiewicz, 2014) in CoNLL-2014 shared task use either of the methods.\n[BOS] Recently, many novel approaches Chollampatt et al., 2016b,a; Rozovskaya and Roth, 2016; Junczys-Dowmunt and Grundkiewicz, 2016; Mizumoto and Matsumoto, 2016; Hoang et al., 2016; Yannakoudakis et al., 2017) have been proposed for GEC.\n[BOS] Among them, seq2seq models Xie et al., 2016; Ji et al., 2017; Schmaltz et al., 2017; Chollampatt and Ng, 2018) have caught much attention.\n[BOS] Unlike the models trained only with original error-corrected data, we propose a novel fluency boost learning mechanism for dynamic data augmentation along with training for GEC, despite some previous studies that explore artificial error generation for GEC (Brockett et al., 2006; Foster and Andersen, 2009; Roth, 2010, 2011; Rozovskaya et al., 2012; Xie et al., 2016; .\n[BOS] Moreover, we propose fluency boost inference which allows the model to repeatedly edit a sentence as long as the sentence's fluency can be improved.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Narrative_cite", "Narrative_cite", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 3, "token_end": 49, "char_start": 9, "char_end": 137, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Brockett et al., 2006;": "757808", "Yoshimoto et al., 2013;": "10672840", "Yuan and Felice, 2013;": "18079336", "Behera and Bhattacharyya, 2013)": "18254763"}}}, {"token_start": 54, "token_end": 83, "char_start": 159, "char_end": 245, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Rozovskaya et al., 2014;": "1547333"}}}, {"token_start": 99, "token_end": 175, "char_start": 315, "char_end": 538, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Rozovskaya and Roth, 2016;": "18563136", "Junczys-Dowmunt and Grundkiewicz, 2016;": "6820419", "Mizumoto and Matsumoto, 2016;": "9640754", "Hoang et al., 2016;": "5609535", "Yannakoudakis et al., 2017)": null}}}, {"token_start": 179, "token_end": 220, "char_start": 557, "char_end": 683, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Xie et al., 2016;": "8880428", "Ji et al., 2017;": "235645", "Schmaltz et al., 2017;": "7682221", "Chollampatt and Ng, 2018)": "19236015"}}}, {"token_start": 242, "token_end": 303, "char_start": 813, "char_end": 1057, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Brockett et al., 2006;": "757808", "Foster and Andersen, 2009;": "8891598", "Rozovskaya et al., 2012;": "60689"}}}]}
{"id": "49664123_0", "paragraph": "[BOS] Most of advanced GEC systems are classifierbased (Chodorow et al., 2007; De Felice and Pulman, 2008; Han et al., 2010; Leacock et al., 2010; Tetreault et al., 2010a; Dale and Kilgarriff, 2011) 7 The state-of-the-art result on CoNLL-2014 dataset has been recently advanced by Chollampatt and Ng (2018) (F0.5=54.79) and Grundkiewicz and Junczys-Dowmunt (2018) (F0.5=56.25) , which are contemporaneous to this paper.\n[BOS] In contrast to the basic seq2seq model in this paper, they used advanced approaches for modeling (e.g., convolutional seq2seq with pre-trained word embedding, using edit operation features, ensemble decoding and advanced model combinations).\n[BOS] It should be noted that their approaches are orthogonal to ours, making it possible to apply our fluency boost learning and inference mechanism to their models.\n[BOS] 8 The recently proposed SMT-NMT hybrid system (Grundkiewicz and Junczys-Dowmunt, 2018) , which is tuned towards GLEU on JFLEG Dev set, reports a higher result (GLEU=61.50 on JFLEG test set).\n[BOS] Table 6 : JFLEG Leaderboard.\n[BOS] Ours denote the single dual-boost models in Table 5 .\n[BOS] The systems with bold fonts are based on seq2seq models.\n[BOS] * denotes the system is tuned on JFLEG.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Narrative_cite", "Other", "Other", "Other", "Other"], "span_citation_mapping": [{"token_start": 5, "token_end": 66, "char_start": 23, "char_end": 198, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chodorow et al., 2007;": "1445945", "De Felice and Pulman, 2008;": "11574043", "Han et al., 2010;": "1231550", "Leacock et al., 2010;": null, "Tetreault et al., 2010a;": "2162894", "Dale and Kilgarriff, 2011)": "18357549"}}}, {"token_start": 67, "token_end": 122, "char_start": 201, "char_end": 363, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Chollampatt and Ng (2018)": "19236015"}}}, {"token_start": 225, "token_end": 248, "char_start": 865, "char_end": 927, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Grundkiewicz and Junczys-Dowmunt, 2018)": "4941839"}}}]}
{"id": "51872853_4", "paragraph": "[BOS] In Japanese, CR has not been actively studied other than Iida et al. (2003) ; Sasano et al. (2007) since the use of zero pronouns is more common and problematic.\n[BOS] Semantic Role Labeling.\n[BOS] Japanese PA is similar to Semantic Role Labeling (SRL) in English.\n[BOS] Neural network-based approaches have improved the performance (Zhou and Xu, 2015; .\n[BOS] In these approaches, an appropriate argument for a predicate is searched among mentions in a text.\n[BOS] The notion entity is not considered.\n[BOS] Other Entity-Centric Study.\n[BOS] There are several studies that consider the notion entity in other areas: text comprehension (Kobayashi et al., 2016; Henaff et al., 2016) and language modeling (Ji et al., 2017) .\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Narrative_cite", "Transition", "Transition", "Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 5, "token_end": 30, "char_start": 19, "char_end": 104, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Iida et al. (2003)": "14982611", "Sasano et al. (2007)": "16743845"}}}, {"token_start": 65, "token_end": 80, "char_start": 277, "char_end": 357, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 132, "token_end": 150, "char_start": 623, "char_end": 687, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kobayashi et al., 2016;": "10239453", "Henaff et al., 2016)": "11243593"}}}, {"token_start": 151, "token_end": 161, "char_start": 692, "char_end": 727, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ji et al., 2017)": "5564363"}}}]}
{"id": "51872853_3", "paragraph": "[BOS] Coreference Resolution.\n[BOS] CR has been actively studied in English and Chinese.\n[BOS] Neural networkbased approaches (Wiseman et al., 2016; Clark and Manning, 2016b,a; outperformed conventional machine learning approaches (Clark and Manning, 2015) .\n[BOS] Wiseman et al. (2016) and Clark and Manning (2016b) learn an entity representation and integrate this into a mentionbased model.\n[BOS] Our work is inspired by Wiseman et al. (2016) , which learn the entity representation by using Recurrent Neural Networks (RNNs).\n[BOS] Clark and Manning (2016b) adopt a clustering approach for the entity representation.\n[BOS] The reason why we do not use this is that if we take a clustering approach in our setting, zero pronouns need to be first identified before clustering, and thus, it is hard to perform CR and PA jointly.\n[BOS] take an end-to-end approach, aiming at not relying on hand-engineering mention detector (consider all spans as potential mentions).\n[BOS] In used Japanese evaluation corpora, since the basic unit for the annotations and our analyses (CR and PA) is fixed, we do not need consider all spans.\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite", "Multi_summ", "Single_summ", "Single_summ", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 18, "token_end": 38, "char_start": 95, "char_end": 173, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wiseman et al., 2016;": "9163756"}}}, {"token_start": 44, "token_end": 55, "char_start": 203, "char_end": 256, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Clark and Manning, 2015)": "17884437"}}}, {"token_start": 57, "token_end": 87, "char_start": 265, "char_end": 393, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wiseman et al. (2016)": "9163756", "Clark and Manning (2016b)": "6235360"}, "Reference": {}}}, {"token_start": 93, "token_end": 117, "char_start": 424, "char_end": 528, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wiseman et al. (2016)": "9163756"}, "Reference": {}}}, {"token_start": 118, "token_end": 135, "char_start": 535, "char_end": 619, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Clark and Manning (2016b)": "6235360"}, "Reference": {}}}]}
{"id": "51872853_2", "paragraph": "[BOS] For Chinese, where zero anaphors are often used, neural network-based approaches (Chen and Ng, 2016; Yin et al., 2017) outperformed conventional machine learning approaches (Zhao and Ng, 2007) .\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 13, "token_end": 32, "char_start": 55, "char_end": 124, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chen and Ng, 2016;": "1195646", "Yin et al., 2017)": "5222450"}}}, {"token_start": 35, "token_end": 45, "char_start": 151, "char_end": 198, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhao and Ng, 2007)": "7739917"}}}]}
{"id": "51872853_1", "paragraph": "[BOS] Although most of studies did not consider the notion entity, Sasano and Kurohashi (2011) consider an entity, and its salience score is calculated based on simple rules.\n[BOS] However, they used gold coreference links to form the entities, and reported the salience score did not improve the performance.\n[BOS] In contrast, we perform CR automatically, and capture the entity salience by using RNNs.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 13, "token_end": 62, "char_start": 67, "char_end": 309, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Sasano and Kurohashi (2011)": "13931451"}, "Reference": {}}}]}
{"id": "51872853_0", "paragraph": "[BOS] Predicate Argument Structure Analysis.\n[BOS] Early studies have handled both intra-and intersentential anaphora (Taira et al., 2008; Sasano and Kurohashi, 2011) , and Hangyo et al. (2013) present a method for handling exophora.\n[BOS] Recent studies, however, focus on only intra-sentential anaphora (Ouchi et al., 2015; Shibata et al., 2016; Iida et al., 2016; Ouchi et al., 2017; Matsubayashi and Inui, 2017) , because the analysis of intersentential anaphora is extremely difficult.\n[BOS] Neural network-based approaches (Shibata et al., 2016; Iida et al., 2016; Ouchi et al., 2017; Matsubayashi and Inui, 2017) have improved its performance.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 13, "token_end": 50, "char_start": 83, "char_end": 193, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hangyo et al. (2013)": "5797690"}, "Reference": {"(Taira et al., 2008;": "15825278", "Sasano and Kurohashi, 2011)": "13931451"}}}, {"token_start": 68, "token_end": 118, "char_start": 279, "char_end": 415, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ouchi et al., 2015;": "5698849", "Shibata et al., 2016;": "17201701", "Iida et al., 2016;": "12452210", "Ouchi et al., 2017;": "30762359", "Matsubayashi and Inui, 2017)": "23265360"}}}, {"token_start": 134, "token_end": 175, "char_start": 497, "char_end": 619, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Shibata et al., 2016;": "17201701", "Iida et al., 2016;": "12452210", "Ouchi et al., 2017;": "30762359", "Matsubayashi and Inui, 2017)": "23265360"}}}]}
{"id": "5931090_3", "paragraph": "[BOS] There are also works on reordering error analysis like Han et al. (2012) which examined an existing reordering method and refined it after a detailed linguistic analysis on reordering issues.\n[BOS] Although they discovered that parsing errors affect the reordering quality, they did not observe the concrete relationship.\n[BOS] On the other hand, Gimnez and Mrquez (2008) proposed an automatic error analysis method of machine translation output, by compiling a set of metric variants.\n[BOS] However, they did not provide insight on what SMT component caused low translation performance.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 60, "char_start": 6, "char_end": 327, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Han et al. (2012)": "539235"}, "Reference": {}}}, {"token_start": 66, "token_end": 113, "char_start": 353, "char_end": 593, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Gim\u00e9nez and M\u00e0rquez (2008)": "12009874"}, "Reference": {}}}]}
{"id": "5931090_2", "paragraph": "[BOS] There are more works on parsing error analysis.\n[BOS] For instance, Hara et al. (2009) defined several types of parsing error patterns on predicate argument relation and tested them with a Headdriven phrase structure grammar (HPSG) (Pollard and Sag, 1994) parser (Miyao and Tsujii, 2008) .\n[BOS] McDonald and Nivre (2007) explored parsing errors for data-driven dependency parsing by comparing a graph-based parser with a transitionbased parser, which are representing two dominant parsing models.\n[BOS] At the same time, Dredze et al. (2007) provided a comparison analysis on differences in annotation guidelines among treebanks which were suspected to be responsible for dependency parsing errors in domain adaptation tasks.\n[BOS] Unlike analyzing parsing errors, authors in Yu et al. (2011) focused on the difficulties in Chinese deep parsing by comparing the linguistic properties between Chinese and English.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 15, "token_end": 67, "char_start": 74, "char_end": 293, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hara et al. (2009)": "5540873"}, "Reference": {"(Pollard and Sag, 1994)": null, "(Miyao and Tsujii, 2008)": "885002"}}}, {"token_start": 69, "token_end": 106, "char_start": 302, "char_end": 503, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 112, "token_end": 149, "char_start": 528, "char_end": 732, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Dredze et al. (2007)": "5811151"}, "Reference": {}}}, {"token_start": 157, "token_end": 182, "char_start": 783, "char_end": 919, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Yu et al. (2011)": "8416967"}, "Reference": {}}}]}
{"id": "5931090_1", "paragraph": "[BOS] One most relevant work to ours is observing the impact of parsing accuracy on a SMT system introduced in Quirk and Corston-Oliver (2006) .\n[BOS] They showed the general idea that syntax-based SMT models are sensitive to syntactic analysis.\n[BOS] However, they did not further analyze concrete parsing error types that affect task accuracy.\n[BOS] Green (2011) explored the effects of noun phrase bracketing in dependency parsing in English, and further on English to Czech machine translation.\n[BOS] But the work focused on using noun phrase structure to improve a machine translation framework.\n[BOS] In the work of Katz-Brown et al. (2011) , they proposed a training method to improve a parser's performance by using reordering quality to examine the parse quality.\n[BOS] But they did not study the relationship between reordering quality and parse quality.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 69, "char_start": 6, "char_end": 345, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Quirk and Corston-Oliver (2006)": "2988643"}, "Reference": {}}}, {"token_start": 70, "token_end": 114, "char_start": 352, "char_end": 600, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Green (2011)": "1226415"}, "Reference": {}}}, {"token_start": 115, "token_end": 168, "char_start": 607, "char_end": 864, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Katz-Brown et al. (2011)": "489244"}, "Reference": {}}}]}
{"id": "5931090_0", "paragraph": "[BOS] Although there are studies on analyzing parsing errors and reordering errors, as far as we know, there is not any work on observing the relationship between these two types of errors.\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "6000982_2", "paragraph": "[BOS] With regard to feature transformation, the work of Ando and Zhang (2005) is similar in spirit to our work.\n[BOS] They studied semi-supervised text chunking by using a large projection matrix to map sparse base features into a small number of high level features.\n[BOS] Their project matrix was trained by transforming the original problem into a large number of auxiliary problems, obtaining training data for the auxiliary problems by automatically labeling raw data and using alternating structure optimization to estimate the matrix across all auxiliary tasks.\n[BOS] In comparison with their approach, our method is simpler in the sense that we do not request any intermediate step of splitting the prediction problem, and obtain meta features directly from self-annotated data.\n[BOS] The training of our meta feature values is highly efficient, requiring the collection of simple statistics over base features from huge amount of data.\n[BOS] Hence our method can potentially be useful to other tasks also.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 100, "char_start": 6, "char_end": 569, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ando and Zhang (2005)": "16629334"}, "Reference": {}}}]}
{"id": "6000982_1", "paragraph": "[BOS] Several previous studies used co-training/selftraining methods.\n[BOS] McClosky et al. (2006) presented a self-training method combined with a reranking algorithm for constituency parsing.\n[BOS] Sagae and Tsujii (2007) applied the standard co-training method for dependency parsing.\n[BOS] In their approaches, some automatically parsed sentences were selected as new training data, which was used together with the original labeled data to retrain a new parser.\n[BOS] We are able to use their approaches on top of the output of our parsers.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 15, "token_end": 42, "char_start": 76, "char_end": 193, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"McClosky et al. (2006)": "1825866"}, "Reference": {}}}, {"token_start": 43, "token_end": 63, "char_start": 200, "char_end": 287, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Sagae and Tsujii (2007)": "2768696"}, "Reference": {}}}]}
{"id": "6000982_0", "paragraph": "[BOS] Our approach is to use unannotated data to generate the meta features to improve dependency parsing.\n[BOS] Several previous studies relevant to our approach have been conducted.\n[BOS] Koo et al. (2008) used a word clusters trained on a large amount of unannotated data and designed a set of new features based on the clusters for dependency parsing models.\n[BOS] Chen et al. (2009) extracted subtree structures from a large amount of data and represented them as the additional features to improve dependency parsing.\n[BOS] Suzuki et al. (2009) extended a Semi-supervised Structured Conditional Model (SS-SCM) of Suzuki and Isozaki (2008) to the dependency parsing problem and combined their method with the word clustering feature representation of Koo et al. (2008) .\n[BOS] Chen et al. (2012) proposed an approach to representing high-order features for graphbased dependency parsing models using a dependency language model and beam search.\n[BOS] In future work, we may consider to combine their methods with ours to improve performance.\n\n", "discourse_tags": ["Reflection", "Reflection", "Single_summ", "Single_summ", "Multi_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 35, "token_end": 74, "char_start": 190, "char_end": 362, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Koo et al. (2008)": "1916754"}, "Reference": {}}}, {"token_start": 75, "token_end": 103, "char_start": 369, "char_end": 523, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Chen et al. (2009)": "14728649"}, "Reference": {}}}, {"token_start": 104, "token_end": 158, "char_start": 530, "char_end": 773, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Suzuki et al. (2009)": "160543"}, "Reference": {"Suzuki and Isozaki (2008)": "647664", "Koo et al. (2008)": "1916754"}}}, {"token_start": 160, "token_end": 191, "char_start": 782, "char_end": 949, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Chen et al. (2012)": "5578635"}, "Reference": {}}}]}
{"id": "550225_2", "paragraph": "[BOS] To the best of our knowledge the closest idea to our work is instance weighting (Jiang and Zhai, 2007) , which is often used for domain adaptation.\n[BOS] They add instance dependent weights to the loss function to help improving the performance.\n[BOS] As a comparison, we focus on using \"difficult\" in-1 Google's Tensor Processing Unit .\n[BOS] stances in training rather than spending training time on easier ones.\n[BOS] We improve the accuracy while simultaneously reducing the cost of training.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 15, "token_end": 25, "char_start": 67, "char_end": 108, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Jiang and Zhai, 2007)": "15036406"}}}]}
{"id": "550225_1", "paragraph": "[BOS] Other methods focus on how to reduce the parameters trained by the model (See et al., 2016) .\n[BOS] They show that with a pruning technique, 40-60% of the parameters can be pruned out.\n[BOS] Similar methods are proposed to reduce the hidden units with knowledge distillation (Crego et al., 2016; Kim and Rush, 2016) .\n[BOS] They re-train a smaller student model using text translated from teacher models.\n[BOS] They report a 70% reduction on the number of parameters and a 30% increase in decoding speed.\n[BOS] Hubara et al. (2016) proposed to reduce the precision of model parameters during training and network activations, which can also bring benefits to training efficiency.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Narrative_cite", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 23, "char_start": 6, "char_end": 97, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(See et al., 2016)": "2973141"}}}, {"token_start": 54, "token_end": 77, "char_start": 240, "char_end": 321, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Crego et al., 2016;": "16641238", "Kim and Rush, 2016)": null}}}, {"token_start": 116, "token_end": 147, "char_start": 517, "char_end": 685, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hubara et al. (2016)": null}, "Reference": {}}}]}
{"id": "550225_0", "paragraph": "[BOS] Training efficiency has been a main concern by many researchers in the field of NMT.\n[BOS] Data parallelism and model parallelism are two commonly used techniques to improve the speed of training .\n[BOS] As a result, multiple GPUs or TPUs 1 are needed which requires additional replication and combination costs.\n[BOS] Data parallelism does not reduce the actual cost of computation, it only saves time by using additional computational power.\n[BOS] Chen et al. (2016b) proposed a modified RNN structure with a full output layer to facilitate the training when using a large amount of data.\n[BOS] Kalchbrenner et al. (2016) proposed ByteNet with two networks to encode the source and decode the target at the same time.\n[BOS] Gehring et al. (2017) use convolutional neural networks to build the system with a nature of parallelization.\n[BOS] Kuchaiev and Ginsburg (2017) focus on how to reduce the computational cost through patitioning or factorizing LSTM matrix.\n[BOS] To compare, our method does not modify the network itself and can be used in any NMT framework.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 83, "token_end": 114, "char_start": 456, "char_end": 596, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Chen et al. (2016b)": "3344840"}, "Reference": {}}}, {"token_start": 115, "token_end": 145, "char_start": 603, "char_end": 725, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kalchbrenner et al. (2016)": "13895969"}, "Reference": {}}}, {"token_start": 146, "token_end": 170, "char_start": 732, "char_end": 841, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Gehring et al. (2017)": "3648736"}, "Reference": {}}}, {"token_start": 171, "token_end": 200, "char_start": 848, "char_end": 970, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kuchaiev and Ginsburg (2017)": "3570621"}, "Reference": {}}}]}
{"id": "5105979_1", "paragraph": "[BOS] In addition to portability experiments with the parsing model of (Collins, 1997) , (Gildea, 2001) provided a comprehensive analysis of parser portability.\n[BOS] On the basis of this analysis, a technique for parameter pruning was proposed leading to a significant reduction in the model size without a large decrease of accuracy.\n[BOS] Gildea (2001) only reports results on sentences of 40 or less words on all the Brown corpus sections combined, for which he reports 80.3%/81.0% recall/precision when training only on data from the WSJ corpus, and 83.9%/84.8% when training on data from the WSJ corpus and all sections of the Brown corpus.\n[BOS] (Roark and Bacchiani, 2003) performed experiments on supervised and unsupervised PCFG adaptation to the target domain.\n[BOS] They propose to use the statistics from a source domain to define priors over weights.\n[BOS] However, in their experiments they used only trivial sub-cases of this approach, namely, count merging and model interpolation.\n[BOS] They achieved very good improvement over their baseline and over (Gildea, 2001) , but the absolute accuracies were still relatively low (as discussed above).\n[BOS] They report results with combined Brown data (on sentences of 100 words or less), achieving 81.3%/80.9% when training only on the WSJ corpus and 85.4%/85.9% with their best method using the data from both domains.\n\n", "discourse_tags": ["Multi_summ", "Multi_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 66, "char_start": 6, "char_end": 335, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Collins, 1997)": "1345", "(Gildea, 2001)": "196105"}, "Reference": {}}}, {"token_start": 67, "token_end": 145, "char_start": 342, "char_end": 646, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Gildea (2001)": "196105"}, "Reference": {}}}, {"token_start": 146, "token_end": 302, "char_start": 653, "char_end": 1382, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Roark and Bacchiani, 2003)": "6376593"}, "Reference": {"(Gildea, 2001)": "196105"}}}]}
{"id": "5105979_0", "paragraph": "[BOS] Most research in the field of parsing has focused on the Wall Street Journal corpus.\n[BOS] Several researchers have addressed the portability of these WSJ parsers to other domains, but mostly without addressing the issue of how a parser can be designed specifically for porting to another domain.\n[BOS] Unfortunately, no direct empirical comparison is possible between our results and results with other parsers, because there is no standard portability benchmark to date where a small amount of data from a target domain is used.\n[BOS] (Ratnaparkhi, 1999) performed portability experiments with a Maximum Entropy parser and demonstrated that the parser trained on WSJ achieves far worse results on the Brown corpus sections.\n[BOS] Adding a small amount of data from the target domain improves the results, but accuracy is still much lower than the results on the WSJ.\n[BOS] They reported results when their parser was trained on the WSJ training set plus a portion of 2,000 sentences from a Brown corpus section.\n[BOS] They achieved 80.9%/80.3% recall/precision for section K, and 80.6%/81.3% for section N. 7 Our analogous method (TOP-Focus) achieved much better accuracy (3.7% and 4.9% better F 1 , respectively).\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 100, "token_end": 257, "char_start": 543, "char_end": 1222, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Ratnaparkhi, 1999)": "3231298"}, "Reference": {}}}]}
{"id": "5057965_1", "paragraph": "[BOS] Language modeling.\n[BOS] Language modeling is a core task in NLP.\n[BOS] Many attempts have been paid to develop better neural language models (Zilly et al., 2017; Inan et al., 2016; Godin et al., 2017; Melis et al., 2017) .\n[BOS] Specifically, with extensive corpora, language models can be well trained to generate high-quality sentences from scratch (Jzefowicz et al., 2016; Grave et al., 2017; Li et al., 2018; Shazeer et al., 2017) .\n[BOS] Meanwhile, initial attempts have been made to improve the performance of other tasks with these methods.\n[BOS] Some methods treat the language modeling as an additional supervision, and conduct co-training for knowledge transfer (Dai and Le, 2015; Liu et al., 2018; Rei, 2017) .\n[BOS] Others, including this paper, aim to construct additional features (referred as contextualized representations) with the pre-trained language models (Peters et al., 2017 (Peters et al., , 2018 .\n[BOS] Neural Network Acceleration.\n[BOS] There are mainly three kinds of NN acceleration methods, i.e., prune network into smaller sizes (Han et al., 2015; Wen et al., 2016) , converting float operation into customized low precision arithmetic (Hubara et al., 2018; Courbariaux et al., 2016) , and using shallower networks to mimic the output of deeper ones (Hinton et al., 2015; Romero et al., 2014) .\n[BOS] However, most of them require costly retraining.\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite", "Narrative_cite", "Transition", "Narrative_cite", "Narrative_cite", "Transition", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 25, "token_end": 62, "char_start": 125, "char_end": 227, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zilly et al., 2017;": "1101453", "Inan et al., 2016;": "7443908", "Godin et al., 2017;": "11744937", "Melis et al., 2017)": "33513311"}}}, {"token_start": 77, "token_end": 120, "char_start": 313, "char_end": 441, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(J\u00f3zefowicz et al., 2016;": "260422", "Grave et al., 2017;": null, "Li et al., 2018;": "19156956", "Shazeer et al., 2017)": "12462234"}}}, {"token_start": 150, "token_end": 179, "char_start": 619, "char_end": 726, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dai and Le, 2015;": "7138078", "Liu et al., 2018;": "19232497", "Rei, 2017)": "16386838"}}}, {"token_start": 201, "token_end": 221, "char_start": 856, "char_end": 927, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Peters et al., 2017": "7197241", "(Peters et al., , 2018": null}}}, {"token_start": 243, "token_end": 264, "char_start": 1034, "char_end": 1103, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Han et al., 2015;": "2238772", "Wen et al., 2016)": "2056019"}}}, {"token_start": 265, "token_end": 293, "char_start": 1106, "char_end": 1221, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hubara et al., 2018;": "15817277", "Courbariaux et al., 2016)": null}}}, {"token_start": 295, "token_end": 323, "char_start": 1228, "char_end": 1330, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hinton et al., 2015;": "7200347", "Romero et al., 2014)": "2723173"}}}]}
{"id": "5057965_0", "paragraph": "[BOS] Sequence labeling.\n[BOS] Linguistic sequence labeling is one of the fundamental tasks in NLP, encompassing various applications including POS tagging, chunking, and NER.\n[BOS] Many attempts have been made to conduct end-to-end learning and build reliable models without handcrafted features (Chiu and Nichols, 2016; Lample et al., 2016; Ma and Hovy, 2016) .\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 40, "token_end": 79, "char_start": 222, "char_end": 361, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chiu and Nichols, 2016;": "6300165", "Lample et al., 2016;": "6042994", "Ma and Hovy, 2016)": "10489017"}}}]}
{"id": "5983351_3", "paragraph": "[BOS] The main theme explored in this paper is to learn a common representation for two views with the end goal of generating a target sequence in a third view.\n[BOS] The idea of learning common representations for multiple views has been explored well in the past (Klementiev et al., 2012; Chandar et al., 2014; Hermann and Blunsom, 2014; Chandar et al., 2016; Rajendran et al., 2015) .\n[BOS] For example, Andrew et al. (2013) propose Deep CCA for learning a common representation for two views.\n[BOS] (Chandar et al., 2014; Chandar et al., 2016) propose correlational neural networks for common representation learning and Rajendran et al. (2015) propose bridge correlational networks for multilingual multimodal representation learning.\n[BOS] From the point of view of representation learning, the work of Rajendran et al. (2015) is very similar to our work except that it focuses only on representation learning and does not consider the end goal of generating sequences in a target language.\n\n", "discourse_tags": ["Reflection", "Narrative_cite", "Single_summ", "Multi_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 37, "token_end": 94, "char_start": 188, "char_end": 385, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Chandar et al., 2014;": null, "Chandar et al., 2016;": null}}}, {"token_start": 99, "token_end": 118, "char_start": 407, "char_end": 496, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Andrew et al. (2013)": null}, "Reference": {}}}, {"token_start": 119, "token_end": 145, "char_start": 503, "char_end": 620, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Chandar et al., 2014;": null, "Chandar et al., 2016)": null}, "Reference": {}}}, {"token_start": 146, "token_end": 167, "char_start": 625, "char_end": 739, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 168, "token_end": 218, "char_start": 746, "char_end": 996, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "5983351_2", "paragraph": "[BOS] Of course, in general the idea of pivot/bridge/interlingua based conversion is not new and has been used previously in several non-neural network settings.\n[BOS] For example (Khapra et al., 2010 ) use a bridge language or pivot language to do machine transliteration.\n[BOS] Similarly, (Wu and Wang, 2007; Zhu et al., 2014) do pivot based machine translation.\n[BOS] Lastly, we would also like to mention the work in interlingua based Machine Translation (Nirenburg, 1994; Dorr et al., 2010) which is clearly the inspiration for this work even though the focus of this work is not on MT.\n\n", "discourse_tags": ["Transition", "Single_summ", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 39, "token_end": 64, "char_start": 180, "char_end": 273, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 67, "token_end": 88, "char_start": 291, "char_end": 364, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wu and Wang, 2007;": "3681367", "Zhu et al., 2014)": "2196577"}}}, {"token_start": 100, "token_end": 121, "char_start": 421, "char_end": 495, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Nirenburg, 1994;": "8237862", "Dorr et al., 2010)": null}}}]}
{"id": "5983351_1", "paragraph": "[BOS] Encoder decoder models in a multi-source, single target setting have been explored by (Elliott et al., 2015) and (Zoph and Knight, 2016) .\n[BOS] Specifically, Elliott et al. (2015) try to generate a German caption from an image and its corresponding English caption.\n[BOS] Similarly, Zoph and Knight (2016) focus on the problem of generating English translations given the same sentence in both French and German.\n[BOS] We would like to highlight that both these models require three-way parallel data while we are focusing on situations where such data is not available.\n[BOS] Single source, multi-target and multi-source, single target settings have been considered in (Luong et al., 2015a) .\n[BOS] Recent work by Firat et al. (2016) explores multi-source to multi-target encoder decoder models in the context of Machine Translation.\n[BOS] However, Firat et al. (2016) focus on multi-task learning with a shared attention mechanism and the goal is to improve the MT performance for a pair of languages for which parallel data is available.\n[BOS] This is clearly different from the goal of this paper which is to design encoder decoder models for a pair of languages for which no parallel data is available but data is available only between each of these languages and a bridge language.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ", "Reflection", "Narrative_cite", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 37, "char_start": 6, "char_end": 142, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Elliott et al., 2015)": null, "(Zoph and Knight, 2016)": "8677917"}}}, {"token_start": 41, "token_end": 66, "char_start": 165, "char_end": 272, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Elliott et al. (2015)": null}, "Reference": {}}}, {"token_start": 69, "token_end": 95, "char_start": 290, "char_end": 419, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zoph and Knight (2016)": "8677917"}, "Reference": {}}}, {"token_start": 125, "token_end": 153, "char_start": 584, "char_end": 698, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Luong et al., 2015a)": "794019"}}}, {"token_start": 158, "token_end": 184, "char_start": 722, "char_end": 841, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Firat et al. (2016)": null}, "Reference": {}}}, {"token_start": 187, "token_end": 227, "char_start": 857, "char_end": 1047, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "5983351_0", "paragraph": "[BOS] Encoder decoder based architectures for sequence to sequence generation were initially proposed in Sutskever et al., 2014) in the context of Machine Translation (MT) and have also been successfully used for generating captions for images (Vinyals et al., 2015b) .\n[BOS] However, such sequence to sequence models are often difficult to train as they aim to encode the entire source sequence using a fixed encoder representation.\n[BOS] introduced attention based models wherein a different representation is fed to the decoder at each time step by focusing the attention on different parts of the input sequence.\n[BOS] Such attention based models have been more successful than vanilla encoderdecoder models and have been used successfully for MT , parsing , speech recognition (Chorowski et al., 2015) , image captioning (Xu et al., 2015) among other applications.\n[BOS] All the above mentioned works focus only on the case when there is one source and one target.\n[BOS] The source can be image, text, or speech signal but the target is always a text sequence.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Narrative_cite", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 7, "token_end": 24, "char_start": 46, "char_end": 128, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 28, "token_end": 56, "char_start": 147, "char_end": 267, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vinyals et al., 2015b)": "1169492"}}}, {"token_start": 141, "token_end": 152, "char_start": 763, "char_end": 806, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chorowski et al., 2015)": "5590763"}}}, {"token_start": 153, "token_end": 165, "char_start": 809, "char_end": 843, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Xu et al., 2015)": "1055111"}}}]}
{"id": "5578635_2", "paragraph": "[BOS] McClosky et al. (2006) presented a self-training approach for phrase structure parsing.\n[BOS] Sagae and Tsujii (2007) used the co-training technique to improve performance.\n[BOS] First, two parsers were used to parse the sentences in unannotated data.\n[BOS] Then they selected some sentences which have the same trees produced by those two parsers.\n[BOS] They retrained a parser on newly parsed sentences and the original labeled data.\n[BOS] We are able to use the output of our systems for co-training/self-training techniques.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 22, "char_start": 6, "char_end": 93, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 23, "token_end": 96, "char_start": 100, "char_end": 441, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Sagae and Tsujii (2007)": "2768696"}, "Reference": {}}}]}
{"id": "5578635_1", "paragraph": "[BOS] Another group of methods are the cotraining/self-training techniques.\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "5578635_0", "paragraph": "[BOS] Several previous studies related to our work have been conducted.\n[BOS] Koo et al. (2008) used a clustering algorithm to produce word clusters on a large amount of unannotated data and represented new features based on the clusters for dependency parsing models.\n[BOS] Chen et al. (2009) proposed an approach that extracted partial tree structures from a large amount of data and used them as the additional features to improve dependency parsing.\n[BOS] They approaches were still restricted in a small number of arcs in the graphs.\n[BOS] Suzuki et al. (2009) presented a semisupervised learning approach.\n[BOS] They extended a Semi-supervised Structured Conditional Model (SS-SCM) (Suzuki and Isozaki, 2008) to the dependency parsing problem and combined their method with the approach of Koo et al. (2008) .\n[BOS] In future work, we may consider apply their methods on our parsers to improve further.\n\n", "discourse_tags": ["Reflection", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 14, "token_end": 53, "char_start": 78, "char_end": 268, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Koo et al. (2008)": "1916754"}, "Reference": {}}}, {"token_start": 54, "token_end": 87, "char_start": 275, "char_end": 453, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Chen et al. (2009)": "14728649"}, "Reference": {}}}, {"token_start": 104, "token_end": 164, "char_start": 545, "char_end": 813, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Suzuki et al. (2009)": "160543"}, "Reference": {"(Suzuki and Isozaki, 2008)": "647664", "Koo et al. (2008)": "1916754"}}}]}
{"id": "51878798_1", "paragraph": "[BOS] On the other hand, the basis of source monolingual corpora, a pre-training method based on an autoencoder has been proposed to enhance the encoder (Zhang and Zong, 2016) .\n[BOS] However, the decoder is not enhanced by this method.\n[BOS] trained two autoencoders using source and target monolingual corpora, while translation models are trained using a parallel corpus.\n[BOS] This method enhances both the encoder and decoder, but it requires two monolingual corpora, respectively.\n[BOS] Our proposed method enhances not only the decoder but also the encoder and attention using target monolingual corpora.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 23, "token_end": 41, "char_start": 100, "char_end": 175, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhang and Zong, 2016)": "17667087"}}}]}
{"id": "51878798_0", "paragraph": "[BOS] One approach of using target monolingual corpora is to construct a recurrent neural network language model and combine the model with the decoder (Glehere et al., 2015; Sriram et al., 2017) .\n[BOS] Similarly, there is a method of training language models, jointly with the translator, using multitask learning (Domhan and Hieber, 2017) .\n[BOS] These Another approach of using monolingual corpora of the target language is to learn models using synthetic parallel sentences.\n[BOS] The method of Sennrich et al. (2016a) generates synthetic parallel corpora through back-translation and learns models from such corpora.\n[BOS] Our proposed method is an extension of this method.\n[BOS] Currey et al. (2017) generated synthetic parallel sentences by copying target sentences to the source.\n[BOS] This method utilizes a feature in which some words, such as named entities, are often identical across the source and target languages and do not require translation.\n[BOS] However, this method provides no benefits to language pairs having different character sets, such as English and Japanese.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Transition", "Single_summ", "Reflection", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 14, "token_end": 43, "char_start": 73, "char_end": 195, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(G\u00fcl\u00e7ehere et al., 2015;": "15352384"}}}, {"token_start": 100, "token_end": 128, "char_start": 486, "char_end": 622, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Sennrich et al. (2016a)": "15600925"}, "Reference": {}}}, {"token_start": 140, "token_end": 213, "char_start": 687, "char_end": 1091, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Currey et al. (2017)": "40575489"}, "Reference": {}}}]}
{"id": "51839897_2", "paragraph": "[BOS] In this paper, we apply active learning for both traditional and overnight data collection with the focus on overnight approach.\n[BOS] In addition, a limitation of prior active learning work is that the hyperparameters are usually predefined in some way, mostly from different work on the same or similar dataset, or from the authors experience Fang et al., 2017) .\n[BOS] In this paper, we investigate how to efficiently set the hyperparameters for the active learning process.\n\n", "discourse_tags": ["Reflection", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 47, "token_end": 70, "char_start": 261, "char_end": 369, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Fang et al., 2017)": "22272492"}}}]}
{"id": "51839897_1", "paragraph": "[BOS] Active learning has been applied to a variety of machine learning and NLP tasks (Thompson et al., 1999; Tang et al., 2002; Chenguang Wang, 2017) employing various algorithms such as least confidence score (Culotta and McCallum, 2005) , large margin (Settles and Craven, 2008) , entropy based sampling, density weighting method (Settles, 2012) , and reinforcement learning (Fang et al., 2017) .\n[BOS] Nevertheless, there has been limited work applying active learning for deep semantic parsing with the exception of Iyer et al. (2017) .\n[BOS] Different from conventional active learning, they used crowd workers to select what data to annotate for traditional semantic parsing data collection.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 11, "token_end": 39, "char_start": 55, "char_end": 150, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Thompson et al., 1999;": "1371723", "Tang et al., 2002;": "5789309", "Chenguang Wang, 2017)": "6825785"}}}, {"token_start": 45, "token_end": 58, "char_start": 194, "char_end": 239, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Culotta and McCallum, 2005)": "11583646"}}}, {"token_start": 59, "token_end": 70, "char_start": 242, "char_end": 281, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Settles and Craven, 2008)": "8197231"}}}, {"token_start": 71, "token_end": 84, "char_start": 284, "char_end": 348, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Settles, 2012)": "207319017"}}}, {"token_start": 86, "token_end": 96, "char_start": 355, "char_end": 397, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Fang et al., 2017)": "22272492"}}}, {"token_start": 98, "token_end": 150, "char_start": 406, "char_end": 698, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Iyer et al. (2017)": "497108"}, "Reference": {}}}]}
{"id": "51839897_0", "paragraph": "[BOS] Sequence-to-sequence models are currently the state-of-the-art for semantic parsing (Jia and Liang, 2016; Dong and Lapata, 2016; Duong et al., 2017) .\n[BOS] In this paper, we also exploit a sequenceto-sequence model to minimise the amount of la-belled training data required to achieve state-ofthe-art semantic parsing results.\n\n", "discourse_tags": ["Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 19, "token_end": 43, "char_start": 73, "char_end": 154, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Jia and Liang, 2016;": "7218315", "Dong and Lapata, 2016;": "15412473", "Duong et al., 2017)": "11320015"}}}]}
{"id": "51996052_0", "paragraph": "[BOS] This work falls under a large body of work on incorporating linguistically sound structures into neural networks for more effective text representation.\n[BOS] One such line of work is sub-lexical models.\n[BOS] In these models, word representations are enriched by explicitly modeling characters (Ma and Hovy, 2016; Kim et al., 2016) or morphemes (Luong et al., 2013; Botha and Blunsom, 2014; Cotterell et al., 2016) .\n[BOS] For languages with complex orthography, sub-character models have also been proposed.\n[BOS] Previous works consider modeling graphical components of Chinese characters called radicals (Sun et al., 2014; Yin et al., 2016) and syllable-blocks of Korean characters-either as atomic (Choi et al., 2017) or as non-linear functions of underlying jamo letters through Unicode decomposition (Stratos, 2017) .\n[BOS] The present work also aims to incorporate subword information into word embeddings, and does so by modeling morphology.\n[BOS] However, this work differs from those above in the means of composition, as our method is based principally on function application.\n[BOS] Here, we take derivational morphemes (i.e. affixes) as functions, and stems as arguments.\n[BOS] Broadly speaking, this work can be seen as an extension of Baroni et al. (2014) 's compositional distributional semantic framework to the sub-word level.\n[BOS] At a more narrow level, our work is reminiscent of Baroni and Zamparelli (2010) , who model adjectives as matrices and nouns as vectors, and work like Hartung et al. (2017) , which seeks to learn composition functions in addition to vector representations.\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite", "Transition", "Narrative_cite", "Transition", "Reflection", "Reflection", "Narrative_cite", "Single_summ"], "span_citation_mapping": [{"token_start": 49, "token_end": 66, "char_start": 281, "char_end": 338, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ma and Hovy, 2016;": "10489017", "Kim et al., 2016)": "686481"}}}, {"token_start": 67, "token_end": 96, "char_start": 342, "char_end": 421, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Luong et al., 2013;": "14276764", "Botha and Blunsom, 2014;": "2838374", "Cotterell et al., 2016)": "8796808"}}}, {"token_start": 123, "token_end": 142, "char_start": 579, "char_end": 650, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sun et al., 2014;": "9459634", "Yin et al., 2016)": "5756244"}}}, {"token_start": 143, "token_end": 161, "char_start": 655, "char_end": 728, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Choi et al., 2017)": "27131087"}}}, {"token_start": 163, "token_end": 182, "char_start": 735, "char_end": 828, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Stratos, 2017)": "8938702"}}}, {"token_start": 269, "token_end": 290, "char_start": 1257, "char_end": 1351, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Baroni et al. (2014)": "61492637"}}}, {"token_start": 302, "token_end": 348, "char_start": 1409, "char_end": 1614, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Baroni and Zamparelli (2010)": "8360910"}, "Reference": {"Hartung et al. (2017)": "14631029"}}}]}
{"id": "51912795_0", "paragraph": "[BOS] There has been considerable work on multitask sequence-to-sequence models for other tasks (Dong et al., 2015; Luong et al., 2015; Elliott and Kdr, 2017) .\n[BOS] There is a wide range of design questions and sharing strategies that we ignore here, focusing instead on under what circumstances the approach advocated in (Bollmann et al., 2017) works.\n[BOS] Our main observation-that the size of the target dataset is most predictive of multi-task learning gains-runs counter previous findings for other NLP tasks (Martnez Alonso and Plank, 2017; Bingel and Sgaard, 2017) .\n[BOS] Martnez Alonso and Plank (2017) find that the label entropy of the auxiliary dataset is more predictive; Bingel and S-gaard (2017) find that the relative differences in the steepness of the two single-task loss curves is more predictive.\n[BOS] Both papers consider sequence tagging problems with a small number of labels; and it is probably not a surprise that their findings do not seem to scale to the case of historical text normalization.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Narrative_cite", "Multi_summ", "Transition"], "span_citation_mapping": [{"token_start": 8, "token_end": 43, "char_start": 42, "char_end": 158, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dong et al., 2015;": "3666937", "Luong et al., 2015;": null}}}, {"token_start": 61, "token_end": 83, "char_start": 253, "char_end": 354, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bollmann et al., 2017)": "30419526"}}}, {"token_start": 111, "token_end": 133, "char_start": 507, "char_end": 574, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mart\u00ednez Alonso and Plank, 2017;": "2418468"}}}, {"token_start": 135, "token_end": 156, "char_start": 583, "char_end": 686, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 157, "token_end": 188, "char_start": 688, "char_end": 820, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Bingel and S\u00f8-gaard (2017)": "30419526"}, "Reference": {}}}]}
{"id": "51882806_2", "paragraph": "[BOS] and looked up their translations (y i ) 5000 i=1 via Google Translate.\n[BOS] Afterwards they used them to find a linear mapping W which minimizes 5000 i=1 W x i  y i .\n[BOS] This linear mapping W was later utilized as the translation mapping to generate a dictionary between two vocabularies and proved to be rather accurate, giving almost 90% top-5 precision.\n[BOS] Lample et al. (2017) extended the approach of (Mikolov et al., 2013) and trained a Generative Adversarial Network (GAN) model to find this mapping without any supervised signal whatsoever.\n[BOS] Generator was set to be this linear mapping, while discriminator should distinct between y and = W x.\n[BOS] This approach worked out: learning random bijection was impossible because of linearity and learning a bad linear mapping was impossible, because many source words would be mapped to nowhere, which is heavily penalized by discriminator.\n[BOS] Authors report 83.3% top-1 precision, which is a significant result for purely unsupervised approach.\n[BOS] Artetxe et al. (2017) built upon described methods to train translation model without any parallel corpora at all.\n[BOS] They trained a shared encoder which should encode sentences into the language-agnostic representations and then two separate decoders to reconstruct them into the desired language.\n[BOS] To make the encoding task non-trivial authors add noise to the input sentence: they randomly swap words, forcing encoder to learn internal structure of the sentence.\n[BOS] They also use backtranslation procedure to make model learn to translate.\n[BOS] This approach obtained 15.56 BLEU on Fr-En pair on WMT'14 dataset.\n[BOS] Artetxe et al. (2017) goes further and use adversarial loss to train their translation system.\n[BOS] They build a single shared encoder and a single shared decoder, using both denoising autoencoder loss and adversarial loss.\n[BOS] Corrupted version of the sentence is given to the encoder and its original form is reconstructed by the decoder.\n[BOS] Discriminator takes encoder's outputs and tries to guess which language was given to the encoder.\n[BOS] Backtranslation is also used to teach model to translate.\n[BOS] Such an approach shows striking performance, obtaining 32.8 BLEU on English-French pair of Multi30k-Task1 dataset.\n[BOS] Zoph et al. (2016) experimented with transferring different components of the translation model trained on a rich language pair (parent model) to a low-resource NMT system (child model).\n[BOS] Such a pretraining proved to be a very strong prior for the child model parameters and improved performance by an average of 5.6 BLEU.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Transition", "Transition", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 80, "token_end": 213, "char_start": 373, "char_end": 1020, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Lample et al. (2017)": null}, "Reference": {"(Mikolov et al., 2013)": "1966640"}}}, {"token_start": 214, "token_end": 337, "char_start": 1027, "char_end": 1653, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Artetxe et al. (2017)": "3515219"}, "Reference": {}}}, {"token_start": 338, "token_end": 426, "char_start": 1660, "char_end": 2107, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Artetxe et al. (2017)": "3515219"}, "Reference": {}}}, {"token_start": 468, "token_end": 509, "char_start": 2299, "char_end": 2485, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zoph et al. (2016)": "16631020"}, "Reference": {}}}]}
{"id": "51882806_1", "paragraph": "[BOS] log p(y t = k|x, y <t ) = log p trans (y t = k|x, y <t ) +  log p trg (y t = k|y <t ), where hyperparameter  denotes how much influence language model has for the prediction.\n[BOS] In deep fusion authors just concatenate hidden states of the translation model and language model and fine-tune the whole thing, keeping parameters of the language model freeze.\n[BOS] Mikolov et al. (2013) used distributed representations of words to learn a linear mapping between vector spaces of languages and showed that this mapping can serve as a good dictionary between the languages.\n[BOS] They pick 5k most frequent words from the source language (x i )\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 96, "token_end": 150, "char_start": 371, "char_end": 647, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Mikolov et al. (2013)": "1966640"}, "Reference": {}}}]}
{"id": "51882806_0", "paragraph": "[BOS] Large amounts of monolingual corpora makes it very appealing to incorporate unsupervised methods into machine translation techniques, and in recent years this trend is becoming more and more prominent.\n[BOS] Cheng et al. (2016) and Sennrich et al. (2015) propose an approach of backtranslation, which is training two translation models: sourcetarget and targetsource, and then generating synthetic training dataset from monolingual corpora to improve the models.\n[BOS] In such a way we incorporate the dual nature of the translation problem.\n[BOS] Authors report significant improvement up to +3.7 BLEU on EnglishGerman pair on IWSLT'14 dataset (Sennrich et al., 2015) .\n[BOS] Glehre et al. (2015) show how one can improve their translation model by shallow or deep fusion of separately trained language model.\n[BOS] Let p(y t = k|x, y <t ) be a probability that t-th word of output sequence y is the k-th word of the vocabulary under some sequence-tosequence model.\n[BOS] Here x is the input sentence, y <t are previous t  1 tokens.\n[BOS] In shallow fusion we combine probabilities from target language model and translation model in the following way:\n\n", "discourse_tags": ["Transition", "Multi_summ", "Reflection", "Narrative_cite", "Single_summ", "Other", "Other", "Other"], "span_citation_mapping": [{"token_start": 35, "token_end": 88, "char_start": 214, "char_end": 468, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Cheng et al. (2016)": "256189", "Sennrich et al. (2015)": "15600925"}, "Reference": {}}}, {"token_start": 122, "token_end": 138, "char_start": 634, "char_end": 674, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 140, "token_end": 168, "char_start": 683, "char_end": 816, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"G\u00fcl\u00e7ehre et al. (2015)": "15352384"}, "Reference": {}}}]}
{"id": "51870098_3", "paragraph": "[BOS] To avoid feature engineering, many deep learning models have been proposed for answer selection.\n[BOS] Among them, Zhang et al. (2017) proposed a novel interactive attention mechanism to address the problem of noise and redundancy prevalent in CQA.\n[BOS] Tay et al. (2017) introduced temporal gates for sequence pairs so that questions and answers are aware of what each other is remembering or forgetting.\n[BOS] Simple as their model are, they did not consider the relationship between question subject and body, which is useful for question condensing.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 22, "token_end": 48, "char_start": 121, "char_end": 254, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhang et al. (2017)": "6423645"}, "Reference": {}}}, {"token_start": 49, "token_end": 108, "char_start": 261, "char_end": 560, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Tay et al. (2017)": "6201557"}, "Reference": {}}}]}
{"id": "51870098_2", "paragraph": "[BOS] Since answer selection is inherently a ranking task, a few recent researches proposed to use local features to make global ranking decision.\n[BOS] was the first work that applies structured prediction model on CQA answer selection task.\n[BOS] Joty et al. (2016) approached the task with a global inference process to exploit the information of all answers in the question-thread in the form of a fully connected graph.\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ"], "span_citation_mapping": [{"token_start": 44, "token_end": 81, "char_start": 249, "char_end": 424, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Joty et al. (2016)": "6243869"}, "Reference": {}}}]}
{"id": "51870098_1", "paragraph": "[BOS] Earlier work of answer selection in CQA relied heavily on feature engineering, linguistic tools, and external resource.\n[BOS] investigated a wide range of feature types including similarity features, content features, thread level/meta features, and automatically generated features for SemEval CQA models.\n[BOS] Tran et al. (2015) studied the use of topic model based features and word vector representation based features in the answer re-ranking task.\n[BOS] Filice et al. (2016) designed various heuristic features and thread-based features that can signal a good answer.\n[BOS] Although achieving good performance, these methods rely heavily on feature engineering, which requires a large amount of manual work and domain expertise.\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 57, "token_end": 86, "char_start": 319, "char_end": 460, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Tran et al. (2015)": "14319089"}, "Reference": {}}}, {"token_start": 87, "token_end": 111, "char_start": 467, "char_end": 580, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Filice et al. (2016)": "8699362"}, "Reference": {}}}]}
{"id": "51870098_0", "paragraph": "[BOS] One main task in community question answering is answer selection, i.e., to rate the answers according to their quality.\n[BOS] The SemEval CQA tasks (Nakov et al., , 2017 provide universal benchmark datasets for evaluating researches on this problem.\n\n", "discourse_tags": ["Transition", "Single_summ"], "span_citation_mapping": [{"token_start": 28, "token_end": 55, "char_start": 133, "char_end": 256, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Nakov et al., , 2017": "3063394"}, "Reference": {}}}]}
{"id": "5634522_4", "paragraph": "[BOS] To the best of our knowledge, our work is the first to use distributed representations of words to improve a bootstrapped system by expanding the training set.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "5634522_3", "paragraph": "[BOS] In most IE systems, including ours, word classes or word vectors are used as features in a classifier (Haghighi and Klein, 2006; Ratinov and Roth, 2009) .\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 10, "token_end": 38, "char_start": 42, "char_end": 158, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Haghighi and Klein, 2006;": "8709299", "Ratinov and Roth, 2009)": "1859014"}}}]}
{"id": "5634522_2", "paragraph": "[BOS] In this paper, we use the setting of bootstrapped pattern-based entity extraction (Riloff, 1996; Thelen and Riloff, 2002) .\n[BOS] This can be viewed as a form of the Yarowsky algorithm, with pattern learning as an additional step.\n[BOS] Pattern based approaches have been widely used for IE (Chiticariu et al., 2013; Fader et al., 2011; Etzioni et al., 2005) .\n[BOS] Patterns are useful in two ways: they are good features, and they identify promising candidate entities.\n[BOS] Recently, Gupta and Manning (2014) improved pattern scoring (Step 2 in Algorithm 1) using predicted labels of unlabeled entities.\n[BOS] For entity scoring (Step 3), they used an average of feature values to predict the scores.\n[BOS] We use the same framework but focus on improving the entity classifiers.\n\n", "discourse_tags": ["Narrative_cite", "Reflection", "Narrative_cite", "Transition", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 11, "token_end": 34, "char_start": 43, "char_end": 127, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Riloff, 1996;": "15894892", "Thelen and Riloff, 2002)": "137155"}}}, {"token_start": 59, "token_end": 96, "char_start": 243, "char_end": 364, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chiticariu et al., 2013;": "17526435", "Fader et al., 2011;": "10318045", "Etzioni et al., 2005)": "7162988"}}}, {"token_start": 120, "token_end": 165, "char_start": 494, "char_end": 710, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Gupta and Manning (2014)": "1367004"}, "Reference": {}}}]}
{"id": "5634522_1", "paragraph": "[BOS] Yarowsky's style of self-training algo-rithms (Yarowsky, 1995) have been shown to be successful at bootstrapping (Collins and Singer, 1999) .\n[BOS] Co-training (Blum and Mitchell, 1998) and its boostrapped adaptation (Collins and Singer, 1999) require disjoint views of the features of the data.\n[BOS] Whitney and Sarkar (2012) proposed a modified Yarowsky algorithm that used label propagation on graphs, inspired by Subramanya et al. (2010) algorithm that used a large labeled data for domain adaptation.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 25, "char_start": 6, "char_end": 68, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yarowsky, 1995)": "1487550"}}}, {"token_start": 32, "token_end": 41, "char_start": 105, "char_end": 145, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Collins and Singer, 1999)": "859162"}}}, {"token_start": 43, "token_end": 54, "char_start": 154, "char_end": 191, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Blum and Mitchell, 1998)": "207228399"}}}, {"token_start": 56, "token_end": 67, "char_start": 200, "char_end": 249, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Collins and Singer, 1999)": "859162"}}}, {"token_start": 78, "token_end": 123, "char_start": 308, "char_end": 512, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Whitney and Sarkar (2012)": "1965764"}, "Reference": {"Subramanya et al. (2010)": "14000702"}}}]}
{"id": "5634522_0", "paragraph": "[BOS] Bootstrapping has many variants, such as self-training, co-training, and label propagation.\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "5063108_1", "paragraph": "[BOS] Beyond word alignment, another branch of approaches for multilingual word embeddings are based on parallel or comparable data, such as parallel sentences (AP Chandar et al., 2014; Gouws et al., 2015; Luong et al., 2015; Hermann and Blunsom, 2014; Schwenk et al., 2017) , phrase translations (Duong et al., 2016) and comparable documents (Vulic and Moens, 2015) .\n[BOS] Moreover, to reduce the need of bilingual alignment, several approaches have been designed to learn crosslingual embeddings based on a small seed dictionary (Vulic and Korhonen, 2016; Artetxe et al., 2017) , or even with no supervision (Cao et al., 2016; Zhang et al., 2017b,a; Conneau et al., 2017) .\n[BOS] However, such methods are still limited to bilingual word embedding learning and remaining to be explored for common semantic space construction.\n[BOS] Figure 1 shows the overview of our neural architecture.\n[BOS] We project all monolingual word embeddings into a common semantic space based on word-level as well as cluster-level alignments and learn the transformation functions.\n[BOS] First, on word-level, we build a neighborhood-consistent CorrNet to augment word representations with neighbor based clusters and align them in the common semantic space.\n[BOS] In addition, we apply a language-independent convolutional neural networks to compose character-level word representation and concatenate it with word representation in the common semantic space.\n[BOS] Finally, we construct clusters based on linguistic properties, such as closed word classes and affixes, and align them in the common semantic space.\n[BOS] We jointly optimize for all the alignments in the common semantic space for each pair of languages.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Transition", "Reflection", "Reflection", "Reflection", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 25, "token_end": 71, "char_start": 141, "char_end": 274, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Chandar et al., 2014;": "217774", "Gouws et al., 2015;": "7021865", "Luong et al., 2015;": "13603998", "Schwenk et al., 2017)": "6660863"}}}, {"token_start": 72, "token_end": 83, "char_start": 277, "char_end": 317, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Duong et al., 2016)": "13888952"}}}, {"token_start": 84, "token_end": 95, "char_start": 322, "char_end": 366, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vulic and Moens, 2015)": "14183678"}}}, {"token_start": 122, "token_end": 143, "char_start": 516, "char_end": 580, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vulic and Korhonen, 2016;": "17515652", "Artetxe et al., 2017)": "13335042"}}}, {"token_start": 145, "token_end": 176, "char_start": 586, "char_end": 674, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cao et al., 2016;": "5757459", "Conneau et al., 2017)": "3470398"}}}]}
{"id": "5063108_0", "paragraph": "[BOS] Multilingual word embeddings have advanced many multilingual natural language processing tasks, such as machine translation (Zou et al., 2013; Mikolov et al., 2013; Madhyastha and Espaa-Bonet, 2017) , dependency parsing (Guo et al., 2015; Ammar et al., 2016a) , and name tagging Tsai and Roth, 2016) .\n[BOS] Using bilingual aligned words, previous methods project multiple monolingual embeddings into a shared semantic space using linear mappings (Mikolov et al., 2013; Rothe et al., 2016; MarcoBaroni, 2015; Xing et al., 2015) or canonical correlation analysis (CCA) (Ammar et al., 2016b; Faruqui and Dyer, 2014; Lu et al., 2015) .\n[BOS] Compared with CCA, which only optimizes the correlation for each individual pair of languages, linear mapping based methods can jointly optimize all the languages in the common semantic space.\n[BOS] We focus on learning linear mappings to construct the common semantic space and adopt correlational neural networks (Chandar et al., 2016; Rajendran et al., 2015) as the basic model.\n[BOS] In contrast to previous work which only exploited monolingual word semantics, we introduce multiple cluster-level alignments.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Transition", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 18, "token_end": 51, "char_start": 110, "char_end": 204, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zou et al., 2013;": "931054", "Mikolov et al., 2013;": "1966640", "Madhyastha and Espa\u00f1a-Bonet, 2017)": "30205929"}}}, {"token_start": 52, "token_end": 71, "char_start": 207, "char_end": 265, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Guo et al., 2015;": "18634877", "Ammar et al., 2016a)": "2868247"}}}, {"token_start": 73, "token_end": 81, "char_start": 272, "char_end": 305, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Tsai and Roth, 2016)": "15156124"}}}, {"token_start": 98, "token_end": 135, "char_start": 416, "char_end": 533, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mikolov et al., 2013;": "1966640", "Rothe et al., 2016;": "15201331", "MarcoBaroni, 2015;": "12187767", "Xing et al., 2015)": "3144258"}}}, {"token_start": 136, "token_end": 169, "char_start": 537, "char_end": 636, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ammar et al., 2016b;": "1227830", "Faruqui and Dyer, 2014;": "3792324", "Lu et al., 2015)": "874413"}}}, {"token_start": 214, "token_end": 241, "char_start": 898, "char_end": 1006, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chandar et al., 2016;": "11533063", "Rajendran et al., 2015)": "8650061"}}}]}
{"id": "51818591_2", "paragraph": "[BOS] Neural sentence compression.\n[BOS] Filippova et al. (2015) proposed a delete-based sentence compression system which took as input a sentence and output a binary sequence corresponding to word deletion decisions in the sentence.\n[BOS] The model was trained on a set of 2 millions sentence pairs which was constructed by the same approach used in Filippova and Altun (2013) .\n[BOS] There are also some neural approaches for abstractive sentence compression.\n[BOS] Rush et al. (2015) proposed a fully data-driven approach which utilized neural language models for abstractive sentence compression.\n[BOS] They tried different kinds of encoders to encode the input sentence into vector representation of fixed dimensions.\n[BOS] Chopra et al. (2016) further improved the model with Recurrent Neural Networks.\n[BOS] However, both works used vocabularies of fixed size for target sentence generation.\n[BOS] Wubben et al. (2016) used a Seq2Seq model with bi-directional LSTMs for abstractive compression of captions.\n[BOS] Toutanova et al. (2016) manually created a multi-reference dataset for sentence and short paragraph compression and studied the correlations between several automatic evaluation metrics and human judgment.\n\n", "discourse_tags": ["Transition", "Single_summ", "Narrative_cite", "Transition", "Single_summ", "Single_summ", "Single_summ", "Transition", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 7, "token_end": 44, "char_start": 41, "char_end": 234, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Filippova et al. (2015)": "1992250"}, "Reference": {}}}, {"token_start": 45, "token_end": 75, "char_start": 241, "char_end": 378, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Filippova and Altun (2013)": "9751546"}}}, {"token_start": 90, "token_end": 116, "char_start": 469, "char_end": 601, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Rush et al. (2015)": "15761498"}, "Reference": {}}}, {"token_start": 137, "token_end": 154, "char_start": 730, "char_end": 809, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Chopra et al. (2016)": "133195"}, "Reference": {}}}, {"token_start": 172, "token_end": 202, "char_start": 906, "char_end": 1014, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wubben et al. (2016)": "9795001"}, "Reference": {}}}, {"token_start": 203, "token_end": 239, "char_start": 1021, "char_end": 1226, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Toutanova et al. (2016)": "8241566"}, "Reference": {}}}]}
{"id": "51818591_1", "paragraph": "[BOS] Abstractive sentence compression.\n[BOS] Abstractive sentence compression extends delete-based compression methods with additional operations, such as substitution, reordering and insertion.\n[BOS] Cohn and Lapata (2008) proposed a discriminative tree-to-tree transduction model which incorporated a grammar extraction method and used a language model for coherent output.\n[BOS] Galanis and Androutsopoulos (2011) presented a dataset for extractive and abstractive sentence compression and proposed a SVR based abstractive sentence compressor which utilized additional PMI-based and LDA-based features.\n[BOS] Shafieibavani et al. (2016) proposed a word graph-based model which can improve both informativeness and grammaticality of the sentence at the same time.\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 32, "token_end": 65, "char_start": 202, "char_end": 376, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Cohn and Lapata (2008)": "2411338"}, "Reference": {}}}, {"token_start": 66, "token_end": 110, "char_start": 383, "char_end": 606, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 111, "token_end": 147, "char_start": 613, "char_end": 766, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Shafieibavani et al. (2016)": null}, "Reference": {}}}]}
{"id": "51818591_0", "paragraph": "[BOS] Delete-based sentence compression.\n[BOS] A large number of work is devoted to delete-based sentence compression.\n[BOS] Jing (2000) presented a system that used multiple sources of knowledge to decide which phrases in a sentence can be removed.\n[BOS] Knight and Marcu (2000) proposed statistical approaches to mimic the sentence compression process, they used both noisy-channel and decision-tree to solve the problem.\n[BOS] McDonald (2006) presented a discriminative large-margin learning framework coupled with a feature set and syntactic representations for sentence compression.\n[BOS] Clarke and Lapata (2006) compared different models for sentence compression across domains and assessed a number of automatic evaluation measures.\n[BOS] Clarke and Lapata (2008) used integer linear programming to infer globally optimal compression with linguistically motivated constraints.\n[BOS] Berg-Kirkpatrick et al. (2011) proposed a joint model of sentence extraction and compression for multi-document summarization.\n[BOS] Filippova and Altun (2013) presented a method for automatically building delete-based sentence compression corpus and proposed an compression method which used structured prediction.\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 24, "token_end": 48, "char_start": 125, "char_end": 249, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Jing (2000)": "8934802"}, "Reference": {}}}, {"token_start": 49, "token_end": 82, "char_start": 256, "char_end": 423, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Knight and Marcu (2000)": "9363872"}, "Reference": {}}}, {"token_start": 83, "token_end": 107, "char_start": 430, "char_end": 587, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"McDonald (2006)": "3014198"}, "Reference": {}}}, {"token_start": 108, "token_end": 132, "char_start": 594, "char_end": 740, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Clarke and Lapata (2006)": "6412912"}, "Reference": {}}}, {"token_start": 133, "token_end": 155, "char_start": 747, "char_end": 884, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Clarke and Lapata (2008)": "3004447"}, "Reference": {}}}, {"token_start": 156, "token_end": 184, "char_start": 891, "char_end": 1017, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Berg-Kirkpatrick et al. (2011)": "15467396"}, "Reference": {}}}, {"token_start": 185, "token_end": 216, "char_start": 1024, "char_end": 1206, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Filippova and Altun (2013)": "9751546"}, "Reference": {}}}]}
{"id": "51875796_2", "paragraph": "[BOS] Finally, other methods have found very simple input modifications can break neural models.\n[BOS] For example, adding character level noise drastically reduces machine translation quality (Belinkov and Bisk, 2018) , while paraphrases can fool textual entailment and visual question answering systems (Iyyer et al., 2018; Ribeiro et al., 2018) .\n[BOS] Jia et al. (2017) place distracting sentences at the end of paragraphs and cause QA systems to incorrectly pick up on the misleading information.\n[BOS] These types of input modifications can evaluate one specific type of phenomenon and are complementary to our approach.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 27, "token_end": 40, "char_start": 165, "char_end": 218, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Belinkov and Bisk, 2018)": "3513372"}}}, {"token_start": 48, "token_end": 74, "char_start": 248, "char_end": 347, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Iyyer et al., 2018;": "4956100", "Ribeiro et al., 2018)": "21740766"}}}, {"token_start": 76, "token_end": 105, "char_start": 356, "char_end": 501, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Jia et al. (2017)": "7228830"}, "Reference": {}}}]}
{"id": "51875796_1", "paragraph": "[BOS] Other work has explored specific limitations of NLP systems.\n[BOS] Rimell et al. (2009) show that parsers struggle on test examples with unbounded dependencies.\n[BOS] The most closely related work to ours is Ettinger et al. (2017) who also use humans as adversaries.\n[BOS] Unlike their Build-it Break-it setting, we have a ready-made audience of \"breakers\" who are motivated and capable of generating adversarial examples.\n[BOS] Our work also differs in that we use model interpretation methods to facilitate the breaking in an interactive manner.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 14, "token_end": 34, "char_start": 73, "char_end": 166, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Rimell et al. (2009)": "9357023"}, "Reference": {}}}, {"token_start": 43, "token_end": 60, "char_start": 214, "char_end": 272, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ettinger et al. (2017)": "25422730"}, "Reference": {}}}]}
{"id": "51875796_0", "paragraph": "[BOS] Creating evaluation datasets to get a fine-grained analysis of particular linguistics features or model attributes has been explored in past work.\n[BOS] The LAM-BADA dataset tests a model's ability to understand the broad contexts present in book passages (Paperno et al., 2016) .\n[BOS] Other work focuses on natural language inference, where challenge examples highlight existing model failures (Wang et al., 2018; Glockner et al., 2018; Naik et al., 2018) .\n[BOS] Our work is unique in that we use human as adversaries to expose model weaknesses, which provides a diverse set of phenomena (from paraphrases to multi-hop reasoning) that models can't solve.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 47, "token_end": 58, "char_start": 248, "char_end": 284, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Paperno et al., 2016)": "2381275"}}}, {"token_start": 73, "token_end": 100, "char_start": 387, "char_end": 463, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wang et al., 2018;": "5034059", "Glockner et al., 2018;": "19204066", "Naik et al., 2018)": "46932607"}}}]}
{"id": "5527031_1", "paragraph": "[BOS] Several works focus on particular types of contexts for learning word embeddings.\n[BOS] Cirik and Yuret (2014) investigates S-CODE word embeddings based on substitute word contexts.\n[BOS] Ling et al. (2015b) and Ling et al. (2015a) propose extensions to the standard window-based context modeling.\n[BOS] Alternatively, another recent popular line of work Kiela et al., 2015) attempts to improve word embeddings by using manually-constructed resources, such as WordNet.\n[BOS] These techniques could be complementary to our work.\n[BOS] Finally, Yin and Schtze (2015) propose word embeddings ensembles (including concatenation) but evaluate mostly on intrinsic tasks and do not consider different types of contexts.\n\n", "discourse_tags": ["Transition", "Single_summ", "Multi_summ", "Single_summ", "Reflection", "Single_summ"], "span_citation_mapping": [{"token_start": 16, "token_end": 36, "char_start": 94, "char_end": 187, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Cirik and Yuret (2014)": "18266205"}, "Reference": {}}}, {"token_start": 37, "token_end": 65, "char_start": 194, "char_end": 303, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ling et al. (2015b)": "14800090", "Ling et al. (2015a)": "1144632"}, "Reference": {}}}, {"token_start": 74, "token_end": 99, "char_start": 361, "char_end": 474, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kiela et al., 2015)": "7981682"}, "Reference": {}}}, {"token_start": 112, "token_end": 143, "char_start": 549, "char_end": 718, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "5527031_0", "paragraph": "[BOS] There are a number of recent works whose goal is a broad evaluation of the performance of different word embeddings on a range of tasks.\n[BOS] However, to the best of our knowledge, none of them focus on embeddings learned with diverse context types as we do.\n[BOS] Levy et al. (2015) , Lapesa and Evert (2014) , and Lai et al. (2015) evaluate several design choices when learning word representations.\n[BOS] However, Levy et al. (2015) and Lapesa and Evert (2014) perform only intrinsic evaluations and restrict context representation to word windows, while Lai et al. (2015) do perform extrinsic evaluations, but restrict their context representation to a word window with the default size of 5.\n[BOS] Schnabel et al. (2015) and Tsvetkov et al. (2015) low correlation between intrinsic and extrinsic results with different word embeddings (they did not evaluate different context types), which is consistent with differences we found between intrinsic and extrinsic performance patterns in all tasks, except parsing.\n[BOS] Bansal et al. (2014) show that functional (dependency-based and small-window) embeddings yield higher parsing improvements than topical (large-window) embeddings, which is consistent with our findings.\n\n", "discourse_tags": ["Transition", "Transition", "Multi_summ", "Multi_summ", "Multi_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 54, "token_end": 89, "char_start": 272, "char_end": 408, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Levy et al. (2015)": null, "Lapesa and Evert (2014)": null, "Lai et al. (2015)": null}, "Reference": {}}}, {"token_start": 92, "token_end": 120, "char_start": 424, "char_end": 557, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Levy et al. (2015)": null, "Lapesa and Evert (2014)": null}, "Reference": {}}}, {"token_start": 122, "token_end": 150, "char_start": 565, "char_end": 703, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Lai et al. (2015)": null}, "Reference": {}}}, {"token_start": 151, "token_end": 210, "char_start": 710, "char_end": 1024, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Schnabel et al. (2015)": "6197592"}, "Reference": {}}}, {"token_start": 211, "token_end": 252, "char_start": 1031, "char_end": 1232, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Bansal et al. (2014)": "7803700"}, "Reference": {}}}]}
{"id": "5698849_2", "paragraph": "[BOS] 3 Graph-Based Joint Models\n\n", "discourse_tags": ["Other"], "span_citation_mapping": []}
{"id": "5698849_1", "paragraph": "[BOS] In these approaches, case arguments were identified per predicate without considering interactions between multiple predicates and candidate arguments in a sentence.\n[BOS] In the semantic role labeling (SRL) task, Yang and Zong (2014) pointed out that information of different predicates and their candidate arguments could help each other for identifying arguments taking part in semantic roles.\n[BOS] They exploited a reranking method to capture the interactions between multiple predicates and candidate arguments, and jointly determine argument structures of all predicates in a sentence (Yang and Zong, 2014) .\n[BOS] In this paper, we propose new joint analysis methods for identifying case arguments of all predicates in a sentence capturing interactions between multiple predicates and candidate arguments.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 37, "token_end": 106, "char_start": 220, "char_end": 619, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Yang and Zong (2014)": null, "(Yang and Zong, 2014)": null}, "Reference": {}}}]}
{"id": "5698849_0", "paragraph": "[BOS] For Japanese PAS analysis research, the NAIST Text Corpus has been used as a standard benchmark (Iida et al., 2007) .\n[BOS] One of the representative researches using the NAIST Text Corpus is Imamura et al. (2009) .\n[BOS] They built three distinct models corresponding to the three case roles by extracting features defined on each pair of a predicate and a candidate argument.\n[BOS] Using each model, they select the best candidate argument for each case per predicate.\n[BOS] Their models are based on maximum entropy model and can easily incorporate various features, resulting in high accuracy.\n[BOS] While in Imamura et al. (2009) one case argument is identified at a time per predicate, the method proposed by Sasano and Kurohashi (2011) simultaneously determines all the three case arguments per predicate by exploiting large-scale case frames obtained from large raw texts.\n[BOS] They focus on identification of implicit arguments (Zero and Inter-Zero), and achieves comparable results to Imamura et al. (2009) .\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 18, "token_end": 29, "char_start": 83, "char_end": 121, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Iida et al., 2007)": "16383740"}}}, {"token_start": 31, "token_end": 117, "char_start": 130, "char_end": 603, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Imamura et al. (2009)": "14712610"}, "Reference": {}}}, {"token_start": 120, "token_end": 138, "char_start": 619, "char_end": 696, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Imamura et al. (2009)": "14712610"}, "Reference": {}}}, {"token_start": 139, "token_end": 174, "char_start": 698, "char_end": 886, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Sasano and Kurohashi (2011)": "13931451"}, "Reference": {}}}, {"token_start": 180, "token_end": 203, "char_start": 925, "char_end": 1023, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Imamura et al. (2009)": "14712610"}}}]}
{"id": "5175754_1", "paragraph": "[BOS] The battered US Navy destroyer Cole has begun its journey home from Yemen ...\n[BOS] Flanked by other US warships and guarded by aircraft, the ship was towed out of Aden Harbor to rendezvous with a huge\n\n", "discourse_tags": ["Other", "Other"], "span_citation_mapping": []}
{"id": "5175754_0", "paragraph": "[BOS] Early computational approaches to coreference resolution were built around what is now referred to as mention-pair models.\n[BOS] Such models use two stage resolution; the first stage calculates pairwise scores reflecting the likelihood that a mention and its candidate antecedents are coreferential while the second phase decodes these scores into coreference clusters.\n[BOS] The simplest way to decode is locationally greedy (Soon et al., 2001) , in that the closest candidate with a compatibility score over some threshold is deemed a mention's antecedent.\n[BOS] Anaphoricity determination (determining whether a mention constitutes a good first mention of an entity) is mediated by the threshold since a mention without a sufficiently good candidate antecedent starts a new cluster.\n[BOS] While these local models achieve a reasonable baseline (Soon et al. (2001) achieves MUC F-scores of 62.6 and 60.4 on MUC 6 and 7), they can make global consistency errors which limit their usefulness downstream.\n[BOS] For instance, in the following excerpt from bn/voa/00/voa 0068 of OntoNotes 5, it is possible that a system uses local evidence such as synonymy to misclassify the ship as the antecedent of a huge Norwegian transport vessel and similarly The battered US Navy destroyer Cole as the antecendent of the ship; unfortunately, these local decisions imply a clustering in which Cole is referred to as a Norwegian transport vessel.\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ", "Transition", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 65, "token_end": 103, "char_start": 382, "char_end": 564, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 142, "token_end": 190, "char_start": 798, "char_end": 1009, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "51879191_2", "paragraph": "[BOS] However, most neural SRL works seldom pay much attention to the impact of input syntactic parse over the resulting SRL performance.\n[BOS] This work is thus more than proposing a high performance SRL model through reviewing the highlights of previous models, and presenting an effective syntactic tree based argument pruning.\n[BOS] Our work is also closely related to (Punyakanok et al., 2008; He et al., 2017) .\n[BOS] Under the traditional methods, Punyakanok et al. (2008) investigated the significance of syntax to SRL system and shown syntactic information most crucial in the pruning stage.\n[BOS] He et al. (2017) presented extensive error analysis with deep learning model for span SRL, including discussion of how constituent syntactic parser could be used to improve SRL performance.\n\n", "discourse_tags": ["Transition", "Transition", "Reflection", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 60, "token_end": 85, "char_start": 337, "char_end": 415, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Punyakanok et al., 2008;": "11162815", "He et al., 2017)": "33626727"}}}, {"token_start": 92, "token_end": 122, "char_start": 455, "char_end": 600, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Punyakanok et al. (2008)": "11162815"}, "Reference": {}}}, {"token_start": 123, "token_end": 159, "char_start": 607, "char_end": 796, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"He et al. (2017)": "33626727"}, "Reference": {}}}]}
{"id": "51879191_1", "paragraph": "[BOS] With the impressive success of deep neural networks in various NLP tasks (Zhang et al., 2016; Qin et al., 2017; Cai et al., 2017) , a series of neural SRL systems have been proposed.\n[BOS] Foland and Martin (2015) presented a dependency semantic role labeler using convolutional and time-domain neural networks, while FitzGerald et al. (2015) exploited neural network to jointly embed arguments and semantic roles, akin to the work (Lei et al., 2015) , which induced a compact feature representation applying tensor-based approach.\n[BOS] Recently, researchers consider multiple ways to effectively integrate syntax into SRL learning.\n[BOS] Roth and Lapata (2016) introduced dependency path embedding to model syntactic information and exhibited a notable success.\n[BOS] leveraged the graph convolutional network to incorporate syntax into neural models.\n[BOS] Differently, proposed a syntax-agnostic model using effective word representation for dependency SRL, which for the first time achieves comparable performance as stateof-the-art syntax-aware SRL models.\n\n", "discourse_tags": ["Narrative_cite", "Multi_summ", "Transition", "Single_summ", "Transition", "Other"], "span_citation_mapping": [{"token_start": 7, "token_end": 37, "char_start": 37, "char_end": 135, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhang et al., 2016;": "17566738", "Qin et al., 2017;": "16273304", "Cai et al., 2017)": "17268102"}}}, {"token_start": 50, "token_end": 72, "char_start": 195, "char_end": 316, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Foland and Martin (2015)": "18951510"}, "Reference": {}}}, {"token_start": 74, "token_end": 120, "char_start": 324, "char_end": 537, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"FitzGerald et al. (2015)": "15048880"}, "Reference": {"(Lei et al., 2015)": null}}}, {"token_start": 137, "token_end": 158, "char_start": 646, "char_end": 769, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Roth and Lapata (2016)": "5779419"}, "Reference": {}}}]}
{"id": "51879191_0", "paragraph": "[BOS] Semantic role labeling was pioneered by Gildea and Jurafsky (2002) .\n[BOS] Most traditional SRL models rely heavily on feature templates (Pradhan et al., 2005; Zhao et al., 2009b; Bjrkelund et al., 2009 ).\n[BOS] Among them, Pradhan et al. (2005) combined features derived from different syntactic parses based on SVM classifier, while Zhao et al. (2009b) presented an integrative approach for dependency SRL by greedy feature selection algorithm.\n[BOS] Later, Collobert et al. (2011) proposed a convolutional neural network model of inducing word embeddings substituting for hand-crafted features, which was a breakthrough for SRL task.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Multi_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 19, "char_start": 6, "char_end": 72, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Gildea and Jurafsky (2002)": "62182406"}}}, {"token_start": 21, "token_end": 58, "char_start": 81, "char_end": 208, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Pradhan et al., 2005;": "2440012", "Zhao et al., 2009b;": "2193825", "Bj\u00f6rkelund et al., 2009": "33777646"}}}, {"token_start": 64, "token_end": 85, "char_start": 230, "char_end": 333, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Pradhan et al. (2005)": "2440012"}, "Reference": {}}}, {"token_start": 87, "token_end": 109, "char_start": 341, "char_end": 452, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhao et al. (2009b)": "2193825"}, "Reference": {}}}, {"token_start": 112, "token_end": 148, "char_start": 466, "char_end": 642, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Collobert et al. (2011)": null}, "Reference": {}}}]}
{"id": "618683_1", "paragraph": "[BOS] The idea of exploiting agreement between two latent variable models is not new; there has been substantial previous work on leveraging the strengths of two complementary models.\n[BOS] Klein and Manning (2004) combine two complementary models for grammar induction, one that models constituency and one that models dependency, in a manner broadly similar to the current work.\n[BOS] Aside from investigating a different domain, one novel aspect of this paper is that we present a formal objective and a training algorithm for combining two generic models.\n\n", "discourse_tags": ["Transition", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 33, "token_end": 69, "char_start": 190, "char_end": 380, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Klein and Manning (2004)": "1364249"}, "Reference": {}}}]}
{"id": "618683_0", "paragraph": "[BOS] Our approach is similar in spirit to co-training, where two classifiers, complementary by the virtue of having different views of the data, are trained jointly to encourage agreement (Blum and Mitchell, 1998; Collins and Singer, 1999) .\n[BOS] One key difference in our work is that we rely exclusively on data likelihood to guide the two models in an unsupervised manner, rather than relying on an initial handful of labeled examples.\n\n", "discourse_tags": ["Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 31, "token_end": 49, "char_start": 158, "char_end": 240, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Blum and Mitchell, 1998;": "207228399", "Collins and Singer, 1999)": "859162"}}}]}
{"id": "51985896_1", "paragraph": "[BOS] Following Kindermans et al. (2018) , however, our approach improves upon the latter methods for the reasons outlined above.\n[BOS] Furthermore, PatternAttribution is related to Montavon et al. (2017) who make use of Taylor decompositions to explain deep models.\n[BOS] PatternAttribution reveals a good root point for the decomposition, the authors explain.\n\n", "discourse_tags": ["Reflection", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 27, "char_start": 6, "char_end": 129, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Kindermans et al. (2018)": "32654687"}}}, {"token_start": 30, "token_end": 74, "char_start": 149, "char_end": 361, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Montavon et al. (2017)": "5731985"}, "Reference": {}}}]}
{"id": "51985896_0", "paragraph": "[BOS] Many of the approaches used to explain and interpret models in NLP mirror methods originally developed in the vision domain, such as the recent approaches by Li et al. (2016) , Arras et al. (2017a) , and Arras et al. (2017b) .\n[BOS] In this paper we implemented a similar strategy.\n\n", "discourse_tags": ["Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 13, "token_end": 58, "char_start": 69, "char_end": 230, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Li et al. (2016)": "14099741", "Arras et al. (2017a)": "2479906", "Arras et al. (2017b)": "19624082"}}}]}
{"id": "51878103_0", "paragraph": "[BOS] NER is a widely studied problem.\n[BOS] Most of previous work rely on the use of CRFs (Finkel et al., 2005; Jun'ichi and Torisawa, 2007; Ratinov and Roth, 2009; Passos et al., 2014; Radford et al., 2015; Luo et al., 2015) .\n[BOS] A recent trend has achieved particularly good results modeling NER as an end-to-end task using neural networks (dos Santos and Guimares, 2015; Chiu and Nichols, 2016; Lample et al., 2016; Yang et al., 2016 ; Gillick et al., 2016) .\n[BOS] While this constitutes a big step forward, certain applications (e.g., in heavily regulated sectors) require a degree of explainability that neural approaches cannot yet provide.\n[BOS] Previous work has already regarded NER as a knowledge intensive task (Florian et al., 2003; Zhang and Johnson, 2003; Jun'ichi and Torisawa, 2007; Ratinov and Roth, 2009; Lin and Wu, 2009; Passos et al., 2014; Radford et al., 2015; Luo et al., 2015) .\n[BOS] Most of these works incorporate background knowledge in the form of entity-type gazetteers (Florian et al., 2003; Zhang and Johnson, 2003; Jun'ichi and Torisawa, 2007; Ratinov and Roth, 2009; Passos et al., 2014) .\n[BOS] Others, used external knowledge by exploiting the association between NER and NED (Durrett and Klein, 2014; Radford et al., 2015; Luo et al., 2015; Nguyen et al., 2016) .\n[BOS] In this study, we attempt to bring more light on the issue by quantifying the effect of different degrees of external knowledge.\n[BOS] Our modular framework allows to test this intuition via novel feature sets that reflect the degree of knowledge contained in available knowledge sources.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite", "Transition", "Narrative_cite", "Narrative_cite", "Narrative_cite", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 19, "token_end": 71, "char_start": 86, "char_end": 226, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Finkel et al., 2005;": "10977241", "Jun'ichi and Torisawa, 2007;": "14343843", "Ratinov and Roth, 2009;": "1859014", "Passos et al., 2014;": "9345583", "Radford et al., 2015;": "8770833", "Luo et al., 2015)": "306227"}}}, {"token_start": 85, "token_end": 135, "char_start": 308, "char_end": 464, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Santos and Guimar\u00e3es, 2015;": "9150889", "Chiu and Nichols, 2016;": "6300165", "Lample et al., 2016;": "6042994", "Yang et al., 2016": "1548828", "Gillick et al., 2016)": "384520"}}}, {"token_start": 180, "token_end": 245, "char_start": 702, "char_end": 906, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Florian et al., 2003;": "10606201", "Zhang and Johnson, 2003;": "7896577", "Jun'ichi and Torisawa, 2007;": "14343843", "Ratinov and Roth, 2009;": "1859014", "Lin and Wu, 2009;": "8148140", "Passos et al., 2014;": "9345583", "Radford et al., 2015;": "8770833", "Luo et al., 2015)": "306227"}}}, {"token_start": 258, "token_end": 305, "char_start": 983, "char_end": 1127, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Florian et al., 2003;": "10606201", "Zhang and Johnson, 2003;": "7896577", "Jun'ichi and Torisawa, 2007;": "14343843", "Ratinov and Roth, 2009;": "1859014", "Passos et al., 2014)": "9345583"}}}, {"token_start": 317, "token_end": 350, "char_start": 1206, "char_end": 1304, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Durrett and Klein, 2014;": "7499734", "Radford et al., 2015;": "8770833", "Luo et al., 2015;": "306227", "Nguyen et al., 2016)": "10327730"}}}]}
{"id": "51875355_7", "paragraph": "[BOS] 3.\n[BOS] SOFTWARE ENGINEERING WORLD: This world describes situations from the first-person perspective of a software development manager, e.g., task assignment to different project team members, stages of software development, bug tickets.\n\n", "discourse_tags": ["Other", "Other"], "span_citation_mapping": []}
{"id": "51875355_6", "paragraph": "[BOS] 2.\n[BOS] HOMEWORK WORLD: This world describes situations from the first-person perspective of a student, e.g., courses taken, assignments in different courses, deadlines of assignments.\n\n", "discourse_tags": ["Other", "Other"], "span_citation_mapping": []}
{"id": "51875355_5", "paragraph": "[BOS] 1.\n[BOS] MEETING WORLD: This world describes situations related to professional meetings, e.g., meetings being set/cancelled, people attending meetings, topics of meetings.\n\n", "discourse_tags": ["Other", "Other"], "span_citation_mapping": []}
{"id": "51875355_4", "paragraph": "[BOS] The set of worlds that we simulate as part of this work are as follows:\n\n", "discourse_tags": ["Other"], "span_citation_mapping": []}
{"id": "51875355_3", "paragraph": "[BOS] Other large-scale QA datasets include Clozestyle datasets such as CNN/Daily Mail (Hermann et al., 2015) , Children's Book Test (Hill et al., 2015) , and Who Did What (Onishi et al., 2016) ; datasets with answers being spans in the document, such as SQuAD (Rajpurkar et al., 2016), NewsQA (Trischler et al., 2016) , and TriviaQA (Joshi et al., 2017) ; and datasets with human generated answers, for instance, MS MARCO (Nguyen et al., 2016) and SearchQA (Dunn et al., 2017) .\n[BOS] One common drawback of these datasets is the difficulty in accessing a system's capability of integrating information across a document context.\n[BOS] Koisk et al. (2017) recently emphasized this issue and proposed NarrativeQA, a dataset of fictional stories with questions that reflect the complexity of narratives: characters, events, and evolving relations.\n[BOS] Our dataset contains similar narrative elements, but it is created with a supporting KB and hence it is easier to analyze and interpret results in a controlled setting.\n[BOS] lated user may introduce new knowledge, update existing knowledge or express a state change (e.g., \"Homework 3 is now due on Friday\" or \"Samantha passed her thesis defense\").\n[BOS] Each narrative is interleaved with questions about the current state of the world, and questions range in complexity depending on the amount of knowledge that needs to be integrated to answer them.\n[BOS] This allows us to benchmark a range of QA models at their ability to answer questions that require different extents of relational reasoning to be answered.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Single_summ", "Reflection", "Transition", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 16, "token_end": 29, "char_start": 72, "char_end": 109, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hermann et al., 2015)": "6203757"}}}, {"token_start": 30, "token_end": 43, "char_start": 112, "char_end": 152, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hill et al., 2015)": "14915449"}}}, {"token_start": 45, "token_end": 57, "char_start": 159, "char_end": 193, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Onishi et al., 2016)": "5761781"}}}, {"token_start": 69, "token_end": 81, "char_start": 255, "char_end": 285, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 82, "token_end": 94, "char_start": 287, "char_end": 318, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Trischler et al., 2016)": "1167588"}}}, {"token_start": 96, "token_end": 108, "char_start": 325, "char_end": 354, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Joshi et al., 2017)": "26501419"}}}, {"token_start": 119, "token_end": 130, "char_start": 414, "char_end": 444, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Nguyen et al., 2016)": "1289517"}}}, {"token_start": 131, "token_end": 141, "char_start": 449, "char_end": 477, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dunn et al., 2017)": "11606382"}}}, {"token_start": 168, "token_end": 208, "char_start": 637, "char_end": 846, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ko\u010disk\u1ef3 et al. (2017)": "2593903"}, "Reference": {}}}]}
{"id": "51875355_2", "paragraph": "[BOS] Our dataset differs in the above datasets in that (i) it contains five different realistic domains permitting cross-domain evaluation to test the ability of models to generalize beyond a fixed set of KB relations, (ii) it exhibits rich referring expressions and linguistic variations (vocabulary much larger than the BABI dataset), (iii) questions in our dataset are designed to be deeply compositional and can cover multiple relations mentioned across multiple sentences.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "51875355_1", "paragraph": "[BOS] There are numerous datasets available for evaluating the capabilities of QA systems.\n[BOS] For example, MCTest (Richardson et al., 2013) contains comprehension questions for fictional stories.\n[BOS] Allen AI Science Challenge (Clark, 2015) contains science questions that can be answered with knowledge from text books.\n[BOS] RACE (Lai et al., 2017) is an English exam dataset for middle and high school Chinese students.\n[BOS] MULTIRC (Khashabi et al., 2018a) is a dataset that focuses on evaluating multi-sentence reasoning skills.\n[BOS] These datasets all require humans to carefully design multiplechoice questions and answers, so that certain aspects of the comprehension and reasoning capabilities are properly evaluated.\n[BOS] As a result, it is difficult to collect them at scale.\n[BOS] Furthermore, as the knowledge required for answering each question is not clearly specified in these datasets, it can be hard to identify the limitations of QA systems and propose improvements.\n[BOS] proposes to use synthetic QA tasks (the BABI dataset) to better understand the limitations of QA systems.\n[BOS] BABI builds on a simulated physical world similar to interactive fiction (Montfort, 2005) with simple objects and relations and includes 20 different reasoning tasks.\n[BOS] Various types of end-to-end neural networks (Sukhbaatar et al., 2015; Lee et al., 2015; Peng et al., 2015) have demonstrated promising accuracies on this dataset.\n[BOS] However, the performance can hardly translate to real-world QA datasets, as BABI uses a small vocabulary (150 words) and short sentences with limited language variations (e.g., nesting sentences, coreference).\n[BOS] A more sophisticated QA dataset with a supporting KB is WIKIMOVIES (Miller et al., 2016) , which contains 100k questions about movies, each of them is answerable by using either a KB or a Wikipedia article.\n[BOS] However, WIKIMOVIES is highly domain-specific, and similar to BABI, the questions are designed to be in simple forms with little compositionality and hence limit the difficulty level of the tasks.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Transition", "Transition", "Transition", "Transition", "Narrative_cite", "Narrative_cite", "Transition", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 19, "token_end": 37, "char_start": 110, "char_end": 198, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Richardson et al., 2013)": "2100831"}, "Reference": {}}}, {"token_start": 38, "token_end": 60, "char_start": 205, "char_end": 325, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Clark, 2015)": "45705325"}, "Reference": {}}}, {"token_start": 61, "token_end": 83, "char_start": 332, "char_end": 427, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Lai et al., 2017)": "6826032"}, "Reference": {}}}, {"token_start": 84, "token_end": 110, "char_start": 434, "char_end": 539, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Khashabi et al., 2018a)": "5112038"}, "Reference": {}}}, {"token_start": 220, "token_end": 229, "char_start": 1166, "char_end": 1202, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 245, "token_end": 277, "char_start": 1303, "char_end": 1392, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sukhbaatar et al., 2015;": null, "Lee et al., 2015;": "8221720", "Peng et al., 2015)": "402276"}}}, {"token_start": 332, "token_end": 379, "char_start": 1671, "char_end": 1877, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "51875355_0", "paragraph": "[BOS] Question answering has been mainly studied in two different settings: KB-based and text-based.\n[BOS] KB-based QA mostly focuses on parsing questions to logical forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2012; Berant et al., 2013; Kwiatkowski et al., 2013; in order to better retrieve answer candidates from a knowledge base.\n[BOS] Text-based QA aims to directly answer questions from the input text.\n[BOS] This includes works on early information retrieval-based methods (Banko et al., 2002; Ahn et al., 2004) and methods that build on extracted structured representations from both the question and the input text (Sachan et al., 2015; Sachan and Xing, 2016; Khot et al., 2017; Khashabi et al., 2018b) .\n[BOS] Although these structured presentations make reasoning more effective, they rely on sophisticated NLP pipelines and suffer from error propagation.\n[BOS] More recently, end-to-end neural architectures have been successfully applied to textbased QA, including Memory-augmented neural networks (Sukhbaatar et al., 2015; Miller et al., 2016; Kumar et al., 2016) and attention-based neural networks (Hermann et al., 2015; Chen et al., 2016; Kadlec et al., 2016; Dhingra et al., 2017; Xiong et al., 2017; Seo et al., 2017; Chen et al., 2017) .\n[BOS] In this work, we focus on QA over text (where the text is generated from a supporting KB) and evaluate several state-of-the-art memoryaugmented and attention-based neural architectures on our QA task.\n[BOS] In addition, we consider a sequence-to-sequence model baseline (Bahdanau et al., 2015) , which has been widely used in dialog (Vinyals and Le, 2015; Ghazvininejad et al., 2017) and recently been applied to generating answer values from Wikidata (Hewlett et al., 2016) .\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Narrative_cite", "Transition", "Narrative_cite", "Reflection", "Narrative_cite"], "span_citation_mapping": [{"token_start": 32, "token_end": 70, "char_start": 158, "char_end": 273, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zelle and Mooney, 1996;": "263135", "Zettlemoyer and Collins, 2012;": "449252"}}}, {"token_start": 104, "token_end": 126, "char_start": 454, "char_end": 528, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 131, "token_end": 177, "char_start": 555, "char_end": 721, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sachan et al., 2015;": "14071482", "Sachan and Xing, 2016;": "15996543", "Khot et al., 2017;": "957320", "Khashabi et al., 2018b)": "20638934"}}}, {"token_start": 223, "token_end": 253, "char_start": 988, "char_end": 1087, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sukhbaatar et al., 2015;": null, "Kumar et al., 2016)": "2319779"}}}, {"token_start": 254, "token_end": 315, "char_start": 1092, "char_end": 1265, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hermann et al., 2015;": "6203757", "Kadlec et al., 2016;": "11022639", "Dhingra et al., 2017;": "6529193", "Xiong et al., 2017;": "3714278", "Seo et al., 2017;": "8535316"}}}, {"token_start": 369, "token_end": 386, "char_start": 1508, "char_end": 1567, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 393, "token_end": 415, "char_start": 1600, "char_end": 1657, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vinyals and Le, 2015;": "12300158", "Ghazvininejad et al., 2017)": "15442925"}}}, {"token_start": 424, "token_end": 437, "char_start": 1717, "char_end": 1748, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hewlett et al., 2016)": "15870937"}}}]}
{"id": "51990124_2", "paragraph": "[BOS] Recently there has been a move away from SRL models which explicitly incorporate syntactic knowledge through features and structured inference towards models which rely on deep neural networks to learn syntactic structure and longrange dependencies from the data.\n[BOS] Zhou and Xu (2015) were the first to achieve state-of-the-art results using 8 layers of bidirectional LSTM combined with inference in a linear-chain conditional random field (Lafferty et al., 2001) .\n[BOS] and also achieved state-of-the-art results using deep LSTMs with no syntactic features.\n[BOS] While most previous work assumes that gold predicates are given, like this work and Strubell et al. (2018) , evaluate on predicted predicates, though they train a separate model for predicate detection.\n[BOS] Most recently, Tan et al. (2018) achieved the state-of-the art on the CoNLL-2005 and 2012 shared tasks with gold predicates and no syntax using 10 layers of selfattention, and on CoNLL-2012 with gold predicates Peters et al. (2018) increase the score of by more than 3 F1 points by incorporating ELMo embeddings into their model, outperforming ensembles from Tan et al. (2018) with a single model.\n[BOS] We are interested in analyzing this relationship further by experimenting with adding ELMo embeddings to models with and without syntax in order to determine whether ELMo can replace explicit syntax in SRL models, or if they can have a synergistic relationship.\n\n", "discourse_tags": ["Transition", "Single_summ", "Other", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 44, "token_end": 91, "char_start": 276, "char_end": 473, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhou and Xu (2015)": "12688069"}, "Reference": {"(Lafferty et al., 2001)": "277918"}}}, {"token_start": 130, "token_end": 155, "char_start": 660, "char_end": 778, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Strubell et al. (2018)": "5068376"}, "Reference": {}}}, {"token_start": 159, "token_end": 197, "char_start": 800, "char_end": 955, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Tan et al. (2018)": "12842009"}, "Reference": {}}}, {"token_start": 199, "token_end": 245, "char_start": 961, "char_end": 1161, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Peters et al. (2018)": "3626819"}, "Reference": {"Tan et al. (2018)": "12842009"}}}]}
{"id": "51990124_1", "paragraph": "[BOS] The idea that an SRL model should incorporate syntactic structure is not new, since many semantic formalities are defined with respect to syntax.\n[BOS] Many of the first approaches to SRL (Pradhan et al., 2005; Surdeanu et al., 2007; Johansson and Nugues, 2008; Toutanova et al., 2008; Punyakanok et al., 2008) , spearheaded by the CoNLL-2005 shared task (Carreras and Mrquez, 2005) , achieved success by relying on syntax-heavy linguistic features as input for a linear model, combined with structured inference which could also take syntax into account.\n[BOS] showed that most of these constraints could more efficiently be enforced by exact inference in a dynamic program.\n[BOS] While most techniques required a predicted parse as input, Sutton and McCallum (2005) modeled syntactic parsing and SRL with a joint graphical model, and Lewis et al. (2015) jointly modeled SRL and CCG semantic parsing.\n[BOS] Collobert et al. (2011) were among the first to use a neural network model for SRL, using a CNN over word embeddings combined with globallynormalized inference.\n[BOS] However, their model failed to out-perform non-neural models, both with and without multi-task learning with other NLP tagging tasks such as part-of-speech tagging and chunking.\n[BOS] FitzGerald et al. (2015) were among the first to successfully employ neural networks, achieving the state-of-the-art by embedding lexicalized features and providing the embeddings as factors in the model of .\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Multi_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 30, "token_end": 87, "char_start": 158, "char_end": 316, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Pradhan et al., 2005;": "2440012", "Surdeanu et al., 2007;": "2757928", "Johansson and Nugues, 2008;": "109294", "Toutanova et al., 2008;": "2243454", "Punyakanok et al., 2008)": "11162815"}}}, {"token_start": 88, "token_end": 110, "char_start": 319, "char_end": 388, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Carreras and M\u00e0rquez, 2005)": "16509032"}}}, {"token_start": 171, "token_end": 191, "char_start": 747, "char_end": 836, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Sutton and McCallum (2005)": "1544330"}, "Reference": {}}}, {"token_start": 193, "token_end": 210, "char_start": 842, "char_end": 907, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Lewis et al. (2015)": "9991841"}, "Reference": {}}}, {"token_start": 211, "token_end": 286, "char_start": 914, "char_end": 1258, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Collobert et al. (2011)": "351666"}, "Reference": {}}}, {"token_start": 287, "token_end": 331, "char_start": 1265, "char_end": 1471, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"FitzGerald et al. (2015)": "15048880"}, "Reference": {}}}]}
{"id": "51990124_0", "paragraph": "[BOS] Our experiments are based on the LISA model of Strubell et al. (2018) , who showed that their method for incorporating syntax into a deep neural network architecture for SRL improves SRL F1 with predicted predicates on CoNLL-2005 and CoNLL-2012 data, including on out-of-domain test data.\n[BOS] Other recent works have also found syntax to improve neural SRL models when evaluated on data from the CoNLL-2009 shared task: Roth and Lapata (2016) use LSTMs to embed syntactic dependency paths, and incorporate syntax using graph convolutional neural networks over predicted dependency parse trees.\n[BOS] In contrast to this work, found that their syntax-aware model did not out-perform a syntaxagnostic model on out-of-domain data.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 8, "token_end": 21, "char_start": 39, "char_end": 75, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Strubell et al. (2018)": "5068376"}, "Reference": {}}}, {"token_start": 89, "token_end": 103, "char_start": 404, "char_end": 450, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Roth and Lapata (2016)": "5779419"}, "Reference": {}}}]}
{"id": "5262888_0", "paragraph": "[BOS] Our work is motivated by JAMR (Flanigan et al., 2014) , which is based on a pipelined model, resulting in a large drop in overall performance when moving from gold concepts to system concepts.\n[BOS] Wang et al. (2015a) uses a two-stage approach; dependency parses are modified by executing a sequence of actions to resolve dis-crepancies between dependency tree and AMR structure.\n[BOS] Goodman et al. (2016) improves the transition-based parser with the imitation learning algorithms, achieving almost the same performance as that of Wang et al.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 7, "token_end": 28, "char_start": 31, "char_end": 97, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Flanigan et al., 2014)": "5000956"}}}, {"token_start": 47, "token_end": 89, "char_start": 205, "char_end": 386, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wang et al. (2015a)": "15344879"}, "Reference": {}}}, {"token_start": 90, "token_end": 122, "char_start": 393, "char_end": 552, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Goodman et al. (2016)": "7779018"}, "Reference": {}}}]}
{"id": "5661541_1", "paragraph": "[BOS] Transfer learning is a machine learning technique effective for many NLP tasks (Pan and Yang, 2010) , which effectively trains a machine learning model by transferring knowledge about a general domain into a target domain.\n[BOS] By applying transfer learning to a stylistically consistent DRG system, once we build a DRG system without stylistic consistency, it is easy to change its style by adding a small stylistically consistent corpus.\n\n", "discourse_tags": ["Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 2, "token_end": 22, "char_start": 6, "char_end": 105, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Pan and Yang, 2010)": null}}}]}
{"id": "5661541_0", "paragraph": "[BOS] The literature includes some prior studies that aim for transforming a style of dialog utterances into a specified one (Walker et al., 2012; Miyazaki et al., 2015) .\n[BOS] Walker et al. (2012) extract rules representing characters from their annotated movie subtitle corpora.\n[BOS] Miyazaki et al. (2015) propose a method of converting utterances using rewriting rules automatically derived from a Twitter corpus.\n[BOS] These approaches have a fundamental problem to need some manual annotations, which is a main issue to be solved in this work.\n[BOS] We propose an end-to-end and data-driven framework which addresses both response generation and stylistic transformation.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 11, "token_end": 37, "char_start": 62, "char_end": 169, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Walker et al., 2012;": "2177536", "Miyazaki et al., 2015)": "1744498"}}}, {"token_start": 39, "token_end": 59, "char_start": 178, "char_end": 281, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Walker et al. (2012)": "2177536"}, "Reference": {}}}, {"token_start": 60, "token_end": 84, "char_start": 288, "char_end": 419, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Miyazaki et al. (2015)": "1744498"}, "Reference": {}}}]}
{"id": "53223062_4", "paragraph": "[BOS] In our work, we generate questions using community questions and answers and apply the encoder-decoder structure.\n[BOS] To boost the performance of our system, we use attention and coverage mechanisms as suggested in See et al. (2017) .\n\n", "discourse_tags": ["Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 33, "token_end": 47, "char_start": 173, "char_end": 240, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"See et al. (2017)": null}}}]}
{"id": "53223062_3", "paragraph": "[BOS] One of the latest studies on the QG task has been conducted by Du et al. (2017) .\n[BOS] Their task is a QG on both sentences and paragraphs for the reading comprehension task, and they adopt an attentionbased sequence learning model.\n[BOS] Another recent work is by Yuan et al. (2017) , they generate questions from documents using supervised and reinforcement learning.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 52, "char_start": 6, "char_end": 239, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Du et al. (2017)": "2172129"}, "Reference": {}}}, {"token_start": 53, "token_end": 77, "char_start": 246, "char_end": 376, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Yuan et al. (2017)": "7562142"}, "Reference": {}}}]}
{"id": "53223062_2", "paragraph": "[BOS] QG can also be combined with its complementary task, Question Answering (QA) for further improvement.\n[BOS] Tang et al. (2017) consider QG and QA as dual tasks and train their relative models simultaneously.\n[BOS] Their training framework takes advantage of the probabilistic correlation between the two tasks.\n[BOS] QG has also entered other communities such as computer vision.\n[BOS] Mostafazadeh et al. (2016) introduced the visual question generation task where the goal of the system is to create a question given an image.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Transition", "Single_summ"], "span_citation_mapping": [{"token_start": 23, "token_end": 60, "char_start": 114, "char_end": 316, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Tang et al. (2017)": "37738077"}, "Reference": {}}}, {"token_start": 74, "token_end": 106, "char_start": 392, "char_end": 534, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Mostafazadeh et al. (2016)": "16227864"}, "Reference": {}}}]}
{"id": "53223062_1", "paragraph": "[BOS] Lately, more approaches have been presented that utilize the neural encoder-decoder architecture.\n[BOS] Serban et al. (2016) address the problem by transducing knowledge graph facts into questions.\n[BOS] They created a factoid question and answer corpus by using the Recurrent Neural Network architecture.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 20, "token_end": 58, "char_start": 110, "char_end": 311, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Serban et al. (2016)": "12241221"}, "Reference": {}}}]}
{"id": "53223062_0", "paragraph": "[BOS] After the first question generation shared task evaluation challenge (Rus et al., 2010) , the question generation task has received a huge attention from the natural language generation community.\n[BOS] Many of the traditional approaches involve human resources to create robust templates and then employing them to generate questions.\n[BOS] For instance, Heilman and Smith (2010) approach is to overgenerate questions by some hand-written rules and then rank them using a logistic regression model.\n[BOS] Labutov et al. (2015) benefit from a low-dimensional ontology for document segments.\n[BOS] They crowdsource a set of promising question templates that are matched with that representation and rank the results based on their relevance to the source.\n[BOS] Lindberg et al. (2013) employed a template-based approach while taking advantage of semantic information to generate natural language questions for on-line learning support.\n[BOS] Chali and Hasan (2015) consider the automatic generation of all possible questions from a topic of interest by exploiting the named entity information and the predicate argument structures of the sentences.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 5, "token_end": 19, "char_start": 22, "char_end": 93, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rus et al., 2010)": "266197"}}}, {"token_start": 61, "token_end": 90, "char_start": 362, "char_end": 505, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Heilman and Smith (2010)": "1809816"}, "Reference": {}}}, {"token_start": 91, "token_end": 139, "char_start": 512, "char_end": 760, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Labutov et al. (2015)": "418759"}, "Reference": {}}}, {"token_start": 140, "token_end": 172, "char_start": 767, "char_end": 940, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Lindberg et al. (2013)": "14313287"}, "Reference": {}}}, {"token_start": 173, "token_end": 209, "char_start": 947, "char_end": 1153, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Chali and Hasan (2015)": null}, "Reference": {}}}]}
{"id": "53222940_1", "paragraph": "[BOS] However, since the size of the data sets that participants must produce in this task is smaller than the number of parallel sentences that are mutual translations, this task is also related to the data selection: selection of a subset of data that maximizes translation quality, avoiding redundancy and matching a given domain (Eetemadi et al., 2015) .\n[BOS] Instead of the widespread language-model based data selection methods (Axelrod et al., 2011) , we replaced words with placeholders in order to not take into account the domain of the text.\n\n", "discourse_tags": ["Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 41, "token_end": 69, "char_start": 219, "char_end": 356, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Eetemadi et al., 2015)": "15514266"}}}, {"token_start": 74, "token_end": 92, "char_start": 380, "char_end": 457, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Axelrod et al., 2011)": "10766958"}}}]}
{"id": "53222940_0", "paragraph": "[BOS] The WMT 2018 parallel corpus filtering shared task partially shares its objectives with the First Automatic Translation Memory Cleaning Shared Task (Barbu et al., 2016) , where participants had to automatically classify translation memory segments according to whether the target language (TL) side was translation of the source language (SL) side or not.\n[BOS] This task is, in turn, very similar to the detection of parallel sentences in comparable corpora, that can be tackled by combining bilingual data and automatic classifiers (Munteanu and Marcu, 2005) , machine translation (AbdulRauf and Schwenk, 2009) or, more recently, word embeddings (Espaa-Bonet et al., 2017) .\n[BOS] In fact, the approach we follow to detect sentences that are mutual translations is similar to the work of Munteanu and Marcu (2005) .\n[BOS] Their approach differs from ours in the fact that we make use of a larger set of shallow features not related to lexical similarity.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 17, "token_end": 33, "char_start": 98, "char_end": 174, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Barbu et al., 2016)": null}}}, {"token_start": 96, "token_end": 109, "char_start": 518, "char_end": 566, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Munteanu and Marcu, 2005)": "15289038"}}}, {"token_start": 110, "token_end": 123, "char_start": 569, "char_end": 618, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 128, "token_end": 142, "char_start": 638, "char_end": 680, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Espa\u00f1a-Bonet et al., 2017)": "12291265"}}}, {"token_start": 164, "token_end": 174, "char_start": 796, "char_end": 821, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Munteanu and Marcu (2005)": "15289038"}}}]}
{"id": "53222445_2", "paragraph": "[BOS] Given the flexibility of neural methods, it would be interesting in future work to examine how well neural sequence-to-sequence generation methods would fare in a direct, head-to-head comparison using the kinds of detailed, deep inputs used with HPSG and CCG.\n[BOS] To the extent that neural approaches continue to hallucinate content and fail to observe constraints and preferences implemented by grammar-based approaches in such a comparison, it would also be worthwhile to investigate additional ways of combining neural and grammarbased methods.\n\n", "discourse_tags": ["Transition", "Transition"], "span_citation_mapping": []}
{"id": "53222445_1", "paragraph": "[BOS] Although approaches using constraint-based grammars are clearly more difficult to implement and deploy, there is some evidence that they are beneficial for parsing, while for realization the question remains largely open.\n[BOS] For parsing, Buys and Blunsom (2017) have recently shown that even though their incremental neural semantic graph parser substantially outperforms standard attentional sequence-to-sequence models, it still lags 4-6% behind an HPSG parser using a simple log-linear model (Toutanova et al., 2005) on a variety of parsing accuracy measures on DeepBank (Flickinger et al., 2012) , a conversion of the Penn Treebank to Minimal Recursion Semantics (Copestake et al., 2005, MRS) .\n[BOS] The MRS representations in DeepBank are qualitatively similar to the OpenCCG semantic graphs used in this work, which are again qualitatively similar to the deep representations used in the First Surface Realization Shared Task (Belz et al., 2010 (Belz et al., , 2011 .\n[BOS] On the deep shared task representations, Bohnet et al. (2011) achieved a BLEU score of 0.7943, which Puduppully et al. (2017) later improved upon with a score of 0.8077.\n[BOS] These scores are substantially lower than our BLEU score of 0.8683 reported here, though since the inputs are not exactly the same, the BLEU scores are of course not directly comparable.\n\n", "discourse_tags": ["Transition", "Single_summ", "Narrative_cite", "Multi_summ", "Multi_summ"], "span_citation_mapping": [{"token_start": 40, "token_end": 148, "char_start": 238, "char_end": 705, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Buys and Blunsom (2017)": "14852353"}, "Reference": {"(Toutanova et al., 2005)": "18241843", "(Flickinger et al., 2012)": null}}}, {"token_start": 182, "token_end": 204, "char_start": 904, "char_end": 981, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Belz et al., 2010": "1783652", "(Belz et al., , 2011": "12040771"}}}, {"token_start": 206, "token_end": 231, "char_start": 990, "char_end": 1083, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Bohnet et al. (2011)": "15726188"}, "Reference": {}}}, {"token_start": 233, "token_end": 296, "char_start": 1091, "char_end": 1352, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Puduppully et al. (2017)": "2247313"}, "Reference": {}}}]}
{"id": "53222445_0", "paragraph": "[BOS] Hypertagging can potentially benefit other grammar-based methods using lexicalized grammars, e.g. using HPSG (Velldal and Oepen, 2005; Carroll and Oepen, 2005; Nakanishi et al., 2005) or TAG (Gardent and Perez-Beltrachini, 2017) .\n[BOS] Much recent work in NLG (Wen et al., 2015; Duek and Jurcicek, 2016; Mei et al., 2016; Kiddon et al., 2016; Konstas et al., 2017; Wiseman et al., 2017) has made use of neural sequenceto-sequence methods for generation rather than grammar-based methods.\n[BOS] The learning flexibility of neural methods make it possible to develop very knowledge lean systems, but they continue to suffer from a tendency to hallucinate content and have not been used with texts exhibiting the full complexity of genres such as news text.\n[BOS] Approaches based on dependency grammar (Guo et al., 2008; Bohnet et al., 2010 Bohnet et al., , 2011 Zhang and Clark, 2015; Liu et al., 2015; Puduppully et al., 2016 Puduppully et al., , 2017 King and White, 2018) are also simpler than constraint-based grammar approaches, making them more robust to unexpected inputs and easier to deploy across languages, but it is difficult to determine whether they can fully substitute for precise grammars because these approaches have not used compatible inputs.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 23, "token_end": 52, "char_start": 110, "char_end": 189, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Velldal and Oepen, 2005;": "14174924", "Carroll and Oepen, 2005;": "5754132", "Nakanishi et al., 2005)": null}}}, {"token_start": 53, "token_end": 68, "char_start": 193, "char_end": 234, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Gardent and Perez-Beltrachini, 2017)": "8646729"}}}, {"token_start": 74, "token_end": 127, "char_start": 263, "char_end": 393, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wen et al., 2015;": "739696", "Du\u0161ek and Jurcicek, 2016;": "6380915", "Mei et al., 2016;": "1354459", "Kiddon et al., 2016;": "9818013", "Konstas et al., 2017;": "8066499", "Wiseman et al., 2017)": "23892230"}}}, {"token_start": 198, "token_end": 263, "char_start": 788, "char_end": 980, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Guo et al., 2008;": "3141720", "Bohnet et al., 2010": "15950784", "Bohnet et al., , 2011": "15726188", "Zhang and Clark, 2015;": null, "Liu et al., 2015;": "13911837", "Puduppully et al., 2016": "11589386", "Puduppully et al., , 2017": "2247313", "King and White, 2018)": "44180499"}}}]}
{"id": "53204155_0", "paragraph": "[BOS] Neural Machine Translation A number of attention-based neural architectures have proven to be very effective for NMT.\n[BOS] For RNMT models, both the encoder and decoder were implemented as deep Recurrent Neural Networks (RNNs), interacting via a soft-attention mechanism Chen et al., 2018) .\n[BOS] It is the pioneering paradigm achieving the state-ofthe-art performance.\n[BOS] Following RNMT, convolutional sequence-to-sequence (ConvS2S) models take advantages of modern fast computing devices which outperform RNMT with faster training speed (Kalchbrenner et al., 2016; Gehring et al., 2017a) .\n[BOS] Most recently, the Transformer model, which is based solely on a self-attention mechanism and feed-forward connections, has further advanced the field of NMT, both in terms of translation quality and speed of convergence (Vaswani et al., 2017; Dehghani et al., 2018) .\n[BOS] The attention mechanism plays a crucial role in the success of all these models to achieve the state-of-the-art results, as the memory capacity of a single dense vector in the typical encoder-decoder model seems not powerful enough to store the necessary information of the source sentence.\n[BOS] Despite the generally good performance, the attention based models have running time that is super-linear in the length of the source sequences, burdening the inference speed as the length of the sequences increases.\n[BOS] Different from the attention based approach, CAPSNMT runs in time that is linear in the length of the sequences.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Narrative_cite", "Narrative_cite", "Transition", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 49, "token_end": 60, "char_start": 253, "char_end": 296, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Chen et al., 2018)": null}}}, {"token_start": 104, "token_end": 132, "char_start": 518, "char_end": 600, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kalchbrenner et al., 2016;": "13895969", "Gehring et al., 2017a)": "3648736"}}}, {"token_start": 170, "token_end": 196, "char_start": 785, "char_end": 875, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vaswani et al., 2017;": "13756489", "Dehghani et al., 2018)": null}}}]}
{"id": "53180670_2", "paragraph": "[BOS] Our work on multilingual MT and sentence representations is closely related to the recently published paper by Lu et al. (2018) .\n[BOS] There, the authors attempt to build a neural interlingua by using language independent encoders and decoders which share an attentive long short-term memory (LSTM) layer.\n[BOS] Our approach differs because our model is able to encode any sequence with variable length into a fixed size representation, without suffering from long-term dependency problems (Lin et al., 2017) and without the need of padding for downstream task testing.\n[BOS] Additionally, we also experiment in a multilingual manyto-many setting, instead of only one-to-many or many-to-one.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 5, "token_end": 27, "char_start": 18, "char_end": 133, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Lu et al. (2018)": "5067886"}}}, {"token_start": 91, "token_end": 104, "char_start": 467, "char_end": 515, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lin et al., 2017)": "15280949"}}}]}
{"id": "53180670_1", "paragraph": "[BOS] Sentence meaning representation has as well been vastly studied under NMT settings.\n[BOS] When introducing the encoder-decoder architectures for MT, Sutskever et al. (2014) showed that the seq2seq models are better at encoding the meaning of sentences into vector spaces than the bagof-words model.\n[BOS] Recent work includes that of Schwenk and Douze (2017) , who use multiple encoders and decoders that are connected through a shared layer, albeit with a different purpose than performing translation.\n[BOS] In Platanios et al. (2018) the authors show an intermediate representation that can be decoded to any target language while describing a parameter generation method for universal NMT.\n[BOS] Cfka and Bojar (2018) introduced an architecture with a self-attentive layer to extract sentence meaning representations of fixed size.\n[BOS] Here we use a similar architecture in a multilingual setting.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 19, "token_end": 63, "char_start": 101, "char_end": 304, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Sutskever et al. (2014)": "7961699"}, "Reference": {}}}, {"token_start": 69, "token_end": 102, "char_start": 340, "char_end": 496, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Schwenk and Douze (2017)": null}, "Reference": {}}}, {"token_start": 106, "token_end": 140, "char_start": 519, "char_end": 699, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Platanios et al. (2018)": "52100117"}, "Reference": {}}}, {"token_start": 141, "token_end": 168, "char_start": 706, "char_end": 841, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"C\u00edfka and Bojar (2018)": "21717477"}, "Reference": {}}}]}
{"id": "53180670_0", "paragraph": "[BOS] Multilingual NMT has been widely studied and developed in different pathways during the last years (Luong et al., 2015; Dong et al., 2015; Chen et al., 2017; Johnson et al., 2016) .\n[BOS] Work has been done with networks that use language specific encoders and decoders, such as Dong et al. (2015) , who used a separate attention mechanism for each decoder on one-to-many translation.\n[BOS] Zoph and Knight (2016) exploited a multi-way parallel corpus in a many-to-one multilingual scenario, while Firat et al. (2016a) used language-specific encoders and decoders that share a traditional attention mechansim in a many-to-many scheme.\n[BOS] Another approach is the use of universal encoderdecoder networks that share embedding spaces to improve the performance of the model, like the one proposed by Gu et al. (2018) for improving translation on low-resourced languages and the one from Johnson et al. (2016) , where the term zero-shot translation was coined.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Multi_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 11, "token_end": 49, "char_start": 51, "char_end": 185, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Luong et al., 2015;": "6954272", "Dong et al., 2015;": "3666937", "Chen et al., 2017;": "2166461", "Johnson et al., 2016)": "6053988"}}}, {"token_start": 51, "token_end": 94, "char_start": 194, "char_end": 390, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Dong et al. (2015)": "3666937"}, "Reference": {}}}, {"token_start": 95, "token_end": 120, "char_start": 397, "char_end": 496, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zoph and Knight (2016)": "8677917"}, "Reference": {}}}, {"token_start": 122, "token_end": 156, "char_start": 504, "char_end": 640, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Firat et al. (2016a)": "6359641"}, "Reference": {}}}, {"token_start": 163, "token_end": 192, "char_start": 678, "char_end": 822, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Gu et al. (2018)": "3295641"}}}, {"token_start": 193, "token_end": 224, "char_start": 827, "char_end": 965, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Johnson et al. (2016)": "6053988"}}}]}
{"id": "53153643_0", "paragraph": "[BOS] Our work is closely aligned with Explainable Artificial Intelligence (XAI) (Gunning, 2017) , which is claimed to be essential if users are to understand, appropriately trust, and effectively manage this incoming generation of artificially intelligent partners.\n[BOS] In artificial intelligence, providing an explanation of individual decisions is a topic that has attracted attention in recent years.\n[BOS] The traditional way of explaining the results is to directly build connections between the input and output, and try to figure out how much each dimension or element in the input contributes to the final output.\n[BOS] Some previous works explain the result from two ways: evaluating the sensitivity of output if input changes and analyze the result from a mathematical way by redistributing the prediction function backward using local redistribution rules (Samek et al., 2017) .\n[BOS] There are some works connecting the result with the classification model.\n[BOS] Ribeiro et al. (2016) try to explain the result from the result itself and provide a global view of the model.\n[BOS] Although the method is promising and mathematically reasonable, they cannot generate explanations in natural forms.\n[BOS] They focus more on how to interpret the result.\n[BOS] Some of the previous works have similar motivations as our work.\n[BOS] Lei et al. (2016) rationalize neural prediction by extracting the phrases from the input texts as the explanations.\n[BOS] They conduct their work in an extractive approach, and focus on rationalizing the predictions.\n[BOS] However, our work aims not only to predict the results, but also to generate abstractive explanations, and our framework can generate explanations both in the forms of texts and numerical scores.\n[BOS] Ouyang et al. (2018) apply explanations to recommendation systems, integrating user information to generate explanation texts and further evaluating these explanations by using them to predict the result.\n[BOS] The problem of their work is that they don't build strong interactions between the explanations and recommendation results, where the strongest connection of the recommendation result and explanations is that they have the same input.\n[BOS] Hancock et al. (2018) proposes to use a classifier with natural language explanations that are annotated by human beings to do the classification.\n[BOS] Our work is different from theirs in that we use the natural attributes as the explanations which are more frequent in reality.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Narrative_cite", "Transition", "Single_summ", "Single_summ", "Single_summ", "Reflection", "Single_summ", "Single_summ", "Reflection", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 8, "token_end": 22, "char_start": 39, "char_end": 96, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Gunning, 2017)": null}}}, {"token_start": 145, "token_end": 157, "char_start": 843, "char_end": 890, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Samek et al., 2017)": "4376915"}}}, {"token_start": 172, "token_end": 241, "char_start": 979, "char_end": 1336, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ribeiro et al. (2016)": "13029170"}, "Reference": {}}}, {"token_start": 242, "token_end": 284, "char_start": 1343, "char_end": 1559, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 322, "token_end": 396, "char_start": 1768, "char_end": 2213, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ouyang et al. (2018)": "49862502"}, "Reference": {}}}, {"token_start": 397, "token_end": 425, "char_start": 2220, "char_end": 2366, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hancock et al. (2018)": "13696741"}, "Reference": {}}}]}
{"id": "53140824_1", "paragraph": "[BOS] Similar to MeetUp, the use of various dialogue game set-ups has lately been established for dialogue data collection.\n[BOS] Das et al. (2017) designed the \"Visual Dialog\" task where a human asks an agent about the content of an image.\n[BOS] De Vries et al. (2017) similarly collected the GuessWhat?\n[BOS] corpus of dialogues in which one player has to ask polar questions in order to identify the correct referent in the pool of images.\n[BOS] de Vries et al. (2018) also develop a new navigation task, where a \"tourist\" has to reach a target location via communication with a \"guide\" given 2D images of various map locations.\n[BOS] While similar in some respects, MeetUp is distinguished by being a symmetrical task (no instruction giver/follower) and broader concerning language data (more phenomena such as repairs, strategy negotiation).\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 28, "token_end": 55, "char_start": 130, "char_end": 240, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Das et al. (2017)": null}, "Reference": {}}}, {"token_start": 56, "token_end": 99, "char_start": 247, "char_end": 442, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Vries et al. (2017)": "36417"}, "Reference": {}}}, {"token_start": 100, "token_end": 144, "char_start": 449, "char_end": 631, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Vries et al. (2018)": "49669712"}, "Reference": {}}}]}
{"id": "53140824_0", "paragraph": "[BOS] As described above, the fact that the seemingly simple task of image captioning can be interpreted differently by crowd-workers has already been recognised in the original publications describing the datasets (Hodosh et al., 2013; Young et al., 2014; Chen et al., 2015) .\n[BOS] However, it has been treated as a problem that can be addressed through the design of instructions (e.g., \"do not give people names\", \"do not describe unimportant details\", (Chen et al., 2015) ).\n[BOS] (van Miltenburg et al., 2016; van Miltenburg, 2017) later investigated the range of pragmatic phenomena to be found in such caption corpora, with the conclusion that the instructions do not sufficiently control for them and leave it to the labellers to make their own decisions.\n[BOS] It is one contribution of the present paper to show that providing a task context results in more constrained descriptions.\n[BOS] Schlangen et al. (2016) similarly noted that referring expressions in a corpus that was collected in a (pseudo-)interactive setting (Kazemzadeh et al., 2014) , where the describers were provided with immediate feedback about whether their expression was understood, were more concise than those collected in a monological setting (Mao et al., 2016) .\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Multi_summ", "Reflection", "Single_summ"], "span_citation_mapping": [{"token_start": 32, "token_end": 60, "char_start": 169, "char_end": 275, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hodosh et al., 2013;": "928608", "Young et al., 2014;": "3104920", "Chen et al., 2015)": "2210455"}}}, {"token_start": 77, "token_end": 111, "char_start": 360, "char_end": 476, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chen et al., 2015)": "2210455"}}}, {"token_start": 114, "token_end": 173, "char_start": 486, "char_end": 764, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 196, "token_end": 272, "char_start": 901, "char_end": 1249, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Schlangen et al. (2016)": "785805"}, "Reference": {"(Kazemzadeh et al., 2014)": "6308361", "(Mao et al., 2016)": "8745888"}}}]}
{"id": "53108063_12", "paragraph": "[BOS] The difference between (13) and (15) Adding these differences gives the second result.\n\n", "discourse_tags": ["Other"], "span_citation_mapping": []}
{"id": "53108063_11", "paragraph": "[BOS] where\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "53108063_10", "paragraph": "[BOS] Thus the difference between (12) and (14) is\n\n", "discourse_tags": ["Other"], "span_citation_mapping": []}
{"id": "53108063_9", "paragraph": "[BOS] and the two terms of its gradient with respect to q (corresponding to (12) and (13)\n\n", "discourse_tags": ["Other"], "span_citation_mapping": []}
{"id": "53108063_8", "paragraph": "[BOS] In contrast, the numerator term estimated as an average over minibatches is\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "53108063_7", "paragraph": "[BOS] 1 N K k=1 z,z log p(z)q(z ) (x,y)B k p(z|x)q(z |y)\n\n", "discourse_tags": ["Other"], "span_citation_mapping": []}
{"id": "53108063_6", "paragraph": "[BOS] The second term is (as a sum over batches)\n\n", "discourse_tags": ["Other"], "span_citation_mapping": []}
{"id": "53108063_5", "paragraph": "[BOS] Hence the difference between (9) and (11) By the product rule, its gradient with respect to q is a sum of two terms.\n[BOS] The first term is (using (10) again)\n\n", "discourse_tags": ["Other", "Other"], "span_citation_mapping": []}
{"id": "53108063_4", "paragraph": "[BOS] In contrast, the gradient of the negative entropy term averaged over minibatches is\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "53108063_3", "paragraph": "[BOS] where we expand q(z) by the identity\n\n", "discourse_tags": ["Other"], "span_citation_mapping": []}
{"id": "53108063_2", "paragraph": "[BOS] (1 + logq(z)) q k (z) (9)\n\n", "discourse_tags": ["Other"], "span_citation_mapping": []}
{"id": "53108063_1", "paragraph": "[BOS] Our POS induction system has some practical advantages over previous approaches.\n[BOS] Many rely on computationally expensive structured inference or pre-optimized features (or both).\n[BOS] Lin et al. (2015) , and He et al. (2018) rely on carefully pretrained lexical representations like Brown clusters and word embeddings.\n[BOS] In contrast, the model presented in this work requires no expensive structured computation or feature engineering and uses word/character embeddings trained from scratch.\n[BOS] It is easy to implement using a standard neural network library and outperforms these previous works in many cases.\n[BOS] whose gradient with respect to q is z (1 + logq(z)) q(z)\n\n", "discourse_tags": ["Reflection", "Transition", "Multi_summ", "Reflection", "Reflection", "Other"], "span_citation_mapping": [{"token_start": 33, "token_end": 63, "char_start": 196, "char_end": 330, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Lin et al. (2015)": "15690223", "He et al. (2018)": "52113103"}, "Reference": {}}}]}
{"id": "53108063_0", "paragraph": "[BOS] Information theory, in particular mutual information, has played a prominent role in NLP (Church and Hanks, 1990; Brown et al., 1992) .\n[BOS] It has intimate connections to the representation learning capabilities of neural networks (Tishby and Zaslavsky, 2015) and underlies many celebrated modern approaches to unsupervised learning such as generative adversarial networks (GANs) (Goodfellow et al., 2014) .\n[BOS] There is a recent burst of effort in learning continuous representations by optimizing various lower bounds on mutual information (Belghazi et al., 2018; Oord et al., 2018; .\n[BOS] These representations are typically evaluated on extrinsic tasks as features.\n[BOS] In contrast, we learn discrete representations by optimizing a novel generalization of the Brown clustering objective (Brown et al., 1992 ) and a variational lower bound on mutual information proposed by McAllester (2018) .\n[BOS] We focus on intrinsic evaluation of these representations on POS induction.\n[BOS] Extrinsic evaluation of these representations in downstream tasks is an important future direction.\n[BOS] The issue of biased stochastic gradient estimators is a common challenge in unsupervised learning (e.g., see Wang et al., 2015) .\n[BOS] This arises mainly because the objective involves a nonlinear transformation of all samples in a training dataset, for instance the whitening constraints in deep canonical correlation analysis (CCA) (Andrew et al., 2013) .\n[BOS] In this work, the problem arises because of entropy.\n[BOS] This issue is not considered in the original work of McAllester (2018) and the error analysis we present in Section 4 is novel.\n[BOS] Our finding is that the feasibility of stochastic optimization greatly depends on the size of the bias in gradient estimates, as we are able to effectively optimize the variational objective while not the generalized Brown objective.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Narrative_cite", "Transition", "Reflection", "Reflection", "Reflection", "Narrative_cite", "Narrative_cite", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 33, "char_start": 6, "char_end": 139, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Church and Hanks, 1990;": "47390681", "Brown et al., 1992)": "10986188"}}}, {"token_start": 45, "token_end": 59, "char_start": 223, "char_end": 267, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Tishby and Zaslavsky, 2015)": "5541663"}}}, {"token_start": 72, "token_end": 90, "char_start": 349, "char_end": 413, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Goodfellow et al., 2014)": "1033682"}}}, {"token_start": 109, "token_end": 128, "char_start": 533, "char_end": 593, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Belghazi et al., 2018;": "44220142"}}}, {"token_start": 157, "token_end": 167, "char_start": 778, "char_end": 824, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Brown et al., 1992": "10986188"}}}, {"token_start": 174, "token_end": 184, "char_start": 860, "char_end": 908, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"McAllester (2018)": null}}}, {"token_start": 226, "token_end": 242, "char_start": 1181, "char_end": 1232, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 269, "token_end": 284, "char_start": 1398, "char_end": 1461, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 308, "token_end": 314, "char_start": 1582, "char_end": 1599, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"McAllester (2018)": null}}}]}
{"id": "53106476_2", "paragraph": "[BOS] Meta-AL learning Several meta-AL approaches have been proposed to learn the AL selection strategy automaticclay from data.\n[BOS] These methods rely on deep reinforcement learning framework (Yue et al., 2012; Wirth et al., 2017) or bandit algorithms (Nguyen et al., 2017) .\n[BOS] Bachman et al. (2017) introduced a policy gradient based method which jointly learns data representation, selection heuristic as well as the model prediction function.\n[BOS] Fang et al. (2017) designed an active learning algorithm based on a deep Q-network, in which the action corresponds to binary annotation decisions applied to a stream of data.\n[BOS] Woodward and Finn (2017) extended one shot learning to active learning and combined reinforcement learning with a deep recurrent model to make labeling decisions.\n[BOS] As far as we know, we are the first one to develop the Meta-AL method to make use of monolingual data for neural machine translation, the method we proposed in this paper can be applied at mini-batch level and conducted in cross lingual settings.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 31, "token_end": 52, "char_start": 157, "char_end": 233, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yue et al., 2012;": "2145272", "Wirth et al., 2017)": null}}}, {"token_start": 53, "token_end": 64, "char_start": 237, "char_end": 276, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Nguyen et al., 2017)": "1635749"}}}, {"token_start": 66, "token_end": 96, "char_start": 285, "char_end": 452, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Bachman et al. (2017)": "30819066"}, "Reference": {}}}, {"token_start": 97, "token_end": 133, "char_start": 459, "char_end": 634, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Fang et al. (2017)": "22272492"}, "Reference": {}}}, {"token_start": 134, "token_end": 163, "char_start": 641, "char_end": 803, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "53106476_1", "paragraph": "[BOS] Expoliting monolingual data for nmt Monolingual data play a key role in neural machine translation systems, previous work have considered training a seperate language model on the target side (Jean et al., 2014; Gulcehre et al., 2015; Domhan and Hieber, 2017) .\n[BOS] Rather than using explicit language model, Cheng et al. (2016) introduced an auto-encoder-based approach, in which the source-to-target and target-to-source translation models act as encoder and decoder respectively.\n[BOS] Moreover, back translation approaches (Sennrich et al., 2015a; Zhang et al., 2018; Hoang et al., 2018) show efficient use of monolingual data to improve neural machine translation.\n[BOS] Dual learning extends back translation by using a deep RL approach.\n[BOS] More recently, unsupervised approaches (Lample et al., 2017b; Artetxe et al., 2017) and phrase-based NMT (Lample et al., 2018) learn how to translate when having access to only a large amount of monolingual corpora, these models also extend the use of back translation and cross-lingual word embeddings are provided as the latent semantic space for sentences from monolingual corpora in different languages.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Narrative_cite", "Transition", "Single_summ"], "span_citation_mapping": [{"token_start": 36, "token_end": 65, "char_start": 186, "char_end": 265, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Jean et al., 2014;": "2863491", "Gulcehre et al., 2015;": "15352384", "Domhan and Hieber, 2017)": "19164342"}}}, {"token_start": 67, "token_end": 113, "char_start": 274, "char_end": 490, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Cheng et al. (2016)": null}, "Reference": {}}}, {"token_start": 116, "token_end": 145, "char_start": 507, "char_end": 599, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sennrich et al., 2015a;": "15600925", "Zhang et al., 2018;": "3645288", "Hoang et al., 2018)": null}}}, {"token_start": 175, "token_end": 196, "char_start": 773, "char_end": 841, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lample et al., 2017b;": "3518190", "Artetxe et al., 2017)": "3515219"}}}, {"token_start": 197, "token_end": 261, "char_start": 846, "char_end": 1165, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Lample et al., 2018)": "5033497"}, "Reference": {}}}]}
{"id": "53106476_0", "paragraph": "[BOS] For statistical MT (SMT), active learning is well explored, e.g. see , where several heuristics for query sentence selection have been proposed, including the entropy over the potential translations (uncertainty sampling), query by committee, and a similarity-based sentence selection method.\n[BOS] However, active learning is largely under-explored for NMT.\n[BOS] The goal of this paper is to provide an approach to learn an active learning strategy for NMT based on a Hierarchical Markov Decision Process (HMDP) formulation of the pool-based AL (Bachman et al., 2017; .\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 106, "token_end": 118, "char_start": 539, "char_end": 574, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "53103613_0", "paragraph": "[BOS] Neural Relation Extraction: In the recent years, NN models have shown superior performance over approaches using hand-crafted features in various tasks.\n[BOS] CNN is the first one of the deep learning models that have been applied to relation extrac- tion (Santos et al., 2015) .\n[BOS] Variants of convolutional networks include piecewise-CNN (PCNN) (Zeng et al., 2015) , instance-level selective attention CNN (Lin et al., 2016) , rank CNN (Ye et al., 2017) , attention and memory CNN and syntax-aware CNN (He et al., 2018) .\n[BOS] Recurrent neural networks (RNN) are another popular choice, and have been used in recent works in the form of attention RNNs (Zhou et al., 2016) , context-aware long short-term memory units (LSTMs) (Sorokin and Gurevych, 2017) , graph-LSTMs (Peng et al., 2017) and ensemble LSTMs .\n[BOS] Capsule Network: Recently, the capsule network has been proposed to improve the representation limitations of CNNs and RNNs.\n[BOS] (Sabour et al., 2017) replaced the scalar-output feature detectors of CNNs with vector-output capsules and max-pooling with routing-by-agreement.\n[BOS] (Hinton et al., 2018) ) proposed a new iterative routing procedure among capsule layers, based on the EM algorithm.\n[BOS] For natural language processing tasks, (Zhao et al., 2018) explored capsule networks for text classification.\n[BOS] (Gong et al., 2018) designed two dynamic routing policies to aggregate the outputs of RNN/CNN encoding layer into a final encoding vector.\n[BOS] (Wang et al., 2018b ) proposed a capsule model based on RNN for sentiment analysis.\n[BOS] To the best of our knowledge, there has been no work that investigates the performance of capsule networks in relation extraction tasks at present.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite", "Narrative_cite", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 46, "token_end": 58, "char_start": 240, "char_end": 283, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Santos et al., 2015)": "15620570"}}}, {"token_start": 65, "token_end": 80, "char_start": 335, "char_end": 375, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zeng et al., 2015)": "2778800"}}}, {"token_start": 81, "token_end": 95, "char_start": 378, "char_end": 435, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lin et al., 2016)": "397533"}}}, {"token_start": 96, "token_end": 106, "char_start": 438, "char_end": 464, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ye et al., 2017)": "1516982"}}}, {"token_start": 112, "token_end": 124, "char_start": 496, "char_end": 530, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(He et al., 2018)": "12390812"}}}, {"token_start": 149, "token_end": 160, "char_start": 649, "char_end": 683, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhou et al., 2016)": "9870160"}}}, {"token_start": 161, "token_end": 186, "char_start": 686, "char_end": 765, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sorokin and Gurevych, 2017)": "6263378"}}}, {"token_start": 187, "token_end": 200, "char_start": 768, "char_end": 799, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Peng et al., 2017)": "2797612"}}}, {"token_start": 231, "token_end": 266, "char_start": 958, "char_end": 1103, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Sabour et al., 2017)": "3603485"}, "Reference": {}}}, {"token_start": 267, "token_end": 293, "char_start": 1110, "char_end": 1225, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Hinton et al., 2018)": "65203110"}, "Reference": {}}}, {"token_start": 295, "token_end": 315, "char_start": 1236, "char_end": 1341, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Zhao et al., 2018)": "4588148"}, "Reference": {}}}, {"token_start": 316, "token_end": 347, "char_start": 1348, "char_end": 1486, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Gong et al., 2018)": "46938675"}, "Reference": {}}}, {"token_start": 348, "token_end": 369, "char_start": 1493, "char_end": 1576, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Wang et al., 2018b": "4886230"}, "Reference": {}}}]}
{"id": "53083677_3", "paragraph": "[BOS] In this paper, we observe two major issues with the exiting neural models: (1) The generated question words do not match the answer type, since the models do not pay much attention to the answers that are critical to generate question words.\n[BOS] (2) The model copies the context words that are far from and irrelevant to the answer, instead of the words that are close and relevant to the answer, since the models are not aware the positions of the context words.\n[BOS] To address these two issues, we propose an answer-focused and position-aware neural question generation model.\n[BOS] As to positionaware models, Zeng et al. (2014) ; Zhang et al. (2017) introduce position feature in the task of relation extraction.\n[BOS] They apply this feature to encode the relative distance to the target noun pairs.\n[BOS] In the task of QG, apply BIO scheme to label answer position, which is a weak representation of relative distance between answer and its context words.\n[BOS] Sequence-to-sequence In recent years, the sequence-to-sequence model has been widely used in the area of natural language generation, including the tasks of abstractive text summarization, response generation in dialogue, poetry generation, etc.\n[BOS] propose a sequence-to-sequence model and apply it to the task of machine translation.\n[BOS] Bahdanau et al. (2014) introduce attention mechanism to the sequence-to-sequence model and it greatly improves the model performance on the task of machine translation.\n[BOS] To deal with the out of vocabulary issue, several variants of the sequenceto-sequence model have been proposed to copy words from source text (Gu et al., 2016; Cao et al., 2017; See et al., 2017) .\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection", "Multi_summ", "Multi_summ", "Transition", "Transition", "Single_summ", "Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 120, "token_end": 167, "char_start": 595, "char_end": 814, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zeng et al. (2014)": "12873739", "Zhang et al. (2017)": "3782112"}, "Reference": {}}}, {"token_start": 268, "token_end": 302, "char_start": 1323, "char_end": 1491, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Bahdanau et al. (2014)": "11212020"}, "Reference": {}}}, {"token_start": 316, "token_end": 352, "char_start": 1564, "char_end": 1693, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Gu et al., 2016;": "8174613", "Cao et al., 2017;": "13531903", "See et al., 2017)": null}}}]}
{"id": "53083677_2", "paragraph": "[BOS] To tackle the issues of rule-based approaches, the neural network-based approaches are applied to the task of QG.\n[BOS] The neural network-based approaches do not rely on hand-crafted rules, and they are instead data driven and trainable in an end-to-end fashion.\n[BOS] Serban et al. (2016) firstly introduce an encoder-decoder framework with attention mechanism to generate factoid questions for the facts (i.e. each fact is a triple composed of a subject, a predicate and an object) from FreeBase.\n[BOS] Du et al. (2017) introduce sequence-to-sequence model with attention mechanism to generate questions for the text from SQuAD dataset, which contains large-scale manually annotated triples composed of question, answer and the context (i.e. the passage).\n[BOS] enrich the sequenceto-sequence model with rich features, e.g. answer position and lexical features, and incorporate copy mechanism that allows it to copy words from the context when generating a question.\n[BOS] Their experiments show the effectiveness of the rich features and the copy mechanism.\n[BOS] propose to combine templates and sequence-tosequence model.\n[BOS] Specifically, they mine question patterns from a question answering community and apply sequence-to-sequence to generate question patterns for a given text.\n[BOS] model question answering and question generation as dual tasks.\n[BOS] It helps generate better questions when training these two tasks together.\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 64, "token_end": 115, "char_start": 276, "char_end": 505, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Serban et al. (2016)": "12241221"}, "Reference": {}}}, {"token_start": 116, "token_end": 169, "char_start": 512, "char_end": 764, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Du et al. (2017)": "2172129"}, "Reference": {}}}]}
{"id": "53083677_1", "paragraph": "[BOS] (1) what to say, i.e. selecting the targets that should be asked.\n[BOS] (2) how to say, i.e. formulating the structure of the question and producing the surface realization.\n[BOS] This is similar to other natural language generation tasks.\n[BOS] In this paper, we focus on the second sub-task, i.e. surface-form realization of questions by assuming the targets are given.\n[BOS] The rule-based approaches usually include the following steps: (1) Preprocess the given text by applying natural language processing techniques, including syntactic parsing, sentence simplification and semantic role labeling.\n[BOS] (2) Identify the targets that should be asked by using rules or semantic roles.\n[BOS] (3) Generate questions using transformation rules or templates.\n[BOS] (4) Rank the over generated questions by well-designed features Smith, 2009, 2010; Chali and Hasan, 2015) .\n[BOS] The major drawbacks of rule-based approaches include: (1) they rely on rules or templates that are expensive to manually create; (2) the rules or templates lack diversity; (3) the targets that they can deal with are limited.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Reflection", "Transition", "Transition", "Transition", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 167, "token_end": 185, "char_start": 813, "char_end": 877, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Chali and Hasan, 2015)": null}}}]}
{"id": "53083677_0", "paragraph": "[BOS] Question Generation Previous work of QG can be classified into two categories: rule-based and neural network-based.\n[BOS] Regardless of the approach taken, QG usually includes two sub-tasks:\n\n", "discourse_tags": ["Transition", "Transition"], "span_citation_mapping": []}
{"id": "53083307_0", "paragraph": "[BOS] Previous work on paraphrase generation falls into three main groups.\n[BOS] Based mainly on monolingual data, earlier approaches use data-driven, (Lin and Pantel, 2001) , grammar-or thesaurus-based methods (Madnani et al., 2007; McKeown, 1983; Hassan et al., 2007; Kozlowski et al., 2003; Quirk et al., 2004; Zhao et al., 2008) .\n[BOS] In contrast, the pivot-based approach exploits bilingual data and machine translation methods to extract and generate paraphrases (Callison-Burch, 2008; Ganitkevitch and Callison-Burch, 2014; .\n[BOS] Finally, neural approaches build upon the encoder-decoder architecture to learn paraphrase generation models (Mallinson et al., 2017; Prakash et al., 2016) .\n[BOS] (Prakash et al., 2016 ) uses a stacked residual LSTM network with residual connections between LSTM layers and show that their model outperforms sequence to sequence, attention-based, and bi-directional LSTM model on three datasets (PPDB, WikiAnswers, and MSCOCO).\n[BOS] (Mallinson et al., 2017 ) introduces a neural model for multi-lingual, multi-pivot backtranslation and show that it outperforms a paraphrase model trained with a commonly used Statistical Machine Translation system (SMT) on three tasks, namely, correlation with human judgments of paraphrase quality; paraphrase and similarity detection; and sentence-level paraphrase generation.\n[BOS] (Iyyer et al., 2018) also use backtranslation as a mean to provide training data.\n[BOS] In addition, it uses syntax to control paraphrase generation.\n[BOS] Given D2T 5best Aktieselskab is the operating organisation for Aarhus Airport which has a runway name of \"10R/28L\" with a length of 2777.\n[BOS] The Aktieselskab is the operating organisation for Aarhus Airport which has a runway name of \"10R/28L\" with a length of 2777.\n[BOS] Aktieselskab is the operating organisation for Aarhus Airport which has a runway length of 2777 and is named \"10R/28L\".\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite", "Multi_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Other", "Other", "Other"], "span_citation_mapping": [{"token_start": 26, "token_end": 38, "char_start": 138, "char_end": 173, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lin and Pantel, 2001)": "12363172"}}}, {"token_start": 39, "token_end": 96, "char_start": 176, "char_end": 332, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Madnani et al., 2007;": "5180342", "McKeown, 1983;": "5368940", "Hassan et al., 2007;": "10296658", "Kozlowski et al., 2003;": "3099330", "Quirk et al., 2004;": "13043395", "Zhao et al., 2008)": "12003953"}}}, {"token_start": 115, "token_end": 143, "char_start": 438, "char_end": 531, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Callison-Burch, 2008;": "2755801"}}}, {"token_start": 159, "token_end": 181, "char_start": 621, "char_end": 696, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mallinson et al., 2017;": "17246494", "Prakash et al., 2016)": "9385494"}}}, {"token_start": 183, "token_end": 244, "char_start": 705, "char_end": 969, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Prakash et al., 2016": "9385494"}, "Reference": {}}}, {"token_start": 245, "token_end": 326, "char_start": 976, "char_end": 1355, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Mallinson et al., 2017": "17246494"}, "Reference": {}}}, {"token_start": 327, "token_end": 363, "char_start": 1362, "char_end": 1511, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Iyyer et al., 2018)": "4956100"}, "Reference": {}}}]}
{"id": "53083122_4", "paragraph": "[BOS] Representation Interpretation Several researchers have tried to visualize the representation of each layer to help better understand what information each layer captures (Zeiler and Fergus, 2014; .\n[BOS] Concerning natural language processing tasks, Shi et al. (2016) find that both local and global source syntax are learned by the NMT encoder and different types of syntax are captured at different layers.\n[BOS] Anastasopoulos and Chiang (2018) show that higher level layers are more representative than lower level layers.\n[BOS] Peters et al. (2018) demonstrate that higher-level layers capture context-dependent aspects of word meaning while lower-level layers model aspects of syntax.\n[BOS] Inspired by these observations, we propose to expose all of these representations to better fuse information across layers.\n[BOS] In addition, we introduce a regularization to encourage different layers to capture diverse information.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 9, "token_end": 32, "char_start": 70, "char_end": 200, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 35, "token_end": 74, "char_start": 210, "char_end": 414, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Shi et al. (2016)": null}, "Reference": {}}}, {"token_start": 75, "token_end": 97, "char_start": 421, "char_end": 532, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Anastasopoulos and Chiang (2018)": "3351788"}, "Reference": {}}}, {"token_start": 98, "token_end": 129, "char_start": 539, "char_end": 696, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Peters et al. (2018)": "3626819"}, "Reference": {}}}]}
{"id": "53083122_3", "paragraph": "[BOS] Concerning machine translation, Meng et al. (2016) and Zhou et al. (2016) have shown that deep networks with advanced connecting strategies outperform their shallow counterparts.\n[BOS] Due to its simplicity and effectiveness, skip connection becomes a standard component of state-of-the-art NMT models (Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017) .\n[BOS] In this work, we prove that deep representation exploitation can further improve performance over simply using skip connections.\n\n", "discourse_tags": ["Multi_summ", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 6, "token_end": 35, "char_start": 38, "char_end": 184, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Meng et al. (2016)": "15358411", "Zhou et al. (2016)": "8586038"}, "Reference": {}}}, {"token_start": 43, "token_end": 86, "char_start": 232, "char_end": 369, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wu et al., 2016;": null, "Gehring et al., 2017;": "3648736", "Vaswani et al., 2017)": "13756489"}}}]}
{"id": "53083122_2", "paragraph": "[BOS] In response to this problem, ResNet (He et al., 2016) uses skip connections to combine layers by simple, one-step operations.\n[BOS] Densely connected network (Huang et al., 2017 ) is designed to better propagate features and losses through skip connections that concatenate all the layers in stages.\n[BOS] Yu et al. (2018) design structures iteratively and hierarchically merge the feature hierarchy to better fuse information in a deep fusion.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 8, "token_end": 32, "char_start": 35, "char_end": 131, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(He et al., 2016)": "206594692"}, "Reference": {}}}, {"token_start": 33, "token_end": 64, "char_start": 138, "char_end": 305, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Huang et al., 2017": "9433631"}, "Reference": {}}}, {"token_start": 65, "token_end": 91, "char_start": 312, "char_end": 450, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Yu et al. (2018)": "30834643"}, "Reference": {}}}]}
{"id": "53083122_1", "paragraph": "[BOS] Deep Representation Learning Deep neural networks have advanced the state of the art in various communities, such as computer vision and natural language processing.\n[BOS] One key challenge of training deep networks lies in how to transform information across layers, especially when the network consists of hundreds of layers.\n\n", "discourse_tags": ["Transition", "Transition"], "span_citation_mapping": []}
{"id": "53083122_0", "paragraph": "[BOS] Representation learning is at the core of deep learning.\n[BOS] Our work is inspired by technological advances in representation learning, specifically in the field of deep representation learning and representation interpretation.\n\n", "discourse_tags": ["Transition", "Reflection"], "span_citation_mapping": []}
{"id": "53082862_3", "paragraph": "[BOS] Other work (Kang et al., 2018; Zhao et al., 2018) aimed to improve model robustness in the framework of generative adversarial networks (Goodfellow et al., 2014) .\n[BOS] Ribeiro et al. (2018) generated semantically equivalent examples using a set of paraphrase rules derived from a machine translation model.\n[BOS] In contrast to these kinds of adversarial examples, we focus on the model not being sensitive enough to small changes that do change meaning.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 4, "token_end": 42, "char_start": 17, "char_end": 167, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kang et al., 2018;": "44122339", "Zhao et al., 2018)": "3513418", "(Goodfellow et al., 2014)": "1033682"}}}, {"token_start": 44, "token_end": 72, "char_start": 176, "char_end": 314, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ribeiro et al. (2018)": "21740766"}, "Reference": {}}}]}
{"id": "53082862_2", "paragraph": "[BOS] Adversarial examples for NLI systems: Jia and Liang (2017) introduced the notion of distraction for reading comprehension systems by trying to fool systems for SQuAD (Rajpurkar et al., 2016) with information nearly matching the question, added to the end of a supporting passage.\n[BOS] Glockner et al. (2018) showed that many NLI systems were confused by hypotheses that were identical to the premise except for the replacement of a word by a synonym, hypernym, co-hyponym, or antonym.\n[BOS] Naik et al. (2018) found that adding the same strings of words to NLI examples without changing the logical relation could significantly change results, because of word overlap, negation, or length mismatches.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 60, "char_start": 6, "char_end": 285, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Jia and Liang (2017)": "7228830"}, "Reference": {"(Rajpurkar et al., 2016)": "11816014"}}}, {"token_start": 61, "token_end": 112, "char_start": 292, "char_end": 491, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Glockner et al. (2018)": "19204066"}, "Reference": {}}}, {"token_start": 113, "token_end": 154, "char_start": 498, "char_end": 707, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Naik et al. (2018)": "46932607"}, "Reference": {}}}]}
{"id": "53082862_1", "paragraph": "[BOS] Neural network based NLI systems: Dozens of neural network based models have been submitted to the SNLI leaderboard.\n[BOS] Some systems have been developed based on sentence representations (Conneau et al., 2017; Nie and Bansal, 2017) , but most common models apply attention between tokens in the premise and hypothesis.\n[BOS] We focus on three influential models of this kind: Decomposable Attention (Parikh et al., 2016) , ESIM (Chen et al., 2017) , and a pre-trained transformer network (Radford et al., 2018) which obtains stateof-the-art results for various NLI datasets including SNLI and SciTail.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 34, "token_end": 53, "char_start": 171, "char_end": 240, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Conneau et al., 2017;": "28971531", "Nie and Bansal, 2017)": "28323588"}}}, {"token_start": 79, "token_end": 92, "char_start": 385, "char_end": 429, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Parikh et al., 2016)": "8495258"}}}, {"token_start": 93, "token_end": 103, "char_start": 432, "char_end": 456, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chen et al., 2017)": null}}}, {"token_start": 106, "token_end": 120, "char_start": 465, "char_end": 519, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Radford et al., 2018)": "49313245"}}}]}
{"id": "53082862_0", "paragraph": "[BOS] Datasets for NLI: SNLI and MultiNLI are both based on crowdsourced annotation.\n[BOS] In SNLI all the premises came from image captions (Young et al., 2014) , whereas MultiNLI collected premises from several genres including fiction, letters, telephone speech, and a government report.\n[BOS] SciTail constructed more complicated hypotheses based on multiple-choice science exams, whose premises were taken from web text.\n[BOS] More recently, FEVER introduced a fact verification task, where claims are to be verified using all of Wikipedia.\n[BOS] As FEVER established ground truth evidence for or against each claim, premises can be collected with a retrieval module and labeled as supporting, contradictory, or neutral for an NLI dataset.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 30, "token_end": 41, "char_start": 126, "char_end": 161, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Young et al., 2014)": "3104920"}}}]}
{"id": "53081680_0", "paragraph": "[BOS] Previous work is related either in terms of the neural architectures for NLI (Section 4.1), or in terms of work on training data permutations in learning (Section 4.2).\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "53081554_2", "paragraph": "[BOS] trains the sequence-to-sequence model with policy gradient for neural machine translation.\n[BOS] Bahdanau et al. (2017) applies the actor-critic model on the same task.\n\n", "discourse_tags": ["Transition", "Single_summ"], "span_citation_mapping": [{"token_start": 19, "token_end": 39, "char_start": 103, "char_end": 174, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "53081554_1", "paragraph": "[BOS] In this paper, to handle this problem, we propose to use adversarial training (Goodfellow et al., 2014; Denton et al., 2015; Li et al., 2017) , which has achieved success in image generation (Radford et al., 2015; Gulrajani et al., 2017; Berthelot et al., 2017) .\n[BOS] However, training GAN is a non-trivial task and there are some previous researches that investigate methods to improve training performance, such as Wasserstein GAN (WGAN) and Energybased GAN (EGAN) (Salimans et al., 2016; Gulrajani et al., 2017; Zhao et al., 2017; Berthelot et al., 2017) .\n[BOS] GAN in text generation has not shown significant improvement as it has in computer vision.\n[BOS] This is partially because text generation is a process of sampling in discrete space where the normal gradient descent solution is not available, which makes it difficult to train.\n[BOS] There are some researches that focus on tackling this problem.\n[BOS] SeqGAN (Yu et al., 2017) incorporates the policy gradient into the model by treating the procedure of generation as a stochastic policy in reinforcement learning.\n[BOS] Ranzato et al. (2016) Figure 1: Illustration of DP-GAN.\n[BOS] Lower: The generator is trained by policy gradient where the reward is provided by the discriminator.\n[BOS] Upper: The discriminator is based on the language model trained over the real text and the generated text.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Transition", "Transition", "Transition", "Single_summ", "Narrative_cite", "Other", "Other"], "span_citation_mapping": [{"token_start": 15, "token_end": 43, "char_start": 63, "char_end": 147, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Goodfellow et al., 2014;": "1033682", "Denton et al., 2015;": "1282515", "Li et al., 2017)": "98180"}}}, {"token_start": 49, "token_end": 79, "char_start": 180, "char_end": 267, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Radford et al., 2015;": "11758569", "Gulrajani et al., 2017;": "10894094", "Berthelot et al., 2017)": "9957731"}}}, {"token_start": 115, "token_end": 157, "char_start": 452, "char_end": 565, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Salimans et al., 2016;": "1687220", "Gulrajani et al., 2017;": "10894094", "Zhao et al., 2017;": "15876696", "Berthelot et al., 2017)": "9957731"}}}, {"token_start": 221, "token_end": 252, "char_start": 927, "char_end": 1089, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Yu et al., 2017)": "3439214"}, "Reference": {}}}, {"token_start": 253, "token_end": 271, "char_start": 1096, "char_end": 1151, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Ranzato et al. (2016)": "7147309"}}}]}
{"id": "53081554_0", "paragraph": "[BOS] A great deal of attention has been paid to developing data-driven methods for natural language dialogue generation.\n[BOS] Conventional statistical approaches tend to rely extensively on hand-crafted rules and templates, require interaction with humans or simulated users to optimize parameters, or produce conversation responses in an information retrieval fashion.\n[BOS] Such properties prevent training on the large corpora that are becoming increasingly available, or fail to produce novel natural language responses.\n[BOS] Currently, a popular model for text generation is the sequence-to-sequence model (Sutskever et al., 2014; .\n[BOS] However, the sequenceto-sequence model tends to generate short, repetitive , and dull text (Luo et al., 2018) .\n[BOS] Recent researches have focused on developing methods to generate informative and diverse text (Li et al., 2017 (Li et al., , 2016 Guu et al., 2017; Shao et al., 2017) .\n[BOS] Reinforcement learning is incorporated into the model of conversation generation to generate more humanlike speeches (Li et al., 2017) .\n[BOS] Moreover, there are also other methods to improve the diversity of the generated text by using mutual-information, prototype editing, and self attention (Li et al., 2016; Guu et al., 2017; Shao et al., 2017) .\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Narrative_cite", "Narrative_cite", "Narrative_cite", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 96, "token_end": 111, "char_start": 587, "char_end": 637, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 130, "token_end": 141, "char_start": 728, "char_end": 756, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Luo et al., 2018)": "52099015"}}}, {"token_start": 151, "token_end": 187, "char_start": 821, "char_end": 931, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Li et al., 2017": "98180", "(Li et al., , 2016": "7287895", "Guu et al., 2017;": "2318481", "Shao et al., 2017)": "5586146"}}}, {"token_start": 200, "token_end": 214, "char_start": 1024, "char_end": 1074, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Li et al., 2017)": "98180"}}}, {"token_start": 233, "token_end": 267, "char_start": 1178, "char_end": 1290, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Li et al., 2016;": "7287895", "Guu et al., 2017;": "2318481", "Shao et al., 2017)": "5586146"}}}]}
{"id": "53081503_3", "paragraph": "[BOS] Unlike all the prior studies, this paper focuses on a very different kind of text representation, i.e., QA-style text level, for sentiment classification.\n[BOS] To the best of our knowledge, this is the first attempt to perform sentiment classification on this text level.\n\n", "discourse_tags": ["Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "53081503_2", "paragraph": "[BOS] Aspect-level sentiment classification is a relatively new research area in the research community of sentiment analysis and it is a fine-grained classification task.\n[BOS] Recently, Wang et al. (2016) proposed an attention-based LSTM neural network to aspect-level sentiment classification by exploring the connection between an aspect and the content of a sentence.\n[BOS] Tang et al. (2016) proposed a deep memory network with multiple attention-based computational layers to improve the performance.\n[BOS] Wang et al. (2018) proposed a hierarchical attention network to explore both word-level and clause-level sentiment information towards a target aspect.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 34, "token_end": 70, "char_start": 188, "char_end": 372, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wang et al. (2016)": null}, "Reference": {}}}, {"token_start": 71, "token_end": 95, "char_start": 379, "char_end": 507, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Tang et al. (2016)": "359042"}, "Reference": {}}}, {"token_start": 96, "token_end": 125, "char_start": 514, "char_end": 665, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wang et al. (2018)": "51609377"}, "Reference": {}}}]}
{"id": "53081503_1", "paragraph": "[BOS] Word-level sentiment classification has been studied in a long period in the research community of sentiment analysis.\n[BOS] Some early studies have devoted their efforts to predicting the sentiment polarity of a word with different learning models and resources.\n[BOS] Turney (2002) proposed an approach to predicting the sentiment polarity of words by calculating Pointwise Mutual Information (PMI) values between the seed words and the search hits.\n[BOS] Hassan and Radev (2010) and Hassan et al. (2011) applied a Markov random walk model to determine the word polarities with a large word relatedness graph, and the synonyms and hypernyms in WordNet (Miller, 1995) .\n[BOS] More recently, some studies aim to learn better word embedding of a word rather than its polarity.\n[BOS] Tang et al. (2014) developed three neural networks to learn word em- bedding by incorporating sentiment polarities of text in loss functions.\n[BOS] Zhou et al. (2015b) employed both unsupervised and supervised neural networks to learn bilingual sentiment word embedding.\n[BOS] Document-level sentiment classification has also been studied in a long period in the research community of sentiment analysis.\n[BOS] On one hand, many early studies have been devoted their efforts to various of aspects on learning approaches, such as supervised learning (Pang et al., 2002; Riloff et al., 2006) , semi-supervised learning (Li et al., 2010; Xia et al., 2015; , and domain adaptation (Blitzer et al., 2007; He et al., 2011) .\n[BOS] On the other hand, many recent studies employ deep learning approaches to enhance the performances in sentiment classification.\n[BOS] Tang et al. (2015) proposed a user-product neural network to incorporate both user and product information for sentiment classification.\n[BOS] Xu et al. (2016) proposed a Cached Long Short-Term Memory neural networks (CLSTM) to capture the overall semantic information in long texts.\n[BOS] More recently, Long et al. (2017) proposed a novel attention model, namely cognition-based attention, for sentiment classification.\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ", "Multi_summ", "Transition", "Single_summ", "Single_summ", "Transition", "Narrative_cite", "Transition", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 46, "token_end": 81, "char_start": 276, "char_end": 457, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Turney (2002)": "484335"}, "Reference": {}}}, {"token_start": 82, "token_end": 134, "char_start": 464, "char_end": 674, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hassan and Radev (2010)": "2565907", "Hassan et al. (2011)": "14877302"}, "Reference": {"(Miller, 1995)": "207846993"}}}, {"token_start": 156, "token_end": 185, "char_start": 788, "char_end": 929, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Tang et al. (2014)": "886027"}, "Reference": {}}}, {"token_start": 186, "token_end": 208, "char_start": 936, "char_end": 1058, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhou et al. (2015b)": "17864960"}, "Reference": {}}}, {"token_start": 253, "token_end": 273, "char_start": 1317, "char_end": 1377, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Pang et al., 2002;": "7105713", "Riloff et al., 2006)": "6208561"}}}, {"token_start": 274, "token_end": 292, "char_start": 1380, "char_end": 1439, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Li et al., 2010;": "15966365"}}}, {"token_start": 295, "token_end": 313, "char_start": 1447, "char_end": 1504, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Blitzer et al., 2007;": "14688775", "He et al., 2011)": "1728741"}}}, {"token_start": 336, "token_end": 361, "char_start": 1647, "char_end": 1783, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Tang et al. (2015)": "1828713"}, "Reference": {}}}, {"token_start": 362, "token_end": 394, "char_start": 1790, "char_end": 1930, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Xu et al. (2016)": "15124020"}, "Reference": {}}}, {"token_start": 398, "token_end": 421, "char_start": 1952, "char_end": 2068, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Long et al. (2017)": "22332464"}, "Reference": {}}}]}
{"id": "53081503_0", "paragraph": "[BOS] Sentiment classification has become a hot research field in NLP since the pioneering work by Pang et al. (2002) .\n[BOS] In general, the research on traditional sentiment classification has been carried out in different text levels, such like word-level, documentlevel and aspect-level.\n\n", "discourse_tags": ["Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 15, "token_end": 27, "char_start": 80, "char_end": 117, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Pang et al. (2002)": "7105713"}}}]}
{"id": "53081403_1", "paragraph": "[BOS] Several researches have shown that explicitly modeling phrases is useful for neural machine translation (Wang et al., 2017; .\n[BOS] Our results confirm these findings.\n[BOS] Concerning attention models, Luong et al. (2015) proposed a modification to look at only a subset of input words at a time.\n[BOS] This can be regarded as a \"hard\" variation of our fixed-window strategy.\n[BOS] In this study, we propose more flexible strategies for placing and zooming the local scope, which yield better results than the fixed scope.\n\n", "discourse_tags": ["Narrative_cite", "Reflection", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 13, "token_end": 23, "char_start": 83, "char_end": 128, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 33, "token_end": 79, "char_start": 180, "char_end": 382, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Luong et al. (2015)": "1998416"}, "Reference": {}}}]}
{"id": "53081403_0", "paragraph": "[BOS] A successful extension of neural language model is attention mechanism, which can directly capture long-distance dependencies by attending to previously generated words.\n[BOS] Daniluk et al. (2017) proposed a key-value-predict attention to separate the key addressing, value reading, and word predicting functions explicitly.\n[BOS] Im and Cho (2017) and Sperber et al. (2018) adopted self-attention networks for acoustic modeling and natural language inference tasks, respectively.\n[BOS] Vaswani et al. (2017) applied the idea of selfattention to neural machine translation.\n[BOS] Shen et al. (2018a) and Shen et al. (2018b) proposed to improve the self-attention model with directional masks and multi-dimensional features.\n[BOS] Although the standard self-attention model can give more bias toward localness, 6 several studies show that explicitly modeling localness for self-attention model can further improve performance.\n[BOS] For example, Sperber et al. (2018) showed that restricting the self-attention model on the neighboring representations performs better for longer sequences in acoustic modeling and natural language inference tasks.\n[BOS] Closely related to this work, Shaw et al. (2018) introduced relative position encoding to consider the relative distances between sequence elements.\n[BOS] While they modeled localness from static position embedding, we improve locality modeling from dynamically revising attention distribution.\n[BOS] Experimental results show that the two models are complementary to each other, and combining them can further improve performance.\n\n", "discourse_tags": ["Transition", "Single_summ", "Multi_summ", "Single_summ", "Multi_summ", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 29, "token_end": 61, "char_start": 182, "char_end": 331, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Daniluk et al. (2017)": "3000562"}, "Reference": {}}}, {"token_start": 62, "token_end": 94, "char_start": 338, "char_end": 487, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Im and Cho (2017)": "20300138", "Sperber et al. (2018)": null}, "Reference": {}}}, {"token_start": 95, "token_end": 116, "char_start": 494, "char_end": 580, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Vaswani et al. (2017)": "13756489"}, "Reference": {}}}, {"token_start": 117, "token_end": 151, "char_start": 587, "char_end": 730, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Shen et al. (2018a)": "19152001", "Shen et al. (2018b)": "4564356"}, "Reference": {}}}, {"token_start": 190, "token_end": 225, "char_start": 952, "char_end": 1153, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Sperber et al. (2018)": null}, "Reference": {}}}, {"token_start": 232, "token_end": 296, "char_start": 1190, "char_end": 1591, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Shaw et al. (2018)": "3725815"}, "Reference": {}}}]}
{"id": "53081355_4", "paragraph": "[BOS] In this work, we extend existing methods and introduce Tree-LSTM for incorporating syntax into SRL.\n[BOS] Rather than proposing completely new model, we synthesize these techniques and present a unified framework to take genuine superiority of syntactic information.\n\n", "discourse_tags": ["Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "53081355_3", "paragraph": "[BOS] In our experiments, we simplify and reformulate the RCNN model.\n[BOS] However, the simplified model performs poorly on the development and the test sets.\n[BOS] The reason might be that the RCNN model with a single global composition parameter is too simple to cover all types of syntactic relation in a dependency tree.\n[BOS] Because of the poor performance of the modified RCNN, we do not include it in this work.\n[BOS] Considering there might be other approach to incorporate the recursive network in SRL model, we leave it as our future work and just provide a brief discussion here.\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "53081355_2", "paragraph": "[BOS] Besides, syntax encoding has also successfully promoted other NLP tasks.\n[BOS] Tree-LSTM (Tai et al., 2015) is a variant of the standard LSTM that can encode a dependency tree with arbitrary branching factors, which has shown effectiveness on semantic relatedness and the sentiment classification tasks.\n[BOS] In this work, we extend the Tree-LSTM with a relation specific gate and employ it to recursively encode the syntactic dependency tree for SRL.\n[BOS] RCNN (Zhu et al., 2015) is an extension of the recursive neural network (Socher et al., 2010) which has been popularly used to encode trees with fixed branching factors.\n[BOS] The RCNN is able to encode a tree structure with arbitrary number of factors and is useful in a re-ranking model for dependency parsing (Zhu et al., 2015) .\n\n", "discourse_tags": ["Transition", "Single_summ", "Reflection", "Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 16, "token_end": 60, "char_start": 85, "char_end": 309, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Tai et al., 2015)": "3033526"}, "Reference": {}}}, {"token_start": 92, "token_end": 133, "char_start": 465, "char_end": 634, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Zhu et al., 2015)": "8755918"}, "Reference": {"(Socher et al., 2010)": "9923502"}}}, {"token_start": 159, "token_end": 169, "char_start": 758, "char_end": 795, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhu et al., 2015)": "8755918"}}}]}
{"id": "53081355_1", "paragraph": "[BOS] Recently, people have attempted to build endto-end systems for span SRL without syntactic input (Zhou and Xu, 2015; He et al., 2017; Tan et al., 2018) .\n[BOS] Similarly, also proposed a syntax-agnostic model for dependency SRL and obtained favorable results.\n[BOS] Despite the success of syntax-agnostic models, there are several works focus on leveraging the advantages of syntax.\n[BOS] Roth and Lapata (2016) employed dependency path embedding to model syntactic information and exhibited a notable success.\n[BOS] Marcheggiani and Titov (2017) leveraged the graph convolutional network to incorporate syntax into a neural SRL model.\n[BOS] Qian et al. (2017) proposed SA-LSTM to model the whole tree structure of dependency relation in an architecture engineering way.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 15, "token_end": 42, "char_start": 69, "char_end": 156, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhou and Xu, 2015;": "12688069", "He et al., 2017;": "33626727", "Tan et al., 2018)": "12842009"}}}, {"token_start": 88, "token_end": 109, "char_start": 394, "char_end": 515, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Roth and Lapata (2016)": "5779419"}, "Reference": {}}}, {"token_start": 110, "token_end": 136, "char_start": 522, "char_end": 640, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 137, "token_end": 165, "char_start": 647, "char_end": 775, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Qian et al. (2017)": "32616705"}, "Reference": {}}}]}
{"id": "53081355_0", "paragraph": "[BOS] Semantic role labeling was pioneered by Gildea and Jurafsky (2002) , also known as shallow semantic parsing.\n[BOS] In early works of SRL, considerable attention has been paid to feature engineering (Pradhan et al., 2005; Zhao and Kit, 2008; Zhao et al., 2009a,b,c; Li et al., 2009; Bjrkelund et al., 2009; Zhao et al., 2013) .\n[BOS] Along with the the impressive success of deep neural networks Cai and Zhao, 2016; Wang et al., 2016b,a; , a series of neural SRL systems have been proposed.\n[BOS] For instance, Foland and Martin (2015) presented a semantic role labeler using convolutional and time-domain neural networks.\n[BOS] FitzGerald et al. (2015) exploited neural network to jointly embed arguments and semantic roles, akin to the work (Lei et al., 2015) , which induced a compact feature representation applying tensor-based approach.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Narrative_cite", "Single_summ", "Multi_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 19, "char_start": 6, "char_end": 72, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Gildea and Jurafsky (2002)": "62182406"}}}, {"token_start": 41, "token_end": 95, "char_start": 184, "char_end": 330, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Pradhan et al., 2005;": "2440012", "Zhao and Kit, 2008;": "8657922", "Li et al., 2009;": "11914496", "Bj\u00f6rkelund et al., 2009;": "33777646", "Zhao et al., 2013)": "1239326"}}}, {"token_start": 104, "token_end": 122, "char_start": 380, "char_end": 441, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Cai and Zhao, 2016;": "14724419"}}}, {"token_start": 139, "token_end": 161, "char_start": 516, "char_end": 627, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Foland and Martin (2015)": "18951510"}, "Reference": {}}}, {"token_start": 162, "token_end": 182, "char_start": 634, "char_end": 729, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"FitzGerald et al. (2015)": "15048880"}, "Reference": {}}}, {"token_start": 187, "token_end": 206, "char_start": 748, "char_end": 837, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Lei et al., 2015)": null}, "Reference": {}}}]}
{"id": "53081308_0", "paragraph": "[BOS] Text Summarization.\n[BOS] Before the successful application of neural generative models, most of the existing works on text summarization (Dorr et al., 2003; Durrett et al., 2016) have focused on extractive methods.\n[BOS] While some of the early approaches have used a rich set of heuristic rules or sparse features to select textual units to include in the summary, more recent works (Cheng and Lapata, 2016; Nallapati et al., 2017) leverage neural models to select words and sentences from the original text.\n[BOS] With the emergence of sequenceto-sequence models (Sutskever et al., 2014 ) and large-scale datasets like CNN/Daily Mail (Hermann et al., 2015; Nallapati et al., 2016) and NYT (Paulus et al., 2018) , abstractive summarization of longer text have become a more feasible and popular task.\n[BOS] Several recent approaches have been proposed to tackle abstractive summarization problem, where Nallapati et al. (2016) exploits hierarchical encoders, See et al. (2017) proposes pointer-generator network and coverage mechanism to overcome OOV and repetition problems, Tan et al. (2017) introduces a graphbased attention mechanism and hierarchical beam search strategy, and (Paulus et al., 2018) proposes to optimize for ROUGE metric via reinforcement learning.\n[BOS] Although impressive progress has been achieved for sentence-level summarization, attempts on abstractive document summarization task are still in early stages where the simple LEAD-3 baseline performance is only very recently matched (Paulus et al., 2018) .\n[BOS] Neural Machine Translation.\n[BOS] With the recent success of encoder-decoder architectures (Sutskever et al., 2014; Bahdanau et al., 2015) , neural machine translation systems has gained a a lot of attention both from academia (Cho et al., 2014; Luong et al., 2015; Luong and Manning, 2016) and industry Vaswani et al., 2017; Ahmed et al., 2018 ) over statistical machine translation, which has been the dominating translation paradigm for years.\n[BOS] Most of these works has focused more on enhancing the architecture design aspect to tackle with various challenges such as different attention mechanisms (Bahdanau et al., 2015; Luong et al., 2015) , a character-level decoder (Chung et al., 2016) , a translation coverage mechanism (Tu et al., 2016) , and so on.\n[BOS] However, only very recently, a few works Ranzato et al., 2016; Norouzi et al., 2016; Shen et al., 2016; Bahdanau et al., 2017; Zhukov and Kretov, 2017; Casas et al., 2018) have investigated sequence-level optimization by training to maximize BLEU score.\n[BOS] Neural Sequence Generation with RL.\n[BOS] Most neural sequence generation models are trained with the objective of maximizing the probability of the next correct word.\n[BOS] However, this results in a major discrepancy between training and test settings of these models because they are trained with cross-entropy loss at word-level, but evaluated based on sequence-level discrete metrics such as ROUGE (Lin and Och, 2004) or BLEU (Papineni et al., 2002) .\n[BOS] On the other hand, directly optimizing for such evaluation metrics is hard due to non-differentiable nature of the exact objective (Rosti et al., 2011) .\n[BOS] Recent works (Ranzato et al., 2016; Bahdanau et al., 2017; Paulus et al., 2018) address the difficulty of differentiating with respect to rewards based on such discrete metrics using variants of reinforcement learning.\n[BOS] These methods essentially propose to mitigate the problem by optimizing the reward weighted log-likelihood of the hypothesis sequences generated by the model distribution.\n[BOS] In this paper, we propose an alternative solution to tackle this problem by introducing a differentiable approximation to exact LCS metric that can be directly optimized by standard gradient-based methods without RL, while still addressing the exposure bias problem.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Multi_summ", "Narrative_cite", "Multi_summ", "Narrative_cite", "Transition", "Narrative_cite", "Narrative_cite", "Narrative_cite", "Transition", "Transition", "Narrative_cite", "Narrative_cite", "Multi_summ", "Multi_summ", "Reflection"], "span_citation_mapping": [{"token_start": 22, "token_end": 42, "char_start": 125, "char_end": 185, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dorr et al., 2003;": "1729177", "Durrett et al., 2016)": "5125975"}}}, {"token_start": 80, "token_end": 110, "char_start": 391, "char_end": 516, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Cheng and Lapata, 2016;": "1499080", "Nallapati et al., 2017)": "6405271"}, "Reference": {}}}, {"token_start": 115, "token_end": 129, "char_start": 545, "char_end": 595, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sutskever et al., 2014": "7961699"}}}, {"token_start": 136, "token_end": 158, "char_start": 628, "char_end": 689, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hermann et al., 2015;": "6203757", "Nallapati et al., 2016)": "8928715"}}}, {"token_start": 159, "token_end": 170, "char_start": 694, "char_end": 719, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Paulus et al., 2018)": "21850704"}}}, {"token_start": 188, "token_end": 216, "char_start": 815, "char_end": 965, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Nallapati et al. (2016)": "8928715"}, "Reference": {}}}, {"token_start": 217, "token_end": 239, "char_start": 967, "char_end": 1082, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"See et al. (2017)": null}, "Reference": {}}}, {"token_start": 240, "token_end": 258, "char_start": 1084, "char_end": 1183, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Tan et al. (2017)": "26698484"}, "Reference": {}}}, {"token_start": 260, "token_end": 280, "char_start": 1189, "char_end": 1276, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Paulus et al., 2018)": "21850704"}, "Reference": {}}}, {"token_start": 310, "token_end": 329, "char_start": 1459, "char_end": 1538, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Paulus et al., 2018)": "21850704"}}}, {"token_start": 341, "token_end": 364, "char_start": 1608, "char_end": 1685, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sutskever et al., 2014;": "7961699", "Bahdanau et al., 2015)": "11212020"}}}, {"token_start": 365, "token_end": 403, "char_start": 1688, "char_end": 1837, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cho et al., 2014;": "5590763", "Luong et al., 2015;": "1998416", "Luong and Manning, 2016)": null}}}, {"token_start": 404, "token_end": 420, "char_start": 1842, "char_end": 1891, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Vaswani et al., 2017;": "13756489", "Ahmed et al., 2018": "29769422"}}}, {"token_start": 457, "token_end": 478, "char_start": 2123, "char_end": 2197, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bahdanau et al., 2015;": "11212020", "Luong et al., 2015)": "1998416"}}}, {"token_start": 480, "token_end": 492, "char_start": 2202, "char_end": 2246, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chung et al., 2016)": "13495961"}}}, {"token_start": 494, "token_end": 505, "char_start": 2251, "char_end": 2299, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Tu et al., 2016)": null}}}, {"token_start": 520, "token_end": 585, "char_start": 2360, "char_end": 2572, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Ranzato et al., 2016;": "7147309", "Norouzi et al., 2016;": "3631537", "Shen et al., 2016;": "3913537", "Zhukov and Kretov, 2017;": "34333661", "Casas et al., 2018)": "3277189"}}}, {"token_start": 655, "token_end": 664, "char_start": 2976, "char_end": 3001, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lin and Och, 2004)": "1586456"}}}, {"token_start": 665, "token_end": 677, "char_start": 3005, "char_end": 3033, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Papineni et al., 2002)": "11080756"}}}, {"token_start": 694, "token_end": 711, "char_start": 3124, "char_end": 3193, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rosti et al., 2011)": "1318875"}}}, {"token_start": 715, "token_end": 789, "char_start": 3215, "char_end": 3598, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Ranzato et al., 2016;": "7147309", "Paulus et al., 2018)": "21850704"}, "Reference": {}}}]}
{"id": "53081291_2", "paragraph": "[BOS] Collective methods: These methods try to model the event interdependency and detect multiple events in one sentence collectively.\n[BOS] However, nearly all of these methods are feature-based methods (McClosky et al., 2011; Yang and Mitchell, 2016; Liu et al., 2016b) , which rely on elaborately designed features and suffer error propagation from existing NLP tools.\n[BOS] Nguyen et al. (2016) exploits a neural-based method to detect multiple events collectively.\n[BOS] However, they only use the sentence-level information and ne-glect document-level clues, and can only capture the interdependencies between the current event candidate and its former predicted events.\n[BOS] Moreover, there method can not handle the multiple words trigger problem.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 33, "token_end": 61, "char_start": 183, "char_end": 272, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(McClosky et al., 2011;": "2941631", "Yang and Mitchell, 2016;": "2367456"}}}, {"token_start": 80, "token_end": 153, "char_start": 379, "char_end": 757, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "53081291_1", "paragraph": "[BOS] Separate methods: These methods regard multiple events in one sentence as independent ones and recognize them separately.\n[BOS] These methods include feature-based methods which exploit a diverse set of strategies to convert classification clues into feature vectors (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Huang and Riloff, 2012) , and neural-based methods which use neural networks to automatically capture clues from plain texts Nguyen and Grishman, 2015; Feng et al., 2016; Chen et al., 2017; Duan et al., 2017; Liu et al., 2017) .\n[BOS] Though effective these methods, they neglect event interdependency by separately predicting each event.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 41, "token_end": 81, "char_start": 257, "char_end": 375, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ahn, 2006;": null, "Ji and Grishman, 2008;": "1320606", "Liao and Grishman, 2010;": "11187670", "Huang and Riloff, 2012)": "6644751"}}}, {"token_start": 83, "token_end": 135, "char_start": 382, "char_end": 578, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Nguyen and Grishman, 2015;": "10913456", "Feng et al., 2016;": null, "Duan et al., 2017;": "9778664"}}}]}
{"id": "53081291_0", "paragraph": "[BOS] Event detection is an increasingly hot and challenging research topic in NLP.\n[BOS] Generally, existing approaches could roughly be divided into two groups: separate and collective methods.\n\n", "discourse_tags": ["Transition", "Transition"], "span_citation_mapping": []}
{"id": "53080784_1", "paragraph": "[BOS] Recent studies show that the conditional random fields (CRFs) can significantly produce higher tagging accuracy in flat (Athavale et al., 2016) or nested (stacking flat NER to nested representation) (Son and Minh, 2017) NERs.\n[BOS] Ju et al. (2018) proposed a novel neural model to address nested entities by dynamically stacking flat NER layers until no outer entities are extracted.\n[BOS] A cascaded CRF layer is used after the LSTM output in each flat layer.\n[BOS] The authors reported that the model outperforms state-of-the-art results by achieving 74.5% in terms of F-score.\n[BOS] Finkel and Manning (2009) proposed a tree-based representation to represent each sentence as a constituency tree of nested entities.\n[BOS] All entities were treated as phrases and represented as subtrees following the whole tree structure and used a CRFbased approach driven by entity-level features to detect nested entities.\n[BOS] We demonstrate that the performance can be improved significantly without CRFs, by training an exhaustive neural model that learns which regions are entity mentions and how to best classify the regions.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 7, "token_end": 33, "char_start": 35, "char_end": 149, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Athavale et al., 2016)": "615401"}}}, {"token_start": 34, "token_end": 51, "char_start": 153, "char_end": 225, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 55, "token_end": 131, "char_start": 238, "char_end": 586, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ju et al. (2018)": "44161048"}, "Reference": {}}}, {"token_start": 132, "token_end": 193, "char_start": 593, "char_end": 919, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "53080784_0", "paragraph": "[BOS] Interests in nested NER detection have increased in recent years, but it is still the case that NER models deals with only one flat level at a time.\n[BOS] Zhou et al. (2004) detected nested entities in a bottom-up way.\n[BOS] They detected the innermost flat entities and then found other NEs containing the flat entities as substrings using rules derived from the detected entities.\n[BOS] The authors reported an improvement of around 3% in the F-score under certain conditions on the GENIA corpus (Collier et al., 1999) .\n[BOS] Katiyar and Cardie (2018) proposed a neural network-based approach that learns hypergraph representation for nested entities using features extracted from a recurrent neural network (RNN).\n[BOS] The authors reported that the model outperformed the existing state-of-the-art featurebased approaches.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 33, "token_end": 110, "char_start": 161, "char_end": 526, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhou et al. (2004)": null}, "Reference": {"(Collier et al., 1999)": "1291519"}}}, {"token_start": 112, "token_end": 171, "char_start": 535, "char_end": 833, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Katiyar and Cardie (2018)": "44170949"}, "Reference": {}}}]}
{"id": "53080574_4", "paragraph": "[BOS] ABSA has also been researched from a question-answering perspective where deep memory networks have played a major role (Tang et al., 2016b; .\n[BOS] However, unlike our proposed method, none of these methods have tried to model the inter-aspect relations.\n\n", "discourse_tags": ["Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 15, "token_end": 31, "char_start": 80, "char_end": 145, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "53080574_3", "paragraph": "[BOS] More recently, Ma et al. (2017) proposed a model where both context and aspect representations interact with each other's attention mechanism to generate the overall representation.\n[BOS] Tay et al. (2017) proposed word-aspect associations using circular correlation as an improvement over Wang et al. (2016) 's work.\n[BOS] Also, Li et al. (2018) used transformer networks for target-oriented sentiment classification.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 5, "token_end": 35, "char_start": 21, "char_end": 187, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ma et al. (2017)": "20407824"}, "Reference": {}}}, {"token_start": 36, "token_end": 67, "char_start": 194, "char_end": 323, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Tay et al. (2017)": "5824248"}, "Reference": {"Wang et al. (2016)": "18993998"}}}, {"token_start": 70, "token_end": 87, "char_start": 336, "char_end": 424, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Li et al. (2018)": "23204325"}, "Reference": {}}}]}
{"id": "53080574_2", "paragraph": "[BOS] The common way of doing ABSA is feeding the aspect-aware sentence representation to the neural network for classification.\n[BOS] This was first proposed by Wang et al. (2016) where they appended aspect embeddings with the each word embeddings of the sentence to generate aspect-aware sentence representation.\n[BOS] This representation was further fed to an attention layer followed by softmax for final classification.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 30, "token_end": 77, "char_start": 162, "char_end": 424, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wang et al. (2016)": "18993998"}, "Reference": {}}}]}
{"id": "53080574_1", "paragraph": "[BOS] In this paper, we focus on ABSA, which is a key task of sentiment analysis that aims to classify sentiment of each aspect individually in a sentence.\n[BOS] In recent days, thanks to the increasing progress of deep neural network research (Young et al., 2018) , novel frameworks have been proposed, achieving notable performance improvement in aspect-based sentiment analysis.\n\n", "discourse_tags": ["Reflection", "Narrative_cite"], "span_citation_mapping": [{"token_start": 44, "token_end": 56, "char_start": 215, "char_end": 264, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Young et al., 2018)": null}}}]}
{"id": "53080574_0", "paragraph": "[BOS] Sentiment analysis is becoming increasingly important due to the rise of the need to process textual data in wikis, micro-blogs, and other social media platforms.\n[BOS] Sentiment analysis requires solving several related NLP problems, like aspect extraction (Poria et al., 2016) .\n[BOS] Aspect based sentiment analysis (ABSA) is a key task of sentiment analysis which focuses on classifying sentiment of each aspect in the sentences.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 46, "token_end": 57, "char_start": 246, "char_end": 284, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Poria et al., 2016)": null}}}]}
{"id": "53080554_2", "paragraph": "[BOS] The term \"frustratingly easy\" in this paper is borrowed from \"frustratingly easy\" papers (Daum III, 2007; Daum III et al., 2010; Tommasi and Caputo, 2013; .\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 18, "token_end": 48, "char_start": 68, "char_end": 159, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Daum\u00e9 III, 2007;": "5360764", "Daum\u00e9 III et al., 2010;": "3036015"}}}]}
{"id": "53080554_1", "paragraph": "[BOS] Hypotheses reranking of language generation has been extensively studied, but most studies focused on discriminative training using costly annotated data (Shen et al., 2004; White and Rajkumar, 2009; Duh et al., 2010; Kim and Mooney, 2013; Mizumoto and Matsumoto, 2016) .\n[BOS] The main stream of our focused unsupervised approach was a reranking method based on a language model (Chen et al., 2006; Vaswani et al., 2013; Luong and Popescu-Belis, 2016) , and other approaches include reranking methods based on key phrase extraction (Boudin and Morin, 2013) , dependency analysis (Hasan et al., 2010) , and search results (Peng et al., 2013) .\n[BOS] All of the above described studies were not used for model ensemble.\n[BOS] Tomeh et al. (2013) used an ensemble learning, but the purpose was to improve the performance of the reranking model for hypotheses reranking of a single model.\n[BOS] Li et al. (2009) , which work is the most related one, proposed a reranking algorithm for model ensemble.\n[BOS] However, their method was constructed to perform at decoding time, so it can be regarded as runtimeensemble.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Transition", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 19, "token_end": 66, "char_start": 108, "char_end": 275, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Shen et al., 2004;": "750809", "White and Rajkumar, 2009;": "7580069", "Duh et al., 2010;": "627008", "Kim and Mooney, 2013;": "7724142", "Mizumoto and Matsumoto, 2016)": "9640754"}}}, {"token_start": 78, "token_end": 116, "char_start": 343, "char_end": 458, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chen et al., 2006;": "5106061", "Vaswani et al., 2013;": "3065236", "Luong and Popescu-Belis, 2016)": "6499370"}}}, {"token_start": 127, "token_end": 139, "char_start": 517, "char_end": 563, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Boudin and Morin, 2013)": "6545121"}}}, {"token_start": 140, "token_end": 151, "char_start": 566, "char_end": 606, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hasan et al., 2010)": "5995203"}}}, {"token_start": 153, "token_end": 163, "char_start": 613, "char_end": 647, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Peng et al., 2013)": "20553139"}}}, {"token_start": 179, "token_end": 216, "char_start": 731, "char_end": 891, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Tomeh et al. (2013)": "6243527"}, "Reference": {}}}, {"token_start": 217, "token_end": 267, "char_start": 898, "char_end": 1118, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Li et al. (2009)": "10000487"}, "Reference": {}}}]}
{"id": "53080554_0", "paragraph": "[BOS] Distillation techniques for an ensemble of multiple models have been widely studied (Kuncoro et al., 2016; Chebotar and Waters, 2016; Kim and Rush, 2016; Freitag et al., 2017; Stahlberg and Byrne, 2017) , especially after a study by Hinton et al. (2015) .\n[BOS] Kuncoro et al. (2016) and Chebotar and Waters (2016) studied distillation techniques for ensembles of multiple dependency parsers and speech recognition models, respectively.\n[BOS] There are several ensemble methods for ensembles of machine translation models (Kim and Rush, 2016; Freitag et al., 2017; Stahlberg and Byrne, 2017) .\n[BOS] For example, Stahlberg and Byrne (2017) proposed a method of unfolding an ensemble of multiple translation models into a single large model once and shrinking it down to a small one.\n[BOS] However, all methods require extra implementation on a deep-learning framework, and it is not easy to apply them to other models.\n[BOS] Our post-ensemble method does not require such coding skills.\n[BOS] In addition, since the predictions of post-ensemble can be regarded as a teacher model, these distillation techniques should be combined with a teacher model based on post-ensemble.\n\n", "discourse_tags": ["Narrative_cite", "Multi_summ", "Narrative_cite", "Single_summ", "Transition", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 74, "char_start": 6, "char_end": 259, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kuncoro et al., 2016;": "905294", "Chebotar and Waters, 2016;": "18195425", "Kim and Rush, 2016;": null, "Freitag et al., 2017;": "9474415", "Stahlberg and Byrne, 2017)": "16994435", "Hinton et al. (2015)": "7200347"}}}, {"token_start": 76, "token_end": 113, "char_start": 268, "char_end": 442, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kuncoro et al. (2016)": "905294", "Chebotar and Waters (2016)": "18195425"}, "Reference": {}}}, {"token_start": 122, "token_end": 152, "char_start": 501, "char_end": 597, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kim and Rush, 2016;": null, "Freitag et al., 2017;": "9474415", "Stahlberg and Byrne, 2017)": "16994435"}}}, {"token_start": 157, "token_end": 194, "char_start": 619, "char_end": 788, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Stahlberg and Byrne (2017)": "16994435"}, "Reference": {}}}]}
{"id": "53079244_1", "paragraph": "[BOS] Multilingual translation has been extensively studied in Dong et al. (2015) , Firat et al. (2016a) , Luong et al. (2016) and Johnson et al. (2017) .\n[BOS] Owing to excellent translation performance and ease of use, many researchers (Blackwood et al., 2018; Lakew et al., 2018) have conduct translation of multiple languages based on the framework of Johnson et al. (2017) and Ha et al. (2016) .\n[BOS] As for low-resource translation scenario Wang et al., 2017b ), similar to above method, Gu et al. (2018) enable sharing of lexical and sentence representation across multiple languages especially for lowresource multilingual NMT.\n[BOS] Different from previous methods, our work mainly focuses on improving the one-to-many multilingual translation framework while sharing as many parameters as possible.\n\n", "discourse_tags": ["Narrative_cite", "Multi_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 44, "char_start": 6, "char_end": 152, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Dong et al. (2015)": "3666937", "Firat et al. (2016a)": "6359641", "Luong et al. (2016)": "6954272", "Johnson et al. (2017)": "6053988"}}}, {"token_start": 58, "token_end": 101, "char_start": 238, "char_end": 398, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Blackwood et al., 2018;": "47005349", "Lakew et al., 2018)": null}, "Reference": {"Johnson et al. (2017)": "6053988", "Ha et al. (2016)": "5234044"}}}, {"token_start": 105, "token_end": 117, "char_start": 414, "char_end": 466, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Wang et al., 2017b": "6199004"}}}, {"token_start": 124, "token_end": 150, "char_start": 495, "char_end": 636, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Gu et al. (2018)": "3295641"}, "Reference": {}}}]}
{"id": "53079244_0", "paragraph": "[BOS] In this work, we explore the balancing problem of shared and unique parameters, and attempt to incorporate the language-dependent presentation features to distinguish different target languages under the scenario of one-to-many multilingual translation.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "53072814_1", "paragraph": "[BOS] The attribute detection task is also similar to the aspect-based sentiment analysis task (Pontiki et al., 2016) , but contains both subjective and objective aspects.\n[BOS] We take a completely different approach in this paper to tackle the problem by using distant supervision and create significantly larger amount of the training data.\n[BOS] It might be an interesting direction to use this distant supervision way to create more training data for the aspect-based sentiment analysis.\n\n", "discourse_tags": ["Narrative_cite", "Reflection", "Transition"], "span_citation_mapping": [{"token_start": 14, "token_end": 27, "char_start": 71, "char_end": 117, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Pontiki et al., 2016)": null}}}]}
{"id": "53072814_0", "paragraph": "[BOS] The idea of distant supervision has been proposed and used widely in Relation Extraction (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) , where the source of labels is an external knowledge base.\n[BOS] The label assignment is done via aligning entities from knowledge base to text.\n[BOS] In alignment, relation extraction has the problem that not every entity pair expresses the semantic relation stored in the knowledge base.\n[BOS] We can view our crowdsourced attribute labels as a knowledge base of places and their attributes.\n[BOS] The label alignment in our case is much simpler, since both attributes and reviews are associated with the place.\n[BOS] The review text, on the other hand, may or may not express the attribute acquired from crowdsourcing.\n[BOS] Recently (Lin et al., 2016) used neural methods to achieve state-of-the-art for distantly supervised relation extraction.\n[BOS] We thus focus on neural methods in our modeling.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Reflection", "Reflection", "Reflection", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 14, "token_end": 51, "char_start": 75, "char_end": 182, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mintz et al., 2009;": "10910955", "Riedel et al., 2010;": "2386383", "Hoffmann et al., 2011;": "16483125", "Surdeanu et al., 2012)": "5869747"}}}, {"token_start": 170, "token_end": 197, "char_start": 821, "char_end": 933, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Lin et al., 2016)": "397533"}, "Reference": {}}}]}
{"id": "53046959_1", "paragraph": "[BOS] pothesis (Harris, 1954) in which the bivariate distribution of target words and contexts is modeled.\n[BOS] Our work deviates from the word embedding literature in two major aspects.\n[BOS] First, our goal is to represent word pairs, not individual words.\n[BOS] Second, our new PMI formulation models the trivariate word-word-context distribution.\n[BOS] Experiments show that our pair embeddings can complement single-word embeddings.\n\n", "discourse_tags": ["Single_summ", "Reflection", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 23, "char_start": 6, "char_end": 106, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Harris, 1954)": null}, "Reference": {}}}]}
{"id": "53046959_0", "paragraph": "[BOS] Pretrained Word Embeddings Many state-ofthe-art models initialize their word representations using pretrained embeddings such as word2vec (Mikolov et al., 2013a) or ELMo .\n[BOS] These representations are typically trained using an interpretation of the Distributional Hy- Table 8 : Given a context c and a word x, we select the top 3 words y from the entire vocabulary using our scoring function R(x, y)  C(c).\n[BOS] The analysis suggests that the model tends to rank correct matches (italics) over others.\n\n", "discourse_tags": ["Narrative_cite", "Reflection", "Other"], "span_citation_mapping": [{"token_start": 23, "token_end": 37, "char_start": 135, "char_end": 167, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mikolov et al., 2013a)": "16447573"}}}]}
{"id": "53025882_1", "paragraph": "[BOS] Memory architectures have also been found to be helpful in other tasks such as question answering.\n[BOS] Work such as (Xu et al. 2016 ) defines a hierarchal memory architecture consisting of sentence level memory followed by word memory for a QA task while (Chandar et al. 2016 ) defines a memory structure that speeds up loading and inferencing over large knowledge bases.\n[BOS] Recent work by (Chen et al. 2018 ) uses a (a) Architecture of our model with multi-level memory attention.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 21, "token_end": 49, "char_start": 116, "char_end": 256, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Xu et al. 2016": "8702399"}, "Reference": {}}}, {"token_start": 50, "token_end": 74, "char_start": 263, "char_end": 379, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 78, "token_end": 101, "char_start": 401, "char_end": 492, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Chen et al. 2018": "4895052"}, "Reference": {}}}]}
{"id": "53025882_0", "paragraph": "[BOS] Recent methods such as (Vinyals and Le 2015; Serban et al. 2016; proposed for end-to-end learning of dialogs were aimed at modeling open-domain dialogs.\n[BOS] While they can be used for learning task oriented dialogs, they are not well suited to interface with a structured KB.\n[BOS] To better adapt them to handle task oriented dialogs: 1) Bordes and Weston (2017b) proposed a memory network based architecture to better encode KB tuples and perform inferencing over them and 2) Fung, Wu, and Madotto (2018) incorporated copy mechanism to enable copying of words from the past utterances and words from KB while generating responses.\n[BOS] All successful end-to-end task oriented dialog networks (Eric et al. 2017; Bordes and Weston 2017b; Fung, Wu, and Madotto 2018) make assumptions while designing the architecture: 1) KB results are assumed to be a triple store, 2) KB triples and past utterances are forced to be represented in a shared memory to enable copying over them.\n[BOS] Both these assumptions makes the task of inferencing much harder.\n[BOS] Any two fields linked directly in the KB tuple are now linked indirectly by the subject of the triples.\n[BOS] Further, placing the KB results and the past utterances in same memory forces the architecture to encode them using a single strategy.\n[BOS] In contrast, our work uses two different memories for past utterances and KB results.\n[BOS] The decoder is equipped with the ability to copy from both memories, while generating the response.\n[BOS] The KB results are represented using a multi-level memory which better reflects the natural hierarchy encoded by sets of queries and their corresponding result sets.\n\n", "discourse_tags": ["Multi_summ", "Multi_summ", "Multi_summ", "Multi_summ", "Transition", "Transition", "Transition", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 6, "token_end": 67, "char_start": 29, "char_end": 283, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Vinyals and Le 2015;": "12300158", "Serban et al. 2016;": "6126582"}, "Reference": {}}}, {"token_start": 81, "token_end": 107, "char_start": 347, "char_end": 478, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 110, "token_end": 139, "char_start": 486, "char_end": 629, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 144, "token_end": 178, "char_start": 662, "char_end": 774, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Bordes and Weston 2017b;": null, "Fung, Wu, and Madotto 2018)": "5068596"}}}]}
{"id": "52986053_2", "paragraph": "[BOS] Finally, Yang et al. (2018) proposes a mixture of softmaxes to enhance the expressiveness of language model, which demonstrate the effectiveness of our S2SMIX model under the matrix factorization framework.\n\n", "discourse_tags": ["Single_summ"], "span_citation_mapping": [{"token_start": 4, "token_end": 44, "char_start": 15, "char_end": 212, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Yang et al. (2018)": "26238954"}, "Reference": {}}}]}
{"id": "52986053_1", "paragraph": "[BOS] Prior to our work, diverse generation has been studied in image captioning, as some of the training set are comprised of images paired with multiple reference captions.\n[BOS] Some work puts their efforts on decoding stages, and form a group of beam search to encourage diversity (Vijayakumar et al., 2016) , while others pay more attention to adversarial training (Shetty et al., 2017; .\n[BOS] Within translation, our method is similar to Schulz et al. (2018b) , where they propose a MT system armed with variational inference to account for translation variations.\n[BOS] Like us, their diversified generation is driven by latent variables.\n[BOS] Albeit the simplicity of our model, it is effective and able to accommodate variation or diversity.\n[BOS] Meanwhile, we propose several diversity metrics to perform quantitative analysis.\n\n", "discourse_tags": ["Reflection", "Narrative_cite", "Single_summ", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 53, "token_end": 67, "char_start": 265, "char_end": 311, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vijayakumar et al., 2016)": "44614"}}}, {"token_start": 74, "token_end": 85, "char_start": 349, "char_end": 390, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 96, "token_end": 136, "char_start": 445, "char_end": 646, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Schulz et al. (2018b)": "44130927"}, "Reference": {}}}]}
{"id": "52986053_0", "paragraph": "[BOS] Obviously, different domains aim at different readers, thus they exhibit distinctive genres compared to other domains.\n[BOS] A well-tuned MT system cannot directly apply to new domains; otherwise, translation quality will degrade.\n[BOS] Based on this factor, out-domain adaptation has been widely studied for MT, ranging from data selection (Li et al., 2010; Wang et al., 2017) , tuning (Luong and Manning, 2015; Farajian et al., 2017) to domain tags (Chu et al., 2017) .\n[BOS] Similarly, in-domain adaptation is also a compelling direction.\n[BOS] Normally, to train an universal MT system, the training data consist of gigantic corpora covering numerous and various domains.This training data is naturally so diverse that Mima et al. (1997) incorporated extralinguistic information to enhance translation quality.\n[BOS] Michel and Neubig (2018) argue even without explicit signals (gender, politeness etc.\n[BOS] ), they can handle domain-specific information via annotation of speakers, and easily gain quality improvement from a larger number of domains.\n[BOS] Our approach is considerably different from the previous work.\n[BOS] We remove any extra annotation, and treat domain-related information as latent variables, which are learned from corpus.\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite", "Transition", "Single_summ", "Single_summ", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 62, "token_end": 79, "char_start": 332, "char_end": 383, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Wang et al., 2017)": "1054586"}}}, {"token_start": 80, "token_end": 99, "char_start": 386, "char_end": 441, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Luong and Manning, 2015;": null, "Farajian et al., 2017)": "22006749"}}}, {"token_start": 100, "token_end": 110, "char_start": 445, "char_end": 475, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chu et al., 2017)": "15201884"}}}, {"token_start": 127, "token_end": 175, "char_start": 564, "char_end": 820, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Mima et al. (1997)": "15464096"}, "Reference": {}}}, {"token_start": 176, "token_end": 222, "char_start": 827, "char_end": 1062, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Michel and Neubig (2018)": "19247366"}, "Reference": {}}}]}
{"id": "52984984_3", "paragraph": "[BOS] In this paper, we mainly investigate the encoderdecoder attention mechanisms.\n[BOS] More specifically, we explore how attention mechanisms work when translating ambiguous nouns.\n\n", "discourse_tags": ["Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "52984984_2", "paragraph": "[BOS] The encoder-decoder attention mechanisms differ in NMT models.\n[BOS] Tang et al. (2018b) evaluate different NMT models, but focusing on NMT architectures.\n[BOS] Tang et al. (2018a) ; Domhan (2018) compare different attention mechanisms.\n[BOS] However, there is no detailed analysis on attention mechanisms.\n\n", "discourse_tags": ["Transition", "Single_summ", "Multi_summ", "Transition"], "span_citation_mapping": [{"token_start": 15, "token_end": 36, "char_start": 75, "char_end": 160, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Tang et al. (2018b)": "52100282"}, "Reference": {}}}, {"token_start": 37, "token_end": 56, "char_start": 167, "char_end": 242, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Tang et al. (2018a)": "49210174", "Domhan (2018)": "51880415"}, "Reference": {}}}]}
{"id": "52984984_1", "paragraph": "[BOS] In NMT, the encoder may encode contextual information into the hidden states.\n[BOS] Marvin and Koehn (2018) explore the ability of hidden states at different encoder layers in WSD, while we focus on exploring the attention mechanisms that connect the encoder and the decoder.\n[BOS] Koehn and Knowles (2017) and Ghader and Monz (2017) investigate the relation between attention mechanisms and the traditional word alignment.\n[BOS] They find that attention mechanisms not only pay attention to the aligned source tokens but also distribute attention to some unaligned source tokens.\n[BOS] In this paper, we perform a more fine-grained investigation of attention mechanisms, focusing on the task of translating ambiguous nouns.\n[BOS] We also explore the advanced attention mechanisms in Transformer models (Vaswani et al., 2017) .\n\n", "discourse_tags": ["Transition", "Single_summ", "Multi_summ", "Multi_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 18, "token_end": 57, "char_start": 90, "char_end": 281, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Marvin and Koehn (2018)": null}, "Reference": {}}}, {"token_start": 58, "token_end": 115, "char_start": 288, "char_end": 586, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Koehn and Knowles (2017)": "8822680", "Ghader and Monz (2017)": "2389139"}, "Reference": {}}}, {"token_start": 151, "token_end": 163, "char_start": 790, "char_end": 831, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vaswani et al., 2017)": "13756489"}}}]}
{"id": "52984984_0", "paragraph": "[BOS] Both Rios et al. (2017) and Liu et al. (2018) propose some techniques to improve the translation of ambiguous words.\n[BOS] Rios et al. (2017) use sense embeddings and lexical chains as additional input features.\n[BOS] Liu et al. (2018) introduce an additional context vector.\n[BOS] There is an apparent difference in evaluation between these two studies.\n[BOS] Rios et al. (2017) design a constrained WSD task.\n[BOS] They create well-designed test sets to evaluate the performance of NMT models in distinguishing different senses of ambiguous words, rather than evaluating the translations of ambiguous words directly.\n[BOS] By contrast, Liu et al. (2018) evaluate the translations of ambiguous words but on a common test set.\n[BOS] Scoring the contrastive translations is not evaluating the real output of NMT models.\n[BOS] In this paper, we directly evaluate the translations generated by NMT models, using ContraWSD as the test set.\n\n", "discourse_tags": ["Multi_summ", "Single_summ", "Single_summ", "Transition", "Single_summ", "Single_summ", "Single_summ", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 30, "char_start": 6, "char_end": 122, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Rios et al. (2017)": "529114", "Liu et al. (2018)": "4410027"}, "Reference": {}}}, {"token_start": 31, "token_end": 50, "char_start": 129, "char_end": 217, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Rios et al. (2017)": "529114"}, "Reference": {}}}, {"token_start": 51, "token_end": 64, "char_start": 224, "char_end": 281, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Liu et al. (2018)": "4410027"}, "Reference": {}}}, {"token_start": 78, "token_end": 127, "char_start": 367, "char_end": 624, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Rios et al. (2017)": "529114"}, "Reference": {}}}, {"token_start": 131, "token_end": 151, "char_start": 644, "char_end": 732, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Liu et al. (2018)": "4410027"}, "Reference": {}}}]}
{"id": "52113461_0", "paragraph": "[BOS] This section describes prior work in machine translation with neural networks as well as semisupervised machine translation.\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "201707194_3", "paragraph": "[BOS] NarrativeQA The summary setting of the Nar-rativeQA dataset (Kocisky et al., 2018) has in the past been addressed with attention mechanisms by the following models: BiAtt + MRU (Tay et al., 2018a) is similar to BiDAF (Seo et al., 2017) .\n[BOS] It is bi-attentive (attends form context-toquery and vice versa) but enhanced with a MRU (Multi-Range Reasoning Units).\n[BOS] MRU is a compositional encoder that splits the context tokens into ranges (n-grams) of different sizes and combines them in summed n-gram representations and fully-connected layers.\n[BOS] DecaProp (Tay et al., 2018b ) is a neural architecture for reading comprehension, that densely connects all pairwise layers, modeling relationships between passage and query across all hierarchical levels.\n[BOS] Bauer et al. (2018) observed that some of the questions require external commonsense knowledge and developed MHPGM-NOIC -a seq2seq generative model with a copy mechanism that also uses commonsense knowledge and ELMo contextual representations.\n[BOS] Hu et al. (2018b) used an implementation of Reinforced Mnemonic Reader (RMR) (Hu et al., 2018a) .\n[BOS] They also proposed RMR + A2D, a novel teacher-student attention distillation method to train a model to mirror the behavior of the ensemble model RMR (Ens).\n\n", "discourse_tags": ["Multi_summ", "Transition", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 9, "token_end": 25, "char_start": 45, "char_end": 88, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kocisky et al., 2018)": "2593903"}}}, {"token_start": 39, "token_end": 54, "char_start": 171, "char_end": 202, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Tay et al., 2018a)": "53080615"}}}, {"token_start": 57, "token_end": 68, "char_start": 217, "char_end": 241, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Seo et al., 2017)": null}}}, {"token_start": 140, "token_end": 179, "char_start": 564, "char_end": 769, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Tay et al., 2018b": "53284053"}, "Reference": {}}}, {"token_start": 180, "token_end": 229, "char_start": 776, "char_end": 1019, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Bauer et al. (2018)": "52290656"}, "Reference": {}}}, {"token_start": 230, "token_end": 300, "char_start": 1026, "char_end": 1286, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hu et al. (2018b)": "52077283"}, "Reference": {"(Hu et al., 2018a)": "13559921"}}}]}
{"id": "201707194_2", "paragraph": "[BOS] Self-Attention Models in NLP Vanilla selfattention models (Vaswani et al., 2017) use positional encoding, sometimes combined with local convolutions (Yu et al., 2018) to model the token order in text.\n[BOS] Although they are scalable due to their recurrence-free nature, most self-attention models do not well work when trained with fixedlength context, due to the fact that they often learn global token positions observed during training, rather than relative.\n[BOS] To address this issue, Shaw et al. (2018) proposes relative position encoding to model the distance between tokens in the context.\n[BOS] Dai et al. (2019) address the problem of moving beyond fixed-length context by adding recurrence to the self-attention model.\n[BOS] Dai et al. (2019) argue that the fixed-length segments used for language modeling hurt the performance due to the fact that they do not respect sentence or any other semantic boundaries.\n[BOS] In this work we also support the claim that the lack of semantic, and also discourse boundaries is an issue, and therefore we aim to introduce structured linguistic information into the self-attention model.\n[BOS] We hypothesize that the lack of local discourse context is a problem for answering narrative questions, where the answer is contained inside the same sentence, or neighbouring sentences and therefore, by offering discourselevel semantic structure to the attention heads, offer ways to restrict, or focus the model to wider or narrower structures, depending on what is needed (Hu et al., 2018b) 48.40 51.50 RMR (Ens) (Hu et al., 2018b) 50.10 53.90 RMR + A2D (Hu et al., 2018b) 50 for specific questions.\n[BOS] Self-attention architectures can be seen as graph architectures (imagine the token (node) interactions as adjacency matrix) and are applied to graph problems (Velikovi et al., 2018; Li et al., 2019) .\n[BOS] Therefore, in very recent work Koncel-Kedziorski et al. (2019) have used a self-attention encoder as a graph encoder for text generation, in a dual encoder model.\n[BOS] A dual-encoder model similar to Koncel-Kedziorski et al. (2019) is suitable for a setting where the input is knowledge from a graph knowledge-base.\n[BOS] For a text-based setting like ours, where word order is important and the tokens are part of semantic arguments, an approach that tries to encode linguistic information in the same architecture (Strubell et al., 2018) is more appropriate.\n[BOS] Therefore our method is most related to LISA (Strubell et al., 2018) , which uses joint multi-task learning of POS and Dependency Parsing to inject syntactic information for Semantic Role Labeling.\n[BOS] In contrast, we do not use multi-task learning, but directly encode semantic information extracted by pre-processing with existing tools.\n\n", "discourse_tags": ["Single_summ", "Transition", "Single_summ", "Single_summ", "Single_summ", "Reflection", "Reflection", "Narrative_cite", "Single_summ", "Single_summ", "Narrative_cite", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 51, "char_start": 6, "char_end": 206, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Vaswani et al., 2017)": "13756489"}, "Reference": {"(Yu et al., 2018)": "52172080"}}}, {"token_start": 101, "token_end": 126, "char_start": 478, "char_end": 605, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Shaw et al. (2018)": "3725815"}, "Reference": {}}}, {"token_start": 127, "token_end": 155, "char_start": 612, "char_end": 737, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Dai et al. (2019)": "57759363"}, "Reference": {}}}, {"token_start": 156, "token_end": 195, "char_start": 744, "char_end": 930, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Dai et al. (2019)": "57759363"}, "Reference": {}}}, {"token_start": 269, "token_end": 354, "char_start": 1355, "char_end": 1653, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hu et al., 2018b)": "52077283"}}}, {"token_start": 381, "token_end": 401, "char_start": 1803, "char_end": 1858, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Veli\u010dkovi\u0107 et al., 2018;": null}}}, {"token_start": 409, "token_end": 444, "char_start": 1898, "char_end": 2029, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Koncel-Kedziorski et al. (2019)": "102354588"}, "Reference": {}}}, {"token_start": 446, "token_end": 483, "char_start": 2038, "char_end": 2183, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Koncel-Kedziorski et al. (2019)": "102354588"}, "Reference": {}}}, {"token_start": 510, "token_end": 529, "char_start": 2320, "char_end": 2407, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 541, "token_end": 575, "char_start": 2475, "char_end": 2632, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Strubell et al., 2018)": "5068376"}, "Reference": {}}}]}
{"id": "201707194_1", "paragraph": "[BOS] Another line of work builds on linguistic knowledge from downstream tasks, such as coreference resolution (Dhingra et al., 2017) or notions of co-occurring candidate mentions (De Cao et al., 2019) and OpenIE triples (Khot et al., 2017) into RNN-based encoders.\n[BOS] Recently, several pretrained language models Radford et al., 2018b; Devlin et al., 2019) have been shown to incrementally boost the performance of well-performing models for several short paragraph reading comprehension tasks Devlin et al., 2019 ) and question answering (Sun et al., 2019) , as well as many tasks from the GLUE benchmark (Wang et al., 2018a) .\n[BOS] Approaches based on BERT (Devlin et al., 2019) usually perform best when the weights are fine-tuned for the specific training task.\n[BOS] Earlier, many papers that do not use self-attention models or even neural methods have also tried to use semantic parse labels (Yih et al., 2016) , or annotations from upstream tasks (Khashabi et al., 2018b) .\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 16, "token_end": 29, "char_start": 89, "char_end": 134, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dhingra et al., 2017)": "15135049"}}}, {"token_start": 32, "token_end": 48, "char_start": 149, "char_end": 202, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(De Cao et al., 2019)": "52116920"}}}, {"token_start": 49, "token_end": 61, "char_start": 207, "char_end": 241, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Khot et al., 2017)": "957320"}}}, {"token_start": 73, "token_end": 95, "char_start": 291, "char_end": 361, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Devlin et al., 2019)": "52967399"}}}, {"token_start": 113, "token_end": 124, "char_start": 471, "char_end": 518, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Devlin et al., 2019": "52967399"}}}, {"token_start": 126, "token_end": 137, "char_start": 525, "char_end": 562, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sun et al., 2019)": "53109787"}}}, {"token_start": 145, "token_end": 156, "char_start": 596, "char_end": 631, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wang et al., 2018a)": "5034059"}}}, {"token_start": 161, "token_end": 172, "char_start": 660, "char_end": 686, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Devlin et al., 2019)": "52967399"}}}, {"token_start": 210, "token_end": 222, "char_start": 883, "char_end": 923, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yih et al., 2016)": null}}}, {"token_start": 226, "token_end": 239, "char_start": 946, "char_end": 985, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Khashabi et al., 2018b)": "20638934"}}}]}
{"id": "201707194_0", "paragraph": "[BOS] Reading Comprehension with Knowledge Recent work has proposed different approaches for integrating external knowledge into neural models for the high-level downstream tasks reading comprehension (RC) and question answering (QA).\n[BOS] One line of work leverages external knowledge from knowledge bases for RC (Xu et al., 2016; Weissenborn et al., 2017a; Ostermann et al., 2018; Mihaylov and Frank, 2018; Bauer et al., 2018; Wang et al., 2018b) and QA (Das et al., 2017; Sun et al., 2018; Tandon et al., 2018) .\n[BOS] These approaches make use of implicit (Weissenborn et al., 2017a) or explicit (Mihaylov and Frank, 2018; Sun et al., 2018; Bauer et al., 2018) attention-based knowledge aggregation or leverage features from knowledge base relations (Wang et al., 2018b) .\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 48, "token_end": 103, "char_start": 292, "char_end": 449, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Xu et al., 2016;": "139787", "Weissenborn et al., 2017a;": "836118", "Ostermann et al., 2018;": "3871146", "Mihaylov and Frank, 2018;": "29151507", "Bauer et al., 2018;": "52290656", "Wang et al., 2018b)": "4715568"}}}, {"token_start": 104, "token_end": 128, "char_start": 454, "char_end": 514, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Das et al., 2017;": "228172", "Sun et al., 2018;": "52154304", "Tandon et al., 2018)": "52136770"}}}, {"token_start": 135, "token_end": 147, "char_start": 552, "char_end": 588, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Weissenborn et al., 2017a)": "836118"}}}, {"token_start": 148, "token_end": 173, "char_start": 592, "char_end": 665, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mihaylov and Frank, 2018;": "29151507", "Sun et al., 2018;": "52154304", "Bauer et al., 2018)": "52290656"}}}, {"token_start": 182, "token_end": 194, "char_start": 730, "char_end": 775, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wang et al., 2018b)": "4715568"}}}]}
{"id": "201686090_2", "paragraph": "[BOS] Multi-tasks models are able to handle several different problems by sharing a subset of shared parameters.\n[BOS] They have been subject to recent interest within the Natural Language Processing community (Hashimoto et al., 2016; Sgaard and Goldberg, 2016; Eger et al., 2017; Yang et al., 2016) .\n[BOS] This type of models is bio-inspired: human beings are able to carry out a multitude of different tasks and can exploit, when necessary, knowledge related to different types of problems, making the learning of new tasks faster and easier.\n[BOS] (Ruder, 2017) states the reasons why this type of model is effective from a machine learning point of view: the use of several different corpora induces an implicit increase in the number of examples available during the training phase.\n[BOS] In addition, the model has to look for characteristics which may be useful for all the tasks to be processed, which limits the noise modeling and thus, leads to a better generalization.\n[BOS] (Sgaard and Goldberg, 2016) showed that inducing a priori knowledge in a multi-task model, by ordering the tasks to be learned, leads to better performance.\n[BOS] (Yang et al., 2016) have shown that driving a multi-task and multi-language model can improve performance on problems where data is only partially annotated.\n[BOS] (Hashimoto et al., 2016) obtained competitive results on several different tasks with a single model.\n[BOS] However, we should note that there is no guarantee on the benefits of using multi-task models, and that their success depends on the data distribution related to the various problems treated (Mou et al., 2016; Alonso and Plank, 2016; Bingel and Sgaard, 2017) .\n[BOS] (Schulz et al., 2018 ) proposed a multi-task framework to perform end-to-end argument mining.\n[BOS] The result they obtained are very promising.\n[BOS] In this paper, we are interested in leveraging auxiliary informa- tions such as Part-Of-Speech and Chunking tags in a multi-task learning setup, in order to perform argument component detection and classification.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Single_summ", "Transition", "Single_summ", "Single_summ", "Single_summ", "Multi_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 31, "token_end": 67, "char_start": 172, "char_end": 299, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hashimoto et al., 2016;": "2213896", "S\u00f8gaard and Goldberg, 2016;": "16661147", "Eger et al., 2017;": "3221856", "Yang et al., 2016)": "1548828"}}}, {"token_start": 116, "token_end": 161, "char_start": 552, "char_end": 788, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Ruder, 2017)": "10175374"}, "Reference": {}}}, {"token_start": 199, "token_end": 234, "char_start": 987, "char_end": 1143, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(S\u00f8gaard and Goldberg, 2016)": "16661147"}, "Reference": {}}}, {"token_start": 235, "token_end": 268, "char_start": 1150, "char_end": 1307, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Yang et al., 2016)": "1548828"}, "Reference": {}}}, {"token_start": 269, "token_end": 290, "char_start": 1314, "char_end": 1415, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Hashimoto et al., 2016)": "2213896"}, "Reference": {}}}, {"token_start": 318, "token_end": 351, "char_start": 1555, "char_end": 1680, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mou et al., 2016;": null, "Alonso and Plank, 2016;": "2418468"}}}, {"token_start": 353, "token_end": 387, "char_start": 1689, "char_end": 1833, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Schulz et al., 2018": null}, "Reference": {}}}]}
{"id": "201686090_1", "paragraph": "[BOS] Determining the type of argument components (premise, conclusion, etc.)\n[BOS] has often been treated as a supervised text classification problem.\n[BOS] (Eckle-Kohler et al., 2015) distinguished premises and conclusions in news articles using Naive Bayes, Random Forest and SVM (Support Vector Machine).\n[BOS] (Park and Cardie, 2014 ) also used a SVM to determine the extent to which claims are justified in citizen's comments related to possible new legislation projects.\n[BOS] (Stab and Gurevych, 2017) classified argumentative components into premises, claims and major claims in essays using a SVM.\n[BOS] (Persing and Ng, 2016) used maximum entropy classification to determine the type of argument components.\n[BOS] (Potash et al., 2016) used sequence-to-sequence recurrent neural networks to infer the type of argument components.\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 29, "token_end": 62, "char_start": 158, "char_end": 308, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Eckle-Kohler et al., 2015)": "88666"}, "Reference": {}}}, {"token_start": 63, "token_end": 96, "char_start": 315, "char_end": 477, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Park and Cardie, 2014": "14764893"}, "Reference": {}}}, {"token_start": 97, "token_end": 125, "char_start": 484, "char_end": 607, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Stab and Gurevych, 2017)": "207616908"}, "Reference": {}}}, {"token_start": 126, "token_end": 146, "char_start": 614, "char_end": 718, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Persing and Ng, 2016)": "1045043"}, "Reference": {}}}, {"token_start": 147, "token_end": 173, "char_start": 725, "char_end": 840, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Potash et al., 2016)": "12355828"}, "Reference": {}}}]}
{"id": "201686090_0", "paragraph": "[BOS] Argument components detection consists in determining the boundaries separating the textual units carrying arguments from the rest of the text.\n[BOS] This task is generally considered as a supervised text segmentation problem at word level.\n[BOS] Models exploiting the sequential aspect of texts, inherent in the construction of a convincing argumentation, seem particularly adapted and are often used.\n[BOS] (Madnani et al., 2012 ) used a CRF (Conditional Random Field) to identify non-argumentative segments within dissertations.\n[BOS] (Levy et al., 2014) identified the boundaries of textual units detailing conclusions which were supporting or attacking topics discussed in threads from Wikipedia.\n[BOS] (Ajjour et al., 2017) used LSTM (Long shortterm memory, recurrent neural network) to extract arguments from essays, editorials, and from user-generated comments.\n[BOS] (Goudas et al., 2014) first identified sentences containing arguments and then detected their boundaries within social media using a CRF.\n[BOS] (Sardianos et al., 2015) determined argument components boundaries in news articles using also using a CRF.\n[BOS] Similarly, (Stab and Gurevych, 2017 ) used a CRF to extract argument components in essays.\n[BOS] (Eger et al., 2017) leveraged deep learning techniques to extract arguments from raw texts.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 66, "token_end": 95, "char_start": 415, "char_end": 537, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Madnani et al., 2012": "554858"}, "Reference": {}}}, {"token_start": 96, "token_end": 126, "char_start": 544, "char_end": 707, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Levy et al., 2014)": "18847466"}, "Reference": {}}}, {"token_start": 127, "token_end": 167, "char_start": 714, "char_end": 875, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Ajjour et al., 2017)": "34131024"}, "Reference": {}}}, {"token_start": 168, "token_end": 195, "char_start": 882, "char_end": 1019, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Goudas et al., 2014)": "9327867"}, "Reference": {}}}, {"token_start": 196, "token_end": 219, "char_start": 1026, "char_end": 1133, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Sardianos et al., 2015)": null}, "Reference": {}}}, {"token_start": 222, "token_end": 243, "char_start": 1151, "char_end": 1230, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Stab and Gurevych, 2017": "207616908"}, "Reference": {}}}, {"token_start": 244, "token_end": 265, "char_start": 1237, "char_end": 1328, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Eger et al., 2017)": "3221856"}, "Reference": {}}}]}
{"id": "201681626_1", "paragraph": "[BOS] At inference time, Freitag and Al-Onaizan (2016) use uniform ensembles of general and finetuned models.\n[BOS] Our Bayesian Interpolation experiments extend previous work by Allauzen and Riley (2011) on Bayesian Interpolation for language model combination.\n\n", "discourse_tags": ["Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 30, "char_start": 6, "char_end": 109, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 39, "token_end": 56, "char_start": 179, "char_end": 262, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Allauzen and Riley (2011)": null}}}]}
{"id": "201681626_0", "paragraph": "[BOS] Transfer learning has been applied to NMT in many forms.\n[BOS] Luong and Manning (2015) use transfer learning to adapt a general model to indomain data.\n[BOS] Zoph et al. (2016) use multilingual transfer learning to improve NMT for lowresource languages.\n[BOS] Chu et al. (2017) introduce mixed fine-tuning, which carries out transfer learning to a new domain combined with some original domain data.\n[BOS] Kobus et al. (2017) train a single model on multiple domains using domain tags.\n[BOS] Khan et al. (2018) sequentially adapt across multiple biomedical domains to obtain one singledomain model.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 15, "token_end": 37, "char_start": 69, "char_end": 158, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Luong and Manning (2015)": null}, "Reference": {}}}, {"token_start": 38, "token_end": 60, "char_start": 165, "char_end": 260, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zoph et al. (2016)": "16631020"}, "Reference": {}}}, {"token_start": 61, "token_end": 90, "char_start": 267, "char_end": 406, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Chu et al. (2017)": "35273027"}, "Reference": {}}}, {"token_start": 91, "token_end": 110, "char_start": 413, "char_end": 492, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kobus et al. (2017)": "7497218"}, "Reference": {}}}, {"token_start": 111, "token_end": 131, "char_start": 499, "char_end": 605, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Khan et al. (2018)": "53235792"}, "Reference": {}}}]}
{"id": "48357847_2", "paragraph": "[BOS] We next consider experiments on 3 pairwise prediction datasets: SNLI (Bowman et al., 2015) , MultiNLI (Williams et al., 2017) and SICK (Marelli et al., 2014) .\n[BOS] The first two are natural language inference tasks and the third is a sentence semantic relatedness task.\n[BOS] We explore the use of two types of sentential encoders: Bag-of-Words (BoW) and BiLSTM-Max (Conneau et al., 2017a (Tai et al., 2015) .\n[BOS] Due to the small size of the dataset, we only experiment with BoW on SICK.\n[BOS] The full details of hyperparameters are discussed in Appendix B.\n[BOS] Table 4 displays our results.\n[BOS] For BoW models, adding Picturebook embeddings to Glove results in significant gains across all three tasks.\n[BOS] For BiLSTM-Max, our contextual gating sets a new state-of-the-art on SNLI sentence encoding methods (methods without interaction layers), outperforming the recently proposed methods of Im and Cho (2017) ; Shen et al. (2018) .\n[BOS] It is worth noting the effect that different encoders have when using our embeddings.\n[BOS] While non-contextual gating is sufficient to improve bag-of-words methods, with BiLSTM-Max it slightly hurts performance over the Glove baseline.\n[BOS] Adding contextual gating was necessary to improve over the Glove baseline on SNLI.\n[BOS] Finally we note the strength of our own Glove baseline over the reported results of Conneau et al. (2017a) , from which we improve on their accuracy from 85.0 to 86.8 on the development set.\n[BOS] 3\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection", "Reflection", "Reflection", "Reflection", "Transition", "Reflection", "Reflection", "Reflection", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 12, "token_end": 23, "char_start": 70, "char_end": 96, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bowman et al., 2015)": "14604520"}}}, {"token_start": 24, "token_end": 34, "char_start": 99, "char_end": 131, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Williams et al., 2017)": "3432876"}}}, {"token_start": 35, "token_end": 45, "char_start": 136, "char_end": 163, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Marelli et al., 2014)": "762228"}}}, {"token_start": 79, "token_end": 110, "char_start": 340, "char_end": 415, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Conneau et al., 2017a": "28971531", "(Tai et al., 2015)": "3033526"}}}, {"token_start": 203, "token_end": 224, "char_start": 864, "char_end": 949, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Im and Cho (2017)": "20300138", "Shen et al. (2018)": "27764139"}}}, {"token_start": 301, "token_end": 319, "char_start": 1331, "char_end": 1397, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Conneau et al. (2017a)": "28971531"}}}]}
{"id": "48357847_1", "paragraph": "[BOS] also fused text-based representations with imagebased representations (Bruni et al., 2014; Lazaridou et al., 2015; Chrupala et al., 2015; Mao et al., 2016; Silberer et al., 2017; Collell et al., 2017; Zablocki et al., 2018) and representations derived from a knowledge-graph (Thoma et al., 2017) .\n[BOS] More recently, gating-based approaches have been developed for fusing traditional word embeddings with visual representations.\n[BOS] Arevalo et al. (2017) introduce a gating mechanism inspired by the LSTM while Kiela et al. (2018) describe an asymmetric gate that allows one modality to 'attend' to the other.\n[BOS] The work that most closely matches ours is that of who also consider fusing Glove embeddings with visual features.\n[BOS] However, their analysis is restricted to word similarity tasks and they require text-to-image regression to obtain visual embeddings for unseen words, due to the use of ImageNet.\n[BOS] The use of image search allows us to obtain visual embeddings for a virtually unlimited vocabulary without needing a mapping function.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Multi_summ", "Transition", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 9, "token_end": 73, "char_start": 49, "char_end": 229, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bruni et al., 2014;": "2618475", "Lazaridou et al., 2015;": "6618571", "Chrupala et al., 2015;": "13276568", "Mao et al., 2016;": "9461243", "Silberer et al., 2017;": null, "Collell et al., 2017;": "27263492", "Zablocki et al., 2018)": "7335121"}}}, {"token_start": 74, "token_end": 90, "char_start": 234, "char_end": 301, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Thoma et al., 2017)": "33793649"}}}, {"token_start": 113, "token_end": 131, "char_start": 443, "char_end": 514, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Arevalo et al. (2017)": "9401721"}, "Reference": {}}}, {"token_start": 132, "token_end": 156, "char_start": 521, "char_end": 619, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kiela et al. (2018)": null}, "Reference": {}}}]}
{"id": "48357847_0", "paragraph": "[BOS] The use of image search for obtaining word representations is not new.\n[BOS] Table 1 illustrates existing methods that utilize image search and the tasks considered in their work.\n[BOS] There has also been other work using other image sources such as ImageNet (Kiela and Bottou, 2014; Collell and Moens, 2016) over the WordNet synset vocabulary, and using Flickr photos and captions (Joulin et al., 2016) .\n[BOS] Our approach differs from the above methods in three main ways: a) we obtain searchgrounded representations for over 2 million words as opposed to a few thousand, b) we apply our representations to a higher diversity of tasks than previously considered, and c) we introduce a multimodal gating mechanism that allows for a more flexible integration of features than mere concatenation.\n[BOS] Our work also relates to existing multimodal models combining different representations of the data .\n[BOS] Various work has Method tasks (Bergsma and Durme, 2011) bilingual lexicons (Bergsma and Goebel, 2011) lexical preference word similarity (Kiela et al., 2015a) lexical entailment detection (Kiela et al., 2015b) bilingual lexicons (Shutova et al., 2016) metaphor identification (Bulat et al., 2015) predicting property norms (Kiela, 2016) toolbox (Vulic et al., 2016) bilingual lexicons word similarity (Anderson et al., 2017) decoding brain activity (Glavas et al., 2017) semantic text similarity (Bhaskar et al., 2017) abstract vs concrete nouns (Hartmann and Sogaard, 2017) bilingual lexicons (Bulat et al., 2017) decoding brain activity Table 1 : Existing methods that use image search for grounding and their corresponding tasks.\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite", "Reflection", "Reflection", "Narrative_cite"], "span_citation_mapping": [{"token_start": 46, "token_end": 65, "char_start": 257, "char_end": 315, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kiela and Bottou, 2014;": "9187880", "Collell and Moens, 2016)": "17308411"}}}, {"token_start": 74, "token_end": 90, "char_start": 356, "char_end": 410, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Joulin et al., 2016)": "5776303"}}}, {"token_start": 182, "token_end": 193, "char_start": 935, "char_end": 973, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bergsma and Durme, 2011)": "5703661"}}}, {"token_start": 193, "token_end": 206, "char_start": 974, "char_end": 1019, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bergsma and Goebel, 2011)": "2994221"}}}, {"token_start": 206, "token_end": 220, "char_start": 1020, "char_end": 1076, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kiela et al., 2015a)": "5986695"}}}, {"token_start": 220, "token_end": 234, "char_start": 1077, "char_end": 1127, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kiela et al., 2015b)": "10847948"}}}, {"token_start": 234, "token_end": 246, "char_start": 1128, "char_end": 1169, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Shutova et al., 2016)": "7516595"}}}, {"token_start": 246, "token_end": 257, "char_start": 1170, "char_end": 1214, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bulat et al., 2015)": "8463304"}}}, {"token_start": 257, "token_end": 266, "char_start": 1215, "char_end": 1254, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kiela, 2016)": "12755666"}}}, {"token_start": 266, "token_end": 276, "char_start": 1255, "char_end": 1283, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vulic et al., 2016)": "18166588"}}}, {"token_start": 276, "token_end": 289, "char_start": 1284, "char_end": 1342, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Anderson et al., 2017)": "3304729"}}}, {"token_start": 289, "token_end": 301, "char_start": 1343, "char_end": 1388, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Glavas et al., 2017)": "38491634"}}}, {"token_start": 301, "token_end": 314, "char_start": 1389, "char_end": 1436, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bhaskar et al., 2017)": "33652080"}}}, {"token_start": 314, "token_end": 328, "char_start": 1437, "char_end": 1492, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hartmann and Sogaard, 2017)": "28841086"}}}, {"token_start": 328, "token_end": 340, "char_start": 1493, "char_end": 1532, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bulat et al., 2017)": "7633375"}}}]}
{"id": "4891749_1", "paragraph": "[BOS] Motivated by structural consistency issues discussed above, significant effort has also been devoted towards cluster-level modeling.\n[BOS] Since global features are notoriously difficult to define (Wiseman et al., 2016) , they often depend heavily on existing pairwise features or architectures (Bjrkelund and Kuhn, 2014; Manning, 2015, 2016b) .\n[BOS] We similarly use an existing pairwise span-ranking architecture as a building block for modeling more complex structures.\n[BOS] In contrast to Wiseman et al. (2016) who use highly expressive recurrent neural networks to model clusters, we show that the addition of a relatively lightweight gating mechanism is sufficient to effectively model higher-order structures.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Reflection", "Single_summ"], "span_citation_mapping": [{"token_start": 24, "token_end": 42, "char_start": 151, "char_end": 225, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wiseman et al., 2016)": "9163756"}}}, {"token_start": 49, "token_end": 72, "char_start": 266, "char_end": 349, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bj\u00f6rkelund and Kuhn, 2014;": "18296459"}}}, {"token_start": 98, "token_end": 138, "char_start": 501, "char_end": 724, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wiseman et al. (2016)": "9163756"}, "Reference": {}}}]}
{"id": "4891749_0", "paragraph": "[BOS] In addition to the end-to-end span-ranking model (Lee et al., 2017 ) that our proposed model builds upon, there is a large body of literature on coreference resolvers that fundamentally rely on scoring span pairs (Ng and Cardie, 2002; Bengtson and Roth, 2008; Denis and Baldridge, 2008; Fernandes et al., 2012; Durrett and Klein, 2013; Wiseman et al., 2015; Clark and Manning, 2016a) .\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 6, "token_end": 22, "char_start": 25, "char_end": 72, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lee et al., 2017": "1222212"}}}, {"token_start": 33, "token_end": 105, "char_start": 123, "char_end": 389, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ng and Cardie, 2002;": null, "Bengtson and Roth, 2008;": "8179642", "Denis and Baldridge, 2008;": "535939", "Durrett and Klein, 2013;": "16039645", "Wiseman et al., 2015;": "15842085", "Clark and Manning, 2016a)": "2012188"}}}]}
{"id": "208332836_4", "paragraph": "[BOS] Sentiment classification is a text classification task with the objective to classify text according to the sentimental polarities.\n[BOS] This has been a widely researched area (Mntyl et al., 2016) and recently there has been a lot of success in this area.\n[BOS] The current state of the art performance on this task is using transformer (Vaswani et al., 2017) based models like BERT (Devlin et al., 2018) and XL-1 https://github.com/srishti-1795/Humour-Detection Net (Yang et al., 2019) .\n[BOS] These models achieve very high accuracy on the binary classification task of sentiment polarity detection but analyzing the failure modes of these models indicate that these models might fail in cases where there are higher order language concepts like sarcasm, humour and hate speech co-occur in the same utterance.\n[BOS] Hence, through this paper, we investigate the performance of sentiment classification when provided with representative features pertaining to these language oriented concepts and at the same time propose a generic approach to compute these feature so as to reuse for multiple downstream tasks.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 24, "token_end": 41, "char_start": 144, "char_end": 203, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 67, "token_end": 78, "char_start": 332, "char_end": 366, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 81, "token_end": 91, "char_start": 385, "char_end": 411, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 92, "token_end": 126, "char_start": 416, "char_end": 493, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "208332836_3", "paragraph": "[BOS] Recent works have attempted to combine feature extraction models trained on some tasks for a different task.\n[BOS] (Poria et al., 2016) , for instance, uses knowledge about sentiment, emotion, and personality to better detect sarcasm.\n[BOS] This finds a parallel in our attempt here, with the difference that these features include non-linguistic features such as user personality, and we focus only on natural language features to test the transferability of knowledge about certain features to detecting others.\n\n", "discourse_tags": ["Transition", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 21, "token_end": 49, "char_start": 121, "char_end": 240, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Poria et al., 2016)": "138716"}, "Reference": {}}}]}
{"id": "208332836_2", "paragraph": "[BOS] Humour Detection has seen a lot of work, with models being developed on several large-scale public datasets, such as the Pun of the Day, 16000 OneLiners, Short Jokes dataset, and the PTT jokes dataset.\n[BOS] (Chen and Soo, 2018) use a Highway Network on top of a CNN on a combination of these datasets.\n[BOS] (Kim, 2014) uses CNN for sentence classification, and these models have also been tested on funny-labelled reviews from the Yelp dataset 1 .\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 52, "token_end": 76, "char_start": 214, "char_end": 308, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 77, "token_end": 107, "char_start": 315, "char_end": 453, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Kim, 2014)": null}, "Reference": {}}}]}
{"id": "208332836_1", "paragraph": "[BOS] Hate Speech in natural language research has traditionally been a loosely-defined term, with one cause being the similarity with other categorizations of hateful utterances, such as offensive language.\n[BOS] In the context of online reviews, we broadly use hate speech to include any form of offensive language.\n[BOS] (Davidson et al., 2017) introduce the seminal dataset in the field, and test a variety of models -Logistic Regression, Naive Bayes, decision trees, random forests, and Support Vector Machines (SVMs), each tested with 5-fold cross validation to find that the Logistic Regression and Linear SVM tend to perform significantly better than other models.\n[BOS] Models such as LSTMs and CNNs have also been tried in works such as (Badjatiya et al., 2017) and (de Gibert et al., 2018) .\n\n", "discourse_tags": ["Transition", "Reflection", "Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 64, "token_end": 134, "char_start": 324, "char_end": 672, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Davidson et al., 2017)": "1733167"}, "Reference": {}}}, {"token_start": 135, "token_end": 174, "char_start": 679, "char_end": 800, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Badjatiya et al., 2017)": "2880908", "(de Gibert et al., 2018)": "52194540"}}}]}
{"id": "208332836_0", "paragraph": "[BOS] Sentiment classification, sarcasm detection, humor detection and hate speech detection have all seen varying levels of interest from the natural language research community, and have evolved over time as better datasets and modeling techniques have come into the picture.\n[BOS] There has been quite a bit of work on sarcasm detection, especially in the context of Twitterbased self-annotated data and Self-Annotated Reddit Corpus.\n[BOS] The seminal work in this area started with (Gonzlez-Ibez et al., 2011) -they used lexical and pragmatic features and found that pragmatic features were more useful in detecting sarcasm.\n[BOS] Addition of context-based features along with text-based features in certain subsequent models helped as well in improving perfor-mance on sarcasm detection.\n[BOS] There was a dramatic shift with the introduction of deep learning as feature engineering took a back seat and deep models began to be used for learning task-specific representations.\n[BOS] (Hazarika et al., 2018) show that using context, user and text embedding provides state of the art performance, which is challenged by Kolchinski (Kolchinski and Potts, 2018) (West et al., 2014) through a more simplistic user embedding based approach that achieves similar performance without other context (like forum embeddings as used by (Hazarika et al., 2018) ).\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ", "Single_summ", "Transition", "Single_summ"], "span_citation_mapping": [{"token_start": 83, "token_end": 154, "char_start": 443, "char_end": 792, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 188, "token_end": 272, "char_start": 988, "char_end": 1352, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Hazarika et al., 2018)": "21721135"}, "Reference": {"(West et al., 2014)": "14443824"}}}]}
{"id": "3936_1", "paragraph": "[BOS] More recently, Li et. al. (2008) describe three models for handling \"single word deletion\" (they discuss, but do not address, word insertion).\n[BOS] The first model uses a fixed probability of deletion P(NULL), independent of the source word, estimated by counting null alignments in the training corpus.\n[BOS] The second model estimates a deletion probability per-word, P(NULL|w), also directly from the aligned corpus, and the third model trains an SVM to predict the probability of deletion given source language context (neighboring and dependency tree-adjacent words and parts-of-speech).\n[BOS] All three models give large gains of 1.5% BLEU or more on Chinese-English translation.\n[BOS] It is interesting to note that the more sophisticated models provide a relatively small improvement over the simplest model in-domain, and no benefit out-of-domain.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 5, "token_end": 126, "char_start": 21, "char_end": 599, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Li et. al. (2008)": null}, "Reference": {}}}]}
{"id": "3936_0", "paragraph": "[BOS] There is surprisingly little prior work in this area.\n[BOS] We previously (Menezes & Quirk, 2005) explored the use of deletion operations such as (3) above, but these were not grounded in any syntactic context, and the estimation was somewhat heuristic 1 .\n[BOS] The tuple translation model of Crego et al. (2005) , a joint model over source and target translations, also provides a means of deleting words.\n[BOS] In training, sentence pairs such as \"nombre de archivo\" / \"file name\" are first word aligned, then minimal bilingual tuples are identified, such as \"nombre / name\", \"de / NULL\" and \"archivo / file\".\n[BOS] The tuples may involve deletion of words by allowing an empty target side, but do not allow insertion tuples with an empty source side.\n[BOS] These inserted words are bound to an adjacent neighbor.\n[BOS] An n-gram model is trained over the tuple sequences.\n[BOS] As a result, deletion probabilities have the desirable property of being conditioned on adjacent context, yet this context is heavily lexicalized, therefore unlikely to generalize well.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 13, "token_end": 55, "char_start": 66, "char_end": 260, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Menezes & Quirk, 2005)": null}, "Reference": {}}}, {"token_start": 57, "token_end": 227, "char_start": 269, "char_end": 1073, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Crego et al. (2005)": null}, "Reference": {}}}]}
{"id": "208330713_3", "paragraph": "[BOS] Multi-Head Attention Multi-head attention mechanism has shown its effectiveness in machine translation (Vaswani et al., 2017) and generative dialog systems.\n[BOS] Recent studies shows that the modeling ability of multi-head attention has not been completely developed.\n[BOS] Several specific guidance cues of different heads without breaking the vanilla multi-head attention mechanism can further boost the performance, e.g., disagreement regularization (Li et al., 2018; Tao et al., 2018) , information aggregation (Li et al., 2019a) , and functional specialization (Fan et al., 2019) on attention heads, the combination of multi-head attention with multi-task learning (Strubell et al., 2018) .\n[BOS] Our work demonstrates that multi-head attention also benefits from the integration of the phrase information.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 16, "token_end": 28, "char_start": 89, "char_end": 131, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vaswani et al., 2017)": "13756489"}}}, {"token_start": 81, "token_end": 98, "char_start": 432, "char_end": 495, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Li et al., 2018;": "53081097", "Tao et al., 2018)": "4564356"}}}, {"token_start": 99, "token_end": 111, "char_start": 498, "char_end": 540, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Li et al., 2019a)": "102352478"}}}, {"token_start": 113, "token_end": 124, "char_start": 547, "char_end": 591, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Fan et al., 2019)": "59599706"}}}, {"token_start": 131, "token_end": 150, "char_start": 631, "char_end": 700, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Strubell et al., 2018)": "5068376"}}}]}
{"id": "208330713_2", "paragraph": "[BOS] Multi Granularity Representation Multigranularity representation, which is proposed to make full use of subunit composition at different levels of granularity, has been explored in various NLP tasks, such as paraphrase identification (Yin and Schtze, 2015) , Chinese word embedding learning (Yin et al., 2016) , universal sentence encoding and machine translation (Nguyen and Joty, 2018; Li et al., 2019b) .\n[BOS] The major difference between our work and Nguyen and Joty (2018); Li et al. (2019b) lies in that we successfully introduce syntactic information into our multi-granularity representation.\n[BOS] Furthermore, it is not well measured how much phrase information are stored in multi-granularity representation.\n[BOS] We conduct the multi-granularity label prediction tasks and empirically verify that the phrase information is embedded in the multi-granularity representation.\n\n", "discourse_tags": ["Narrative_cite", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 38, "token_end": 51, "char_start": 214, "char_end": 262, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yin and Sch\u00fctze, 2015)": "17578970"}}}, {"token_start": 52, "token_end": 64, "char_start": 265, "char_end": 315, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yin et al., 2016)": "5756244"}}}, {"token_start": 65, "token_end": 88, "char_start": 318, "char_end": 411, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Li et al., 2019b)": "53085166"}}}, {"token_start": 97, "token_end": 129, "char_start": 462, "char_end": 607, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Li et al. (2019b)": "53085166"}}}]}
{"id": "208330713_1", "paragraph": "[BOS] Another thread of work is to implicitly promote the generation of phrase-aware representation, such as the integration of external phrase boundary (Wang et al., 2017; Nguyen and Joty, 2018; Li et al., 2019b) , prior attention bias (Yang et al., , 2019 Guo et al., 2019) .\n[BOS] Our work differs at that we explicitly model phrase patterns at different granularities, which is then attended by different attention heads.\n\n", "discourse_tags": ["Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 23, "token_end": 50, "char_start": 128, "char_end": 213, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wang et al., 2017;": "462553", "Li et al., 2019b)": "53085166"}}}, {"token_start": 51, "token_end": 71, "char_start": 216, "char_end": 275, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yang et al., , 2019": null, "Guo et al., 2019)": "57823006"}}}]}
{"id": "208330713_0", "paragraph": "[BOS] Phrase Modeling for NMT Several works have proven that the introduction of phrase modeling in NMT can obtain promising improvement on translation quality.\n[BOS] Tree-based encoders, which explicitly take the constituent tree (Eriguchi et al., 2016) or dependency tree (Bastings et al., 2017) into consideration, are proposed to produce treebased phrase representations.\n[BOS] The difference of our work from these studies is that they adopt the RNN-based encoder to form the tree-based encoder while we explicitly introduce the phrase structure into the the state-of-the-art multi-layer multi-head SANs-based encoder, which we believe is more challenging.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 39, "token_end": 51, "char_start": 214, "char_end": 254, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Eriguchi et al., 2016)": "12851711"}}}, {"token_start": 52, "token_end": 64, "char_start": 258, "char_end": 297, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "203657379_2", "paragraph": "[BOS] The current work also relates to more recent work in data-augmentation for dependency parsing (Sahin and Steedman, 2018) and more generally in NLP (Sennrich et al., 2016) .\n[BOS] The augmentation methods are designed to address data scarcity by exploiting monolingual corpora or generating synthetic samples in multilingual applications.\n[BOS] However, the underlying abstractions used to generate the synthetic data are induced from auxiliary corpora.\n[BOS] Jonson (2006) show that synthetic corpora generated using a GF grammar can be used to build language models for speech recognition.\n[BOS] Experiments in their work show that synthetic indomain examples generated using the grammar when combined with large out-of-domain data result in significant reduction of word error rate of the speech recognizer.\n[BOS] This work falls in line with similar approaches to combine corpus driven approaches with rule-based systems (Bangalore and Johnston, 2004) , as a way to combine the statistical information available from corpora with good coverage resulting from rule-based abstractions especially when working with restricted domains.\n[BOS] In this paper, we restrict ourselves to utilizing synthetic treebanks for parsing, and leave the discussion on ways to combine synthetic treebanks with real treebanks as future work.\n[BOS] This choice is primarily motivated by our interest in grammar-based development of dependency treebanks as opposed to the traditional way of treebanking -by training human annotators.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Single_summ", "Single_summ", "Narrative_cite", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 12, "token_end": 28, "char_start": 59, "char_end": 126, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 32, "token_end": 44, "char_start": 149, "char_end": 176, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sennrich et al., 2016)": "15600925"}}}, {"token_start": 88, "token_end": 209, "char_start": 465, "char_end": 1140, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Jonson (2006)": "17992480"}, "Reference": {"(Bangalore and Johnston, 2004)": "9550077"}}}]}
{"id": "203657379_1", "paragraph": "[BOS]  we assume multilingual abstraction and the concrete syntaxes are available, namely the GF-RGL to generate language-independent samples in the form of ASTs.\n[BOS]  we also assume that a distribution of the target language is not available and what is available is a distribution on the abstract syntax that generalizes to other languages.\n[BOS] Hence, the resulting treebank is licensed by a grammar, and high-precision cross-linguistic information is specified, but the distribution over the resulting treebank is different from the distribution obtained using the real treebanks.\n[BOS] An alternative to the method of bootstrapping UD treebanks is to use ud2gf as a way to translate existing UD treebanks to GF treebanks, that are licensed by a grammar.\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection", "Transition"], "span_citation_mapping": []}
{"id": "203657379_0", "paragraph": "[BOS] The current trend in dependency parsing is directed towards using synthetic treebanks in an attempt to cover unknown languages for which (a) Learning curves for English with N between 1K and 5K samples (b) Learning curves for English with N between 5K and 10K samples Figure 6 : Learning curves shown using bar plots for parsing models of English resources are minimal or do not exist altogether.\n[BOS] Such treebanks rely on various auxiliary resources: parallel corpora (Tiedemann, 2014) , multilingual word-embeddings (Xiao and Guo, 2014) , MT system for the target language (Tiedemann and Agic, 2016; Tyers et al., 2018) or more minimally, tagged corpora in the target language (Wang and Eisner, 2018) .\n[BOS] Tiedemann and Agic (2016) propose a method to generate synthetic treebanks for new languages using machine translation systems to transfer cross-linguistic information from resource-rich language to under-resourced languages.\n[BOS] This work builds on top of many previous approaches to cross-lingual parsing using parallel corpora and multilingual word-embeddings.\n[BOS] The synthetic treebanks generated in the current work are are different in two ways:\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Single_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 91, "token_end": 99, "char_start": 461, "char_end": 495, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Tiedemann, 2014)": "216772"}}}, {"token_start": 100, "token_end": 112, "char_start": 498, "char_end": 547, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Xiao and Guo, 2014)": "15749718"}}}, {"token_start": 113, "token_end": 136, "char_start": 550, "char_end": 630, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Tyers et al., 2018)": "53622891"}}}, {"token_start": 140, "token_end": 154, "char_start": 650, "char_end": 711, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wang and Eisner, 2018)": "52829985"}}}, {"token_start": 156, "token_end": 224, "char_start": 720, "char_end": 1085, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Tiedemann and Agic (2016)": "7805664"}, "Reference": {}}}]}
{"id": "4937880_4", "paragraph": "[BOS] Reproducibility.\n[BOS] All code, data, and experiments for this paper are available on the CodaLab platform at https://worksheets.\n[BOS] codalab.org/worksheets/ 0xe3eb416773ed4883bb737662b31b4948/.\n[BOS] From factual to humorous (CAPTIONS) SOURCE a black and white dog is running through shallow water .\n[BOS] CROSSALIGNED two dogs are playing on a field to win the water .\n[BOS] STYLEEMBEDDING a black and white dog is running through shallow water .\n[BOS] MULTIDECODER a black and white dog is running through grassy water .\n[BOS] TEMPLATEBASED a black and white dog is running through shallow water looking for .\n[BOS] RETRIEVEONLY a black and white dog is slowly running through a snowy field .\n[BOS] DELETEANDRETRIEVE a black and white dog is running through shallow water to search for bones .\n[BOS] DELETEONLY a black and white dog is running through shallow water like a fish .\n[BOS] From positive to negative (AMAZON) SOURCE i would definitely recommend this for a cute case .\n[BOS] CROSSALIGNED i would not recommend this for a long time .\n[BOS] STYLEEMBEDDING i would definitely recommend this for a cute case .\n[BOS] MULTIDECODER i would definitely recommend this for a bra does it .\n[BOS] TEMPLATEBASED skip this one for a cute case .\n[BOS] RETRIEVEONLY cute while it lasted .\n[BOS] .\n[BOS] .\n[BOS] so if you want to have a NUM night stand case , this is your case .\n[BOS] DELETEONLY i would not recommend it for a cute case .\n[BOS] DELETEANDRETRIEVE i would not recommend this for a cute case .\n\n", "discourse_tags": ["Reflection", "Reflection", "Transition", "Other", "Other", "Other", "Other", "Other", "Other", "Other", "Other", "Other", "Other", "Other", "Other", "Other", "Other", "Other", "Other", "Other", "Other", "Other"], "span_citation_mapping": []}
{"id": "4937880_3", "paragraph": "[BOS] To conclude, we have described a simple method for text attribute transfer that outperforms previous models based on adversarial training.\n[BOS] The main leverage comes from the inductive bias that attributes are usually manifested in localized discriminative phrases.\n[BOS] While many prior works on linguistic style analysis confirm our observation that attributes often manifest in idiosyncratic phrases (Recasens et al., 2013; Schwartz et al., 2017; Newman et al., 2003) , we recognize the fact that in some problems (e.g., Pavlick and Tetreault (2017) ), content and attribute cannot be so cleanly separated along phrase boundaries.\n[BOS] Looking forward, a fruitful direction is to develop a notion of attributes more general than n-grams, but with more inductive bias than arbitrary latent vectors.\n\n", "discourse_tags": ["Reflection", "Reflection", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 59, "token_end": 87, "char_start": 379, "char_end": 480, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Recasens et al., 2013;": "2772094", "Schwartz et al., 2017;": "1994584", "Newman et al., 2003)": null}}}, {"token_start": 88, "token_end": 126, "char_start": 483, "char_end": 643, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Pavlick and Tetreault (2017)": "57565759"}}}]}
{"id": "4937880_2", "paragraph": "[BOS] Our method of detecting attribute markers is reminiscent of Naive Bayes, which is a strong baseline for tasks like sentiment classification (Wang and Manning, 2012) .\n[BOS] Deleting these attribute markers can be viewed as attacking a Naive Bayes classifier by deleting the most informative features (Globerson and Roweis, 2006) , similarly to how adversarial methods are trained to fool an attribute classifier.\n[BOS] One difference is that our classifier is fixed, not jointly trained with the model.\n\n", "discourse_tags": ["Reflection", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 22, "token_end": 32, "char_start": 121, "char_end": 170, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wang and Manning, 2012)": "217537"}}}, {"token_start": 52, "token_end": 65, "char_start": 285, "char_end": 334, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Globerson and Roweis, 2006)": "2116006"}}}]}
{"id": "4937880_1", "paragraph": "[BOS] tually removing the attribute information.\n[BOS] Therefore, we explicitly separate attribute and content by taking advantage of the prior knowledge that the attribute is localized to parts of the sentence.\n[BOS] To address the problem of unaligned data, Hu et al. (2017) relies on an attribute classifier to guide the generator to produce sentences with a desired attribute (e.g. sentiment, tense) in the Variational Autoencoder (VAE) framework.\n[BOS] Similarly, used a regularized autoencoder in the adversarial training framework; however, they also find that these models require extensive hyperparameter tuning and the content tends to be changed during the transfer.\n[BOS] Shen et al. (2017) used a discriminator to align target sentences and sentences transfered to the target domain from the source domain.\n[BOS] More recently, unsupervised machine translation models (Artetxe et al., 2017; Lample et al., 2017) used a cycle loss similar to Jun-Yan et al. (2017) to ensure that the content is preserved during the transformation.\n[BOS] These methods often rely on bilinguial word vectors to provide word-for-word translations, which are then finetune by back-translation.\n[BOS] Thus they can be used to further improve our results.\n\n", "discourse_tags": ["Reflection", "Reflection", "Single_summ", "Single_summ", "Single_summ", "Multi_summ", "Multi_summ", "Reflection"], "span_citation_mapping": [{"token_start": 37, "token_end": 133, "char_start": 218, "char_end": 677, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hu et al. (2017)": "20981275"}, "Reference": {}}}, {"token_start": 134, "token_end": 162, "char_start": 684, "char_end": 819, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Shen et al. (2017)": null}, "Reference": {}}}, {"token_start": 166, "token_end": 245, "char_start": 841, "char_end": 1184, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Artetxe et al., 2017;": "3515219", "Lample et al., 2017)": "3518190"}, "Reference": {"Jun-Yan et al. (2017)": "206770979"}}}]}
{"id": "4937880_0", "paragraph": "[BOS] Our work is closely related to the recent body of work on text attribute transfer with unaligned data, where the key challenge to disentangle attribute and content in an unsupervised way.\n[BOS] Most existing work (Shen et al., 2017; Fu et al., 2018; Melnyk et al., 2017) uses adversarial training to separate attribute and content: the content encoder aims to fool the attribute discriminator by removing attribute information from the content embedding.\n[BOS] However, we find that empirically it is often easy to fool the discriminator without ac- Figure 3: Trade-off curves between matching the target attribute (measured by classifier scores) and preserving the content (measured by BLEU).\n[BOS] Bigger points on the curve correspond to settings used for both training and our official evaluation.\n\n", "discourse_tags": ["Reflection", "Multi_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 40, "token_end": 97, "char_start": 200, "char_end": 460, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Shen et al., 2017;": null, "Fu et al., 2018;": "6484065", "Melnyk et al., 2017)": "1215120"}, "Reference": {}}}]}
{"id": "49363931_4", "paragraph": "[BOS] components common to many systems in addition to pre-assembled models for standard NLP tasks, such as coreference resolution, constituency parsing, named entity recognition, question answering and textual entailment.\n[BOS] In comparison with ALLENNLP, JACK supports both TENSORFLOW and PYTORCH.\n[BOS] Furthermore, JACK can also learn from Knowledge Graphs (discussed in Section 4), while ALLENNLP focuses on textual inputs.\n[BOS] Finally, JACK is structured following a modular architecture, composed by input-, model-, and output modules, facilitating code reuse and the inclusion and prototyping of new methods.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Transition"], "span_citation_mapping": []}
{"id": "49363931_3", "paragraph": "[BOS] Multiple general machine learning frameworks, such as SCIKIT-LEARN (Pedregosa et al., 2011 ), PYTORCH, THEANO (Theano Development Team, 2016 and TENSORFLOW (Abadi et al., 2015) , among others, enable quick prototyping and deployment of ML models.\n[BOS] However, unlike JACK, they do not offer a simple framework for defining and evaluating MR models.\n[BOS] The framework closest in objectives to JACK is ALLENNLP (Gardner et al., 2017) , which is a research-focused open-source NLP library built on PYTORCH.\n[BOS] It provides the basic low-level On the left, the responsibilities covered by the IN-PUT, MODEL and OUTPUT modules that compose a JTREADER instance.\n[BOS] On the right, the data format that is used to interact with a JTREADER (dotted lines indicate that the component is optional).\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 10, "token_end": 23, "char_start": 60, "char_end": 96, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Pedregosa et al., 2011": "10659969"}}}, {"token_start": 25, "token_end": 38, "char_start": 100, "char_end": 146, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 39, "token_end": 50, "char_start": 151, "char_end": 182, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Abadi et al., 2015)": null}}}, {"token_start": 84, "token_end": 186, "char_start": 363, "char_end": 800, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Gardner et al., 2017)": "3994096"}, "Reference": {}}}]}
{"id": "49363931_2", "paragraph": "[BOS] All these frameworks offer pre-built models for standard NLP preprocessing tasks, such as tokenisation, sentence splitting, named entity recognition and parsing.\n[BOS] GATE (Cunningham et al., 2002) and UIMA (Ferrucci and Lally, 2004) are toolkits that allow quick assembly of baseline NLP pipelines, and visualisation and annotation via a Graphical User Interface.\n[BOS] GATE can utilise NLTK and CORENLP models and additionally enable development of rule-based methods using a dedicated pattern language.\n[BOS] UIMA offers a text analysis pipeline which, unlike GATE, also includes retrieving information, but does not offer its own rule-based language.\n[BOS] It is further worth mentioning the Information Retrieval frameworks APACHE LUCENE and APACHE SOLR which can be used for building simple, keyword-based question answering systems, but offer no ML support.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Multi_summ", "Multi_summ", "Transition"], "span_citation_mapping": [{"token_start": 32, "token_end": 136, "char_start": 174, "char_end": 661, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Cunningham et al., 2002)": "18358569", "(Ferrucci and Lally, 2004)": "26266327"}, "Reference": {}}}]}
{"id": "49363931_1", "paragraph": "[BOS] General NLP frameworks include CORENLP (Manning et al., 2014) , NLTK (Bird et al., 2009) , OPENNLP 6 and SPACY.\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 7, "token_end": 19, "char_start": 37, "char_end": 67, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Manning et al., 2014)": "14068874"}}}, {"token_start": 20, "token_end": 30, "char_start": 70, "char_end": 94, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bird et al., 2009)": null}}}]}
{"id": "49363931_0", "paragraph": "[BOS] Machine Reading requires a tight integration of Natural Language Processing and Machine Learning models.\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "48353884_2", "paragraph": "[BOS] In this work, our model learns to produce a particular representation of a tree in parallel.\n[BOS] Representations can be computed in parallel, and the conversion from representation to a full tree can efficiently be done with a divide-and-conquer algorithm.\n[BOS] As our model outputs decisions in parallel, our model doesn't suffer from the exposure bias.\n[BOS] Interestingly, a series of recent works, both in machine translation (Gu et al., 2018) and speech synthesis (Oord et al., 2017) , considered the sequence of output variables conditionally independent given the inputs.\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection", "Narrative_cite"], "span_citation_mapping": [{"token_start": 82, "token_end": 92, "char_start": 419, "char_end": 456, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Gu et al., 2018)": "3480671"}}}, {"token_start": 93, "token_end": 104, "char_start": 461, "char_end": 497, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Oord et al., 2017)": "27706557"}}}]}
{"id": "48353884_1", "paragraph": "[BOS] Other parsing methods ensure structural consistency by operating in a transition-based setting (Chen and Manning, 2014) by parsing either in the top-down direction (Dyer et al., 2016; Liu and Zhang, 2017b) , bottom-up (Zhu et al., 2013; Watanabe and Sumita, 2015; Cross and Huang, 2016) and recently in-order (Liu and Zhang, 2017a) .\n[BOS] Transition-based methods generally suffer from compounding errors due to exposure bias: during testing, the model is exposed to a very different regime (i.e. decisions sampled from the model itself) than what was encountered during training (i.e. the ground-truth decisions) (Daum et al., 2009; Goldberg and Nivre, 2012) .\n[BOS] This can have catastrophic effects on test performance but can be mitigated to a certain extent by using beamsearch instead of greedy decoding.\n[BOS] (Stern et al., 2017b) proposes an effective inference method for generative parsing, which enables direct decoding in those models.\n[BOS] More complex training methods have been devised in order to alleviate this problem (Goldberg and Nivre, 2012; Cross and Huang, 2016) .\n[BOS] Other efforts have been put into neural chart-based parsing (Durrett and Klein, 2015; Stern et al., 2017a) which ensure structural consistency and offer exact inference with CYK algorithm.\n[BOS] (Gaddy et al., 2018 ) includes a simplified CYK-style inference, but the complexity still remains in O(n 3 ).\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Transition", "Single_summ", "Narrative_cite", "Narrative_cite", "Single_summ"], "span_citation_mapping": [{"token_start": 12, "token_end": 24, "char_start": 76, "char_end": 125, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chen and Manning, 2014)": "11616343"}}}, {"token_start": 29, "token_end": 49, "char_start": 151, "char_end": 211, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dyer et al., 2016;": "1949831", "Liu and Zhang, 2017b)": "2617867"}}}, {"token_start": 50, "token_end": 74, "char_start": 214, "char_end": 292, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhu et al., 2013;": null, "Watanabe and Sumita, 2015;": null, "Cross and Huang, 2016)": "15407650"}}}, {"token_start": 75, "token_end": 87, "char_start": 297, "char_end": 337, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Liu and Zhang, 2017a)": "31713824"}}}, {"token_start": 107, "token_end": 161, "char_start": 450, "char_end": 666, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Daum\u00e9 et al., 2009;": "704519", "Goldberg and Nivre, 2012)": "1195002"}}}, {"token_start": 190, "token_end": 216, "char_start": 825, "char_end": 956, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Stern et al., 2017b)": "1604520"}, "Reference": {}}}, {"token_start": 217, "token_end": 244, "char_start": 963, "char_end": 1095, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Goldberg and Nivre, 2012;": "1195002", "Cross and Huang, 2016)": "15407650"}}}, {"token_start": 252, "token_end": 273, "char_start": 1137, "char_end": 1210, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Durrett and Klein, 2015;": "543551", "Stern et al., 2017a)": "8280711"}}}, {"token_start": 287, "token_end": 317, "char_start": 1299, "char_end": 1408, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Gaddy et al., 2018": null}, "Reference": {}}}]}
{"id": "48353884_0", "paragraph": "[BOS] Parsing natural language with neural network models has recently received growing attention.\n[BOS] These models have attained state-of-the-art results for dependency parsing (Chen and Manning, 2014) and constituency parsing (Dyer et al., 2016; Cross and Huang, 2016; Coavoux and Crabb, 2016) .\n[BOS] Early work in neural network based parsing directly use a feed-forward neural network to predict parse trees (Chen and Manning, 2014) .\n[BOS] Vinyals et al. (2015) use a sequence-tosequence framework where the decoder outputs a linearized version of the parse tree given an input sentence.\n[BOS] Generally, in these models, the correctness of the output tree is not strictly ensured (although empirically observed).\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 29, "token_end": 39, "char_start": 161, "char_end": 204, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chen and Manning, 2014)": "11616343"}}}, {"token_start": 40, "token_end": 68, "char_start": 209, "char_end": 297, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dyer et al., 2016;": "1949831", "Cross and Huang, 2016;": "15407650", "Coavoux and Crabb\u00e9, 2016)": "13939903"}}}, {"token_start": 87, "token_end": 97, "char_start": 403, "char_end": 439, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chen and Manning, 2014)": "11616343"}}}, {"token_start": 99, "token_end": 154, "char_start": 448, "char_end": 721, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Vinyals et al. (2015)": "14223"}, "Reference": {}}}]}
{"id": "4858508_1", "paragraph": "[BOS] Focusing on machine translation, Devlin (2017) implemented 16-bit fixed-point integer math to speed up matrix multiplication operations, seeing a 2.59x improvement.\n[BOS] They show competitive BLEU scores on WMT English-French NewsTest2014 while offering significant speedup.\n[BOS] Similarly, (Wu et al., 2016 ) applies 8-bit end-toend quantization in translation models.\n[BOS] They also show that automatic metrics do not suffer as a result.\n[BOS] In this work, quantization requires modification to model training to limit the size of matrix outputs.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 58, "char_start": 6, "char_end": 281, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 61, "token_end": 115, "char_start": 299, "char_end": 558, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "4858508_0", "paragraph": "[BOS] Reducing the resources required for decoding neural nets in general and neural machine translation in particular has been the focus of some attention in recent years.\n[BOS] Vanhoucke et al. (2011) explored accelerating convolutional neural nets with 8-bit integer decoding for speech recognition.\n[BOS] They demonstrated that low precision computation could be used with no significant loss of accuracy.\n[BOS] Han et al. (2015) investigated highly compressing image classification neural networks using network pruning, quantization, and Huffman coding so as to fit completely into on-chip cache, seeing significant improvements in speed and energy efficiency while keeping accuracy losses small.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 30, "token_end": 72, "char_start": 179, "char_end": 409, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Vanhoucke et al. (2011)": "15196840"}, "Reference": {}}}, {"token_start": 73, "token_end": 124, "char_start": 416, "char_end": 702, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Han et al. (2015)": "2134321"}, "Reference": {}}}]}
{"id": "208222253_3", "paragraph": "[BOS] Multi-Head Attention Multi-head attention mechanism has shown its effectiveness in machine translation (Vaswani et al., 2017) and generative dialog systems.\n[BOS] Recent studies shows that the modeling ability of multi-head attention has not been completely developed.\n[BOS] Several specific guidance cues of different heads without breaking the vanilla multi-head attention mechanism can further boost the performance, e.g., disagreement regularization (Li et al., 2018; Tao et al., 2018) , information aggregation (Li et al., 2019a) , and functional specialization (Fan et al., 2019) on attention heads, the combination of multi-head attention with multi-task learning (Strubell et al., 2018) .\n[BOS] Our work demonstrates that multi-head attention also benefits from the integration of the phrase information.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 16, "token_end": 28, "char_start": 89, "char_end": 131, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vaswani et al., 2017)": "13756489"}}}, {"token_start": 81, "token_end": 98, "char_start": 432, "char_end": 495, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Li et al., 2018;": "53081097", "Tao et al., 2018)": "4564356"}}}, {"token_start": 99, "token_end": 111, "char_start": 498, "char_end": 540, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Li et al., 2019a)": "102352478"}}}, {"token_start": 113, "token_end": 124, "char_start": 547, "char_end": 591, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Fan et al., 2019)": "59599706"}}}, {"token_start": 131, "token_end": 150, "char_start": 631, "char_end": 700, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Strubell et al., 2018)": "5068376"}}}]}
{"id": "208222253_2", "paragraph": "[BOS] Multi Granularity Representation Multigranularity representation, which is proposed to make full use of subunit composition at different levels of granularity, has been explored in various NLP tasks, such as paraphrase identification (Yin and Schtze, 2015) , Chinese word embedding learning (Yin et al., 2016) , universal sentence encoding and machine translation (Nguyen and Joty, 2018; Li et al., 2019b) .\n[BOS] The major difference between our work and Nguyen and Joty (2018); Li et al. (2019b) lies in that we successfully introduce syntactic information into our multi-granularity representation.\n[BOS] Furthermore, it is not well measured how much phrase information are stored in multi-granularity representation.\n[BOS] We conduct the multi-granularity label prediction tasks and empirically verify that the phrase information is embedded in the multi-granularity representation.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 38, "token_end": 51, "char_start": 214, "char_end": 262, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yin and Sch\u00fctze, 2015)": "17578970"}}}, {"token_start": 52, "token_end": 64, "char_start": 265, "char_end": 315, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yin et al., 2016)": "5756244"}}}, {"token_start": 65, "token_end": 88, "char_start": 318, "char_end": 411, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Li et al., 2019b)": "53085166"}}}, {"token_start": 90, "token_end": 129, "char_start": 420, "char_end": 607, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Li et al. (2019b)": "53085166"}}}]}
{"id": "208222253_1", "paragraph": "[BOS] Another thread of work is to implicitly promote the generation of phrase-aware representation, such as the integration of external phrase boundary (Wang et al., 2017; Nguyen and Joty, 2018; Li et al., 2019b) , prior attention bias (Yang et al., , 2019 Guo et al., 2019) .\n[BOS] Our work differs at that we explicitly model phrase patterns at different granularities, which is then attended by different attention heads.\n\n", "discourse_tags": ["Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 23, "token_end": 50, "char_start": 128, "char_end": 213, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wang et al., 2017;": "462553", "Li et al., 2019b)": "53085166"}}}, {"token_start": 51, "token_end": 71, "char_start": 216, "char_end": 275, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yang et al., , 2019": null, "Guo et al., 2019)": "57823006"}}}]}
{"id": "208222253_0", "paragraph": "[BOS] Phrase Modeling for NMT Several works have proven that the introduction of phrase modeling in NMT can obtain promising improvement on translation quality.\n[BOS] Tree-based encoders, which explicitly take the constituent tree (Eriguchi et al., 2016) or dependency tree (Bastings et al., 2017) into consideration, are proposed to produce treebased phrase representations.\n[BOS] The difference of our work from these studies is that they adopt the RNN-based encoder to form the tree-based encoder while we explicitly introduce the phrase structure into the the state-of-the-art multi-layer multi-head SANs-based encoder, which we believe is more challenging.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 39, "token_end": 51, "char_start": 214, "char_end": 254, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Eriguchi et al., 2016)": "12851711"}}}, {"token_start": 52, "token_end": 64, "char_start": 258, "char_end": 297, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bastings et al., 2017)": "6206777"}}}]}
{"id": "208331826_2", "paragraph": "[BOS] Closely related to our work, Hao et al. (2019b) find that the integration of the recurrence in SANs encoder can provide more syntactic structure fea-tures to the encoder representations.\n[BOS] Our work follows this direction and empirically evaluates the structure modelling on the related tasks.\n\n", "discourse_tags": ["Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 8, "token_end": 43, "char_start": 35, "char_end": 192, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hao et al. (2019b)": "198186958"}, "Reference": {}}}]}
{"id": "208331826_1", "paragraph": "[BOS] Structure Modeling for Neural Networks in NLP Structure modeling in NLP has been studied for a long time as the natural language sentences inherently have hierarchical structures (Chomsky, 1965; Bever, 1970) .\n[BOS] With the emergence of deep learning, tree-based models have been proposed to integrate syntactic tree structure into Recursive Neural Networks (Socher et al., 2013) , LSTMs (Tai et al., 2015) , CNNs (Mou et al., 2016) .\n[BOS] As for SANs, Hao et al. (2019a) , Ma et al. (2019) and enhance the SANs with neural syntactic distance, multigranularity attention scope and structural position representations, which are generated from the syntactic tree structures.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Multi_summ"], "span_citation_mapping": [{"token_start": 29, "token_end": 42, "char_start": 161, "char_end": 213, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chomsky, 1965;": null, "Bever, 1970)": null}}}, {"token_start": 64, "token_end": 76, "char_start": 339, "char_end": 386, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Socher et al., 2013)": "990233"}}}, {"token_start": 77, "token_end": 88, "char_start": 389, "char_end": 413, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Tai et al., 2015)": "3033526"}}}, {"token_start": 89, "token_end": 100, "char_start": 416, "char_end": 439, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mou et al., 2016)": "1914494"}}}, {"token_start": 106, "token_end": 154, "char_start": 461, "char_end": 681, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hao et al. (2019a)": "202637966", "Ma et al. (2019)": "174799954"}, "Reference": {}}}]}
{"id": "208331826_0", "paragraph": "[BOS] Improved Self-Attention Networks Recently, there is a large body of work on improving SANs in various NLP tasks (Yang et al., 2018; Yang et al., 2019a,b; Guo et al., 2019; Sukhbaatar et al., 2019) , as well as image classification (Bello et al., 2019) and automatic speech recognition (Mohamed et al., 2019) tasks.\n[BOS] In these works, several strategies are proposed to improve the utilize SANs with the enhancement of local and global information.\n[BOS] In this work, we enhance the SANs with the On-Lstm to form a hybrid model , and thoroughly evaluate the performance on machine translation, targeted linguistic evaluation, and logical inference tasks.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 21, "token_end": 62, "char_start": 108, "char_end": 202, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yang et al., 2018;": "53081403", "Guo et al., 2019;": "57823006", "Sukhbaatar et al., 2019)": "159041867"}}}, {"token_start": 66, "token_end": 78, "char_start": 216, "char_end": 257, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bello et al., 2019)": null}}}, {"token_start": 79, "token_end": 92, "char_start": 262, "char_end": 313, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mohamed et al., 2019)": "135465923"}}}]}
{"id": "4940747_2", "paragraph": "[BOS] Adversarial Training It has been shown in the field of image classification that training with adversarial examples produces more robust and error-resistant models (Goodfellow et al., 2015; Kurakin et al., 2017) .\n[BOS] In the field of Q&A, Jia and Liang (2017) attempted to retrain the BiDAF (Seo et al., 2017) model with data generated with AddSent algorithm.\n[BOS] Despite performing well when evaluated on AddSent, the retrained model suffers a more than 30% decrease in F1 performance when tested on a slightly different adversarial dataset generated by AddSentMod (which differs from AddSent in two superficial ways: using a different set of fake answers and prepending instead of appending the distractor sentence to the context).\n[BOS] We show that using AddSent to generate adversarial training data introduces new superficial trends for a model to exploit; and instead we propose the AddSentDiverse algorithm that generates highly varied data for adversarial training, resulting in more robust models.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 23, "token_end": 48, "char_start": 136, "char_end": 217, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Goodfellow et al., 2015;": "6706414", "Kurakin et al., 2017)": "9059612"}}}, {"token_start": 50, "token_end": 163, "char_start": 226, "char_end": 743, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Jia and Liang (2017)": "7228830"}, "Reference": {"(Seo et al., 2017)": null}}}]}
{"id": "4940747_1", "paragraph": "[BOS] In the field of Q&A, Jia and Liang (2017) introduced the AddSent algorithm, which generates adversaries that punish model failure in the other direction: overstability, or the inability to detect semantic-altering noise.\n[BOS] It does so by generating distractor sentences that only resemble the questions syntactically and appending them to the context paragraphs (detailed description included in Sec.\n[BOS] 3).\n[BOS] When tested on these adversarial examples, Jia and Liang (2017) showed that even the most 'robust' amongst published models (the Mnemonic Reader (Hu et al., 2017) ) only achieved 46.6% F1 (compared to 79.6% F1 on the regular task).\n[BOS] Since then, the FusionNet model (Huang et al., 2018) used history-of-word representations and multi-level attention mechanism to obtain an improved 51.4% F1 score under adversarial evaluation, but that is still a 30% decrease from the model's performance on the regular task.\n[BOS] We show, however, that one can make a pre-existing model significantly more robust by simply retraining it with better, higher variance adversarial training data, and improve it further with minor semantic feature additions to its inputs.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 81, "char_start": 6, "char_end": 409, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Jia and Liang (2017)": "7228830"}, "Reference": {}}}, {"token_start": 94, "token_end": 149, "char_start": 469, "char_end": 657, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Jia and Liang (2017)": "7228830"}, "Reference": {"(Hu et al., 2017)": "64515663"}}}, {"token_start": 153, "token_end": 213, "char_start": 676, "char_end": 939, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Huang et al., 2018)": "11480374"}, "Reference": {}}}]}
{"id": "4940747_0", "paragraph": "[BOS] Adversarial Evaluation In computer vision, adversarial examples are frequently used to punish model oversensitivity, where semantic-preserving perturbations (usually in the form of small noise vectors) are added to an image to fool the classifier into giving it a different label (Szegedy et al., 2014; Goodfellow et al., 2015) .\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 41, "token_end": 72, "char_start": 224, "char_end": 333, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Szegedy et al., 2014;": "604334", "Goodfellow et al., 2015)": "6706414"}}}]}
{"id": "2248785_5", "paragraph": "[BOS] X and Y denote the source word and a related term candidate, respectively.\n[BOS] P (X) and P (Y ) denote probabilities of X and Y , respectively.\n[BOS] P (X, Y ) denotes the joint probability of X and Y .\n[BOS] To estimate the above three probabilities, we followed the method proposed by Turney (2001).\n[BOS] We used the Yahoo!JAPAN 6 search engine and replaced P (A) in Equation (3) with the number of pages retrieved by the query A.\n[BOS] Here, \"A\" can be \"X\", \"Y \", or \"X and Y \".\n[BOS] Then, we selected up to 10 Y s with the greatest I(X, Y ) and translated them into Chinese using the Yahoo!JAPAN machine translation system.\n[BOS] Table 1 shows examples of related terms for the source word \" (mass)\", such as \" (ceremony)\" and \" (dedication)\".\n[BOS] Irrelevant candidates, such as \" (meeting)\" and \" (thing)\", were discarded successfully.\n\n", "discourse_tags": ["Other", "Other", "Other", "Reflection", "Reflection", "Other", "Reflection", "Other", "Other"], "span_citation_mapping": [{"token_start": 53, "token_end": 72, "char_start": 217, "char_end": 309, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "2248785_4", "paragraph": "[BOS] We used mutual information (Turney, 2001 ) to measure the degree of relation between the source word and a related term candidate by Equation (3).\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": [{"token_start": 4, "token_end": 11, "char_start": 14, "char_end": 46, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Turney, 2001": "5509836"}}}]}
{"id": "2248785_3", "paragraph": "[BOS] 3.\n[BOS] We extracted nouns and adjectives as related term candidates.\n\n", "discourse_tags": ["Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "2248785_2", "paragraph": "[BOS] 2.\n[BOS] We deleted HTML tags from the result page and performed morphological analysis by ChaSen 5 .\n\n", "discourse_tags": ["Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "2248785_1", "paragraph": "[BOS] 1.\n[BOS] We consulted the Japanese Wikipedia for the source word and obtained the result page.\n\n", "discourse_tags": ["Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "2248785_0", "paragraph": "[BOS] To extract related terms for a source word, we used Wikipedia 4 , which is a free encyclopedia on the Web and includes general words, persons, places, companies, and products, as headwords.\n[BOS] We extracted related term candidates for a source word as follows.\n\n", "discourse_tags": ["Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "203690908_4", "paragraph": "[BOS] Other works explore abstractive sentence compression with paraphrasing (Nayeem et al., 2019), different network training regimes (Ayana et al., 2016) or architectures that jointly learn summarization and semantic parsing (Fan et al., 2018) .\n[BOS] The authors in (Guo et al., 2018 ) propose a multitask model with parallel training of three tasks: summary generation, question generation, and entailment generation and find it provides useful guidance for summarization.\n[BOS] While we share their motivation to make the model input richer, our work presents a much simpler approach.\n[BOS] Another recent attempt to produce rich pre-trained encoder representations for many downstream tasks, including summarization, is BERT (Dev (Lin et al., 2018) ).\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Reflection", "Narrative_cite"], "span_citation_mapping": [{"token_start": 5, "token_end": 24, "char_start": 26, "char_end": 98, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 26, "token_end": 38, "char_start": 110, "char_end": 155, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ayana et al., 2016)": "18318429"}}}, {"token_start": 41, "token_end": 56, "char_start": 178, "char_end": 245, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Fan et al., 2018)": "53113851"}}}, {"token_start": 58, "token_end": 101, "char_start": 254, "char_end": 476, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Guo et al., 2018": "44105751"}, "Reference": {}}}, {"token_start": 123, "token_end": 155, "char_start": 596, "char_end": 754, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lin et al., 2018)": "13707541"}}}]}
{"id": "203690908_3", "paragraph": "[BOS] In (Lin et al., 2018) the authors use a convolutional gated unit to help control the information flow between the encoder and decoder networks aiming to filter the secondary and preserve only the core information, while Zhou et al. (2017) de- sign a selective gate network with the same goal.\n[BOS] In order to avoid generating fake facts in a summary, Cao et al. (2018b) extract actual factual descriptions from the source text leveraging information retrieval techniques.\n[BOS] A task-agnostic diverse beam search procedure is proposed in (Vijayakumar et al., 2018) that modifies the standard beam search algorithm in the direction of more diverse text generation.\n\n", "discourse_tags": ["Multi_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 3, "token_end": 41, "char_start": 9, "char_end": 218, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Lin et al., 2018)": "13707541"}, "Reference": {}}}, {"token_start": 43, "token_end": 62, "char_start": 226, "char_end": 298, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhou et al. (2017)": "1770102"}, "Reference": {}}}, {"token_start": 75, "token_end": 98, "char_start": 359, "char_end": 479, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Cao et al. (2018b)": "19198109"}, "Reference": {}}}, {"token_start": 99, "token_end": 139, "char_start": 486, "char_end": 672, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Vijayakumar et al., 2018)": "19224034"}, "Reference": {}}}]}
{"id": "203690908_2", "paragraph": "[BOS] However, problems like repetitive, generic, or ungrammatical summary generation, with limited abstraction and easily fooled by irrelevant information remained intact for the standard neural network summarizers.\n[BOS] Several extensions to their basic encoder-decoder architecture or their end-toend learning strategy developed accordingly.\n\n", "discourse_tags": ["Transition", "Transition"], "span_citation_mapping": []}
{"id": "203690908_1", "paragraph": "[BOS] The need for more human-like, abstractive summary creation led to the modern sequence-tosequence models with attention.\n[BOS] These neural networks are able to generate any word from their vocabulary, even novel words and phrases unseen in the source document, but can also copy from it when generating an out of vocabulary word is called for.\n\n", "discourse_tags": ["Transition", "Transition"], "span_citation_mapping": []}
{"id": "203690908_0", "paragraph": "[BOS] Early approaches to text summarization were based in first finding and then reordering (reranking) the most important sentences in a document based on their word frequency or some sentence-similarity metric.\n[BOS] Then, a simple extraction of the top k highest scoring sentences from the source document could produce a grammatical correct, albeit incoherent, summary.\n\n", "discourse_tags": ["Transition", "Transition"], "span_citation_mapping": []}
{"id": "52180771_2", "paragraph": "[BOS] In our paper, we exploit two types of plausible decoding functions, including linear projection and bijective functions with neural networks (Dinh et al., 2014) , and with proper design, the inverse of each of the decoding functions can be derived without expensive calculation after learning.\n[BOS] Thus, the decoder function can be utilised along with the encoder for building sentence representations.\n[BOS] We show that the ensemble of the encoder and the inverse of the decoder outperforms each of them.\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 19, "token_end": 34, "char_start": 106, "char_end": 166, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dinh et al., 2014)": "13995862"}}}]}
{"id": "52180771_1", "paragraph": "[BOS] Our approach is to invert the decoding function in order to use it as another encoder to assist the original encoder.\n[BOS] In order to make computation of the inverse function well-posed and tractable, careful design of the decoder is needed.\n[BOS] A simple instance of an invertible decoder is a linear projection with an orthonormal square matrix, whose transpose is its inverse.\n[BOS] A family of bijective transformations with non-linear functions (Dinh et al., 2014; Rezende and Mohamed, 2015; Kingma et al., 2016) can also be considered as it empowers the decoder to learn a complex data distribution.\n\n", "discourse_tags": ["Reflection", "Reflection", "Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 82, "token_end": 112, "char_start": 438, "char_end": 526, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dinh et al., 2014;": "13995862", "Rezende and Mohamed, 2015;": "12554042", "Kingma et al., 2016)": "11514441"}}}]}
{"id": "52180771_0", "paragraph": "[BOS] Learning vector representations for words with a word embedding matrix as the encoder and a context word embedding matrix as the decoder (Mikolov et al., 2013a; Lebret and Collobert, 2014; Pennington et al., 2014; can be considered as a word-level example of our approach, as the models learn to predict the surrounding words in the context given the current word, and the context word embeddings can also be utilised to augment the word embeddings (Pennington et al., 2014; Levy et al., 2015) .\n[BOS] We are thus motivated to explore the use of sentence decoders after learning instead of ignoring them as most sentence encoder-decoder models do.\n\n", "discourse_tags": ["Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 17, "token_end": 50, "char_start": 98, "char_end": 218, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mikolov et al., 2013a;": "5959482", "Lebret and Collobert, 2014;": null}}}, {"token_start": 93, "token_end": 111, "char_start": 439, "char_end": 499, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Pennington et al., 2014;": "1957433", "Levy et al., 2015)": "5159281"}}}]}
{"id": "52100401_2", "paragraph": "[BOS] Using adversarial examples of word and character level embeddings for natural language text generation has been explored in (Rajeswar et al., 2017) .\n[BOS] Models trained using generative adversarial networks or variational autoencoders have been shown to learn representations of continuous structures by leveraging deep latent variables such as text embeddings (Zhao et al., 2017) .\n[BOS] This work explores injecting sentence embeddings produced using the Skip Thought architecture into GANs with different setups.\n\n", "discourse_tags": ["Single_summ", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 31, "char_start": 6, "char_end": 153, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Rajeswar et al., 2017)": "397556"}, "Reference": {}}}, {"token_start": 57, "token_end": 72, "char_start": 323, "char_end": 388, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "52100401_1", "paragraph": "[BOS] Supervised learning with deep neural networks in the framework of encoder-decoder models has become the state-of-the-art methods for approaching NLP problems (Young et al., 2017) .\n[BOS] Recent text generation models use a wide variety of GANs such as gradient policy based sequence generation framework (Yu et al., 2016) and an actor-critic conditional GAN to fill missing text conditioned on surrounding text (Fedus et al., 2018) for performing natural language generation tasks.\n[BOS] Other architectures such as those proposed in with RNN and variational autoencoder generator with CNN discriminator and in arXiv:1808.08703v2 [cs.CL] 13 Nov 2018 (Guo et al., 2017) with leaky discriminator to guide generator through high-level extracted features have also shown great results.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 29, "token_end": 40, "char_start": 151, "char_end": 184, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Young et al., 2017)": null}}}, {"token_start": 55, "token_end": 69, "char_start": 258, "char_end": 327, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yu et al., 2016)": null}}}, {"token_start": 70, "token_end": 93, "char_start": 332, "char_end": 437, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 112, "token_end": 149, "char_start": 553, "char_end": 674, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Guo et al., 2017)": "3389583"}}}]}
{"id": "52100401_0", "paragraph": "[BOS] Deep neural network architectures have demonstrated strong results on natural language generation tasks (Xie, 2017) .\n[BOS] Recurrent neural networks using combinations of shared parameter matrices across time-steps (Sutskever et al., 2014; Mikolov et al., 2010; Cho et al., 2014) with different gating mechanisms for easing optimization (Hochreiter & Schmidhuber, 1997; Cho et al., 2014) have found some success in modeling natural language.\n[BOS] Another approach is to use convolutional neural networks that reuse kernels across time-steps with attention mechanism to perform language generation tasks (Kalchbrenner et al., 2016; .\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 11, "token_end": 20, "char_start": 76, "char_end": 121, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Xie, 2017)": "28473248"}}}, {"token_start": 22, "token_end": 61, "char_start": 130, "char_end": 286, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sutskever et al., 2014;": "7961699", "Mikolov et al., 2010;": "17048224"}}}, {"token_start": 62, "token_end": 88, "char_start": 292, "char_end": 394, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hochreiter & Schmidhuber, 1997;": "1915014"}}}, {"token_start": 103, "token_end": 132, "char_start": 482, "char_end": 637, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "52011877_2", "paragraph": "[BOS] For a good exposition on the field of argument mining refer to (Lippi and Torroni, 2016) .\n[BOS] Some notable works include (Palau and Moens, 2009 ) who first suggested the argument mining task, Rinott et al., 2015) who focused on mining claims/evidence in the context of a user given controversial topic and several works related to specific text genres such as student essays (Stab and Gurevych, 2014) , legal documents (Wyner et al., 2010; Moens et al., 2007; Grabmair et al., 2015) , user comments on proposed regulations (Park and Cardie, 2014) and newspaper articles (Feng and Hirst, 2011) .\n\n", "discourse_tags": ["Narrative_cite", "Multi_summ"], "span_citation_mapping": [{"token_start": 10, "token_end": 23, "char_start": 44, "char_end": 94, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lippi and Torroni, 2016)": "9561587"}}}, {"token_start": 29, "token_end": 45, "char_start": 130, "char_end": 199, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Palau and Moens, 2009": null}, "Reference": {}}}, {"token_start": 46, "token_end": 70, "char_start": 201, "char_end": 310, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Rinott et al., 2015)": "1804771"}, "Reference": {}}}, {"token_start": 81, "token_end": 94, "char_start": 369, "char_end": 409, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Stab and Gurevych, 2014)": "71907"}}}, {"token_start": 95, "token_end": 123, "char_start": 412, "char_end": 491, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wyner et al., 2010;": null, "Moens et al., 2007;": null, "Grabmair et al., 2015)": "12998931"}}}, {"token_start": 124, "token_end": 137, "char_start": 494, "char_end": 555, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Park and Cardie, 2014)": "14764893"}}}, {"token_start": 138, "token_end": 149, "char_start": 560, "char_end": 601, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Feng and Hirst, 2011)": "1805147"}}}]}
{"id": "52011877_1", "paragraph": "[BOS] Several works used DNN to tackle a variety of computational argumentation tasks, such as argument mining (Eger et al., 2017) , predicting argument convincingness (Habernal and Gurevych, 2016) , detecting context dependent claims and evidence (Laha and Raykar, 2016) and attack and support relations between arguments (Cocarascu and Toni, 2017) .\n[BOS] However, these works used the fully-supervised learning paradigm, which is inherently demanding, especially in the context of argument mining where obtaining labeled data is notoriously difficult .\n[BOS] In addition, Al-Khatib et al. (2016) used a distant supervision approach trained over debate portals' data, to develop a classifier for argumentative texts stored in these portals.\n[BOS] To the best of our knowledge, the present work is the first to demonstrate the value of DNN trained solely with weak supervision (Hearst, 1992) in this challenging field.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 18, "token_end": 29, "char_start": 95, "char_end": 130, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Eger et al., 2017)": "3221856"}}}, {"token_start": 30, "token_end": 45, "char_start": 133, "char_end": 197, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Habernal and Gurevych, 2016)": "3083231"}}}, {"token_start": 46, "token_end": 61, "char_start": 200, "char_end": 271, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Laha and Raykar, 2016)": "8507616"}}}, {"token_start": 62, "token_end": 78, "char_start": 276, "char_end": 349, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cocarascu and Toni, 2017)": "28193257"}}}, {"token_start": 118, "token_end": 156, "char_start": 575, "char_end": 742, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Al-Khatib et al. (2016)": "16416820"}, "Reference": {}}}, {"token_start": 171, "token_end": 188, "char_start": 812, "char_end": 892, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hearst, 1992)": "15763200"}}}]}
{"id": "52011877_0", "paragraph": "[BOS] Recently, Wachsmuth et al. (2017) suggested an argument search framework and a corresponding search engine prototype.\n[BOS] However, the proposed system relies on arguments crawled from dedicated resources that suggest pre-written arguments for various topics, and hence, is only relevant for topics covered in these resources, and cannot be used directly over unstructured textual data.\n[BOS] Stab et al. (2018) tackled the argument mining task in heterogeneous texts retrieved by Google search when queried with a controversial topic.\n[BOS] They show that it is feasible to annotate the retrieved documents via crowd-sourcing and to use these labels in order to build a supervised learning system that finds arguments in the given documents.\n[BOS] Similar to our work, sentences are treated in isolation (ignoring the document context).\n[BOS] The only work we are aware of that tackles corpus wide claim detection, is the work by (Levy et al., 2017) .\n[BOS] Here, we demonstrate how this work can be leveraged to define weak signals for training DNNs to obtain significantly greater performance.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 4, "token_end": 73, "char_start": 16, "char_end": 393, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wachsmuth et al. (2017)": "19259545"}, "Reference": {}}}, {"token_start": 74, "token_end": 142, "char_start": 400, "char_end": 749, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Stab et al. (2018)": "3339724"}, "Reference": {}}}, {"token_start": 171, "token_end": 188, "char_start": 894, "char_end": 957, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Levy et al., 2017)": "12346560"}}}]}
{"id": "52136770_4", "paragraph": "[BOS] State tracking also appears in other areas of AI, such as dialog.\n[BOS] A typical dialog state tracking task (e.g., the DSTC competitions) involves gradually uncovering the user's state (e.g., their constraints, preferences, and goals for booking a restaurant), until an answer can be provided.\n[BOS] Although this context is somewhat different (the primary goal being state discovery from weak dialog evidence), state tracking techniques originally designed for procedural text have been successfully applied in this context also (Liu and Perez, 2017) .\n[BOS] Finally, our model learns to search over the best candidate structures using hard constraints and soft KB priors.\n[BOS] Previous work in Neural Machine Translation (NMT) has used sets of example-specific lexical constraints in beam search decoding to only produce translations that satisfy every constraint in the set (Hokamp and Liu, 2017) .\n[BOS] In contrast, our work uses a set of global example-free constraints to prune the set of possible paths the search algorithm can explore.\n[BOS] Simultaneously, a recent body of work has explored encoding soft constraints as an additional loss term in the training objective for dialogue (Wen et al., 2015) , machine translation (Tu et al., 2016) , and recipe generation (Kiddon et al., 2016) .\n[BOS] Our work instead uses soft constraints to re-rank candidate structures and is not directly encoded in the loss function.\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite", "Reflection", "Narrative_cite", "Reflection", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 105, "token_end": 116, "char_start": 519, "char_end": 558, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Liu and Perez, 2017)": "2325035"}}}, {"token_start": 164, "token_end": 182, "char_start": 823, "char_end": 907, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hokamp and Liu, 2017)": "15281983"}}}, {"token_start": 235, "token_end": 244, "char_start": 1193, "char_end": 1220, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wen et al., 2015)": "739696"}}}, {"token_start": 245, "token_end": 255, "char_start": 1223, "char_end": 1260, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Tu et al., 2016)": "16113848"}}}, {"token_start": 257, "token_end": 269, "char_start": 1267, "char_end": 1306, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kiddon et al., 2016)": "9818013"}}}]}
{"id": "52136770_3", "paragraph": "[BOS] Neural methods for structure prediction have been used extensively in other areas of NLP, and we leverage these methods here.\n[BOS] In particular we use a neural encoder-decoder architecture with beam search decoding, representative of several current state-of-the-art systems (Bahdanau et al., 2014; Wiseman and Rush, 2016; Vinyals et al., 2015) .\n[BOS] As our model's only supervision signal comes from the final prediction (of state changes), our work is similar to previous work in semantic parsing that extracts structured outputs from text with no intermediate supervision (Krishnamurthy et al., 2017) .\n\n", "discourse_tags": ["Reflection", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 41, "token_end": 80, "char_start": 224, "char_end": 352, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bahdanau et al., 2014;": "11212020", "Wiseman and Rush, 2016;": "2783746", "Vinyals et al., 2015)": "14223"}}}, {"token_start": 112, "token_end": 133, "char_start": 514, "char_end": 613, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Krishnamurthy et al., 2017)": "1675452"}}}]}
{"id": "52136770_2", "paragraph": "[BOS] Our work here can viewed as incorporating these approaches within the neural paradigm.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "52136770_1", "paragraph": "[BOS] Prior to these neural approaches, some earlier systems for process comprehension did make use of world knowledge, and motivated this work.\n[BOS] Like us, the system ProRead (Berant et al., 2014; Scaria et al., 2013 ) also treated process comprehension as structure prediction, using an Integer Linear Programming (ILP) formalism to enforce global constraints (e.g., if the result of event1 is the agent of event2, then event1 must enable event2).\n[BOS] Similarly, Kiddon et al. (2015) used corpus-based priors to guide extraction of an \"action graph\" from recipes.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 31, "token_end": 101, "char_start": 164, "char_end": 452, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Berant et al., 2014;": "8471750", "Scaria et al., 2013": "13856836"}, "Reference": {}}}, {"token_start": 104, "token_end": 130, "char_start": 470, "char_end": 570, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kiddon et al. (2015)": "6736944"}, "Reference": {}}}]}
{"id": "52136770_0", "paragraph": "[BOS] Our work builds off a recent body of work that focuses on using neural networks to explicitly track the states of entities while reading long texts.\n[BOS] These works have focused on answering simple commonsense questions (Henaff et al., 2017) , tracking entity states in scientific processes , tracking ingredients in cooking recipes , and tracking the emotional reactions and motivations of characters in simple stories (Rashkin et al., 2018) .\n[BOS] Our work extends these methods and addresses their most common issues by using background knowledge about entities to prune the set of state changes they can experience as the model reads new text.\n\n", "discourse_tags": ["Reflection", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 36, "token_end": 49, "char_start": 199, "char_end": 249, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Henaff et al., 2017)": "11243593"}}}, {"token_start": 70, "token_end": 85, "char_start": 384, "char_end": 450, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rashkin et al., 2018)": "21689288"}}}]}
{"id": "52053467_6", "paragraph": "[BOS] On the other side, Grammar Error Correction has been extensively studied, with significant incremental advances made recently by treating GEC as an MT task: among others, Junczys-Dowmunt and Grundkiewicz (2016) used phrased-based MT, Ji et al. (2017) used hybrid character-word neural sequence-to-sequence systems, Sakaguchi et al. (2017b) used reinforcement learning, and combined several techniques with NMT to achieve the current state-of-the-art.\n[BOS] Synthetic errors for training GEC systems have also been studied and applied with mixed success (Rozovskaya and Roth, 2010; Rozovskaya et al., 2014; Xie et al., 2016) , while more recently Xie et al. (2018) used backtranslation techniques for adding synthetic noise useful for GEC.\n\n", "discourse_tags": ["Multi_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 33, "token_end": 54, "char_start": 177, "char_end": 238, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Junczys-Dowmunt and Grundkiewicz (2016)": "6820419"}, "Reference": {}}}, {"token_start": 55, "token_end": 74, "char_start": 240, "char_end": 319, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ji et al. (2017)": "235645"}, "Reference": {}}}, {"token_start": 75, "token_end": 107, "char_start": 321, "char_end": 456, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Sakaguchi et al. (2017b)": "7833469"}, "Reference": {}}}, {"token_start": 108, "token_end": 153, "char_start": 463, "char_end": 629, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rozovskaya and Roth, 2010;": "15175549", "Rozovskaya et al., 2014;": "1547333", "Xie et al., 2016)": "8880428"}}}, {"token_start": 157, "token_end": 178, "char_start": 652, "char_end": 744, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Xie et al. (2018)": "21730715"}, "Reference": {}}}]}
{"id": "52053467_5", "paragraph": "[BOS] 1.\n[BOS] Belinkov and Bisk (2018) and Sperber et al. (2017) train their NMT systems on fairly small datasets: 235K (Fr-En), 210K (De-En), 122K (Cz-En), and 138K sentences (Es-En) respectively.\n[BOS] Even though they use systems like Nematus (Sennrich et al., 2017) or XNMT (Neubig et al., 2018) which generally achieve nearly SOTA results, it is unclear whether their results generalize to larger training data.\n[BOS] In contrast, we train our system on almost 2M sentences.\n[BOS] 2.\n[BOS] All three systems introduce somewhat unrealistic amounts of noise in the data.\n[BOS] The natural noise of Belinkov and Bisk (2018) consists of word substitutions based on Wikipedia errors or corrected essays (in the Czech case) but they substitute all possible correct words with their erroneous version, We suspect that such a solution would indeed be appropriate for dealing with typos and other character-level noise, but not for more general grammatical noise.\n[BOS] Our method could potentially be combined with GloVe (Pennington et al., 2014) or fastText (Bojanowski et al., 2017) embeddings that can deal with slight spelling variations, but we leave this for future work.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Multi_summ", "Reflection", "Reflection", "Transition", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 5, "token_end": 120, "char_start": 15, "char_end": 417, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Belinkov and Bisk (2018)": "3513372", "Sperber et al. (2017)": "21657379"}, "Reference": {"(Sennrich et al., 2017)": "905565", "(Neubig et al., 2018)": "3628568"}}}, {"token_start": 153, "token_end": 194, "char_start": 585, "char_end": 799, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Belinkov and Bisk (2018)": "3513372"}, "Reference": {}}}, {"token_start": 233, "token_end": 244, "char_start": 1013, "char_end": 1044, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Pennington et al., 2014)": "1957433"}}}, {"token_start": 245, "token_end": 257, "char_start": 1048, "char_end": 1082, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bojanowski et al., 2017)": "207556454"}}}]}
{"id": "52053467_4", "paragraph": "[BOS] We consider our work to be complementary to the works of Heigold et al. (2017) ; Belinkov and Bisk (2018) , and Sperber et al. (2017) .\n[BOS] However, there are several important differences:\n\n", "discourse_tags": ["Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 13, "token_end": 22, "char_start": 63, "char_end": 84, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Heigold et al. (2017)": null}}}, {"token_start": 23, "token_end": 32, "char_start": 87, "char_end": 111, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Belinkov and Bisk (2018)": "3513372"}}}, {"token_start": 34, "token_end": 43, "char_start": 118, "char_end": 139, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "52053467_3", "paragraph": "[BOS] We present a summary of relevant previous work in Table 9 .\n[BOS] Synthetic errors refer to noise introduced according an artificially created distribu-tion, and natural errors refer to actual errorful text produced by humans.\n[BOS] As for semi-natural, it refers to either noise introduced according to a distribution learned from data (as in our work), or to errors that are learned from data but introduced according to an artificial distribution (as is part of the work of Belinkov and Bisk (2018) ).\n\n", "discourse_tags": ["Reflection", "Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 72, "token_end": 102, "char_start": 367, "char_end": 507, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Belinkov and Bisk (2018)": "3513372"}}}]}
{"id": "52053467_2", "paragraph": "[BOS] The notion of linguistically plausible corruption is also explored by Li et al. (2017) , who created adversarial examples with syntactic and semantic noise (reordering and word substitutions respectively).\n[BOS] When training with these noisy datasets, they obtained better performance on several text classification tasks.\n[BOS] Furthermore, in accordance with our results, their best system is the one that combines different types of noise.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 5, "token_end": 80, "char_start": 20, "char_end": 449, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Li et al. (2017)": "17730607"}, "Reference": {}}}]}
{"id": "52053467_1", "paragraph": "[BOS] In addition, Heigold et al. (2017) evaluated the robustness of word embeddings against word scrambling noise, and showed that performance in downstream tasks like POS-tagging and MT is especially hurt.\n[BOS] Sakaguchi et al. (2017a) studied word scrambling and the Cmabrigde Uinervtisy (Cambridge University) effect, where humans are able to understand the meaning of sentences with scrambled words, performing word recognition (word level spelling correction) with a semi-character RNN system.\n[BOS] Focusing only on character-level NMT models, Belinkov and Bisk (2018) showed that they ex- .\n[BOS] In line with our findings, they also showed that slightly better performance can be achieved by training on data artificially induced with the same kind of noise as the test set.\n[BOS] Sperber et al. (2017) proposed a noiseintroduction system reminiscent of WER, based on insertions, deletions, and substitutions.\n[BOS] An NMT system tested on correct transcriptions achieves a BLEU score of 55 (4 references), but tested on the ASR transcriptions it only achieves a BLEU score of 35.7.\n[BOS] By introducing similar noise in the training data, they were able to make the NMT system slightly more robust.\n[BOS] Interestingly, they found that the optimal amount of noise on the training data is smaller than the amount of noise on the test data.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 5, "token_end": 45, "char_start": 19, "char_end": 207, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Heigold et al. (2017)": null}, "Reference": {}}}, {"token_start": 46, "token_end": 112, "char_start": 214, "char_end": 500, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Sakaguchi et al. (2017a)": "11217839"}, "Reference": {}}}, {"token_start": 116, "token_end": 172, "char_start": 524, "char_end": 784, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Belinkov and Bisk (2018)": "3513372"}, "Reference": {}}}, {"token_start": 173, "token_end": 293, "char_start": 791, "char_end": 1349, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Sperber et al. (2017)": "21657379"}, "Reference": {}}}]}
{"id": "52053467_0", "paragraph": "[BOS] The effect of noise in NMT was recently studied by Khayrallah and Koehn (2018) , who explored noisy situations during training due to webcrawled data.\n[BOS] This type of noise includes misaligned, mistranslated, or untranslated sentences which, when used during training, significantly degrades the performance of NMT.\n[BOS] Unlike our work, they primarily focus on a setting where the training set is noisy but the test set is clean.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 3, "token_end": 97, "char_start": 10, "char_end": 440, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Khayrallah and Koehn (2018)": "44090489"}, "Reference": {}}}]}
{"id": "52817936_0", "paragraph": "[BOS] There is a growing body of research that uses the MSRPC to build models of paraphrase.\n[BOS] As noted, the most successful work has used edit distance (Zhang and Patrick, 2005) or bag-of-words features to measure sentence similarity, along with shallow syntactic features (Finch et al., 2005; Wan et al., 2006; Corley and Mihalcea, 2005) .\n[BOS] Qiu et al. (2006) used predicate-argument annotations.\n[BOS] Most related to our approach, Wu (2005) used inversion transduction grammars-a synchronous context-free formalism (Wu, 1997) -for this task.\n[BOS] Wu reported only positive-class (p) precision (not accuracy) on the test set.\n[BOS] He obtained 76.1%, while our PoE model achieves 79.6% on that measure.\n[BOS] Wu's model can be understood as a strict hierarchical maximum-alignment method.\n[BOS] In contrast, our alignments are soft (we sum over them), and we do not require strictly isomorphic syntactic structures.\n[BOS] Most importantly, our approach is founded on a stochastic generating process and estimated discriminatively for this task, while Wu did not estimate any parameters from data at all.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 33, "token_end": 42, "char_start": 143, "char_end": 182, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhang and Patrick, 2005)": "10351213"}}}, {"token_start": 56, "token_end": 85, "char_start": 251, "char_end": 343, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Finch et al., 2005;": "7314969", "Wan et al., 2006;": "12813043", "Corley and Mihalcea, 2005)": "4948322"}}}, {"token_start": 87, "token_end": 101, "char_start": 352, "char_end": 406, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Qiu et al. (2006)": "1028939"}, "Reference": {}}}, {"token_start": 108, "token_end": 192, "char_start": 443, "char_end": 800, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wu (2005)": "4955511"}, "Reference": {"(Wu, 1997)": "912349"}}}]}
{"id": "52164624_0", "paragraph": "[BOS] Our system is inspired by previous work on multitask learning and multi-lingual learning, mainly building on two intuitions: (1) jointly learning related tasks tends to be beneficial (Caruana, 1997; Bjerva et al., 2016; Bjerva, 2017b) ; and (2) jointly learning related languages in an MTL-inspired framework tends to be beneficial (Bjerva, 2017a; Johnson et al., 2017; de Lhoneux et al., 2018) .\n[BOS] In the context of computational morphology, multilingual approaches have previously been employed for morphological reinflection (Bergmanis et al., 2017) and for paradigm completion .\n[BOS] In both of these cases, however, the available datasets covered more languages, 40 and 21, respectively, which allowed for linguistically-motivated language groupings and for parameter sharing directly on the level of characters.\n[BOS] De Lhoneux et al. (2018) explore parameter sharing between related languages for dependency parsing, and find that sharing is more beneficial in the case of closely related languages.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Transition", "Single_summ"], "span_citation_mapping": [{"token_start": 30, "token_end": 61, "char_start": 135, "char_end": 240, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Caruana, 1997;": "207752942", "Bjerva et al., 2016;": "11690925", "Bjerva, 2017b)": "31415243"}}}, {"token_start": 66, "token_end": 106, "char_start": 251, "char_end": 400, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bjerva, 2017a;": null, "Johnson et al., 2017;": "6053988", "de Lhoneux et al., 2018)": "52111211"}}}, {"token_start": 123, "token_end": 137, "char_start": 511, "char_end": 562, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bergmanis et al., 2017)": "6826327"}}}, {"token_start": 186, "token_end": 221, "char_start": 835, "char_end": 1018, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Lhoneux et al. (2018)": "52111211"}, "Reference": {}}}]}
{"id": "52115592_0", "paragraph": "[BOS] N -ary relation extraction N -ary relation extractions can be traced back to MUC-7 (Chinchor,\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 20, "token_end": 27, "char_start": 83, "char_end": 98, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "52186890_3", "paragraph": "[BOS] Unsupervised Domain Transfer: Generally speaking, learning the cross-lingual transfer of word embedding can be viewed as a domain transfer problem, where the domains are word sets in different languages.\n[BOS] Thus various work in the field of unsupervised domain adaptation or unsupervised transfer learning can shed light on our problem.\n[BOS] For example, He et al. (2016) proposed a semi-supervised method for machine translation to utilize large monolingual corpora.\n[BOS] Shen et al. (2017) used unsupervised learning to transfer sentences of different sentiments.\n[BOS] Recent work in computer vision addresses the problem of image style transfer without any annotated training data (Zhu et al., 2017; Taigman et al., 2016; Yi et al., 2017) .\n[BOS] Among those, our work is mostly inspired by the work on CycleGAN (Zhu et al., 2017) , and we adopt their cycled consistent loss over images into our back-translation loss.\n[BOS] One key difference of our method from CycleGAN is that they used the training loss of an adversarial classifier as an indicator of the distributional distance, but instead, we introduce the Sinkhorn distance in our objective function and demonstrate its superiority over the representative method using adversarial loss (Zhang et al., 2017a) .\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ", "Single_summ", "Narrative_cite", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 64, "token_end": 87, "char_start": 365, "char_end": 477, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"He et al. (2016)": "5758868"}, "Reference": {}}}, {"token_start": 88, "token_end": 106, "char_start": 484, "char_end": 576, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Shen et al. (2017)": "7296803"}, "Reference": {}}}, {"token_start": 116, "token_end": 148, "char_start": 639, "char_end": 753, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhu et al., 2017;": "206770979", "Taigman et al., 2016;": "10756563", "Yi et al., 2017)": "1082740"}}}, {"token_start": 162, "token_end": 172, "char_start": 818, "char_end": 845, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhu et al., 2017)": "206770979"}}}, {"token_start": 242, "token_end": 254, "char_start": 1243, "char_end": 1281, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhang et al., 2017a)": "26873455"}}}]}
{"id": "52186890_2", "paragraph": "[BOS] Unsupervised Methods have been studied to establish cross-lingual mapping without any humanannotated supervision.\n[BOS] Earlier work simply relied on word occurrence information only (Rapp, 1995; Fung, 1996) while later efforts have considered more sophisticated statistics in addition (Haghighi et al., 2008) .\n[BOS] The main difficulty in unsupervised learning of cross-lingual mapping is the formulation of the objective function, i.e., how to measure the goodness of an induced mapping without any supervision is a non-trivial question.\n[BOS] Cao et al. (2016) tried to match the mean and standard deviation of the embedded word vectors in two different languages after mapping the words in the source language to the target language.\n[BOS] However, such an approach has shown to be sub-optimal because the objective function only carries the first and second order statistics of the mapping.\n[BOS] Artetxe et al. (2017) tried to impose an orthogonal constraint to their linear transformation model and minimize the distance between the transferred source-word embedding and its nearest neighbor in the target embedding space.\n[BOS] Their method, however, requires a seed bilingual dictionary as the labeled training data and hence is not fully unsupervised.\n[BOS] (Zhang et al., 2017a; Barone, 2016 ) adapted a generative adversarial network (GAN) to make the transferred embedding of each source-language word indistinguishable from its true translation in the target embedding space (Goodfellow et al., 2014) .\n[BOS] The adversarial model could be optimized in a purely unsupervised manner but is often suffered from unstable training, i.e. the adversarial learning does not always improve the performance over simpler baselines.\n[BOS] Zhang et al. (2017b) , Conneau et al. (2017) and Artetxe et al. (2017) also tried adversarial approaches for the induction of seed bilingual dictionaries, as a sub-problem in the crosslingual transfer of word embedding.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Single_summ", "Transition", "Single_summ", "Single_summ", "Multi_summ", "Transition", "Multi_summ"], "span_citation_mapping": [{"token_start": 28, "token_end": 42, "char_start": 156, "char_end": 213, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rapp, 1995;": "7671180", "Fung, 1996)": null}}}, {"token_start": 48, "token_end": 62, "char_start": 255, "char_end": 315, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Haghighi et al., 2008)": "7185434"}}}, {"token_start": 109, "token_end": 146, "char_start": 553, "char_end": 744, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Cao et al. (2016)": null}, "Reference": {}}}, {"token_start": 176, "token_end": 240, "char_start": 909, "char_end": 1268, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Artetxe et al. (2017)": "13335042"}, "Reference": {}}}, {"token_start": 241, "token_end": 296, "char_start": 1275, "char_end": 1521, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Zhang et al., 2017a;": "26873455", "Barone, 2016": "1513472"}, "Reference": {"(Goodfellow et al., 2014)": "1033682"}}}, {"token_start": 338, "token_end": 395, "char_start": 1749, "char_end": 1968, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhang et al. (2017b)": "32923127", "Conneau et al. (2017)": "3470398", "Artetxe et al. (2017)": "13335042"}, "Reference": {}}}]}
{"id": "52186890_1", "paragraph": "[BOS] Supervised Methods: There is a rich body of supervised methods for learning cross-lingual transfer of word embeddings based on bilingual dictionaries (Mikolov et al., 2013; Faruqui and Dyer, 2014; Artetxe et al., 2016; Xing et al., 2015; Duong et al., 2016; Gouws and Sgaard, 2015) , sentence-aligned corpora (Koisk et al., 2014; Hermann and Blunsom, 2014; and document-aligned corpora (Vuli and Moens, 2016; Sgaard et al., 2015) .\n[BOS] The most relevant line of work is that by Mikolov et al. (2013) where they showed monolingual word embeddings are likely to share similar geometric properties across languages although they are trained separately and hence cross-lingual mapping can be captured by a linear transformation across embedding spaces.\n[BOS] Several follow-up studies tried to improve the cross-lingual transformation in various ways (Faruqui and Dyer, 2014; Artetxe et al., 2016; Xing et al., 2015; Duong et al., 2016; Ammar et al., 2016; Artetxe et al., 2016; Zhang et al., 2016; Shigeto et al., 2015) .\n[BOS] Nevertheless, all these methods require bilingual lexicons for supervised learning.\n[BOS] Vuli and Korhonen (2016) showed that 5000 high-quality bilingual lexicons are sufficient for learning a reasonable cross-lingual mapping.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Narrative_cite", "Transition", "Single_summ"], "span_citation_mapping": [{"token_start": 25, "token_end": 83, "char_start": 133, "char_end": 287, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mikolov et al., 2013;": "5959482", "Faruqui and Dyer, 2014;": "3792324", "Artetxe et al., 2016;": "1040556", "Xing et al., 2015;": "3144258", "Duong et al., 2016;": "13888952", "Gouws and S\u00f8gaard, 2015)": null}}}, {"token_start": 84, "token_end": 105, "char_start": 290, "char_end": 361, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ko\u010disk\u1ef3 et al., 2014;": "5809776"}}}, {"token_start": 107, "token_end": 129, "char_start": 367, "char_end": 435, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vuli\u0107 and Moens, 2016;": "16452496"}}}, {"token_start": 140, "token_end": 188, "char_start": 486, "char_end": 756, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Mikolov et al. (2013)": "5959482"}, "Reference": {}}}, {"token_start": 196, "token_end": 275, "char_start": 798, "char_end": 1024, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Faruqui and Dyer, 2014;": "3792324", "Artetxe et al., 2016;": "1040556", "Xing et al., 2015;": "3144258", "Duong et al., 2016;": "13888952", "Ammar et al., 2016;": "1227830", "Zhang et al., 2016;": "158652"}}}, {"token_start": 291, "token_end": 321, "char_start": 1123, "char_end": 1260, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Vuli\u0107 and Korhonen (2016)": "17515652"}, "Reference": {}}}]}
{"id": "52186890_0", "paragraph": "[BOS] We divide the related work into supervised and unsupervised categories.\n[BOS] Representative methods in both categories are included in our comparative evaluation (Section 3.4).\n[BOS] We also discuss some related work in unsupervised domain transfer in addition.\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "52009541_0", "paragraph": "[BOS] We describe related work from two perspectives.\n[BOS] Gate mechanism.\n[BOS] Our work is partially inspired by studies of gate mechanism for neural networks.\n[BOS] Following the success of LSTM (Hochreiter and Schmidhuber, 1997) and GRU (Cho et al., 2014a; Cho et al., 2014b) , the gate mechanism has become standard components in RNN.\n[BOS] Recently, Srivastava et al. (2015) employ gate units to regulate information flow, called highway networks.\n[BOS] The most relevant work to ours is , in which they propose context gate to control the ratios of the source context (i.e., c t ) and target context (i.e., y t1 and s t1 ) for computing next target state s t .\n[BOS] On the contrary, we use gate units to regulate information flow in computing the output state o t .\n[BOS] Moreover, we propose adaptive weighting for GRU through gate units.\n[BOS] Interpretation for neural networks.\n[BOS] Attention mechanism (Bahdanau et al., 2015; Lin et al., 2017; Vaswani et al., 2017 ) offers a way of understanding the contribution of every source words to the generation of a target word.\n[BOS] Ding et al. (2017) propose to use layer-wise relevance propagation (LRP) to interpret the internal workings of NMT and analyze translation errors.\n[BOS] Moreover, Karpathy et al. (2015) and propose to visualize and understand RNNs for natural language processing.\n[BOS] In this work, we use the proposed gates in both encoder and decoder to analyze what types of information encoded in the encoder and what types of information influences the generation of a target word.\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection", "Narrative_cite", "Single_summ", "Transition", "Reflection", "Reflection", "Reflection", "Narrative_cite", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 34, "token_end": 48, "char_start": 194, "char_end": 233, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hochreiter and Schmidhuber, 1997)": "1915014"}}}, {"token_start": 49, "token_end": 68, "char_start": 238, "char_end": 280, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cho et al., 2014a;": null, "Cho et al., 2014b)": "5590763"}}}, {"token_start": 83, "token_end": 105, "char_start": 357, "char_end": 454, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 204, "token_end": 231, "char_start": 897, "char_end": 979, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bahdanau et al., 2015;": "11212020", "Lin et al., 2017;": "15280949"}}}, {"token_start": 252, "token_end": 285, "char_start": 1093, "char_end": 1239, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ding et al. (2017)": "27930067"}, "Reference": {}}}, {"token_start": 288, "token_end": 310, "char_start": 1256, "char_end": 1356, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Karpathy et al. (2015)": "988348"}, "Reference": {}}}]}
{"id": "52113519_5", "paragraph": "[BOS] As one of our goals was to investigate whether easy questions are dominant in recent datasets, it did not necessarily require a detailed classification of reasoning types.\n[BOS] Nonetheless, we recognize there are more fine-grained classifications of required skills for NLU.\n[BOS] For example, Weston et al. (2015) defined 20 skills as a set of toy tasks.\n[BOS] Sugawara et al. (2017) also organized 10 prerequisite skills for MRC.\n[BOS] LoBue and Yates (2011) and Sammons et al. (2010) analyzed entailment phenomena using detailed classifications in RTE.\n[BOS] For the ARC dataset, Boratko et al. (2018) proposed knowledge and reasoning types.\n\n", "discourse_tags": ["Reflection", "Reflection", "Single_summ", "Single_summ", "Multi_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 55, "token_end": 73, "char_start": 301, "char_end": 362, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Weston et al. (2015)": "3178759"}, "Reference": {}}}, {"token_start": 74, "token_end": 91, "char_start": 369, "char_end": 438, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Sugawara et al. (2017)": "30866421"}, "Reference": {}}}, {"token_start": 92, "token_end": 120, "char_start": 445, "char_end": 562, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Sammons et al. (2010)": "1002552"}, "Reference": {}}}, {"token_start": 123, "token_end": 141, "char_start": 577, "char_end": 651, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Boratko et al. (2018)": "44133715"}, "Reference": {}}}]}
{"id": "52113519_4", "paragraph": "[BOS] The theory behind evaluating AI distinguishes between taskand skill-oriented approaches (Hernndez-Orallo, 2017) .\n[BOS] In the task-oriented approach, we usually develop a system and test it on a specific dataset.\n[BOS] Sometimes the developed system lacks generality but achieves the state of the art for that specific dataset.\n[BOS] Further, it becomes difficult to verify and explain the solution to tasks.\n[BOS] The situation in which we are biased to the specific tasks is called evaluation overfitting (Whiteson et al., 2011) .\n[BOS] By contrast, with the skill-oriented approach, we aim to interpret the relationships between tasks and skills.\n[BOS] This orientation can encourage the development of more realistic NLU systems.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Transition", "Narrative_cite", "Reflection", "Transition"], "span_citation_mapping": [{"token_start": 9, "token_end": 25, "char_start": 60, "char_end": 117, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hern\u00e1ndez-Orallo, 2017)": "207079473"}}}, {"token_start": 95, "token_end": 107, "char_start": 491, "char_end": 537, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Whiteson et al., 2011)": "6231557"}}}]}
{"id": "52113519_3", "paragraph": "[BOS] Evaluation overfitting:\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "52113519_2", "paragraph": "[BOS] Similarly, in MRC, Weissenborn et al. (2017) proposed context/type matching heuristic to develop a simple neural system.\n[BOS] Min et al. (2018) observed that 92% of answerable questions in SQuAD can be answered only using a single context sentence.\n[BOS] In visual question answering, Agrawal et al. (2016) analyzed the behavior of models with the variable length of the first question words.\n[BOS] More recently, Khashabi et al. (2018) proposed a dataset that has questions for multisentence reasoning.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 5, "token_end": 29, "char_start": 20, "char_end": 126, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Weissenborn et al. (2017)": "2592133"}, "Reference": {}}}, {"token_start": 30, "token_end": 58, "char_start": 133, "char_end": 255, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Min et al. (2018)": "29161506"}, "Reference": {}}}, {"token_start": 60, "token_end": 88, "char_start": 265, "char_end": 399, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Agrawal et al. (2016)": "12304778"}, "Reference": {}}}, {"token_start": 92, "token_end": 114, "char_start": 421, "char_end": 510, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Khashabi et al. (2018)": "5112038"}, "Reference": {}}}]}
{"id": "52113519_1", "paragraph": "[BOS] Unintended biases: The MRC task tests a reading process that involves retrieving stored information and performing inferences (Sutcliffe et al., 2013) .\n[BOS] However, it is difficult to construct datasets that comprehensively require those skills.\n[BOS] As Levesque (2014) discussed as a desideratum for testing AI, we should avoid creating questions that can be solved by matching patterns, using unintended biases, and selectional restrictions.\n[BOS] For the unintended biases, one suggestive example is the Story Cloze Test (Mostafazadeh et al., 2016) , in which a system chooses a sentence among candidates to conclude a given paragraph of the story.\n[BOS] A recent attempt at this task showed that recognizing superficial features in the correct candidate is critical to achieve the state of the art (Schwartz et al., 2017) .\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Multi_summ", "Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 16, "token_end": 32, "char_start": 76, "char_end": 156, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sutcliffe et al., 2013)": null}}}, {"token_start": 51, "token_end": 91, "char_start": 264, "char_end": 453, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Levesque (2014)": null}, "Reference": {}}}, {"token_start": 104, "token_end": 139, "char_start": 517, "char_end": 661, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Mostafazadeh et al., 2016)": null}, "Reference": {}}}, {"token_start": 148, "token_end": 172, "char_start": 710, "char_end": 835, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Schwartz et al., 2017)": "7962030"}}}]}
{"id": "52113519_0", "paragraph": "[BOS] Our heuristics and annotation are motivated by unintended biases (Levesque, 2014) and evaluation overfitting (Whiteson et al., 2011) , respectively.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": [{"token_start": 9, "token_end": 20, "char_start": 53, "char_end": 87, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Levesque, 2014)": null}}}, {"token_start": 21, "token_end": 33, "char_start": 92, "char_end": 138, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Whiteson et al., 2011)": "6231557"}}}]}
{"id": "52009569_4", "paragraph": "[BOS] Another research line explores a kind of end-to-end method for relation classification.\n[BOS] For example, Miwa et al. (2016) proposed a novel end-to-end neural model to extract entities and the relations between them.\n[BOS] Their model captures both word sequence and dependency tree substructure information by stacking bidirectional tree-structured LSTM-RNNs on bidirectional sequential LSTM-RNNs, which allows the model to jointly represent both entities and relations with shared parameters in a single model.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 23, "token_end": 103, "char_start": 113, "char_end": 520, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "52009569_3", "paragraph": "[BOS] Some researchers combine CNN and RNN together for relation classification.\n[BOS] Recently, the attention method is achieving more and more research attention.\n[BOS] Some researchers also add the attention method in their relation classification models.\n[BOS] For example, proposed a multi-level attention CNN model for relation classification.\n[BOS] In their method, two levels of attentions are used in order to better discern patterns in heterogeneous contexts.\n[BOS] Zhou et al. (2016) proposed an attention-based bidirectional LSTM model for relation classification.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 81, "token_end": 101, "char_start": 476, "char_end": 576, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "52009569_2", "paragraph": "[BOS] CNN is a very popular learning framework for relation classification and lots of methods are based on it.\n[BOS] For example, Zeng et al. (2014) proposed a CNN based approach for relation classification.\n[BOS] In their method, sentence level features are learned through a CNN model that takes word embedding features and position embedding features as input.\n[BOS] In parallel, lexical level features are extracted from some context windows that are around the labeled entities.\n[BOS] Then the sentence level features and the lexical level features are concatenated into a single vector.\n[BOS] This vector is then fed into a softmax classifier for relation prediction.\n[BOS] Another representative CNN based relation classification method is CR-CNN (Dos Santos et al., 2015) , which tackles the relation classification task with a CNN model that performs classification by ranking.\n[BOS] They proposed a new pairwise ranking loss function that is easy to reduce the impact of the artificial relation \"Other\".\n[BOS] Their method is also the unique one that takes a specific processing strategy for the \"Other\" relation.\n[BOS] Xu et al. (2016) pointed out that compared with a raw word sequence or a whole parse tree, the SDP between two entities has two main advantages.\n[BOS] First, it reduces irrelevant information; second, grammatical relations between words focus on the action and agents in a sentence and are naturally suitable for a relation classification task.\n[BOS] Thus many researchers integrate SDP into DNN based learning frameworks for relation classification.\n[BOS] For example, based on SDP, Xu et al. (2016) proposed deep recurrent neural networks (DRNNs) for relation classification.\n[BOS] Their method can be roughly regarded as a \"RNN + SDP\" relation classification method.\n[BOS] Xu et al. (2015a) proposed a neural relation classification architecture that picks up heterogeneous information along the left and right sub-path of the SDP respectively, leveraging RNN with multichannel long short term memory (LSTM) units.\n[BOS] And their method can be roughly regarded as a \"LSTM + SDP\" relation classification method.\n[BOS] Other similar work, Xu et al. (2015b) proposed to learn more robust relation representations from SDP through a CNN model; proposed augmented dependency path (ADP), which is a variant of SDP.\n[BOS] Both of these two methods can be roughly regarded as a \"CNN + SDP\" relation classification method.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 25, "token_end": 120, "char_start": 131, "char_end": 674, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 123, "token_end": 204, "char_start": 704, "char_end": 1124, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Dos Santos et al., 2015)": "15620570"}, "Reference": {}}}, {"token_start": 205, "token_end": 271, "char_start": 1131, "char_end": 1475, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 292, "token_end": 338, "char_start": 1601, "char_end": 1800, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 339, "token_end": 412, "char_start": 1807, "char_end": 2145, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Xu et al. (2015a)": "12203896"}, "Reference": {}}}, {"token_start": 417, "token_end": 456, "char_start": 2172, "char_end": 2343, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Xu et al. (2015b)": "5403702"}, "Reference": {}}}]}
{"id": "52009569_1", "paragraph": "[BOS] Recently, DNN based methods have been widely explored and have achieved state-of-the-art experimental results.\n[BOS] The core of these methods is to embed features into real-valued vectors, and then feed these vectors into some DNN based learning frameworks.\n[BOS] Generally, there are three widely used DNN frameworks for relation classification: convolutional neural networks (CNN), recurrent neural networks (RNN), and their combination.\n[BOS] In most recent years, inspired by both the success of DNN methods and the broad consensus that syntactic tree structures are of great help for relation classification, more and more research attention is being paid to the methods that integrate syntactic tree features into DNN based learning frameworks.\n[BOS] Among the syntactic tree features, the Shortest Dependent Path (SDP) is one of the most frequently used.\n[BOS] In Table 1 From Table 1 we can see that there are many similarities among the state-of-the-art relation classification methods.\n[BOS] For example, most of them use a cross entropy loss function, use WordNet, and use the stochastic gradient descent (SGD) method for optimization, etc.\n[BOS] The main differences among them mainly lie in the learning frameworks.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Transition", "Transition", "Transition", "Transition", "Transition"], "span_citation_mapping": []}
{"id": "52009569_0", "paragraph": "[BOS] Up to now, lots of novel relation classification methods have been proposed.\n[BOS] Early research mainly focuses on features based methods.\n[BOS] Usually, these methods firstly select some syntactic and semantic features from the given sentences.\n[BOS] Then the selected features are fed into some classification models like support vector machines, maximum entropy, etc.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Transition"], "span_citation_mapping": []}
{"id": "52013542_0", "paragraph": "[BOS] Text classification develops from rule-based method, statistical machine learning method to deep learning method (Aggarwal and Zhai, 2012) .\n[BOS] Many researches on text classification focus on the sentence representation (Liu et al., 2018) .\n[BOS] Structure representation and attention mechanism are important for sentence representation.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 16, "token_end": 29, "char_start": 98, "char_end": 144, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Aggarwal and Zhai, 2012)": null}}}, {"token_start": 39, "token_end": 49, "char_start": 205, "char_end": 247, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Liu et al., 2018)": "195347078"}}}]}
{"id": "52098015_5", "paragraph": "[BOS] (1) where J T is the objective function for the sequence-to-sequence model, J V is the objective function for joint embedding learning,  T are the parameters in the translation model, and  V are the parameters for the shared vision-language embedding learning, and  determines the contribution of the machine translation loss versus the visual grounding loss.\n[BOS] Both J T and J V share the parameters of the encoder from the neural machine translation model.\n[BOS] We describe details of the two objective functions in Section 3.2.\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "52098015_4", "paragraph": "[BOS] The joint objective function is defined as:\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "52098015_3", "paragraph": "[BOS] Our VAG-NMT mechanism is inspired by (Elliott and Kdr, 2017) , but has important differences.\n[BOS] First, we modify the auxiliary task as a visual-text shared space learning objective instead of the simple image reconstruction objective.\n[BOS] Second, we create a visual-text attention mechanism that captures the words that share a strong semantic meaning with the image, where the grounded visual-context has more impact on the translation.\n[BOS] We show that these enhancements lead to improvement in multimodal translation quality over (Elliott and Kdr, 2017) .\n[BOS] Given a set of parallel sentences in language X and Y , and a set of corresponding images V paired with each sentence pair, the model aims to translate sentences {x i } N i=1  X in language X to sentences {y i } N i=1  Y in language Y with the assistance of images {v i } N i=1  V .\n[BOS] We treat the problem of multimodal machine translation as a joint optimization of two tasks: (1) learning a robust translation model and (2) constructing a visual-language shared embedding that grounds the visual semantics with text.\n[BOS] Figure 1 shows an overview of our VAG-NMT model.\n[BOS] We adopt a state-of-the-art attention-based sequenceto-sequence structure (Bahdanau et al., 2014) for translation.\n[BOS] For the joint embedding, we obtain the text representation using a weighted sum of hidden states from the encoder of the sequenceto-sequence model and we obtain the image representation from a pre-trained convnet.\n[BOS] We learn the weights using a visual attention mechanism, which represents the semantic relatedness between the image and each word in the encoded text.\n[BOS] We learn the shared space with a ranking loss and the translation model with a cross entropy loss.\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection", "Narrative_cite", "Reflection", "Reflection", "Reflection", "Reflection", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 3, "token_end": 20, "char_start": 10, "char_end": 66, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 97, "token_end": 112, "char_start": 496, "char_end": 570, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 255, "token_end": 273, "char_start": 1191, "char_end": 1260, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "52098015_2", "paragraph": "[BOS] The proposed visual context grounding process in our model is closely related to the literature on multimodal shared space learning.\n[BOS] (Kiros et al., 2014) propose a neural language model to learn a visual-semantic embedding space by optimizing a ranking objective, where the distributed representation helps generate image captions.\n[BOS] (Karpathy and Li, 2014) densely align different objects in the image with their corresponding text captions in the shared space, which further improves the quality of the generated caption.\n[BOS] In later work, multimodal shared space learning was extended to multimodal multilingual shared space learning.\n[BOS] (Calixto et al., 2017c ) learn a multi-modal multilingual shared space through optimization of a modified pairwise contrastive function, where the extra multilingual signal in the shared space leads to improvements in image-sentence ranking and semantic textual similarity task.\n[BOS] (Gella et al., 2017) extend the work from (Calixto et al., 2017c) by using the image as the pivot point to learn the multilingual multimodal shared space, which does not require large parallel corpora during training.\n[BOS] Finally, (Elliott and Kdr, 2017) is the first to integrate the idea of multimodal shared space learning to help multimodal machine translation.\n[BOS] Their multi-task architecture called \"imagination\" shares an encoder between a primary task of the classical encoder-decoder NMT and an auxiliary task of visual feature reconstruction.\n\n", "discourse_tags": ["Reflection", "Single_summ", "Single_summ", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 25, "token_end": 63, "char_start": 145, "char_end": 343, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Kiros et al., 2014)": "7732372"}, "Reference": {}}}, {"token_start": 64, "token_end": 102, "char_start": 350, "char_end": 539, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Karpathy and Li, 2014)": null}, "Reference": {}}}, {"token_start": 122, "token_end": 176, "char_start": 663, "char_end": 941, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Calixto et al., 2017c": "12152396"}, "Reference": {}}}, {"token_start": 177, "token_end": 229, "char_start": 948, "char_end": 1165, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Gella et al., 2017)": "19396711"}, "Reference": {"(Calixto et al., 2017c)": "12152396"}}}, {"token_start": 232, "token_end": 294, "char_start": 1181, "char_end": 1506, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Elliott and K\u00e1d\u00e1r, 2017)": "20272964"}, "Reference": {}}}]}
{"id": "52098015_1", "paragraph": "[BOS] The model that performs best in the multimodal machine translation task employed image context in a different way.\n[BOS] (Huang et al., 2016) combine region features extracted from a region-proposal network (Ren et al., 2015) with the word sequence feature as the input to the encoder, which leads to significant improvement over their NMT baseline.\n[BOS] The best multimodal machine translation system in WMT 2017 (Caglayan et al., 2017) performs element-wise multiplication of the target language embedding with an affine transformation of the convnet image feature vector as the mixed input to the decoder.\n[BOS] While this method outperforms all other methods in WMT 2017 shared task workshop, the advantage over the monomodal translation system is still minor.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 22, "token_end": 71, "char_start": 127, "char_end": 355, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Huang et al., 2016)": "11391667"}, "Reference": {"(Ren et al., 2015)": "10328909"}}}, {"token_start": 74, "token_end": 149, "char_start": 371, "char_end": 771, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Caglayan et al., 2017)": null}, "Reference": {}}}]}
{"id": "52098015_0", "paragraph": "[BOS] In the machine translation literature, there are two major streams for integrating visual information: approaches that (1) employ separate attention for different (text and vision) modalities, and (2) fuse visual information into the NMT model as part of the input.\n[BOS] The first line of work learns independent context vectors from a sequence of text encoder hidden states and a set of location-preserving visual features extracted from a pre-trained convnet, and both sets of attentions affect the decoder's translation (Calixto et al., 2017a; Helcl and Libovick, 2017) .\n[BOS] The second line of work instead extracts a global semantic feature and initializes either the NMT encoder or decoder to fuse the visual context (Calixto et al., 2017b; Ma et al., 2017) .\n[BOS] While both approaches demonstrate significant improvement over their Text-Only NMT baselines, they still perform worse than the best monomodal machine translation system from the WMT 2017 shared task (Zhang et al., 2017) .\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 59, "token_end": 121, "char_start": 301, "char_end": 579, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Calixto et al., 2017a;": "6881637", "Helcl and Libovick\u00fd, 2017)": "868646"}}}, {"token_start": 131, "token_end": 167, "char_start": 631, "char_end": 772, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Calixto et al., 2017b;": "11387463", "Ma et al., 2017)": "4380496"}}}, {"token_start": 192, "token_end": 212, "char_start": 914, "char_end": 1001, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhang et al., 2017)": null}}}]}
{"id": "52011208_5", "paragraph": "[BOS] In the same period of our work, Lin et al. (2018) and Xia et al. (2017) first turn eyes to the target side attention of NMT architecture.\n[BOS] Our approach share the similar idea with these work.\n[BOS] The diffrence lies in that we concernd more about the integrating of the souce side context and the target side context and designed three types of combination functions.\n[BOS] In addition, we approached in a multi-layer way which is more effective.\n\n", "discourse_tags": ["Multi_summ", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 10, "token_end": 38, "char_start": 38, "char_end": 143, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Lin et al. (2018)": "3646455"}, "Reference": {}}}]}
{"id": "52011208_4", "paragraph": "[BOS] Exploiting Contextual Information A thread of work in sequence to sequence learning attempts to exploit auxiliary context information (Wang and Cho, 2016; Li et al., 2017; Zhang and Zong, 2016) .\n[BOS] Recently Tu et al. (2016a) propose using context gates in NMT to dynamically control the contributions from the source contexts and the RNN hidden state.\n[BOS] Our approach focuses on integrating the decoding history and the source side context to NMT architecture.\n[BOS] In addition, we have a multi-layer approach to better utilize the contextual information.\n[BOS] Experiments in Section 4.3 show the superiority of DHEA.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 17, "token_end": 41, "char_start": 110, "char_end": 199, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wang and Cho, 2016;": null, "Li et al., 2017;": "12637374", "Zhang and Zong, 2016)": "17667087"}}}, {"token_start": 44, "token_end": 75, "char_start": 217, "char_end": 361, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Tu et al. (2016a)": "1658155"}, "Reference": {}}}]}
{"id": "52011208_3", "paragraph": "[BOS] Recurrent Residual Networks Our work is also related to residual connections, which have been shown to improve the learning process of deep neural networks by addressing the vanishing gradient problem (He et al., 2015; Szegedy et al., 2016) .\n[BOS] Recently, several architectures using residual connections with LSTMs have been proposed (Kim et al., 2017; Wang, 2017; for sequence prediction.\n[BOS] These connections create a direct path from previous layers, helping the transmission of information.\n[BOS] Related to our work, Miculicich et al. (2018) propose a target sideattentive residual recurrent network for decoding, where attention over previous words contributes directly to the prediction of the next word.\n[BOS] Comparatively, DHEA attends to the previous hidden state and make a combination with the source context.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Transition", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 19, "token_end": 50, "char_start": 109, "char_end": 246, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(He et al., 2015;": "206594692", "Szegedy et al., 2016)": "1023605"}}}, {"token_start": 57, "token_end": 77, "char_start": 293, "char_end": 373, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kim et al., 2017;": "15709478"}}}, {"token_start": 105, "token_end": 142, "char_start": 535, "char_end": 724, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Miculicich et al. (2018)": "3514091"}, "Reference": {}}}]}
{"id": "52011208_2", "paragraph": "[BOS] The application of self-attention mechanisms in RNNs have been previously studied, and in general, it appears to capture syntactic dependencies among distant words (Liu and Lapata, 2017; Kim et al., 2017; Lin et al., 2017) .\n[BOS] Vaswani et al. (2017) resort to self-attention mechanism and showed outstanding performance.\n[BOS] Our approach is diffrent from their work in two aspect.\n[BOS] First, our method can be viewed as a variant of RNN decoder which allows a form of memory, thus has the the potential to better handle sentences of arbitrary length.\n[BOS] Second, we forcus on controlling the information flow between the source side memory and the target side memory and design a gate to balance the contribution of the two-sides.\n\n", "discourse_tags": ["Multi_summ", "Single_summ", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 24, "token_end": 52, "char_start": 119, "char_end": 228, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Liu and Lapata, 2017;": "39871772", "Kim et al., 2017;": "15709478", "Lin et al., 2017)": "15280949"}}}, {"token_start": 54, "token_end": 74, "char_start": 237, "char_end": 329, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Vaswani et al. (2017)": "13756489"}, "Reference": {}}}]}
{"id": "52011208_1", "paragraph": "[BOS] Attention Mechanism Attention in neural networks (Bahdanau et al., 2014; Luong et al., 2015) is designed to assign weights to different inputs instead of threating all input sequences equally as original neural networks do.\n[BOS] A number of efforts have been made to improve the attention mechanism (Tu et al., 2016b; Mi et al., 2016; .\n[BOS] Some of them incorporated the previous attention history into the current attention for better alignment, but none of them are based on the decoding history.\n\n", "discourse_tags": ["Multi_summ", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 2, "token_end": 48, "char_start": 6, "char_end": 229, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Bahdanau et al., 2014;": "61556494"}, "Reference": {"Luong et al., 2015)": "1998416"}}}, {"token_start": 59, "token_end": 77, "char_start": 286, "char_end": 341, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Tu et al., 2016b;": "146843", "Mi et al., 2016;": "14900221"}}}]}
{"id": "52011208_0", "paragraph": "[BOS] Memory Networks There are variety of studies proposed to increase the LSTM memory capacity by using memory networks.\n[BOS] The two most salient examples are Neural Turing Machine (NTM) (Graves et al., 2014) and Memory Network (Weston et al., 2014) .\n[BOS] Cheng et al. (2016) propose a machine reading simulator which processes text incrementally from left to right.\n[BOS] In the NMT task, present a decoder enhanced decoder with an external shared memory which extends the capacity of the network and has the potential to read, write, and forget information.\n[BOS] In fact DHEA can be viewed as a special case of memory networks, with only reading mechanism for the translation task.\n[BOS] Quite remarkably DHEA incorporates two different types of memory (source memory and decoding history memory) and significantly improves upon state-of-the-arts.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Single_summ", "Transition", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 29, "token_end": 45, "char_start": 163, "char_end": 212, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Graves et al., 2014)": "15299054"}}}, {"token_start": 46, "token_end": 57, "char_start": 217, "char_end": 253, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 59, "token_end": 81, "char_start": 262, "char_end": 372, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Cheng et al. (2016)": null}, "Reference": {}}}]}
{"id": "52117484_4", "paragraph": "[BOS] Bilingual Word Embeddings There have been two general paradigms in obtaining bilingual word vectors besides using dictionaries: through parallel corpora and through joint training.\n[BOS] Approaches based on parallel corpora usually learn bilingual word embeddings that can produce similar representations for aligned sentences (Hermann and Blunsom, 2014; Chandar et al., 2014) .\n[BOS] Jointlytrained models combine the common monolingual training objective with a cross-lingual training objective that often comes from parallel corpus (Zou et al., 2013; Gouws et al., 2015) .\n[BOS] Recently, unsupervised approaches also have been used to align two sets of word embeddings by learning a mapping through adversarial learning or selflearning (Zhang et al., 2017; Artetxe et al., 2017; Lample et al., 2018) .\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 36, "token_end": 66, "char_start": 238, "char_end": 382, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Chandar et al., 2014)": "217774"}}}, {"token_start": 90, "token_end": 110, "char_start": 525, "char_end": 579, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zou et al., 2013;": "931054", "Gouws et al., 2015)": "7021865"}}}, {"token_start": 128, "token_end": 163, "char_start": 682, "char_end": 809, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhang et al., 2017;": "26873455", "Artetxe et al., 2017;": "13335042", "Lample et al., 2018)": "3470398"}}}]}
{"id": "52117484_3", "paragraph": "[BOS] Another way of performing language independent transfer resorts to multi-task learning, where a model is trained jointly across different languages by sharing parameters to allow for knowledge transfer (Ammar et al., 2016a; Cotterell and Duh, 2017; Lin et al., 2018) .\n[BOS] However, such approaches usually require some amounts of training data in the target language for bootstrapping, which is different from our unsupervised approach that requires no labeled resources in the target language.\n\n", "discourse_tags": ["Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 12, "token_end": 60, "char_start": 73, "char_end": 272, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ammar et al., 2016a;": "2868247", "Cotterell and Duh, 2017;": null, "Lin et al., 2018)": "51870433"}}}]}
{"id": "52117484_2", "paragraph": "[BOS] Language independent transfer-based approaches build models using language independent and delexicalized features.\n[BOS] For instance, Zirikly and Hagiwara (2015) transfers word cluster and gazetteer features through the use of comparable copora.\n[BOS] Tsai et al. (2016) links words to Wikipedia entries and uses the entry category as features to train language independent NER models.\n[BOS] Recently, Ni et al. (2017) propose to project word embeddings into a common space as language independent features.\n[BOS] These approaches utilize such features by training a model on the source language and directly applying it to the target language.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 24, "token_end": 50, "char_start": 141, "char_end": 252, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zirikly and Hagiwara (2015)": null}, "Reference": {}}}, {"token_start": 51, "token_end": 77, "char_start": 259, "char_end": 392, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Tsai et al. (2016)": "2889848"}, "Reference": {}}}, {"token_start": 80, "token_end": 101, "char_start": 409, "char_end": 514, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ni et al. (2017)": "13410520"}, "Reference": {}}}]}
{"id": "52117484_1", "paragraph": "[BOS] Annotation projection methods create training data by using parallel corpora to project annotations from the source to the target language.\n[BOS] Such approaches have been applied to many tasks under the cross-lingual setting, such as POS tagging (Yarowsky et al., 2001; Das and Petrov, 2011; Tckstrm et al., 2013; Fang and Cohn, 2016) , mention detection (Zitouni and Florian, 2008) and parsing (Hwa et al., 2005; McDonald et al., 2011) .\n\n", "discourse_tags": ["Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 42, "token_end": 78, "char_start": 241, "char_end": 341, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yarowsky et al., 2001;": "15279538", "Das and Petrov, 2011;": "8396953", "T\u00e4ckstr\u00f6m et al., 2013;": "14760908"}}}, {"token_start": 79, "token_end": 91, "char_start": 344, "char_end": 389, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 92, "token_end": 109, "char_start": 394, "char_end": 443, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hwa et al., 2005;": "157167"}}}]}
{"id": "52117484_0", "paragraph": "[BOS] Cross-Lingual Learning Cross-lingual learning approaches can be loosely classified into two categories: annotation projection and languageindependent transfer.\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "52865081_5", "paragraph": "[BOS] Story generation Work in story generation has incorporated structure and context through event representations (Martin et al., 2017) or semantic representations, like story graphs (Rishes et al., 2013; Elson and McKeown, 2009) .\n[BOS] In this work, we provide evidence for the value of entity representations as an additional form of structure, following work by Walker et al. (2011), Cavazza and Charles (2005) , and Cavazza et al. (2002) .\n\n", "discourse_tags": ["Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 12, "token_end": 24, "char_start": 79, "char_end": 138, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Martin et al., 2017)": "19127777"}}}, {"token_start": 25, "token_end": 49, "char_start": 142, "char_end": 232, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rishes et al., 2013;": "6800803", "Elson and McKeown, 2009)": null}}}, {"token_start": 62, "token_end": 101, "char_start": 292, "char_end": 445, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Charles (2005)": "9832478", "Cavazza et al. (2002)": "206468170"}}}]}
{"id": "52865081_4", "paragraph": "[BOS] The mention only version of the mention generation task is related to cloze tests like the Children's Book Test (Hill et al., 2016) , the \"Who-didWhat\" Test (Onishi et al., 2016) , and the CNN and Daily Mail test described by Hermann et al. (2015) .\n[BOS] However, unlike these tests, we predict all entity mentions in the text and from a dynamically expanding candidate list, typically much larger than those in other cloze tests.\n\n", "discourse_tags": ["Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 19, "token_end": 32, "char_start": 97, "char_end": 137, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hill et al., 2016)": "14915449"}}}, {"token_start": 35, "token_end": 50, "char_start": 145, "char_end": 184, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Onishi et al., 2016)": "5761781"}}}, {"token_start": 53, "token_end": 68, "char_start": 195, "char_end": 253, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Hermann et al. (2015)": "6203757"}}}]}
{"id": "52865081_3", "paragraph": "[BOS] Mention generation Our novel mention generation task is inspired by both referring expression generation (Dale and Reiter, 1995) and entity prediction (Modi et al., 2017) .\n[BOS] The major difference is that, unlike referring expression generation, our task includes all the mentions used for entities, including pronouns; we believe it is a more realistic test of a model's handling of entities.\n[BOS] Krahmer and Van Deemter (2012) give a comprehensive survey on early work of referring expression generation.\n\n", "discourse_tags": ["Narrative_cite", "Reflection", "Single_summ"], "span_citation_mapping": [{"token_start": 14, "token_end": 25, "char_start": 89, "char_end": 134, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dale and Reiter, 1995)": "7018595"}}}, {"token_start": 26, "token_end": 36, "char_start": 139, "char_end": 176, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Modi et al., 2017)": "8075816"}}}, {"token_start": 82, "token_end": 105, "char_start": 409, "char_end": 517, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Krahmer and Van Deemter (2012)": null}, "Reference": {}}}]}
{"id": "52865081_2", "paragraph": "[BOS] Choosing an appropriate entity and its mention has a big influence on the coherence of a text, as studied in Centering Theory (Grosz et al., 1995) .\n[BOS] Recently, the ENTI-TYNLM proposed by Ji et al. (2017) shows that adding entity related information can improve the performance of language modeling, which potentially provides a method for entity related text generation.\n[BOS] We build on ENTITYNLM, combining entity context with previous-sentence context, and demonstrate the importance of the latter in a coherence test ( 6).\n[BOS] The max pooling combination we propose is simple but effective.\n[BOS] Another line of related work on recipe generation included special treatment of entities as candidates in generating sentences, but not as context (Kiddon et al., 2016) .\n[BOS] Bosselut et al. (2018) also generated recipes, using neural process networks to track and update entity representations with the goal of modeling actions and their causal effects on entities.\n[BOS] However, the entity representations are frozen during generation, rather than dynamically updated.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Reflection", "Reflection", "Narrative_cite", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 23, "token_end": 35, "char_start": 115, "char_end": 152, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Grosz et al., 1995)": "11660053"}}}, {"token_start": 40, "token_end": 78, "char_start": 175, "char_end": 381, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ji et al. (2017)": "5564363"}, "Reference": {}}}, {"token_start": 132, "token_end": 155, "char_start": 674, "char_end": 783, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kiddon et al., 2016)": "9818013"}}}, {"token_start": 157, "token_end": 210, "char_start": 792, "char_end": 1088, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Bosselut et al. (2018)": "31816657"}, "Reference": {}}}]}
{"id": "52865081_1", "paragraph": "[BOS] Neural models for text generation Natural language generation is a classic problem in artificial intelligence.\n[BOS] Recent use of RNNs (Sutskever et al., 2011) has reignited interest in this area.\n[BOS] Our work provides an additional way to address the wellknown drawback of RNNs: they use only limited context.\n[BOS] This has been noted as a serious problem in conversational modeling (Sordoni et al., 2015) and text generation with multiple sentences (Lau et al., 2017) .\n[BOS] Recent work on context-aware text generation (or the related task, language modeling) has studied the possibilities of using different granularity of context.\n[BOS] For example, in the scenario of response generation, Sordoni et al. (2015) showed a consistent gain by including one more utterance from context.\n[BOS] Similar effects are also observed by adding topical information for language modeling and generation (Lau et al., 2017) .\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Reflection", "Narrative_cite", "Transition", "Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 22, "token_end": 34, "char_start": 137, "char_end": 166, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sutskever et al., 2011)": "8843166"}}}, {"token_start": 76, "token_end": 89, "char_start": 370, "char_end": 416, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sordoni et al., 2015)": "94285"}}}, {"token_start": 90, "token_end": 103, "char_start": 421, "char_end": 479, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lau et al., 2017)": "7074119"}}}, {"token_start": 139, "token_end": 165, "char_start": 673, "char_end": 798, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Sordoni et al. (2015)": "94285"}, "Reference": {}}}, {"token_start": 176, "token_end": 188, "char_start": 873, "char_end": 924, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lau et al., 2017)": "7074119"}}}]}
{"id": "52865081_0", "paragraph": "[BOS] Beyond past work already discussed, we note a few additional important areas of research relevant to our work.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "53224724_2", "paragraph": "[BOS] To our knowledge, multi-source MT has also been studied only using the RNN-based models.\n[BOS] Dabre et al. (2017) use simple concatenation of source sentences in various languages and process them with a single multilingual encoder.\n[BOS] Zoph and Knight (2016) try context concatenation and hierarchical gating method for combining context vectors in attention models with multiple inputs encoded by separate encoders.\n[BOS] In all of their experiments, the multi-source methods significantly surpass the single-source baseline.\n[BOS] Nishimura et al. (2018) extend the former approach for situations when of the source languages is missing, so that the translation system does not overly rely on a single source language like some of the models presented in this work.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 24, "token_end": 51, "char_start": 101, "char_end": 239, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Dabre et al. (2017)": "3088415"}, "Reference": {}}}, {"token_start": 52, "token_end": 104, "char_start": 246, "char_end": 536, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zoph and Knight (2016)": "8677917"}, "Reference": {}}}, {"token_start": 105, "token_end": 152, "char_start": 543, "char_end": 777, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Nishimura et al. (2018)": "210931395"}, "Reference": {}}}]}
{"id": "53224724_1", "paragraph": "[BOS] Except for using the image features direct input to the model, they can be used as an auxiliary objective (Elliott and Kdr, 2017) .\n[BOS] In this setup, the visually grounded representation, improves the MMT significantly, achieving similar results that our models achieved using only the Multi30k dataset.\n\n", "discourse_tags": ["Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 6, "token_end": 31, "char_start": 27, "char_end": 135, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "53224724_0", "paragraph": "[BOS] MMT was so far solved only within the RNNbased architectures.\n[BOS] Elliott et al. (2015) report significant improvements with a non-attentive model.\n[BOS] With attentive models (Bahdanau et al., 2014) , the additional visual information usually did not improve the models significantly (Caglayan et al., 2016; in terms of BLEU score.\n[BOS] Our models slightly outperform these models in the single model setup.\n\n", "discourse_tags": ["Transition", "Single_summ", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 17, "token_end": 36, "char_start": 74, "char_end": 155, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Elliott et al. (2015)": "18550051"}, "Reference": {}}}, {"token_start": 38, "token_end": 51, "char_start": 167, "char_end": 207, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bahdanau et al., 2014)": "11212020"}}}, {"token_start": 53, "token_end": 72, "char_start": 214, "char_end": 315, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "52010712_2", "paragraph": "[BOS] Apart from these models, there are others that do not directly address the problem of morphology but their solutions could be quite useful to translate MRLs.\n[BOS] Jean et al. (2015) proposed a model to handle very large vocabularies.\n[BOS] Luong et al. (2015) addressed the problem of rare and unseen words with the help of a post-translation phase to exchange unknown tokens with their potential translations.\n[BOS] Dalvi et al. (2017) did not propose a new model but studied the impact of morphological information in NMT.\n[BOS] They evaluated the behaviour of an encoder-decoder model to see what sort of morphological information is learned via the model and how the model deals with complex structures.\n[BOS] Passban (2018) extensively discussed the problem of morphology at the word and sequence level and proposed solutions for modeling and translating sequences in monolingual and bilingual settings.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 32, "token_end": 50, "char_start": 170, "char_end": 240, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Jean et al. (2015)": "2863491"}, "Reference": {}}}, {"token_start": 51, "token_end": 86, "char_start": 247, "char_end": 417, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Luong et al. (2015)": "1245593"}, "Reference": {}}}, {"token_start": 87, "token_end": 144, "char_start": 424, "char_end": 714, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Dalvi et al. (2017)": "37605145"}, "Reference": {}}}, {"token_start": 145, "token_end": 177, "char_start": 721, "char_end": 915, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Passban (2018)": "196007197"}, "Reference": {}}}]}
{"id": "52010712_1", "paragraph": "[BOS] Models reviewed so far address the problem of morphology on the source side.\n[BOS] In contrast, there is a group of models which study the same problem for the target side.\n[BOS] Huck et al. (2017) compared different word-segmentation models, including linguistically motivated as well as statistical techniques, to find the most appropriate segmentation scheme when translating into MRLs.\n[BOS] Chung et al. (2016) tried to design a suitable architecture when the target language is an MRL.\n[BOS] They benefit from using a character-based decoder which partially resolves the problem.\n[BOS] proposed a similar approach in which they equipped the character-based decoder with an additional morphology table to inform the decoder with the target language's morphological structures.\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 37, "token_end": 75, "char_start": 185, "char_end": 395, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Huck et al. (2017)": "6885919"}, "Reference": {}}}, {"token_start": 76, "token_end": 115, "char_start": 402, "char_end": 591, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Chung et al. (2016)": "13495961"}, "Reference": {}}}, {"token_start": 116, "token_end": 147, "char_start": 598, "char_end": 787, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "52010712_0", "paragraph": "[BOS] The problem of morphology and dealing with MCWs is an important issue in the field of sequence modeling as it directly affects the model's architecture and its performance.\n[BOS] There is a fair amount of research carried out to address this problem, which can be discussed in two main categories.\n[BOS] One group of models tries to cope with the problem on the encoder side where the goal is to understand the rich morphology of the source language.\n[BOS] Kim et al. (2016) proposed a convolutional module to process complex inputs for the problem of language modeling.\n[BOS] Costa-juss and Fonollosa (2016) and Lee et al. (2017) adapted the same convolutional encoder to NMT.\n[BOS] Luong and Manning (2016) designed a hybrid character-and word-based encoder to try to solve the out-of-vocabulary problem.\n[BOS] Vylomova et al. (2016) tackled the problem by comparing the impact of different representation schemes on the encoder.\n[BOS] Similarly, Burlot et al. (2017) investigated the impact of different word representation models in the context of factored NMT.\n[BOS] Our work is also an example of models which try to provide richer information when the source side is an MRL.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Single_summ", "Multi_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 88, "token_end": 110, "char_start": 463, "char_end": 576, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kim et al. (2016)": "686481"}, "Reference": {}}}, {"token_start": 111, "token_end": 139, "char_start": 583, "char_end": 683, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Costa-juss\u00e0 and Fonollosa (2016)": "1712853", "Lee et al. (2017)": "10509498"}, "Reference": {}}}, {"token_start": 140, "token_end": 170, "char_start": 690, "char_end": 812, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Luong and Manning (2016)": "13972671"}, "Reference": {}}}, {"token_start": 171, "token_end": 197, "char_start": 819, "char_end": 937, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Vylomova et al. (2016)": "7193975"}, "Reference": {}}}, {"token_start": 200, "token_end": 225, "char_start": 955, "char_end": 1071, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Burlot et al. (2017)": "4246"}, "Reference": {}}}]}
{"id": "52166727_2", "paragraph": "[BOS] Our method is also connected to some previous approaches to improve machine translation using monolingual corpora.\n[BOS] In particular, the generation of a synthetic parallel corpus through backtranslation (Sennrich et al., 2016) , which is a key component of our unsupervised tuning and iterative refinement procedures, has been previously used to improve NMT.\n[BOS] In addition, there have been several proposals to extend the phrase table of SMT systems by inducing translation candidates and/or scores from monolingual corpora, using either statistical decipherment methods Knight, 2012, 2013) or cross-lingual embeddings (Zhao et al., 2015; Wang et al., 2016) .\n[BOS] While all these methods exploit monolingual corpora to enhance an existing machine translation system previously trained on parallel corpora, our approach learns a fully featured phrase-based SMT system from monolingual corpora alone.\n\n", "discourse_tags": ["Reflection", "Narrative_cite", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 25, "token_end": 45, "char_start": 146, "char_end": 235, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sennrich et al., 2016)": "15600925"}}}, {"token_start": 102, "token_end": 114, "char_start": 551, "char_end": 603, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 115, "token_end": 135, "char_start": 607, "char_end": 670, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhao et al., 2015;": "8284107", "Wang et al., 2016)": "14855044"}}}]}
{"id": "52166727_1", "paragraph": "[BOS] More recently, Artetxe et al. (2018c) and have managed to train a standard attentional encoder-decoder NMT system from monolingual corpora alone.\n[BOS] For that purpose, they use a shared encoder for both languages with pretrained cross-lingual embeddings, and train the entire system using a combination of denoising, backtranslation and, in the case of , adversarial training.\n[BOS] This method was further improved by Yang et al. (2018) , who use a separate encoder for each language, sharing only a subset of their parameters, and incorporate two generative adversarial networks.\n[BOS] However, our results in Section 6.1 show that our SMT-based approach obtains substantially better results.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 5, "token_end": 82, "char_start": 21, "char_end": 384, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Artetxe et al. (2018c)": "3515219"}, "Reference": {}}}, {"token_start": 89, "token_end": 122, "char_start": 427, "char_end": 589, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Yang et al. (2018)": "13748556"}, "Reference": {}}}]}
{"id": "52166727_0", "paragraph": "[BOS] Similar to our approach, statistical decipherment also attempts to build machine translation systems from monolingual corpora.\n[BOS] For that purpose, existing methods treat the source language as ciphertext, and model its generation through a noisy channel model involving two steps: the generation of the original English sentence and the probabilistic replacement of the words in it (Ravi and Knight, 2011; Dou and Knight, 2012) .\n[BOS] The English generative process is modeled using an n-gram language model, and the channel model parameters are estimated using either expectation maximization or Bayesian inference.\n[BOS] Subsequent work has attempted to enrich these models with additional information like syntactic knowledge (Dou and Knight, 2013) and word embeddings (Dou et al., 2015) .\n[BOS] Nevertheless, these systems work in a word-by-word basis and have only been shown to work in limited settings, being often evaluated in word-level translation.\n[BOS] In contrast, our method builds a fully featured phrasebased SMT system, and achieves competitive performance in a standard machine translation task.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Narrative_cite", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 59, "token_end": 83, "char_start": 347, "char_end": 437, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ravi and Knight, 2011;": "6060648", "Dou and Knight, 2012)": "5727182"}}}, {"token_start": 127, "token_end": 138, "char_start": 720, "char_end": 762, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dou and Knight, 2013)": "12959203"}}}, {"token_start": 139, "token_end": 150, "char_start": 767, "char_end": 801, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dou et al., 2015)": "6508869"}}}]}
{"id": "52830097_2", "paragraph": "[BOS] In terms of the regularization to the representation, Duong et al. (2015) used l2 regularization between the parameters of the same part of two models in multi-task learning.\n[BOS] Their method is a kind of soft-parameter sharing, which does not involve sharing any part of the model directly.\n[BOS] Fu et al. (2017) applied domain adversarial networks (Ganin and Lempitsky, 2015) to relation extraction and obtained improvement on out-of-domain evaluation.\n[BOS] Inspired by the adversarial training, we attempt to use it as a regularization tool in our multi-task model and find some improvement.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 6, "token_end": 63, "char_start": 22, "char_end": 299, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Duong et al. (2015)": "17263016"}, "Reference": {}}}, {"token_start": 64, "token_end": 100, "char_start": 306, "char_end": 463, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Fu et al. (2017)": "31779941"}, "Reference": {"(Ganin and Lempitsky, 2015)": "6755881"}}}]}
{"id": "52830097_1", "paragraph": "[BOS] In order to further improve the representation learning for relation extraction, tried to transfer knowledge through bilingual representation.\n[BOS] They used their multi-task model to train on the bilingual ACE05 datasets and obtained improvement when there is less training available (10%-50%).\n[BOS] Our experiments will show our multitask model can make significant improvement on the full training set.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": []}
{"id": "52830097_0", "paragraph": "[BOS] Relation extraction is typically reduced to a classification problem.\n[BOS] A supervised machine learning model is designed and trained on a single dataset to predict the relation type of pairs of entities.\n[BOS] Traditional methods rely on linguistic or semantic features (Zhou et al., 2005; Jing and Zhai, 2007) , or kernels based on syntax or sequences (Bunescu and Mooney, 2005a,b; Plank and Moschitti, 2013) to represent sentences of relations.\n[BOS] More recently, deep neural nets start to show promising results.\n[BOS] Most rely on convolutional neural nets (Zeng et al., 2014 (Zeng et al., , 2015 Grishman, 2015, 2016; Fu et al., 2017) or recurrent neural nets (Zhang et al., 2015; Zhou et al., 2016; Miwa and Bansal, 2016) to learn the representation of relations.\n[BOS] Our supervised base model will be similar to (Zhou et al., 2016) .\n[BOS] Our initial experiments did not use syntactic features (Nguyen and Grishman, 2016; Fu et al., 2017 ) that require additional parsers.\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite", "Transition", "Narrative_cite", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 41, "token_end": 60, "char_start": 247, "char_end": 319, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhou et al., 2005;": "3160937", "Jing and Zhai, 2007)": "15036406"}}}, {"token_start": 62, "token_end": 90, "char_start": 325, "char_end": 418, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Plank and Moschitti, 2013)": "3011134"}}}, {"token_start": 113, "token_end": 146, "char_start": 546, "char_end": 650, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zeng et al., 2014": "12873739", "(Zeng et al., , 2015": "2778800", "Fu et al., 2017)": "31779941"}}}, {"token_start": 147, "token_end": 173, "char_start": 654, "char_end": 738, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhang et al., 2015;": "8950084", "Zhou et al., 2016;": "9870160", "Miwa and Bansal, 2016)": "2476229"}}}, {"token_start": 189, "token_end": 197, "char_start": 832, "char_end": 851, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhou et al., 2016)": "9870160"}}}, {"token_start": 205, "token_end": 222, "char_start": 896, "char_end": 958, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Nguyen and Grishman, 2016;": "5831803", "Fu et al., 2017": "31779941"}}}]}
{"id": "52144168_1", "paragraph": "[BOS] Pipelines.\n[BOS] In a typical pipeline, T 1 and T 2 are separately trained, with the output of T 2 used to define the inputs to T 1 (Wolpert, 1992) .\n[BOS] Using syntax as T 2 in a pipeline is perhaps the most common approach for semantic structure prediction (Toutanova et al., 2008; Yang and Mitchell, 2017; Wiseman et al., 2016) .\n[BOS] 2 However, pipelines introduce the problem of cascading errors (T 2 's mistakes affect the performance, and perhaps the training, of T 1 ; He et al., 2013) .\n[BOS] To date, remedies to cascading errors are so computationally expensive as to be impractical (e.g., Finkel et al., 2006) .\n[BOS] A syntactic scaffold is quite different from a pipeline since the output of T 2 is never explicitly used.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite", "Narrative_cite", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 28, "token_end": 40, "char_start": 113, "char_end": 153, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wolpert, 1992)": null}}}, {"token_start": 57, "token_end": 85, "char_start": 236, "char_end": 337, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Toutanova et al., 2008;": "2243454", "Yang and Mitchell, 2017;": "5638092", "Wiseman et al., 2016)": "9163756"}}}, {"token_start": 111, "token_end": 125, "char_start": 462, "char_end": 501, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"He et al., 2013)": "105260"}}}]}
{"id": "52144168_0", "paragraph": "[BOS] We briefly contrast the syntactic scaffold with existing alternatives.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "52167799_7", "paragraph": "[BOS] The persona is a set of sentences representing the personality of the responding agent, the context is the utterance that it responds to, and the response is the answer to be predicted.\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "52167799_6", "paragraph": "[BOS]  Response: \"Me too!\n[BOS] But only on weekends.\"\n\n", "discourse_tags": ["Other", "Other"], "span_citation_mapping": []}
{"id": "52167799_5", "paragraph": "[BOS]  Context: \"I love running.\"\n\n", "discourse_tags": ["Other"], "span_citation_mapping": []}
{"id": "52167799_4", "paragraph": "[BOS]  Persona: [\"I like sport\", \"I work a lot\"]\n\n", "discourse_tags": ["Other"], "span_citation_mapping": []}
{"id": "52167799_3", "paragraph": "[BOS] Our goal is to learn to predict responses based on a persona for a large variety of personas.\n[BOS] To that end, we build a dataset of examples of the following form using data from REDDIT:\n\n", "discourse_tags": ["Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "52167799_2", "paragraph": "[BOS] 3 Building a dataset of millions of persona-based dialogues\n\n", "discourse_tags": ["Other"], "span_citation_mapping": []}
{"id": "52167799_1", "paragraph": "[BOS] In this paper, we use a pre-existing REDDIT data dump as data source.\n[BOS] REDDIT is a massive online message board.\n[BOS] Dodge et al. (2015) used it to assess chit-chat qualities of generic dialogue models.\n[BOS] Yang et al. (2018) used response prediction on REDDIT as an auxiliary task in order to improve prediction performance on natural language inference problems.\n\n", "discourse_tags": ["Reflection", "Reflection", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 34, "token_end": 56, "char_start": 130, "char_end": 215, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Dodge et al. (2015)": "2239496"}, "Reference": {}}}, {"token_start": 57, "token_end": 87, "char_start": 222, "char_end": 379, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Yang et al. (2018)": "5033484"}, "Reference": {}}}]}
{"id": "52167799_0", "paragraph": "[BOS] With the rise of end-to-end dialogue systems, personalized trained systems have started to appear.\n[BOS] Li et al. (2016) proposed to learn latent variables representing each speaker's bias/personality in a dialogue model.\n[BOS] Other classic strategies include extracting explicit variables from structured knowledge bases or other symbolic sources as in (Ghazvininejad et al., 2017; Joshi et al., 2017; Young et al., 2017) .\n[BOS] Still, in the context of per-sonal chatbots, it might be more desirable to condition on data that can be generated and interpreted by the user itself such as text rather than relying on some knowledge base facts that might not exist for everyone or a great variety of situations.\n[BOS] PERSONA-CHAT (Zhang et al., 2018) recently introduced a dataset of conversations revolving around human habits and preferences.\n[BOS] In their experiments, they showed that conditioning on a text description of each speaker's habits, their persona, improved dialogue modeling.\n\n", "discourse_tags": ["Transition", "Single_summ", "Narrative_cite", "Transition", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 23, "token_end": 48, "char_start": 111, "char_end": 228, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Li et al. (2016)": "2955580"}, "Reference": {}}}, {"token_start": 61, "token_end": 94, "char_start": 333, "char_end": 430, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Joshi et al., 2017;": "29473470", "Young et al., 2017)": "19179559"}}}, {"token_start": 154, "token_end": 208, "char_start": 725, "char_end": 1001, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Zhang et al., 2018)": null}, "Reference": {}}}]}
{"id": "52812226_2", "paragraph": "[BOS] Other Seq2seq models in NLP.\n[BOS] Even though neural seq2seq models were originally designed for machine translation (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015) , their application has not stayed limited to this area.\n[BOS] Similar architectures have been successfully applied to many seq2seq tasks in NLP, e.g., syntactic parsing (Vinyals et al., 2015) , language correction (Xie et al., 2016) , normalization of historical texts (Bollmann et al., 2017) , or text simplification (Nisioi et al., 2017) .\n[BOS] Transductive inference is similar to domain adaptation, e.g., in machine translation (Luong and Manning, 2015) .\n[BOS] One difference is that training set and test set can hardly be called different domains in paradigm completion.\n[BOS] Another difference is that explicit structured labels (the morphological tags of the forms in the input subset) are available at test time in paradigm completion.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite", "Narrative_cite", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 23, "token_end": 51, "char_start": 104, "char_end": 189, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sutskever et al., 2014;": "7961699"}}}, {"token_start": 85, "token_end": 97, "char_start": 342, "char_end": 382, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vinyals et al., 2015)": "14223"}}}, {"token_start": 98, "token_end": 108, "char_start": 385, "char_end": 423, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Xie et al., 2016)": "8880428"}}}, {"token_start": 109, "token_end": 123, "char_start": 426, "char_end": 483, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 125, "token_end": 137, "char_start": 489, "char_end": 530, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Nisioi et al., 2017)": "36364048"}}}, {"token_start": 154, "token_end": 165, "char_start": 604, "char_end": 649, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Luong and Manning, 2015)": null}}}]}
{"id": "52812226_1", "paragraph": "[BOS] Several ways to employ neural models for morphological generation with limited data have been proposed, e.g., semi-supervised training (Zhou and Neubig, 2017; or simultaneous training on multiple languages (Kann et al., 2017b) .\n[BOS] The total number of sources in the training set in some of our settings may be comparable to this earlier work, but our training sets are less diverse since many forms come from the same paradigm.\n[BOS] We argue in 1 that the number of paradigms (not the number of sources) measures the effective size of the training set.\n\n", "discourse_tags": ["Narrative_cite", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 23, "token_end": 34, "char_start": 116, "char_end": 163, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 36, "token_end": 50, "char_start": 168, "char_end": 232, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kann et al., 2017b)": "519236"}}}]}
{"id": "52812226_0", "paragraph": "[BOS] Morphological generation.\n[BOS] In the last two years, most work on paradigm completion has been done in the context of the SIGMORPHON 2016 and the CoNLL-SIGMORPHON 2017 shared tasks (Cotterell et al., 2016 (Cotterell et al., , 2017a ).\n[BOS] Due to the success of neural seq2seq models in 2016 (Kann and Schtze, 2016b; Aharoni et al., 2016) , systems developed for the 2017 edition were mostly neural (Makarov et al., 2017; Bergmanis et al., 2017; Zhou and Neubig, 2017 ).\n[BOS] Besides the shared task systems, presented a paradigm completion model for a multi-source setting that made use of an attention mechanism to decide which input form to attend to at each time step.\n[BOS] They used randomly chosen, independent pairs of source and target forms for training.\n[BOS] This differs crucially from the setting we consider in that no complete paradigms were available in their training sets.\n[BOS] Only Cotterell et al. (2017b) addressed essentially the same task we do, but they only considered the high-resource setting: their models were trained on hundreds of complete paradigms.\n[BOS] The experiments reported in 5.3 empirically confirm that inductive-only models perform poorly in our setting.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite", "Single_summ", "Single_summ", "Transition", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 30, "token_end": 60, "char_start": 150, "char_end": 239, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cotterell et al., , 2017a": "482862"}}}, {"token_start": 68, "token_end": 94, "char_start": 271, "char_end": 347, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kann and Sch\u00fctze, 2016b;": "6387118"}}}, {"token_start": 103, "token_end": 129, "char_start": 401, "char_end": 476, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Makarov et al., 2017;": "28704517", "Zhou and Neubig, 2017": "13875246"}}}, {"token_start": 208, "token_end": 247, "char_start": 908, "char_end": 1093, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "52100101_2", "paragraph": "[BOS] Illustration In Fig.\n[BOS] 2 , we contrast transfer learning, multilingual learning and meta-learning using three source language pairs (Fr-En, Es-En and Pt-En) and two target pairs (Ro-En and Lv-En).\n[BOS] Transfer learning trains an NMT system specifically for a source language pair (Es-En) and finetunes the system for each target language pair (RoEn, Lv-En).\n[BOS] Multilingual learning often trains a single NMT system that can handle many different language pairs (Fr-En, Pt-En, Es-En), which may or may not include the target pairs (Ro-En, LvEn).\n[BOS] If not, it finetunes the system for each target pair, similarly to transfer learning.\n[BOS] Both of these however aim at directly solving the source tasks.\n[BOS] On the other hand, meta-learning trains the NMT system to be useful for fine-tuning on various tasks including the source and target tasks.\n[BOS] This is done by repeatedly simulating the learning process on low-resource languages using many high-resource language pairs (Fr-En, Pt-En, Es-En).\n\n", "discourse_tags": ["Reflection", "Reflection", "Transition", "Transition", "Transition", "Transition", "Transition", "Transition"], "span_citation_mapping": []}
{"id": "52100101_1", "paragraph": "[BOS] where D k is the training set of the k-th task, or language pair.\n[BOS] The target low-resource language pair could either be a part of joint training or be trained separately starting from the solution  0 found from solving the above problem.\n[BOS] The major difference between the proposed MetaNMT and these multilingual transfer approaches is that the latter do not consider how learning happens with the target, low-resource language pair.\n[BOS] The former explicitly incorporates the learning process within the framework by simulating it repeatedly in Eq.\n[BOS] (2).\n[BOS] As we will see later in the experiments, this results in a substantial gap in the final performance on the low-resource task.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Transition", "Transition", "Reflection"], "span_citation_mapping": []}
{"id": "52100101_0", "paragraph": "[BOS] The proposed MetaNMT differs from the existing framework of multilingual translation (Lee et al., 2016; Johnson et al., 2016; Gu et al., 2018b) or transfer learning (Zoph et al., 2016) .\n[BOS] The latter can be thought of as solving the following problem:\n\n", "discourse_tags": ["Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 12, "token_end": 38, "char_start": 66, "char_end": 149, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lee et al., 2016;": "10509498", "Johnson et al., 2016;": "6053988", "Gu et al., 2018b)": "3295641"}}}, {"token_start": 39, "token_end": 50, "char_start": 153, "char_end": 190, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zoph et al., 2016)": null}}}]}
{"id": "52119281_2", "paragraph": "[BOS] One of our primary contributions is an extensive invesigation of the efficacy of a typical LSTM-based NMT system when operating at the character-level.\n[BOS] The vast majority of existing studies compare a specialized character-level architecture to a distinct word-level one.\n[BOS] To the best of our knowledge, only a small number of papers have explored running NMT unmodified on character sequences; these include: Luong and Manning (2016) on WMT'15 English-Czech, Wu et al. (2016) on WMT'14 English-German, and Bradbury et al. (2016) on IWSLT German-English.\n[BOS] All report scores that either trail behind or reach parity with word-level models.\n[BOS] Only Wu et al. (2016) compare to word fragment models, which they show to outperform characters by a sizeable margin.\n[BOS] We revisit the question of character-versus fragment-level NMT here, and reach quite different conclusions.\n\n", "discourse_tags": ["Reflection", "Transition", "Narrative_cite", "Transition", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 82, "token_end": 98, "char_start": 425, "char_end": 473, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 99, "token_end": 114, "char_start": 475, "char_end": 516, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Wu et al. (2016)": "3603249"}}}, {"token_start": 116, "token_end": 132, "char_start": 522, "char_end": 569, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Bradbury et al. (2016)": "51559"}}}, {"token_start": 151, "token_end": 176, "char_start": 670, "char_end": 782, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wu et al. (2016)": "3603249"}, "Reference": {}}}]}
{"id": "52119281_1", "paragraph": "[BOS] With the advent of word-fragment approaches, interest in character-level processing fell off, but has recently been reignited with the work of Lee et al. (2017) .\n[BOS] They propose a specialized character-level encoder, connected to an unmodified character-level RNN decoder.\n[BOS] They address the modeling and efficiency challenges of long character sequences using a convolutional layer, max-pooling over time, and highway layers.\n[BOS] We agree with their conclusion that character-level translation is effective, but revisit the question of whether their specific encoder produces a desirable speed-quality tradeoff in the context of a much stronger baseline translation system.\n[BOS] We draw inspiration from their pooling solution for reducing sequence length, along with similar ideas from the speech community (Chan et al., 2016) , when devising fixed-schedule reduction strategies in Section 3.3.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 87, "char_start": 6, "char_end": 440, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Lee et al. (2017)": "10509498"}, "Reference": {}}}, {"token_start": 148, "token_end": 158, "char_start": 809, "char_end": 845, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chan et al., 2016)": "18165915"}}}]}
{"id": "52119281_0", "paragraph": "[BOS] Early work on modeling characters in NMT focused on solving the out-of-vocabulary and softmax bottleneck problems associated with wordlevel models (Ling et al., 2015; Costa-juss and Fonollosa, 2016; Luong and Manning, 2016) .\n[BOS] These took the form of word-boundary-aware hierarchical models, with word-level models delegating to character-level models to generate representations in the encoder and words in the decoder.\n[BOS] Our work will not assume fixed word boundaries are given in advance.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 26, "token_end": 56, "char_start": 136, "char_end": 229, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ling et al., 2015;": "5799549", "Costa-juss\u00e0 and Fonollosa, 2016;": "1712853", "Luong and Manning, 2016)": "13972671"}}}]}
{"id": "52190658_4", "paragraph": "[BOS] There are several approaches to perform constrained translation.\n[BOS] One possibility is including this information in training, but this requires knowing the constraints at training time (Crego et al., 2016) .\n[BOS] Post-processing the hypotheses is another possibility, but this comes with the downside that offline modification of the hypotheses happens out of context.\n[BOS] A third possibility is to do constrained decoding (Hokamp and Liu, 2017; Chatterjee et al., 2017; Hasler et al., 2018; Post and Vilar, 2018 ).\n[BOS] This does not require knowledge of the constraints at training time, and it also allows dynamic changes of the rest of the hypothesis when the constraints are activated.\n[BOS] We perform experiments where the translation is guided online during decoding.\n[BOS] We focus on the case where translation suggestions are to be used when a word in the source sentence matches the source side of a pre-defined dictionary entry.\n[BOS] We show that alignment-assisted transformer-based NMT outperforms standard transformer models in such a task.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Narrative_cite", "Transition", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 24, "token_end": 39, "char_start": 154, "char_end": 215, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Crego et al., 2016)": "16641238"}}}, {"token_start": 75, "token_end": 109, "char_start": 415, "char_end": 525, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hokamp and Liu, 2017;": "15281983", "Chatterjee et al., 2017;": "31481386", "Hasler et al., 2018;": "13663540", "Post and Vilar, 2018": "4936344"}}}]}
{"id": "52190658_3", "paragraph": "[BOS] There is plenty of work on modifying attention models to capture more complex dependencies.\n[BOS] Cohn et al. (2016) introduce structural biases from word-based alignment concepts like fertility and Markov conditioning.\n[BOS] These are internal modifications that leave the model self-contained.\n[BOS] Our modifications introduce alignments as external information to the model.\n[BOS] Arthur et al. (2016) include lexical probabilities to bias attention.\n[BOS] Chen et al. (2016) and Mi et al. (2016) add an extra term dependent on the alignments to the training objective function to guide neural training.\n[BOS] This is only applied during training but not during decoding.\n[BOS] Our work makes use of alignments during training and also during decoding.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Reflection", "Single_summ", "Multi_summ", "Multi_summ", "Reflection"], "span_citation_mapping": [{"token_start": 20, "token_end": 56, "char_start": 104, "char_end": 301, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Cohn et al. (2016)": "1964946"}, "Reference": {}}}, {"token_start": 69, "token_end": 84, "char_start": 391, "char_end": 460, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Arthur et al. (2016)": "10086161"}, "Reference": {}}}, {"token_start": 85, "token_end": 130, "char_start": 467, "char_end": 681, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Chen et al. (2016)": "17078659", "Mi et al. (2016)": "18193214"}, "Reference": {}}}]}
{"id": "52190658_2", "paragraph": "[BOS] Deriving neural models for translation based on the hidden Markov model (HMM) framework can also be found in (Yang et al., 2013; Yu et al., 2017) .\n[BOS] Alignment-based neural models were also applied to perform summarization and morphological inflection (Yu et al., 2016) .\n[BOS] Their work used a monotonous alignment model, where training was done by marginalizing over the alignment hidden variables, which is computationally expensive.\n[BOS] In this work, we use non-monotonous alignment models.\n[BOS] In addition, we train using pre-computed Viterbi alignments which speeds up neural training.\n[BOS] In (Yu et al., 2017) , alignmentbased neural models were used to model alignment and translation from the target to the source side (inverse direction), and a language model was included in addition.\n[BOS] They showed results on a small translation task.\n[BOS] In this work, we present results on translation tasks containing tens of millions of words.\n[BOS] We do not include a language model in any of our systems.\n\n", "discourse_tags": ["Multi_summ", "Single_summ", "Single_summ", "Reflection", "Reflection", "Single_summ", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 37, "char_start": 6, "char_end": 151, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Yang et al., 2013;": "148051", "Yu et al., 2017)": "15816492"}, "Reference": {}}}, {"token_start": 39, "token_end": 93, "char_start": 160, "char_end": 447, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Yu et al., 2016)": "371926"}, "Reference": {}}}, {"token_start": 129, "token_end": 181, "char_start": 613, "char_end": 867, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Yu et al., 2017)": "15816492"}, "Reference": {}}}]}
{"id": "52190658_1", "paragraph": "[BOS] Alignment-based neural models were proposed in (Alkhouli et al., 2016) to perform neural machine translation.\n[BOS] They mainly used feedforward alignment and lexical models in decoding.\n[BOS] Alkhouli and Ney (2017) used recurrent models instead, and presented an attention component biased using external alignment information.\n[BOS] In this work, we explore the use of transformer models in ANMT instead of recurrent models.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 38, "char_start": 6, "char_end": 192, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Alkhouli et al., 2016)": "13419043"}, "Reference": {}}}, {"token_start": 39, "token_end": 64, "char_start": 199, "char_end": 335, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Alkhouli and Ney (2017)": "1077209"}, "Reference": {}}}]}
{"id": "52190658_0", "paragraph": "[BOS] Alignment-based neural models have explicit dependence on the alignment information either at the input or at the output of the network.\n[BOS] They have been extensively and successfully applied on top of conventional phrase-based systems (Sundermeyer et al., 2014; Tamura et al., 2014; Devlin et al., 2014) .\n[BOS] In this work, we focus on using the models directly to perform standalone neural machine translation.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 37, "token_end": 69, "char_start": 211, "char_end": 313, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sundermeyer et al., 2014;": "13631598", "Tamura et al., 2014;": "9316168", "Devlin et al., 2014)": "7417943"}}}]}
{"id": "52111191_1", "paragraph": "[BOS] Seq2seq using Side Information: In Neural Machine Translation (NMT) field, recent work (Zhang et al., 2018 ) explored modifications to the decoder of seq2seq models to improve translation results.\n[BOS] They used a search engine to retrieve sentences and their translation (referred to as translation pieces) that have high similarity with the source sentence.\n[BOS] When similar n-grams from a source document were found in the translation pieces, they rewarded the presence of those ngrams during the decoding process through a scoring mechanism calculating the similarity between source sentence and the source side of the translation pieces.\n[BOS] Zhang et al. (2018) reported improvements in translation results up to 6 BLEU points over their seq2seq NMT baseline.\n[BOS] In this paper we use the same principle and reward n-grams that are found in the source document during the AMRto-Text generation process.\n[BOS] However we use a simpler approach using a probabilistic language model in the scoring mechanism.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 10, "token_end": 122, "char_start": 41, "char_end": 651, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Zhang et al., 2018": "4698173"}, "Reference": {}}}, {"token_start": 123, "token_end": 150, "char_start": 658, "char_end": 775, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhang et al. (2018)": "4698173"}, "Reference": {}}}]}
{"id": "52111191_0", "paragraph": "[BOS] Abstractive Summarization using AMR: In Liu et al. (2015) work, the source document's sentences were parsed into AMR graphs, which were then combined through merging, collapsing and graph expansion into a single AMR graph representing the source document.\n[BOS] Following this, a summary AMR graph was extracted, from which a bag of concept words was obtained without attempting to form fluent text.\n[BOS] Vilca and Cabezudo (2017) performed a summary AMR graph extraction augmented with discourse-level information and the PageRank (Page et al., 1998) algorithm.\n[BOS] For text generation, Vilca and Cabezudo (2017) used a rule-based syntactic realizer (Gatt and Reiter, 2009 ) which requires substantial human input to perform adequately.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 57, "char_start": 6, "char_end": 261, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Liu et al. (2015)": "5001921"}, "Reference": {}}}, {"token_start": 87, "token_end": 124, "char_start": 412, "char_end": 569, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Page et al., 1998)": null}}}, {"token_start": 125, "token_end": 165, "char_start": 576, "char_end": 746, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Gatt and Reiter, 2009": "14547126"}}}]}
{"id": "52186560_3", "paragraph": "[BOS] Our work is related to research areas on deep learning models for program induction and question answering from a knowledge base (Neelakantan et al., 2016; Liang et al., 2017; .\n[BOS] Neelakantan et al. (2016) solve the problem of semantic parsing from structured data and generate programs using pre-defined arithmetic operations.\n[BOS] Liang et al. (2017) design a set of executable operators and obtain the answers by the generated logic forms.\n[BOS] design a set of operators to generate the latent program for math problem solving.\n[BOS] However, data-to-text is a different task.\n[BOS] The operations for these methods are designed to find the answers, while we use the operations to guide the process of generation.\n\n", "discourse_tags": ["Reflection", "Single_summ", "Single_summ", "Single_summ", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 17, "token_end": 40, "char_start": 94, "char_end": 180, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Neelakantan et al., 2016;": "6715185"}}}, {"token_start": 43, "token_end": 72, "char_start": 190, "char_end": 337, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Neelakantan et al. (2016)": "6715185"}, "Reference": {}}}, {"token_start": 73, "token_end": 96, "char_start": 344, "char_end": 453, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Liang et al. (2017)": "2742513"}, "Reference": {}}}]}
{"id": "52186560_2", "paragraph": "[BOS] In the field of neural text generation, Mei et al. (2016) uses a neural encoder-decoder approach for end-to-end training.\n[BOS] Some have focused on conditional language generation based on tables (Yang et al., 2017) , short biographies generation from Wikipedia tables (Lebret et al., 2016; Chisholm et al., 2017) and comments generation based on stock prices (Murakami et al., 2017) .\n[BOS] However, none of these methods consider incorporating the facts that can be inferred from the input data to guide the process of generation.\n[BOS] Murakami et al. (2017) post-process the price by extending the copy mechanism and replacing numerical values with defined arithmetic operations after generation.\n[BOS] While our model, OpAtt utilizes information from pre-computed operations on raw data to guide the generation.\n\n", "discourse_tags": ["Single_summ", "Narrative_cite", "Transition", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 10, "token_end": 33, "char_start": 46, "char_end": 127, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Mei et al. (2016)": "1354459"}, "Reference": {}}}, {"token_start": 38, "token_end": 52, "char_start": 155, "char_end": 222, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yang et al., 2017)": "1899153"}}}, {"token_start": 53, "token_end": 80, "char_start": 225, "char_end": 320, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lebret et al., 2016;": "1238927", "Chisholm et al., 2017)": "14633379"}}}, {"token_start": 81, "token_end": 97, "char_start": 325, "char_end": 390, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Murakami et al., 2017)": "28939987"}}}, {"token_start": 125, "token_end": 155, "char_start": 546, "char_end": 707, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Murakami et al. (2017)": "28939987"}, "Reference": {}}}]}
{"id": "52186560_1", "paragraph": "[BOS] Recent work avoids the distinction of the content selection and sentence realization.\n[BOS] Chen and Mooney (2008) use an SMT based approach to learn alignments between comments and their corresponding event records.\n[BOS] Angeli et al. (2010) transform the problem into a sequence of local decisions using a log-linear model.\n[BOS] Konstas and Lapata (2012) employ a PCFG to simultaneously optimize the content selection and surface realization problem.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 16, "token_end": 40, "char_start": 98, "char_end": 222, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Chen and Mooney (2008)": "2488088"}, "Reference": {}}}, {"token_start": 41, "token_end": 65, "char_start": 229, "char_end": 332, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Angeli et al. (2010)": "13402912"}, "Reference": {}}}, {"token_start": 66, "token_end": 90, "char_start": 339, "char_end": 460, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Konstas and Lapata (2012)": null}, "Reference": {}}}]}
{"id": "52186560_0", "paragraph": "[BOS] Data-to-text generation is a task of natural language generation (NLG) (Gatt and Krahmer, 2018) .\n[BOS] Previous research has focused on individual content selection (Kukich, 1983; Reiter and Dale, 1997; Dubou and McKeown, 2003; Barzilay and Lapata, 2005) and surface realization (Goldberg et al., 1994; Soricut and Marcu, 2006; Wong and Mooney, 2007) .\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 12, "token_end": 29, "char_start": 43, "char_end": 101, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Gatt and Krahmer, 2018)": "16946362"}}}, {"token_start": 36, "token_end": 72, "char_start": 143, "char_end": 261, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kukich, 1983;": "10056961", "Reiter and Dale, 1997;": "8460470", "Dubou\u00e9 and McKeown, 2003;": "2120088", "Barzilay and Lapata, 2005)": "9482302"}}}, {"token_start": 73, "token_end": 99, "char_start": 266, "char_end": 357, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Goldberg et al., 1994;": null, "Soricut and Marcu, 2006;": "934325", "Wong and Mooney, 2007)": "799077"}}}]}
{"id": "52895001_0", "paragraph": "[BOS] Other proposals exist other than the three we analyzed that expand on features in SQuAD (Rajpurkar et al., 2016) .\n[BOS] For example, maintaining question independence of context to reduce the role of string matching and having long context length (Joshi et al., 2017; Kocisk et al., 2017) , higher level reasoning (Khashabi et al., 2018; Yang et al., 2018) , multi-turn information seeking interactions, in either table settings (Iyyer et al., 2017; Talmor and Berant, 2018; Saha et al., 2018) , regulation settings (Saeidi et al., 2018) , or Quiz Bowl settings (Elgohary et al., 2018) .\n[BOS] Other work considers multi-modal contexts where interactions are a single turn (Tapaswi et al., 2016; Antol et al., 2015; Lei et al., 2018) or multi-turn (Das et al., 2017; Pasunuru and Bansal, 2018) .\n[BOS] These efforts contain alternative challenges than ones we analyze in this paper.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 16, "token_end": 28, "char_start": 88, "char_end": 118, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rajpurkar et al., 2016)": "11816014"}}}, {"token_start": 46, "token_end": 68, "char_start": 227, "char_end": 295, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Joshi et al., 2017;": "26501419", "Kocisk\u00fd et al., 2017)": "2593903"}}}, {"token_start": 69, "token_end": 89, "char_start": 298, "char_end": 363, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Khashabi et al., 2018;": "5112038", "Yang et al., 2018)": "52822214"}}}, {"token_start": 90, "token_end": 126, "char_start": 366, "char_end": 500, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Iyyer et al., 2017;": "2623009", "Talmor and Berant, 2018;": null, "Saha et al., 2018)": "19240019"}}}, {"token_start": 127, "token_end": 138, "char_start": 503, "char_end": 544, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Saeidi et al., 2018)": "52165754"}}}, {"token_start": 140, "token_end": 156, "char_start": 550, "char_end": 592, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Elgohary et al., 2018)": "52228931"}}}, {"token_start": 169, "token_end": 196, "char_start": 668, "char_end": 740, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Tapaswi et al., 2016;": "1017389", "Antol et al., 2015;": "3180429", "Lei et al., 2018)": "52171684"}}}, {"token_start": 197, "token_end": 218, "char_start": 744, "char_end": 800, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Das et al., 2017;": "64711781", "Pasunuru and Bansal, 2018)": null}}}]}
{"id": "52099904_1", "paragraph": "[BOS] Adversarial Neural Networks have been successfully applied to various cross-lingual NLP tasks where annotated data is not available, such as cross-lingual text classification (Chen et al., 2016) , unsupervised BWE induction (Zhang et al., 2017; Lample et al., 2018b) and unsupervised machine translation (Lample et al., 2018a; Artetxe et al., 2018b) .\n[BOS] These works, however, only consider the case of two languages, and our MAT method ( 3.1) is a generalization to multiple languages.\n[BOS] Mikolov et al. (2013a) first propose to learn cross-lingual word representations by learning a linear mapping between the monolingual embedding spaces of a pair of languages.\n[BOS] It has then been observed that enforcing the linear mapping to be orthogonal could significantly improve performance (Xing et al., 2015; Artetxe et al., 2016; Smith et al., 2017) .\n[BOS] These methods solve a linear equation called the orthogonal Procrustes problem for the optimal orthogonal linear mapping between two languages, given a set of word pairs as supervision.\n[BOS] Artetxe et al. (2017) find that when using weak supervision (e.g. digits in both languages), applying this Procrustes process iteratively achieves higher performance.\n[BOS] Lample et al. (2018b) adopt the iterative Procrustes method with pseudo-supervision in a fully unsupervised setting and also obtain good results.\n[BOS] In the MWE task, however, the multilingual mappings no longer have a closed-form solution, and we hence propose the MPSR algorithm ( 3.2) for learning multilingual embeddings using gradient-based optimization methods.\n\n", "discourse_tags": ["Narrative_cite", "Reflection", "Single_summ", "Multi_summ", "Transition", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 28, "token_end": 42, "char_start": 147, "char_end": 200, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chen et al., 2016)": "9387600"}}}, {"token_start": 43, "token_end": 64, "char_start": 203, "char_end": 272, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhang et al., 2017;": "26873455", "Lample et al., 2018b)": "3470398"}}}, {"token_start": 65, "token_end": 88, "char_start": 277, "char_end": 355, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lample et al., 2018a;": "3518190", "Artetxe et al., 2018b)": "3515219"}}}, {"token_start": 120, "token_end": 157, "char_start": 502, "char_end": 676, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Mikolov et al. (2013a)": "1966640"}, "Reference": {}}}, {"token_start": 164, "token_end": 201, "char_start": 714, "char_end": 861, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Xing et al., 2015;": "3144258", "Artetxe et al., 2016;": "1040556", "Smith et al., 2017)": "11591887"}}}, {"token_start": 236, "token_end": 273, "char_start": 1062, "char_end": 1228, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Artetxe et al. (2017)": "13335042"}, "Reference": {}}}, {"token_start": 274, "token_end": 305, "char_start": 1235, "char_end": 1380, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Lample et al. (2018b)": "3470398"}, "Reference": {}}}]}
{"id": "52099904_0", "paragraph": "[BOS] There is a plethora of literature on learning crosslingual word representations, focusing either on a pair of languages, or multiple languages at the same time (Klementiev et al., 2012; Zou et al., 2013; Mikolov et al., 2013a; Gouws et al., 2015; Coulmance et al., 2015; Ammar et al., 2016; Duong et al., 2017, inter alia) .\n[BOS] One shortcoming of these methods is the dependence on crosslingual supervision such as parallel corpora or bilingual lexica.\n[BOS] Abundant research efforts have been made to alleviate such dependence (Vuli and Moens, 2015; Artetxe et al., 2017; Smith et al., 2017) , but consider only the case of a single pair of languages (BWEs).\n[BOS] Furthermore, fully unsupervised methods exist for learning BWEs (Zhang et al., 2017; Lample et al., 2018b; Artetxe et al., 2018a) .\n[BOS] For unsupervised MWEs, however, previous methods merely rely on a number of independent BWEs to separately map each language into the embedding space of a chosen target language (Smith et al., 2017; Lample et al., 2018b) .\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Narrative_cite", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 21, "token_end": 99, "char_start": 108, "char_end": 328, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Klementiev et al., 2012;": "6758088", "Zou et al., 2013;": "931054", "Mikolov et al., 2013a;": "1966640", "Gouws et al., 2015;": "7021865", "Coulmance et al., 2015;": null, "Ammar et al., 2016;": "1227830"}}}, {"token_start": 134, "token_end": 177, "char_start": 527, "char_end": 669, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vuli\u0107 and Moens, 2015;": "14183678", "Artetxe et al., 2017;": "13335042", "Smith et al., 2017)": "11591887"}}}, {"token_start": 180, "token_end": 215, "char_start": 689, "char_end": 805, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhang et al., 2017;": "26873455", "Lample et al., 2018b;": "3470398", "Artetxe et al., 2018a)": "21728524"}}}, {"token_start": 230, "token_end": 266, "char_start": 880, "char_end": 1034, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Smith et al., 2017;": "11591887", "Lample et al., 2018b)": "3470398"}}}]}
{"id": "52100894_3", "paragraph": "[BOS] There are also some work share a somehow simillar idea with our work: character-level NMT (Chung et al., 2016; Lee et al., 2016) and chunkbased NMT Ishiwatari et al., 2017) .\n[BOS] Unlike the SAT, these models are not able to produce multiple tokens (characters or words) each time step.\n[BOS] Oda et al. (2017) proposed a bitlevel decoder, where a word is represented by a binary code and each bit of the code can be predicted in parallel.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Single_summ"], "span_citation_mapping": [{"token_start": 17, "token_end": 37, "char_start": 76, "char_end": 134, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chung et al., 2016;": "13495961", "Lee et al., 2016)": "10509498"}}}, {"token_start": 38, "token_end": 52, "char_start": 139, "char_end": 178, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Ishiwatari et al., 2017)": "38048605"}}}, {"token_start": 77, "token_end": 112, "char_start": 300, "char_end": 446, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Oda et al. (2017)": "13266502"}, "Reference": {}}}]}
{"id": "52100894_2", "paragraph": "[BOS] A related study on realistic speech synthesis is the parallel WaveNet (Oord et al., 2017) .\n[BOS] The paper introduced probability density distillation, a new method for training a parallel feed-forward network from a trained WaveNet (Van Den Oord et al., 2016) with no significant difference in quality.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 11, "token_end": 23, "char_start": 59, "char_end": 95, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Oord et al., 2017)": "27706557"}}}, {"token_start": 38, "token_end": 61, "char_start": 176, "char_end": 267, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Van Den Oord et al., 2016)": "6254678"}}}]}
{"id": "52100894_1", "paragraph": "[BOS] The most relevant to our proposed semiautoregressive model is (Kaiser et al., 2018) .\n[BOS] They first autoencode the target sequence into a shorter sequence of discrete latent variables, which at inference time is generated autoregressively, and finally decode the output sequence from this shorter latent sequence in parallel.\n[BOS] What we have in common with their idea is that we have not entirely abandoned autoregressive, but rather shortened the autoregressive path.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 64, "char_start": 6, "char_end": 334, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Kaiser et al., 2018)": "4720016"}, "Reference": {}}}]}
{"id": "52100894_0", "paragraph": "[BOS] Almost all state-of-the-art NMT models are autoregressive Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017) , meaning that the model generates words one by one and is not friendly to modern hardware optimized for parallel execution.\n[BOS] A recent work (Gu et al., 2017) attempts to accelerate generation by introducing a non-autoregressive model.\n[BOS] Based on the Transformer (Vaswani et al., 2017) , they made lots of modifications.\n[BOS] The most significant modification is that they avoid feeding the previously generated target words to the decoder, but instead feeding the source words, to predict the next target word.\n[BOS] They also introduced a set of latent variables to model the fertilities of source words to tackle the multimodality problem in translation.\n[BOS] Lee et al. (2018) proposed another non-autoregressive sequence model based on iterative refinement.\n[BOS] The model can be viewed as both a latent variable model and a conditional denoising autoencoder.\n[BOS] They also proposed a learning algorithm that is hybrid of lower-bound maximization and reconstruction error minimization.\n\n", "discourse_tags": ["Multi_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 15, "token_end": 42, "char_start": 49, "char_end": 124, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Wu et al., 2016;": "3603249", "Gehring et al., 2017;": "3648736", "Vaswani et al., 2017)": "13756489"}}}, {"token_start": 68, "token_end": 170, "char_start": 270, "char_end": 791, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Gu et al., 2017)": null}, "Reference": {"(Vaswani et al., 2017)": "13756489"}}}, {"token_start": 171, "token_end": 231, "char_start": 798, "char_end": 1128, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Lee et al. (2018)": "3438497"}, "Reference": {}}}]}
{"id": "52913933_2", "paragraph": "[BOS] More recently, Boratko et al. (2018) developed a labeling interface to obtain high quality labels for the ARC dataset.\n[BOS] One interesting finding is that human annotators tend to retrieve better evidence after they reformulate the search queries which are originally constructed by a simple concatenation of question and answer choice.\n[BOS] By feeding the evidence obtained by human-reformulated queries into a pre-trained MRC model (i.e. DrQA (Chen et al., 2017) ) they achieved an accuracy increase of 42% on a subset of 47 questions.\n[BOS] This shows the potential for a \"human-like\" retriever to boost performance on this task.\n[BOS] Inspired by this work, we focus on selecting essential terms to reformulate more efficient queries, similar to those that a human would construct.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 5, "token_end": 134, "char_start": 21, "char_end": 641, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Boratko et al. (2018)": null}, "Reference": {"(Chen et al., 2017)": null}}}]}
{"id": "52913933_1", "paragraph": "[BOS] Scientific Question Answering (SQA) is a representative open-domain task that requires capability in both retrieval and reading comprehension.\n[BOS] In this paper, we study question answering on the AI2 Reasoning Challenge (ARC) scientific QA dataset .\n[BOS] This dataset contains elementary-level multiple-choice scientific questions from standardized tests and a large corpus of relevant information gathered from search engines.\n[BOS] The dataset is partitioned into \"Challenge\" and \"Easy\" sets.\n[BOS] The challenge set consists of questions that cannot be answered correctly by any of the solvers based on Pointwise Mutual Information (PMI) or Information Retrieval (IR).\n[BOS] Existing models tend to achieve only slightly better and sometimes even worse performance than random guessing, which demonstrates that existing models are not well suited to this kind of QA task.\n[BOS] Khashabi et al. (2017) worked on the problem of finding essential terms in a question for solving SQA problems.\n[BOS] They handcrafted over 100 features and used an SVM classifier to uncover essential terms within a question.\n[BOS] They also published a dataset containing over 2,200 science questions annotated with essential terms.\n[BOS] We leverage this dataset to build an essential term selector.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Transition", "Transition", "Transition", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 160, "token_end": 225, "char_start": 891, "char_end": 1224, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Khashabi et al. (2017)": "33698356"}, "Reference": {}}}]}
{"id": "52913933_0", "paragraph": "[BOS] There has recently been growing interest in building better retrievers for open-domain QA.\n[BOS] proposed a Reinforced Ranker-Reader model that ranks retrieved evidence and assigns different weights to evidence prior to processing by the reader.\n[BOS] Min et al. (2018) demonstrated that for several popular MRC datasets (e.g. SQuAD, TriviaQA) most questions can be answered using only a few sentences rather than the entire document.\n[BOS] Motivated by this observation, they built a sentence selector to gather this potential evidence for use by the reader model.\n[BOS] Nishida et al. (2018) developed a multi-task learning (MTL) method for a retriever and reader in order to obtain a strong retriever that considers certain passages including the answer text as positive samples during training.\n[BOS] The proposed MTL framework is still limited to the scenario when it is feasible to discover whether the passages contain the answer span.\n[BOS] Although these works have achieved progress on open-domain QA by improving the ranking or selection of given evidence, few have focused on the scenario where the model needs to start by searching for the evidence itself.\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 46, "token_end": 112, "char_start": 258, "char_end": 571, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Min et al. (2018)": "29161506"}, "Reference": {}}}, {"token_start": 113, "token_end": 186, "char_start": 578, "char_end": 948, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Nishida et al. (2018)": "52145665"}, "Reference": {}}}]}
{"id": "52221821_2", "paragraph": "[BOS] The design of recurrent networks, such as SRU and related architectures, raises questions about representational power and interpretability (Chen et al., 2018; Peng et al., 2018) .\n[BOS] Balduzzi and Ghifary (2016) applies type-preserving transformations to discuss the capacity of various simplified RNN architectures.\n[BOS] Recent work (Anselmi et al., 2015; Daniely et al., 2016; Lei et al., 2017) relates the capacity of neural networks to deep kernels.\n[BOS] We empirically demonstrate SRU can achieve compelling results by stacking multiple layers.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Multi_summ", "Reflection"], "span_citation_mapping": [{"token_start": 19, "token_end": 40, "char_start": 102, "char_end": 184, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Peng et al., 2018)": "52114454"}}}, {"token_start": 42, "token_end": 68, "char_start": 193, "char_end": 325, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 71, "token_end": 106, "char_start": 344, "char_end": 463, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Anselmi et al., 2015;": "7621349", "Lei et al., 2017)": "12975952"}}}]}
{"id": "52221821_1", "paragraph": "[BOS] Various strategies have been proposed to scale network training (Goyal et al., 2017) and to speed up recurrent networks (Diamos et al., 2016; Kuchaiev and Ginsburg, 2017) .\n[BOS] For instance, Diamos et al. (2016) utilize hardware infrastructures by stashing RNN parameters on cache (or fast memory).\n[BOS] and Kuchaiev and Ginsburg (2017) improve the computation via conditional computing and matrix factorization respectively.\n[BOS] Our implementation for SRU is inspired by the cuDNNoptimized LSTM (Appleyard et al., 2016) , but enables more parallelism -while cuDNN LSTM requires six optimization steps, SRU achieves more significant speed-up via two optimizations.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 8, "token_end": 20, "char_start": 47, "char_end": 90, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 22, "token_end": 45, "char_start": 98, "char_end": 176, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Kuchaiev and Ginsburg, 2017)": "3570621"}}}, {"token_start": 50, "token_end": 76, "char_start": 199, "char_end": 306, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 78, "token_end": 99, "char_start": 317, "char_end": 434, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kuchaiev and Ginsburg (2017)": "3570621"}, "Reference": {}}}, {"token_start": 109, "token_end": 127, "char_start": 487, "char_end": 531, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "52221821_0", "paragraph": "[BOS] Improving on common architectures for sequence processing has recently received significant attention (Greff et al., 2017; Balduzzi and Ghifary, 2016; Miao et al., 2016; Zoph and Le, 2016; Lee et al., 2017) .\n[BOS] One area of research involves incorporating word-level convolutions (i.e. n-gram filters) into recurrent computation (Lei et al., 2015; Bradbury et al., 2017; Lei et al., 2017) .\n[BOS] For example, Quasi-RNN (Bradbury et al., 2017) proposes to alternate convolutions and a minimalist recurrent pooling function and achieves significant speed-up over LSTM.\n[BOS] While Bradbury et al. (2017) focus on the speed advantages of the network, Lei et al. (2017) study the theoretical characteristics of such computation and possible extensions.\n[BOS] Their results suggest that simplified recurrence retains strong modeling capacity through layer stacking.\n[BOS] This finding motivates the design of SRU for both high parallelization and representational power.\n[BOS] SRU also relates to IRNN (Le et al., 2015) , which uses an identity diagonal matrix to initialize hidden-to-hidden connections.\n[BOS] SRU uses point-wise multiplication for hidden connections, which is equivalent to using a diagonal weight matrix.\n[BOS] This can be seen as a constrained version of diagonal initialization.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Single_summ", "Multi_summ", "Transition", "Transition", "Single_summ", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 4, "token_end": 55, "char_start": 19, "char_end": 212, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Miao et al., 2016;": "14215400", "Zoph and Le, 2016;": "12713052"}}}, {"token_start": 79, "token_end": 104, "char_start": 316, "char_end": 397, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lei et al., 2015;": "2146847", "Lei et al., 2017)": "12975952"}}}, {"token_start": 109, "token_end": 144, "char_start": 419, "char_end": 576, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 146, "token_end": 162, "char_start": 589, "char_end": 656, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 163, "token_end": 181, "char_start": 658, "char_end": 758, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Lei et al. (2017)": "12975952"}, "Reference": {}}}, {"token_start": 217, "token_end": 248, "char_start": 982, "char_end": 1109, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "52577367_0", "paragraph": "[BOS] Due to the increasing demand for model interpretability, many previous works have been proposed for making sense of NLP models by examining individual predictions as well as the model mechanism as a whole.\n[BOS] In recent work, Li et al. (2015) investigated the composability of the vector-based text representations using instancelevel attribution techniques that originated from the vision community (e.g., Zeiler and Fergus, 2014) .\n[BOS] In a study of the representation of erasure, Li et al. (2016) explained neural model decisions by exploring the impact of altering or removing the components of the model (i.e., changing the dimension count of hidden units or input words) on the prediction performance.\n[BOS] Besides interpreting the model via carefully designed experiments, several interactive demo/visualization systems, such as AllenNLP's demos (http://demo.allennlp.org/), often rely on visual encodings to summarize the model predictions.\n[BOS] These systems provide a flexible environment in which the user can experiment with the various inputs and perform error analysis.\n[BOS] The hidden state properties of the LSTM are visualized and investigated in the LSTMvis visualization system (Strobelt et al., 2018) .\n[BOS] Lee et al. (2017) visualized the beam search and attention component in neural machine translation models, in which the user can dynamically change the probability for the next step of the search tree or change the weight of the attention.\n[BOS] In the visualization work on question answering (Rckl and Gurevych, 2017) , the system shows the text context and highlights the critical phrase that is used to answer the question.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Transition", "Transition", "Narrative_cite", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 44, "token_end": 87, "char_start": 234, "char_end": 439, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Li et al. (2015)": "14099741"}, "Reference": {"Zeiler and Fergus, 2014)": "3960646"}}}, {"token_start": 94, "token_end": 146, "char_start": 466, "char_end": 717, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Li et al. (2016)": "13017314"}, "Reference": {}}}, {"token_start": 238, "token_end": 253, "char_start": 1181, "char_end": 1233, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Strobelt et al., 2018)": "25127323"}}}, {"token_start": 255, "token_end": 300, "char_start": 1242, "char_end": 1481, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Lee et al. (2017)": "9549525"}, "Reference": {}}}, {"token_start": 306, "token_end": 338, "char_start": 1517, "char_end": 1669, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(R\u00fcckl\u00e9 and Gurevych, 2017)": "20020443"}, "Reference": {}}}]}
{"id": "52013416_5", "paragraph": "[BOS] Different from the latest strong syntax-agnostic models in and (He et al., 2018) which both adopted sequence labeling formulization for the SRL task, this work adopts word pair classification scheme implemented by LSTM encoder and biaffine scorer.\n[BOS] Compared to the previous state-of-the-art syntax-agnostic model in (He et al., 2018) whose performance boosting (more than 1% absolute gain) is mostly due to introducing the enhanced representation, namely, the CNNBiLSTM character embedding from (Peters et al., 2018) , our performance promotion mainly roots from model architecture improvement, which results in quite different syntax-aware enhanced impacts.\n[BOS] Using the same latest syntax-aware k-order pruning, the syntax-agnostic backbone in (He et al., 2018 ) may receive about 1% performance gain, while our model is furthermore enhanced little.\n[BOS] This comparison also suggests the possibility that maybe our model can be further improved by incorporating with the same character embedding as (He et al., 2018 ) does 5 .\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 14, "token_end": 34, "char_start": 69, "char_end": 154, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(He et al., 2018)": "51879191"}}}, {"token_start": 66, "token_end": 117, "char_start": 302, "char_end": 527, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(He et al., 2018)": "51879191"}, "Reference": {"(Peters et al., 2018)": "3626819"}}}, {"token_start": 153, "token_end": 174, "char_start": 732, "char_end": 816, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(He et al., 2018": "51879191"}}}, {"token_start": 203, "token_end": 213, "char_start": 994, "char_end": 1033, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(He et al., 2018": "51879191"}}}]}
{"id": "52013416_4", "paragraph": "[BOS] This work is also closely related to the attentional mechanism.\n[BOS] The traditional attention mechanism was proposed by Bahdanau et al. (2015) in the NMT literature.\n[BOS] Following the work (Luong et al., 2015) that encouraged substituting the MLP in the attentional mechanism with a single bilinear transformation, Dozat and Manning (2017) introduced the bias terms into the primitive form of bilinear attention and applied it for dependency parsing.\n[BOS] They demonstrate that the bias terms help their model to capture the uneven prior distribution of the data, which is again verified by our practice on SRL in this paper.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Multi_summ", "Multi_summ"], "span_citation_mapping": [{"token_start": 15, "token_end": 36, "char_start": 80, "char_end": 173, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Bahdanau et al. (2015)": "61556494"}}}, {"token_start": 40, "token_end": 63, "char_start": 199, "char_end": 323, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Luong et al., 2015)": "1998416"}, "Reference": {}}}, {"token_start": 64, "token_end": 125, "char_start": 325, "char_end": 636, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Dozat and Manning (2017)": "7942973"}, "Reference": {}}}]}
{"id": "52013416_3", "paragraph": "[BOS] However, almost all of previous works treated the predicate disambiguation as individual subtasks, apart from (Zhao and Kit, 2008; Zhao et al., 2009a; Zhao et al., 2009c; Zhao et al., 2013) , who presented the first end-to-end system for dependency SRL.\n[BOS] For the neural models of dependency SRL, we have presented the first end-to-end solution that handles both semantic labeling subtasks in one single model.\n[BOS] At the same time, our model enjoys the advantage that does not rely any syntactic information.\n\n", "discourse_tags": ["Narrative_cite", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 21, "token_end": 67, "char_start": 116, "char_end": 259, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhao and Kit, 2008;": "8657922", "Zhao et al., 2009a;": "11146492", "Zhao et al., 2009c;": "1850092", "Zhao et al., 2013)": "1239326"}}}]}
{"id": "52013416_2", "paragraph": "[BOS] Besides the above-mentioned works who relied on syntactic information, several works attempted to build SRL systems without or with little syntactic information.\n[BOS] Zhou and Xu (2015) came up with an end-to-end model for span-based SRL and obtained surprising performance putting syntax aside.\n[BOS] He et al. (2017) further extended their work with the highway network.\n[BOS] Simultaneously, proposed a syntax-agnostic model with effective word representation for dependency-based SRL.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 30, "token_end": 60, "char_start": 174, "char_end": 302, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhou and Xu (2015)": "12688069"}, "Reference": {}}}, {"token_start": 61, "token_end": 77, "char_start": 309, "char_end": 379, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"He et al. (2017)": "33626727"}, "Reference": {}}}]}
{"id": "52013416_1", "paragraph": "[BOS] In recent years, applying neural networks in SRL task has gained a lot of attention due to the impressive success of deep neural networks in various NLP tasks (Zhang et al., 2016; Qin et al., 2017; .\n[BOS] Collobert et al. (2011) initially introduced neural networks into the SRL task.\n[BOS] They developed a feed-forward network that employed a convolutional network as sentence encoder and a conditional random field as a role classifier.\n[BOS] Foland and Martin (2015) extended their model to further use syntactic information by including binary indicator features.\n[BOS] FitzGerald et al. (2015) exploited a neural network to unifiedly embed arguments and semantic roles, similar to the work (Lei et al., 2015) which induced a compact feature representation applying tensor-based approach.\n[BOS] Roth and Lapata (2016) introduced the dependency path embedding to incorporate syntax and exhibited a notable success, while employed the graph convolutional network to integrate syntactic information into their neural model.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 25, "token_end": 47, "char_start": 123, "char_end": 202, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhang et al., 2016;": "17566738"}}}, {"token_start": 50, "token_end": 94, "char_start": 212, "char_end": 446, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Collobert et al. (2011)": null}, "Reference": {}}}, {"token_start": 95, "token_end": 116, "char_start": 453, "char_end": 575, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Foland and Martin (2015)": "18951510"}, "Reference": {}}}, {"token_start": 117, "token_end": 139, "char_start": 582, "char_end": 681, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"FitzGerald et al. (2015)": "15048880"}, "Reference": {}}}, {"token_start": 144, "token_end": 164, "char_start": 703, "char_end": 800, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Lei et al., 2015)": "5730387"}, "Reference": {}}}, {"token_start": 165, "token_end": 201, "char_start": 807, "char_end": 1032, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Roth and Lapata (2016)": "5779419"}, "Reference": {}}}]}
{"id": "52013416_0", "paragraph": "[BOS] Semantic role labeling was pioneered by Gildea and Jurafsky (2002) .\n[BOS] Most traditional SRL models heavily rely on complex feature engineering (Pradhan et al., 2005; Zhao et al., 2009a; Bjrkelund et al., 2009) .\n[BOS] Among those early works, Pradhan et al. (2005) combined features derived from different syntactic parses based on SVM classifier, while Zhao et al. (2009a) exploited the abundant set of language-specific features that were carefully designed for SRL task.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Multi_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 19, "char_start": 6, "char_end": 72, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Gildea and Jurafsky (2002)": "62182406"}}}, {"token_start": 29, "token_end": 60, "char_start": 125, "char_end": 219, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Pradhan et al., 2005;": "2440012", "Zhao et al., 2009a;": "11146492", "Bj\u00f6rkelund et al., 2009)": "33777646"}}}, {"token_start": 67, "token_end": 88, "char_start": 253, "char_end": 356, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Pradhan et al. (2005)": "2440012"}, "Reference": {}}}, {"token_start": 90, "token_end": 116, "char_start": 364, "char_end": 483, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhao et al. (2009a)": "11146492"}, "Reference": {}}}]}
{"id": "52938038_2", "paragraph": "[BOS] 2015).\n[BOS] These approaches can be roughly divided into two broad categories: computing the representation of the full document-level context (Jean et al., 2017; Tiedemann and Scherrer, 2017; Wang et al., 2017; Maruf and Haffari, 2018; Voita et al., 2018) and using a cache to memorize most relevant information in the document-level context (Kuang et al., 2017; Tu et al., 2018) .\n[BOS] Our approach falls into the first category.\n[BOS] We use multi-head attention to represent and integrate document-level context.\n[BOS] Voita et al. (2018) also extended Transformer to model document-level context, but our work is different in modeling and training strategies.\n[BOS] The experimental part is also different.\n[BOS] While Voita et al. (2018) focus on anaphora resolution, our model is able to improve the overall translation quality by integrating document-level context.\n\n", "discourse_tags": ["Other", "Narrative_cite", "Reflection", "Reflection", "Single_summ", "Reflection", "Single_summ"], "span_citation_mapping": [{"token_start": 17, "token_end": 68, "char_start": 86, "char_end": 263, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Jean et al., 2017;": "16627807", "Tiedemann and Scherrer, 2017;": "2496355", "Wang et al., 2017;": "9768369", "Maruf and Haffari, 2018;": "21686013", "Voita et al., 2018)": "44062236"}}}, {"token_start": 69, "token_end": 100, "char_start": 268, "char_end": 387, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kuang et al., 2017;": "195346279", "Tu et al., 2018)": "7421176"}}}, {"token_start": 127, "token_end": 156, "char_start": 531, "char_end": 672, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Voita et al. (2018)": "44062236"}, "Reference": {}}}, {"token_start": 166, "token_end": 197, "char_start": 732, "char_end": 881, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Voita et al. (2018)": "44062236"}, "Reference": {}}}]}
{"id": "52938038_1", "paragraph": "[BOS] Most existing work on document-level NMT has focused on integrating document-level context into the RNNsearch model (Bahdanau et al., Context   ziji ye yinwei queshao jingzheng duishou er dui saiche youxie yanjuan shi   Source wo rengran feichang rezhong yu zhexiang yundong.\n[BOS] Reference I'm still very fond of the sport.\n[BOS] Transformer I am still very enthusiastic about this movement.\n[BOS] Our work I am still very keen on this sport.\n[BOS] Table 10 : An example of Chinese-English translation.\n[BOS] In the source sentence, \"yundong\" (sport or political movement) is a multi-sense word and \"rezhong\" (fond of) is an emotional word whose meaning is dependent on its context.\n[BOS] Our model takes advantage of the words \"saiche\" (car racing) and \"yanjuan\" (tired of) in the documentlevel context to translate the source words correctly.\n\n", "discourse_tags": ["Single_summ", "Other", "Other", "Other", "Other", "Other", "Reflection"], "span_citation_mapping": [{"token_start": 21, "token_end": 32, "char_start": 106, "char_end": 138, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "52938038_0", "paragraph": "[BOS] Developing document-level models for machine translation has been an important research direction, both for conventional SMT (Gong et al., 2011; Hardmeier et al., 2012; Xiong et al., 2013a,b; Garcia et al., 2014) and NMT (Jean et al., 2017; Kuang et al., 2017; Tiedemann and Scherrer, 2017; Wang et al., 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Tu et al., 2018; Voita et al., 2018) .\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 19, "token_end": 57, "char_start": 114, "char_end": 218, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Gong et al., 2011;": "13257146", "Hardmeier et al., 2012;": "16112861", "Garcia et al., 2014)": "1045460"}}}, {"token_start": 58, "token_end": 125, "char_start": 223, "char_end": 398, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Jean et al., 2017;": "16627807", "Kuang et al., 2017;": "195346279", "Tiedemann and Scherrer, 2017;": "2496355", "Wang et al., 2017;": "9768369", "Maruf and Haffari, 2018;": "21686013", "Bawden et al., 2018;": "5016370", "Tu et al., 2018;": "7421176", "Voita et al., 2018)": "44062236"}}}]}
{"id": "52009101_2", "paragraph": "[BOS] SQuAD: SQuAD is an MRC dataset accommodating 107,785 question-and-answer (QA) pairs on 536 Wikipedia articles.\n[BOS] A passage is associated with, at most, five QA pairs, and each question has the corresponding answer, forming a span in the associated passage.\n[BOS] The evaluation metrics are exact match (EM) and F1.\n[BOS] An EM score measures the percentage of predictions that match any one of the ground truth answers exactly.\n[BOS] An F1 score is a metric that measures the average overlap between the prediction and the ground truth answer.\n[BOS] Figure 1 , adopted from (Rajpurkar et al., 2016) , exemplifies three QA pairs taken from a paragraph in a Wikipedia article, whose topic is precipitation.\n[BOS] Note that each of the three questions refers to the same passage, and the corresponding answer can be found as a span within the passage.\n[BOS] As detailed in the next section, we create an MRC dataset with NAQs by modifying an existing dataset.\n[BOS] If a good source dataset is properly chosen, this strategy could prevent the shortage of QAs and enjoy the advantages of the existing dataset.\n[BOS] Among the many MRC datasets, we choose SQuAD as the source, because it is a rather large-scaled dataset of the answer-in-the-text style, which functions as a good starting point to pursue the study of genuine machine understanding of a language.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Transition", "Transition", "Single_summ", "Transition", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 124, "token_end": 154, "char_start": 584, "char_end": 714, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Rajpurkar et al., 2016)": "11816014"}, "Reference": {}}}]}
{"id": "52009101_1", "paragraph": "[BOS] WIKIQA: WIKIQA is one of the few MRC datasets that contains questions without answers, which is a main concern of this paper.\n[BOS] WIKIQA was created to capture the characteristics of natural queries asked by people in the real world.\n[BOS] Each question in this dataset was taken from a search engine's actual query log, and the corresponding Wikipedia summary paragraph is employed as a text passage, where each of the contained sentences is treated as an answer candidate.\n[BOS] This means that a machine is only required to infer the positive or negative status of each sentence.\n[BOS] This problem can be often solved by only looking at the questions; the ability of reading comprehension is not necessarily required.\n[BOS] WIKIQA maintains 3,047 questions, of which about two thirds are NAQs, as they never have a positive sentence in the corresponding paragraphs.\n[BOS] Moreover, 20.3% of the answers share no content words with the questions, contributing to the elevated difficulty level of the dataset.\n[BOS] Unfortunately, this dataset is relatively small, and the amount of training data is inevitably limited.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Transition", "Transition", "Transition", "Transition", "Transition"], "span_citation_mapping": []}
{"id": "52009101_0", "paragraph": "[BOS] Recently, several MRC datasets have been developed.\n[BOS] The characteristics of each MRC dataset varies, depending on the purpose.\n[BOS] For example, the Childrens Book Test dataset (Hill et al., 2016) was built from books for children, where the 21st sentence that follows the preceeding 20 sentences is employed as a \"question.\"\n[BOS] Additionally, the CNN/Daily dataset (Chen et al., 2016) collects news articles with bullet-pointed summaries, in which each summary is converted into a question.\n[BOS] Among the many MRC datasets, this section introduces two datasets, WIKIQA (Yang et al., 2015) and SQuAD (Rajpurkar et al., 2016) .\n[BOS] Of these, the latter is used in this work.\n[BOS] Additionally, we address potential problems with SQuAD.\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ", "Single_summ", "Narrative_cite", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 30, "token_end": 70, "char_start": 161, "char_end": 336, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Hill et al., 2016)": "14915449"}, "Reference": {}}}, {"token_start": 75, "token_end": 106, "char_start": 362, "char_end": 505, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Chen et al., 2016)": "6360322"}, "Reference": {}}}, {"token_start": 119, "token_end": 130, "char_start": 579, "char_end": 605, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yang et al., 2015)": "1373518"}}}, {"token_start": 131, "token_end": 143, "char_start": 610, "char_end": 640, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rajpurkar et al., 2016)": "11816014"}}}]}
{"id": "52167541_1", "paragraph": "[BOS] Aharoni and Goldberg (2017), Wu et al. (2017) , and Eriguchi et al. (2017) experimented with NMT models that utilise target side structural syntax.\n[BOS] Aharoni and Goldberg (2017) treated constituency trees as sequential strings (linearised-tree) and trained a Seq2Seq model to produce such sequences.\n[BOS] Wu et al. (2017) proposed SD-NMT, which models dependency syntax trees by adding a shift-reduce neural parser to a standard RNN decoder.\n[BOS] Eriguchi et al. (2017) in addition to Wu et al. (2017) 's work, proposed NMT+RNNG which uses a modified RNNG generator (Dyer et al., 2016) to process dependency instead of constituency information as originally proposed by Dyer et al. (2016) , making it consequently a StackLSTM sequential decoder with additional RNN units so it is still a bottom-up tree-structured decoder rather than a top-down decoder like ours.\n[BOS] Nevertheless, all of these research showed that target side syntax could improve NMT systems.\n[BOS] We believe these models could also be augmented with SynC connections (with NMT+RNNG one has to instead use constituency information).\n\n", "discourse_tags": ["Multi_summ", "Single_summ", "Single_summ", "Multi_summ", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 43, "char_start": 6, "char_end": 153, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wu et al. (2017)": "41550072", "Eriguchi et al. (2017)": "14519034"}, "Reference": {}}}, {"token_start": 44, "token_end": 77, "char_start": 160, "char_end": 309, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Aharoni and Goldberg (2017)": "8078153"}, "Reference": {}}}, {"token_start": 78, "token_end": 111, "char_start": 316, "char_end": 452, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wu et al. (2017)": "41550072"}, "Reference": {}}}, {"token_start": 112, "token_end": 214, "char_start": 459, "char_end": 875, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Eriguchi et al. (2017)": "14519034", "Dyer et al. (2016)": "1949831"}, "Reference": {"Wu et al. (2017)": "41550072", "(Dyer et al., 2016)": "1949831"}}}]}
{"id": "52167541_0", "paragraph": "[BOS] Recent research shows that modelling syntax is useful for various neural NLP tasks.\n[BOS] Dyer et al. (2015 Dyer et al. ( , 2016 ; Vinyals et al. (2015) ; Luong et al. (2016) have works on language modelling and parsing, Tai et al. (2015) on semantic analysis, and Zhang et al. (2016) on sentence completion, etc.\n[BOS] Eriguchi et al. (2017) showed that NMT model can benefit from neural syntactical parsing models.\n[BOS] Choe and Charniak (2016) showed that a neural parsing problem shares similarity to neural language modelling problem, which forms a building block of an NMT system.\n[BOS] We can then make the assumption that structural syntactic information utilised in neural parsing models should be able to aid NMT, which is shown to be true here.\n[BOS] Zhang et al. (2016) proposed TreeLSTM which is another structured neural decoder.\n[BOS] TreeL-STM is not only structurally more complicated but also uses external classifiers.\n[BOS] Dong and Lapata (2016) also proposed a sequence-to-tree (Seq2Tree) model for question answering.\n[BOS] Both of these models are not designed for NMT and lack a language model.\n[BOS] While operate from top-tobottom like Seq2DRNN(+SynC), TreeLSTM and Seq2Tree produce components that lack sequential continuity which we have shown to be nonnegligible for language generation.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Single_summ", "Single_summ", "Transition", "Single_summ", "Transition", "Single_summ", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 18, "token_end": 59, "char_start": 96, "char_end": 225, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Dyer et al. (2015": null, "Dyer et al. ( , 2016": "1949831", "Vinyals et al. (2015)": "14223", "Luong et al. (2016)": "6954272"}}}, {"token_start": 60, "token_end": 70, "char_start": 227, "char_end": 265, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Tai et al. (2015)": "3033526"}}}, {"token_start": 72, "token_end": 82, "char_start": 271, "char_end": 313, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Zhang et al. (2016)": "18436460"}}}, {"token_start": 86, "token_end": 109, "char_start": 326, "char_end": 422, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Eriguchi et al. (2017)": "14519034"}, "Reference": {}}}, {"token_start": 110, "token_end": 144, "char_start": 429, "char_end": 593, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Choe and Charniak (2016)": "81026"}, "Reference": {}}}, {"token_start": 177, "token_end": 195, "char_start": 769, "char_end": 850, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhang et al. (2016)": "18436460"}, "Reference": {}}}, {"token_start": 213, "token_end": 238, "char_start": 951, "char_end": 1047, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Dong and Lapata (2016)": "15412473"}, "Reference": {}}}]}
{"id": "52962051_8", "paragraph": "[BOS] In order to apply S2S models, a list of attributes in an MR has to be linearized into a sequence of tokens (Konstas et al., 2017; Ferreira et al., 2017) .\n[BOS] Not all attributes have to appear for all inputs, and each attribute might have multi-token values, such as area: city centre.\n[BOS] We use special start and stop tokens for each possible attribute to mark value boundaries; for example, an attribute area: city centre becomes start area city centre end area .\n[BOS] These fragments are concatenated into a single sequence to represent the original MR as an input sequence to our models.\n[BOS] In this approach, no values are delexicalized, in contrast to Juraska et al. (2018) and others who delexicalize a subset of attributes.\n[BOS] An alternative approach by Freitag and Roy (2018) treats the attribute type as an additional feature and learn embeddings for words and types separately.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Reflection", "Reflection", "Multi_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 12, "token_end": 46, "char_start": 38, "char_end": 158, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Konstas et al., 2017;": "8066499"}}}, {"token_start": 147, "token_end": 167, "char_start": 672, "char_end": 745, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 172, "token_end": 197, "char_start": 779, "char_end": 905, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "52962051_7", "paragraph": "[BOS] We evaluate the performance of both the LSTM (Hochreiter and Schmidhuber, 1997) and Transformer (Vaswani et al., 2017) architecture.\n[BOS] We additionally experiment with two attention formulations.\n[BOS] The first uses a dot-product between the hidden states of the encoder and decoder (Luong et al., 2015) .\n[BOS] The second uses a multi-layer perceptron with the hidden states as inputs (Bahdanau et al., 2014) .\n[BOS] We refer to them as dot and MLP respectively.\n[BOS] Since dot attention does not require additional parameters, we hypothesize that it performs well in a limited data environment.\n\n", "discourse_tags": ["Reflection", "Reflection", "Narrative_cite", "Narrative_cite", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 9, "token_end": 23, "char_start": 46, "char_end": 85, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 24, "token_end": 35, "char_start": 90, "char_end": 124, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 60, "token_end": 72, "char_start": 273, "char_end": 313, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Luong et al., 2015)": "1998416"}}}, {"token_start": 85, "token_end": 99, "char_start": 372, "char_end": 419, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "52962051_6", "paragraph": "[BOS] S2S aims to learn a distribution parametrized by  to maximize the conditional probability of p  (y|x).\n[BOS] We assume that the target is generated from left to right, such that p  (y|x) = n t=1 p  (y t |y [t1] , x), and that p  (y t |y [t1] , x) takes the form of an encoder-decoder architecture with attention.\n[BOS] The training aims to maximize the log-likelihood of the observed training data.\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "52962051_5", "paragraph": "[BOS] be a set of N aligned source and target sequence pairs, with (x (i) , y (i) ) denoting the ith element in (X , Y) pairs.\n[BOS] Further, let x = x 1 , .\n[BOS] .\n[BOS] .\n[BOS] , x m be the sequence of m tokens in the source, and y = y 1 , .\n[BOS] .\n[BOS] .\n[BOS] , y n the target sequence of length n. Let V be the vocabulary of possible tokens, and [n] the list of integers up to n, [1, .\n[BOS] .\n[BOS] .\n[BOS] , n].\n\n", "discourse_tags": ["Other", "Other", "Other", "Other", "Other", "Other", "Other", "Other", "Other", "Other", "Other"], "span_citation_mapping": []}
{"id": "52962051_4", "paragraph": "[BOS] We start by introducing the standard a text-totext problem and discuss how to map structured data into a sequential form.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "52962051_3", "paragraph": "[BOS] 3 Background: Sequence-to-Sequence Generation\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "52962051_2", "paragraph": "[BOS] (2018) describe a heuristic based on word-overlap that provides unsupervised slot alignment between meaning representations and open slots in sentence plans.\n[BOS] This method allows a model to operate with a smaller vocabulary and to be agnostic to actual values in the meaning representations.\n[BOS] To account for syntactic structure in templates, Su et al. (2018) describe a hierarchical decoding strategy that generates different part of speech at different steps, filling in slots between previously generated tokens.\n[BOS] In contrast, our model uses copyattention to fill in latent slots inside of learned templates.\n[BOS] Juraska et al. (2018) also describe a data selection process in which they use heuristics to filter a dataset to the most natural sounding examples according to a set of rules.\n[BOS] Our work aims at the unsupervised segmentation of data such that one model learns the most natural sounding sentence plans.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Reflection", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 53, "char_start": 6, "char_end": 301, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 55, "token_end": 92, "char_start": 311, "char_end": 529, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Su et al. (2018)": "44091475"}, "Reference": {}}}, {"token_start": 113, "token_end": 150, "char_start": 637, "char_end": 813, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "52962051_1", "paragraph": "[BOS] For the task of text generation from simple keyvalue pairs, as in the E2E task, Juraska et al.\n\n", "discourse_tags": ["Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 27, "char_start": 6, "char_end": 100, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "52962051_0", "paragraph": "[BOS] Traditional approaches to natural language generation separate the generation of a sentence plan from the surface realization.\n[BOS] First, an input is mapped into a format that represents the layout of the output sentence, for example, an adequate pre-defined template.\n[BOS] Then, the surface realization transforms the intermediary structure into text (Stent et al., 2004) .\n[BOS] These representations often model the hierarchical structure of discourse relations (Walker et al., 2007) .\n[BOS] Early data-driven approach used phrase-based language models for generation (Oh and Rudnicky, 2000; Mairesse and Young, 2014) , or aimed to predict the best fitting cluster of semantically similar templates (Kondadadi et al., 2013) .\n[BOS] More recent work combines both steps by learning plan and realization jointly using end-to-end trained models (e.g. Wen et al., 2015) .\n[BOS] Several approaches have looked at generation from abstract meaning representations (AMR), and Peng et al. (2017) apply S2S models to the problem.\n[BOS] However, Ferreira et al. (2017) show that S2S models are outperformed by phrase-based machine translation models in small datasets.\n[BOS] To address this issue, Konstas et al. (2017) propose a semi-supervised training method that can utilize English sentences outside of the training set to train parts of the model.\n[BOS] We address the issue by using copy-attention to enable the model to copy words from the source, which helps to generate out of vocabulary and rare words.\n[BOS] We note that end-to-end trained models, including our approach, often do not explicitly model the sentence planning stage, and are thus not directly comparable to previous work on sentence planning.\n[BOS] This is especially limiting for generation of complex argument structures that rely on hierarchical structure.\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite", "Narrative_cite", "Narrative_cite", "Narrative_cite", "Single_summ", "Single_summ", "Single_summ", "Reflection", "Reflection", "Transition"], "span_citation_mapping": [{"token_start": 53, "token_end": 69, "char_start": 293, "char_end": 381, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Stent et al., 2004)": "1543141"}}}, {"token_start": 76, "token_end": 89, "char_start": 428, "char_end": 495, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Walker et al., 2007)": "2937525"}}}, {"token_start": 97, "token_end": 120, "char_start": 536, "char_end": 629, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Oh and Rudnicky, 2000;": "2425383", "Mairesse and Young, 2014)": "14875383"}}}, {"token_start": 130, "token_end": 144, "char_start": 680, "char_end": 735, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kondadadi et al., 2013)": "13550936"}}}, {"token_start": 159, "token_end": 178, "char_start": 828, "char_end": 877, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Wen et al., 2015)": "739696"}}}, {"token_start": 180, "token_end": 212, "char_start": 886, "char_end": 1031, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Peng et al. (2017)": "7730925"}, "Reference": {}}}, {"token_start": 215, "token_end": 244, "char_start": 1047, "char_end": 1169, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 250, "token_end": 283, "char_start": 1199, "char_end": 1354, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Konstas et al. (2017)": "8066499"}, "Reference": {}}}]}
{"id": "52009850_5", "paragraph": "[BOS] In the source side ,for a word x, HL-EncDec not only utilizes the word-level embedding vector x w obtained by looking up embedding matrix, but also calculates an equivalent word-level embedding vector x c of x.\n[BOS] To calculate the x c , HL-EncDec employs its corresponding character sequence and a Convolutional Neural Network based network.\n[BOS] The Encoder of HL-EncDec maintains two vocabularies V e w and V e c , the first vocabulary V e w records a finite number of words and their embedding vectors, the second vocabulary V e c records all characters of the language used by current system.\n[BOS] In general, the total number of all characters of a language is fixed and small, hence recently GPU devices are very easy to load all characters to its RAM.\n[BOS] For example, there are about 128 characters in English alphabet (ASCII).\n[BOS] Obviously, the concept of out-vocabulary-character has gone with the wind, and each OOV word could be represented as a sequence of fully known characters.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Transition", "Transition", "Transition"], "span_citation_mapping": []}
{"id": "52009850_4", "paragraph": "[BOS] In addition, previous models are designed for machine translation task which is actually a different task compared with response generation while they are both SEQ2SEQ tasks.\n[BOS] This paper proposes a model HL-EncDec, which utilizes the hybrid-level representation technique to improve the generation quality for response generation task.\n\n", "discourse_tags": ["Transition", "Reflection"], "span_citation_mapping": []}
{"id": "52009850_3", "paragraph": "[BOS] While a character-level model or a subword-level model may address the Unknown Words Issue, but it could sharpen the long-term dependencies issue and lost much semantic and syntactic information due to each word must be decomposed to several characters or subunits.\n[BOS] The loss may more than gains, and we consider this is why most models operate sentences at word-level.\n\n", "discourse_tags": ["Transition", "Reflection"], "span_citation_mapping": []}
{"id": "52009850_2", "paragraph": "[BOS] In this paper, we focus on two issues discussed in Section 2: Unknown Words Issue and Preference Issue.\n[BOS] Actually, many researchers have raised their attention to these two issues, and proposed models operate at non-word-level such as character-level and subword-level.\n[BOS] (Kim et al., 2016) solved the representation of unknown words and selection preference for language modelling by utilizing characters to calculate an equivalent word embedding instead of looking up a word embedding matrix.\n[BOS] (Costajussa and Fonollosa, 2016) absorbed this idea and proposed a character-level Encoder for machine translation.\n[BOS] Recently, inspired by (Kim et al., 2016; Costajussa and Fonollosa, 2016) , (Lee et al., 2016 ) introduced a fully character-level EncDec model.\n[BOS] (Sennrich et al., 2016) mined subword units to represent all words by applying BPE algorithm.\n[BOS] (Chung et al., 2016) utilized the BPE to build a subword-level Encoder and proposed a character-level Decoder.\n\n", "discourse_tags": ["Reflection", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 59, "token_end": 97, "char_start": 287, "char_end": 509, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Kim et al., 2016)": null}, "Reference": {}}}, {"token_start": 98, "token_end": 124, "char_start": 516, "char_end": 631, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Costajussa and Fonollosa, 2016)": null}, "Reference": {}}}, {"token_start": 129, "token_end": 136, "char_start": 660, "char_end": 677, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 137, "token_end": 148, "char_start": 679, "char_end": 710, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Costajussa and Fonollosa, 2016)": null}}}, {"token_start": 149, "token_end": 167, "char_start": 713, "char_end": 781, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Lee et al., 2016": "10509498"}, "Reference": {}}}, {"token_start": 168, "token_end": 193, "char_start": 788, "char_end": 881, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Sennrich et al., 2016)": null}, "Reference": {}}}, {"token_start": 194, "token_end": 222, "char_start": 888, "char_end": 998, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Chung et al., 2016)": "13495961"}, "Reference": {}}}]}
{"id": "52009850_1", "paragraph": "[BOS] The word-level EncDec models have achieved great improvements on neural machine translations Jean et al., 2015) , which inspired researchers to apply this idea to response generation task to build a generation based neural conversation systems (Vinyals and Le, 2015; Shang et al., 2015; Xing et al., 2016; .\n[BOS] This technique enables a conversation system to be end-to-end trained from a large-scale corpus of message-response pairs, but researchers found it tends to generate some general and 'safe' responses (Vinyals and Le, 2015) .\n[BOS] To address this challenge, (Li et al., 2016) introduced a new objective function with MMI (maximum mutual information) to penalize too general responses.\n[BOS] (Shao et al., 2017) presented a novel attentional model to generate long and diverse responses.\n[BOS] (Wu et al., 2017) enhanced the quality and diversity of generated responses by constructing a dynamic vocabulary.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 14, "token_end": 24, "char_start": 71, "char_end": 117, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Jean et al., 2015)": null}}}, {"token_start": 39, "token_end": 68, "char_start": 205, "char_end": 310, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vinyals and Le, 2015;": "12300158", "Shang et al., 2015;": "7356547"}}}, {"token_start": 103, "token_end": 120, "char_start": 477, "char_end": 542, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vinyals and Le, 2015)": "12300158"}}}, {"token_start": 127, "token_end": 155, "char_start": 578, "char_end": 704, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Li et al., 2016)": "7287895"}, "Reference": {}}}, {"token_start": 156, "token_end": 177, "char_start": 711, "char_end": 806, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Shao et al., 2017)": "5586146"}, "Reference": {}}}, {"token_start": 178, "token_end": 200, "char_start": 813, "char_end": 926, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Wu et al., 2017)": "19253447"}, "Reference": {}}}]}
{"id": "52009850_0", "paragraph": "[BOS] In linguistics, a word is the most basic unit, and other larger language elements such as phrases and sentences, etc., are built upon words.\n[BOS] Due to this fact, it is natural that most previous works regard a word as the smallest unit in their models, i.e. these models operate at word-level.\n\n", "discourse_tags": ["Transition", "Transition"], "span_citation_mapping": []}
{"id": "52009111_3", "paragraph": "[BOS] The idea of adding a reordering layer into neural MT models has also been studied by Huang et al. (2018) .\n[BOS] They use a simple feed-forward soft and local reordering layer similar to the soft attention mechanism.\n[BOS] A fixed window size is used for local reordering.\n[BOS] Our RNN reordering layer can handle long distance reordering.\n[BOS] Another important difference is that we use discrete variables (permutations) for reordering while the soft reordering mechanism has no latent variables.\n[BOS] We leave it as future work to train the end-to-end system by treating ITG transitions and permutations as latent variables.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 5, "token_end": 60, "char_start": 18, "char_end": 278, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Huang et al. (2018)": "26551251"}, "Reference": {}}}]}
{"id": "52009111_2", "paragraph": "[BOS] For the approach of incorporating syntactic constraints into neural translation, the following papers are most relevant.\n[BOS] Eriguchi et al. (2016) and Chen et al. (2017) assume the existence of source parse trees and enhance the encoder and the attention mechanism to attend to both words and syntactic phrases.\n[BOS] We do not rely on external parsers.\n[BOS] Stahlberg et al. (2016) let a hierarchical phrase-based decoder guide neural machine translation decoding.\n[BOS] Reordering decisions can only be indirectly influenced by the hierarchical decoder.\n[BOS] In contrast, we have an explicit hierarchical reordering model applied pre-translation.\n[BOS] Eriguchi et al. (2017) train a joint parsing and translation model to maximize the log likelihood of output sequence and input parsing action sequence.\n[BOS] This is similar to our multi-task training setup.\n[BOS] The key difference is our subtask is ITG parsing for reordering instead of linguistically-motivated parsing.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Reflection", "Single_summ", "Single_summ", "Reflection", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 21, "token_end": 62, "char_start": 133, "char_end": 320, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Eriguchi et al. (2016)": "12851711", "Chen et al. (2017)": "3504277"}, "Reference": {}}}, {"token_start": 73, "token_end": 109, "char_start": 369, "char_end": 565, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Stahlberg et al. (2016)": "11642690"}, "Reference": {}}}, {"token_start": 127, "token_end": 157, "char_start": 666, "char_end": 817, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Eriguchi et al. (2017)": "14519034"}, "Reference": {}}}]}
{"id": "52009111_1", "paragraph": "[BOS] There are two papers most relevant to ours that relate to combining transition systems with RNNs.\n[BOS] Dyer et al. (2015) propose a stack-LSTM for transition-based parsing.\n[BOS] The difference with our approach to modeling is that we do not encode the entire stack explicitly.\n[BOS] At each time step, we only feed the context indexed by the current stack and buffer configuration.\n[BOS] We do not have an stack RNN, which can be expensive.\n[BOS] Our transition RNN can be viewed as a special case of DRAGNN (Kong et al., 2017) which combines fixed features as input with recurrence links at each time step.\n[BOS] Our recurrence link is only to the previous time step.\n\n", "discourse_tags": ["Transition", "Single_summ", "Reflection", "Reflection", "Reflection", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 21, "token_end": 41, "char_start": 110, "char_end": 179, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Dyer et al. (2015)": "6278207"}, "Reference": {}}}, {"token_start": 108, "token_end": 132, "char_start": 509, "char_end": 615, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Kong et al., 2017)": "16981435"}, "Reference": {}}}]}
{"id": "52009111_0", "paragraph": "[BOS] There are three papers that are most relevant to ours that relate to ITG pre-ordering.\n[BOS] DeNero and Uszkoreit (2011) induce binary source trees first and learn pre-reordering rules for these binary trees from parallel data.\n[BOS] Neubig et al. (2012) discriminatively train an ITG parser with CYK parsing for pre-reordering, essentially combining the two steps in DeNero and Uszkoreit (2011) into one.\n[BOS] Nakagawa (2015) improve upon Neubig et al. (2012) with a linear time top-down ITG parsing algorithm.\n[BOS] They all rely on feature engineering as they use linear models for training.\n[BOS] None of them does transition-based parsing for ITG.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 22, "token_end": 52, "char_start": 99, "char_end": 233, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 53, "token_end": 97, "char_start": 240, "char_end": 411, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Neubig et al. (2012)": "7646419"}, "Reference": {}}}, {"token_start": 98, "token_end": 140, "char_start": 418, "char_end": 601, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {"Neubig et al. (2012)": "7646419"}}}]}
{"id": "52302229_4", "paragraph": "[BOS] In future work, we plan to replace the GloVebased measure of coherence with a trained discriminator that distinguishes between coherent and incoherent responses (Li and Jurafsky, 2017 ).\n[BOS] This will allow us to use extend the notion of coherence to account for phenomena such as topic shifts.\n[BOS] We also plan to verify the results with a human evaluation study.\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 25, "token_end": 38, "char_start": 133, "char_end": 189, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Li and Jurafsky, 2017": "18706304"}}}]}
{"id": "52302229_3", "paragraph": "[BOS] We showed that explicitly modeling coherence and optimizing towards coherence and diversity leads to better-quality outputs in dialogue response generation.\n[BOS] We introduced three extensions to current encoder-decoder response generation models: (1) we defined a measure of coherence based on GloVe embeddings (Pennington et al., 2014) , (2) we filtered the OpenSubtitles training corpus (Lison and Meena, 2016) based on this measure to obtain coherent and diverse training instances, (3) we trained a cVAE model based on (Hu et al., 2017) and (Tu et al., 2017 ) that uses our coherence measure as one of the training signals.\n[BOS] Our experimental results showed a considerable improvement in the output quality over competitive models, which demonstrates the effectiveness of our approach.\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 47, "token_end": 62, "char_start": 283, "char_end": 344, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Pennington et al., 2014)": "1957433"}}}, {"token_start": 69, "token_end": 84, "char_start": 367, "char_end": 420, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lison and Meena, 2016)": "9847013"}}}, {"token_start": 102, "token_end": 123, "char_start": 511, "char_end": 569, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hu et al., 2017)": "20981275", "(Tu et al., 2017": "1658155"}}}]}
{"id": "52302229_2", "paragraph": "[BOS] We also draw on ideas from other areas than dialogue generation to build our models: Tu et al. (2017) 's context gates originate from machine translation and Hu et al. (2017) 's cVAE training stems from free-text generation.\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 18, "token_end": 33, "char_start": 91, "char_end": 159, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Tu et al. (2017)": "1658155"}}}, {"token_start": 34, "token_end": 53, "char_start": 164, "char_end": 230, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Hu et al. (2017)": "20981275"}}}]}
{"id": "52302229_1", "paragraph": "[BOS] A lot of recent works explore the use of additional training signals and VAE setups in dialogue generation.\n[BOS] In contrast to this paper, they do not focus explicitly on coherence: Asghar et al. (2017) use reinforcement learning with human-provided feedback, Li et al. (2017a) use a RL scenario with length as reward signal.\n[BOS] Li et al. (2017b) add an adversarial discriminator to provide RL rewards (discriminating between human and machine outputs), Xu et al. (2017) use a full adversarial training setup.\n[BOS] The most recent works explore the usage of VAEs: Cao and Clark (2017) explore a vanilla VAE setup conditioned on dual encoder (for contexts and responses) during training, the model of Serban et al. (2017) uses a VAE in a hierarchical E-D model.\n[BOS] Shen et al. (2017) use a cVAE conditioned on sentiment and response genericity (based on a handwritten list of phrases).\n[BOS] Shen et al. (2018) combine a cVAE with a plain VAE in an adversarial fashion.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Multi_summ", "Multi_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 38, "token_end": 55, "char_start": 190, "char_end": 266, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Asghar et al. (2017)": "644194"}, "Reference": {}}}, {"token_start": 56, "token_end": 74, "char_start": 268, "char_end": 333, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Li et al. (2017a)": "14635535"}, "Reference": {}}}, {"token_start": 75, "token_end": 101, "char_start": 340, "char_end": 463, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Li et al. (2017b)": "98180"}, "Reference": {}}}, {"token_start": 102, "token_end": 117, "char_start": 465, "char_end": 520, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Xu et al. (2017)": "11030403"}, "Reference": {}}}, {"token_start": 126, "token_end": 154, "char_start": 570, "char_end": 697, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 158, "token_end": 178, "char_start": 712, "char_end": 772, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Serban et al. (2017)": "14857825"}, "Reference": {}}}, {"token_start": 179, "token_end": 208, "char_start": 779, "char_end": 899, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Shen et al. (2017)": "9346726"}, "Reference": {}}}, {"token_start": 209, "token_end": 231, "char_start": 906, "char_end": 983, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Shen et al. (2018)": "3620643"}, "Reference": {}}}]}
{"id": "52302229_0", "paragraph": "[BOS] Our work fits into the context of the very active area of end-to-end generative conversation models, where neural E-D approaches have been first applied by Vinyals and Le (2015) and extended by many others since.\n[BOS] Many works address the lack of diversity and coherence in E-D outputs (Sountsov and Sarawagi, 2016; but do not attempt to model coherence directly, unlike our work: Li et al. (2016a) use anti-LM reranking; Li et al. (2016c) modify the beam search decoding algorithm, similar to Shao et al. (2017) in addition to using a self-attention model.\n[BOS] Mou et al. (2016) predict keywords for the output in a preprocessing step while Wu et al. (2018) preselect a vocabulary subset to be used for decoding.\n[BOS] Li et al. (2016b) focus specifically on personality generation (using personality embeddings) and promote topic-specific outputs by language-model rescoring and sampling.\n\n", "discourse_tags": ["Reflection", "Multi_summ", "Multi_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 24, "token_end": 42, "char_start": 113, "char_end": 183, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Vinyals and Le (2015)": "12300158"}}}, {"token_start": 54, "token_end": 75, "char_start": 248, "char_end": 323, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 89, "token_end": 105, "char_start": 390, "char_end": 430, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Li et al. (2016a)": "7287895"}, "Reference": {}}}, {"token_start": 105, "token_end": 140, "char_start": 431, "char_end": 566, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Li et al. (2016c)": "14563749"}, "Reference": {"Shao et al. (2017)": "17431796"}}}, {"token_start": 141, "token_end": 158, "char_start": 573, "char_end": 646, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Mou et al. (2016)": "5165773"}, "Reference": {}}}, {"token_start": 159, "token_end": 178, "char_start": 653, "char_end": 724, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wu et al. (2018)": "19253447"}, "Reference": {}}}, {"token_start": 179, "token_end": 212, "char_start": 731, "char_end": 901, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Li et al. (2016b)": "2955580"}, "Reference": {}}}]}
{"id": "52971170_8", "paragraph": "[BOS] A promising line of future work is to get NORMA-Linear to bridge the 2-3 point gap on related languages by exploring a best of both worlds approach, combining neighborhood sensitivity with the methods that achieve superior performance on nearby languages.\n[BOS] Table 3 : Performance for en-pt on rare words (RARE), and the en-pt MUSE dataset, which as shown in Figure  3 contains a lot of frequent words.\n\n", "discourse_tags": ["Transition", "Other"], "span_citation_mapping": []}
{"id": "52971170_7", "paragraph": "[BOS] We show experiments on English to related languages in the last three columns of Table 1 .\n[BOS] On these languages, indeed the most recently proposed methods (Artetxe et al., 2018a; Conneau et al., 2018) produce the best performing maps.\n[BOS] However, NORMA-Linear is only 2-3 points behind these methods.\n[BOS] This in contrast to English to Chinese where both (Artetxe et al., 2018a) and (Conneau et al., 2018) are behind NORMA -Linear, by more than 10 points.\n\n", "discourse_tags": ["Reflection", "Multi_summ", "Transition", "Multi_summ"], "span_citation_mapping": [{"token_start": 20, "token_end": 56, "char_start": 103, "char_end": 244, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Artetxe et al., 2018a;": "4334731"}, "Reference": {}}}, {"token_start": 76, "token_end": 118, "char_start": 328, "char_end": 470, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Artetxe et al., 2018a)": "4334731"}, "Reference": {}}}]}
{"id": "52971170_6", "paragraph": "[BOS] Translation Retrieval Methods.\n[BOS] The most commonly used way to obtain a translation t of a source language word s is nearest neighbor retrieval, given by: t = arg max t cos(Mx s , y t ).\n[BOS] Alternative retrieval methods have been proposed, such as the inverted nearest neighbor retrieval (Dinu et al., 2014) , inverted softmax (Smith et al., 2017) and Cross-Domain Similarity Local Scaling (CSLS) (Conneau et al., 2018) .\n[BOS] Since we are interested in evaluating the quality of mapping functions, our experiments use standard nearest neighbor retrieval for all methods.\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 56, "token_end": 69, "char_start": 265, "char_end": 320, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dinu et al., 2014)": "17910711"}}}, {"token_start": 70, "token_end": 81, "char_start": 323, "char_end": 360, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Smith et al., 2017)": "11591887"}}}, {"token_start": 82, "token_end": 102, "char_start": 365, "char_end": 432, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "52971170_5", "paragraph": "[BOS] Forms of Supervision.\n[BOS] The methods we have described so far fall under supervised learning.\n[BOS] In the supervised setting, a seed dictionary (5k word pairs is a typical size) is used to induce the mapping function.\n[BOS] In (Artetxe et al., 2017) a semi-supervised approach is explored, whereby the method alternates between learning the map and generating an increasingly large dictionary.\n[BOS] Completely unsupervised methods have recently been proposed using adversarial training (Barone, 2016; Zhang et al., 2017; Conneau et al., 2018) .\n[BOS] However, the underlying methods for learning the mapping function are similar to prior work such as (Xing et al., 2015) .\n[BOS] The limitations and strengths of unsupervised methods are detailed in (Sgaard et al., 2018) Although in our our experiments we work in the supervised setting, NORMA can work with any form of supervision.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Multi_summ", "Narrative_cite", "Narrative_cite", "Single_summ"], "span_citation_mapping": [{"token_start": 48, "token_end": 82, "char_start": 237, "char_end": 403, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Artetxe et al., 2017)": "13335042"}, "Reference": {}}}, {"token_start": 91, "token_end": 116, "char_start": 476, "char_end": 553, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Barone, 2016;": null, "Zhang et al., 2017;": "26873455"}}}, {"token_start": 124, "token_end": 144, "char_start": 598, "char_end": 681, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Xing et al., 2015)": "3144258"}}}, {"token_start": 147, "token_end": 166, "char_start": 694, "char_end": 781, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(S\u00f8gaard et al., 2018)": null}, "Reference": {}}}]}
{"id": "52971170_4", "paragraph": "[BOS] Work on phrase translation proposed to induce many local maps that are individually trained (Zhao et al., 2015) on local neighborhoods.\n[BOS] In contrast, our approach trains a single function while taking into account neighborhood sensitivity.\n[BOS] Our underlying motivation of neighborhood sensitivity is similar in spirit to the use of locally linear embeddings for nonlinear dimensionality reduction (Roweis and Saul, 2000) .\n\n", "discourse_tags": ["Narrative_cite", "Reflection", "Narrative_cite"], "span_citation_mapping": [{"token_start": 10, "token_end": 24, "char_start": 57, "char_end": 117, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhao et al., 2015)": "8284107"}}}, {"token_start": 64, "token_end": 77, "char_start": 376, "char_end": 434, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Roweis and Saul, 2000)": null}}}]}
{"id": "52971170_3", "paragraph": "[BOS] Most of the prior methods can be characterized as a series of linear transformations.\n[BOS] In particular, (Artetxe et al., 2018a) propose a framework to differentiate prior methods in terms of which transformations they perform: embedding normalization, whitening, re-weighting, de-whitening, and dimensionality reduction.\n\n", "discourse_tags": ["Transition", "Single_summ"], "span_citation_mapping": [{"token_start": 21, "token_end": 66, "char_start": 113, "char_end": 329, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Artetxe et al., 2018a)": "4334731"}, "Reference": {}}}]}
{"id": "52971170_2", "paragraph": "[BOS] here X and Y are matrices that contain word embedding vectors for the source and target language (Mikolov et al., 2013a; Dinu et al., 2014; Vulic and Korhonen, 2016) .\n[BOS] Improved results were obtained by imposing an orthogonality constraint on M (Xing et al., 2015; Smith et al., 2017) .\n[BOS] Another loss function used in prior work is the max-margin loss, which has been shown to significantly outperform the least squares loss (Lazaridou et al., 2015; Nakashole and Flauger, 2017) .\n[BOS] Another approach is to use canonical correlation analysis (CCA) to map two languages to a shared embedding space (Haghighi et al., 2008; Faruqui and Dyer, 2014; Lu et al., 2015; Ammar et al., 2016) .\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 17, "token_end": 47, "char_start": 87, "char_end": 171, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mikolov et al., 2013a;": "1966640", "Dinu et al., 2014;": "17910711", "Vulic and Korhonen, 2016)": "17515652"}}}, {"token_start": 56, "token_end": 77, "char_start": 226, "char_end": 295, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Xing et al., 2015;": "3144258", "Smith et al., 2017)": "11591887"}}}, {"token_start": 101, "token_end": 125, "char_start": 422, "char_end": 494, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lazaridou et al., 2015;": "12187767", "Nakashole and Flauger, 2017)": "22421874"}}}, {"token_start": 139, "token_end": 182, "char_start": 570, "char_end": 700, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Haghighi et al., 2008;": "7185434", "Faruqui and Dyer, 2014;": "3792324", "Lu et al., 2015;": "874413", "Ammar et al., 2016)": "1227830"}}}]}
{"id": "52971170_1", "paragraph": "[BOS] Map Induction Methods.\n[BOS] The earliest and simplest approach is to use a regularized least squares loss to induce a linear map M as follows:\n\n", "discourse_tags": ["Transition", "Transition"], "span_citation_mapping": []}
{"id": "52971170_0", "paragraph": "[BOS] The common approach to learning cross embedding space mapping functions is: first monolingual word embeddings for each language are trained independently; and second, a mapping function is learned, using supervised or unsupervised methods.\n[BOS] The resulting mapping function enables translating words from the source to the target language.\n\n", "discourse_tags": ["Transition", "Transition"], "span_citation_mapping": []}
{"id": "52126537_2", "paragraph": "[BOS] In our approach, we aim to generate a set of ifthen-else rules that approximate the interaction between the most important features and classes for a trained model.\n[BOS] As opposed to Lakkaraju et al. (2017) , before learning an explanation model, we modify the input data based on the importance of the features in the trained network.\n[BOS] In doing so, we already encode some information about the network's performance within these input features.\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 38, "token_end": 54, "char_start": 191, "char_end": 253, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Lakkaraju et al. (2017)": "19598815"}}}]}
{"id": "52126537_1", "paragraph": "[BOS] Most of the above-mentioned techniques output a ranked list of the most significant features for a model.\n[BOS] Several approaches, especially when the input is an image, visualize these features as image segments (Erhan et al., 2009; Simonyan et al., 2013; Olah et al., 2018) .\n[BOS] These act as visual cues about the salient objects in an image for the classifier.\n[BOS] However, such visual understanding is limited when we use either structured or textual input.\n[BOS] Heatmaps are often used to visualize interpretations of text-based models (Hermann et al., 2015; Li et al., 2016a,b; Yang et al., 2016; Aubakirova and Bansal, 2016; Arras et al., 2017) .\n[BOS] However, the interaction between different features and their relative contribution towards class labels remains unknown in this qualitative representation.\n[BOS] To overcome this limitation, in the same vein as our work, rule induction for interpreting neural networks has been proposed (Andrews et al., 1995; Lakkaraju et al., 2017) .\n[BOS] Thrun (1993) have proposed a technique to find disjunctive rules by identifying valid intervals of input values for the correct classification.\n[BOS] Intervals are expanded starting with the known values for instances.\n[BOS] Lakkaraju et al. (2017) use the input data and the model predictions to learn decision sets that are optimized to jointly maximize the interpretability of the explanations and the extent to which the original model is explained.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Transition", "Narrative_cite", "Transition", "Multi_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 34, "token_end": 65, "char_start": 177, "char_end": 282, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Olah et al., 2018)": "67440606"}}}, {"token_start": 111, "token_end": 159, "char_start": 536, "char_end": 664, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hermann et al., 2015;": "6203757"}}}, {"token_start": 196, "token_end": 223, "char_start": 895, "char_end": 1007, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Lakkaraju et al., 2017)": "19598815"}}}, {"token_start": 225, "token_end": 263, "char_start": 1016, "char_end": 1234, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 264, "token_end": 309, "char_start": 1241, "char_end": 1469, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Lakkaraju et al. (2017)": "19598815"}, "Reference": {}}}]}
{"id": "52126537_0", "paragraph": "[BOS] There has been a lot of recent interest in making machine learning models interpretable.\n[BOS] Different approaches can be broadly grouped under two headings-1) the use of interpretable models, and 2) model-agnostic interpretability techniques.\n[BOS] In the first case, the choice of machine learning methods is limited to the more interpretable models such as linear models and decision trees (Molnar; Caruana et al., 2015) .\n[BOS] The drawback of incorporating model interpretability through specific model choices is that these models may not perform well enough for a given task or a given dataset.\n[BOS] To overcome this, the second set of approaches try to explain either a complete model, or an individual prediction by using the input data and the model output(s).\n[BOS] Several approaches involve manipulation of the trained network to identify the most significant input features.\n[BOS] In some cases, the input features are deleted one by one, and the corresponding effect on the output is recorded (Li et al., 2016b; Avati et al., 2017; Suresh et al., 2017) .\n[BOS] The features that cause the maximum change in the output are ranked the highest.\n[BOS] Another computational approach uses gradient ascent to learn the input vector that maximizes a given output in a trained network (Erhan et al., 2009; Simonyan et al., 2013) .\n[BOS] In some other cases, the gradient of the output with respect to the input is computed, which corresponds to the effect of an infinitesimal change of the input on the output (Engelbrecht and Cloete, 1998; Simonyan et al., 2013; Aubakirova and Bansal, 2016; Sushil et al., 2018) .\n[BOS] Another approach computes feature importance using layerwise relevance propagation (LRP) (Bach et al., 2015; Montavon et al., 2017; Arras et al., 2017) , which has been shown to be equivalent to the prod-uct of the gradient value and the input (Kindermans et al., 2016) .\n[BOS] Sometimes the importance of a feature is analyzed by setting its value to a reference value, and then backpropagating the difference (DeepLIFT) (Shrikumar et al., 2017) .\n[BOS] In another approach, a separate 'explanation model' is trained to fit the predictions of the original model (Ribeiro et al., 2016; Lundberg and Lee, 2017; Lakkaraju et al., 2017) .\n[BOS] In an information theoretic approach, the mutual information between feature subsets and the model output is approximated to identify the most important features, similar to feature selection techniques (Chen et al., 2018) .\n[BOS] For recurrent neural networks with an attention mechanism, attention weights are often used as feature importance scores (Hermann et al., 2015; Yang et al., 2016; Choi et al., 2016) .\n[BOS] Poerner et al. (2018) have investigated several of the previously discussed techniques and have found LRP and DeepLIFT to be the most effective approaches for explaining deep neural networks in NLP.\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite", "Transition", "Transition", "Transition", "Narrative_cite", "Transition", "Narrative_cite", "Narrative_cite", "Narrative_cite", "Narrative_cite", "Narrative_cite", "Narrative_cite", "Narrative_cite", "Single_summ"], "span_citation_mapping": [{"token_start": 72, "token_end": 90, "char_start": 367, "char_end": 430, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Molnar;": null}}}, {"token_start": 179, "token_end": 221, "char_start": 922, "char_end": 1075, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Li et al., 2016b;": "13017314"}}}, {"token_start": 243, "token_end": 277, "char_start": 1207, "char_end": 1343, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 304, "token_end": 349, "char_start": 1477, "char_end": 1628, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 357, "token_end": 390, "char_start": 1688, "char_end": 1788, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Montavon et al., 2017;": "5731985"}}}, {"token_start": 400, "token_end": 420, "char_start": 1836, "char_end": 1906, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kindermans et al., 2016)": "6290904"}}}, {"token_start": 431, "token_end": 461, "char_start": 1968, "char_end": 2083, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 468, "token_end": 510, "char_start": 2115, "char_end": 2270, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ribeiro et al., 2016;": "13029170", "Lundberg and Lee, 2017;": null, "Lakkaraju et al., 2017)": "19598815"}}}, {"token_start": 539, "token_end": 550, "char_start": 2453, "char_end": 2501, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 567, "token_end": 593, "char_start": 2605, "char_end": 2691, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hermann et al., 2015;": "6203757"}}}, {"token_start": 595, "token_end": 636, "char_start": 2700, "char_end": 2898, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Poerner et al. (2018)": "51692044"}, "Reference": {}}}]}
{"id": "52011869_4", "paragraph": "[BOS] Other researchers have attempted to learn from the limit-scale validation set and augment the capability of the relation extractor.\n[BOS] Schwartz (Schwartz et al., 2017) is the champion of the LSDSem 2017 Shared Figure 1 : Architecture of our model.\n[BOS] 1 Task, which achieved a score of 75.2% by associating writing style features in endings and training a linear regression.\n[BOS] HCM (Chaturvedi et al., 2017 ) trained a joint model with feature engineering to obtain representations of event sequence, sentiment, and topic from validation set and a hidden variable approach as a voter, thereby obtaining 77.6%.\n[BOS] The previous NN-based models did not perform well.\n[BOS] Cai (Cai et al., 2017) constructed a model with hierarchical long short-term memory network (LSTM) to encode plot and an ending2sentence attention, then concatenated the two representations through feedforward network and outputting the final prediction, obtaining 74.7% accuracy.\n[BOS] We pursue the same strategy to construct our principal model MANN and see opportunity to utilize external knowledge in the technique of combining semantic sequence information.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Transition", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 26, "token_end": 81, "char_start": 144, "char_end": 385, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Schwartz et al., 2017)": "1994584"}, "Reference": {}}}, {"token_start": 82, "token_end": 130, "char_start": 392, "char_end": 621, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Chaturvedi et al., 2017": "27249900"}, "Reference": {}}}, {"token_start": 145, "token_end": 203, "char_start": 687, "char_end": 967, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Cai et al., 2017)": "2325074"}, "Reference": {}}}]}
{"id": "52011869_3", "paragraph": "[BOS] However, the published ROCStories could not be used directly in supervised learning.\n[BOS] Considering the use of the training set without negative endings, researchers proposed strategies to generate incorrect options.\n[BOS] A conditional generative adversarial network has been proposed, achieving a moderate result with an accuracy of 60.9% (Wang et al., 2017) .\n[BOS] Roemmele (Roemmele et al., 2017) designed four generative models for fake options, namely, random, backward, nearest-ending and language model.\n[BOS] The best result is produced from samples of all four types of endings (67.2%).\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 42, "token_end": 70, "char_start": 246, "char_end": 369, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wang et al., 2017)": "31728143"}}}, {"token_start": 72, "token_end": 131, "char_start": 378, "char_end": 606, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Roemmele et al., 2017)": "11293070"}, "Reference": {}}}]}
{"id": "52011869_2", "paragraph": "[BOS] Common sense learning is a challenging aspect in NLP.\n[BOS] The limitation of other rich knowledge structures is that they mostly either focus on shallower representations, such as semantic roles like PropBank (Palmer et al., 2005) , or pay attention to specific types of knowledge, i.e., unsupervised co-reference in the text (Chambers and Jurafsky, 2009 ) and event temporal relation (Modi and Titov, 2014) .\n[BOS] Learning from structural event knowledge is proposed to enrich this field, including narrative schema (Chambers and Jurafsky, 2009 ) and event frames (Sha et al., 2016) .\n[BOS] Unlike the above tasks, SCT (Mostafazadeh et al., 2016 ) provides large-scale supervised training stories of temporal and causal relations, ensuring a high-quality evaluation for common sense knowledge understanding of mechanisms.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite", "Single_summ"], "span_citation_mapping": [{"token_start": 37, "token_end": 47, "char_start": 207, "char_end": 237, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Palmer et al., 2005)": "2486369"}}}, {"token_start": 62, "token_end": 77, "char_start": 295, "char_end": 361, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chambers and Jurafsky, 2009": "10299779"}}}, {"token_start": 79, "token_end": 90, "char_start": 368, "char_end": 414, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Modi and Titov, 2014)": "5636607"}}}, {"token_start": 105, "token_end": 115, "char_start": 508, "char_end": 553, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chambers and Jurafsky, 2009": "10299779"}}}, {"token_start": 117, "token_end": 127, "char_start": 560, "char_end": 591, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sha et al., 2016)": "5741899"}}}, {"token_start": 134, "token_end": 175, "char_start": 624, "char_end": 830, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "52011869_1", "paragraph": "[BOS] Reading comprehension is the ability to read and understand text, and it has attracted much attention in natural language processing (NLP) to evaluate the level a machine can reach in understanding text.\n[BOS] Two popular forms of evaluation tasks exist in this field: cloze-style query and text-span matching.\n[BOS] Cloze-style query, such as SQuAD published by Stanford University, focuses on predicting existing text from the original corpus when given a relevant context.\n[BOS] Text-span matching is different from selecting a possible word from the provided text to replenish the blank areas, such as CNN/DailyMail by Hermann and Hinton.\n[BOS] Existing tasks are constructed with fragments, whereas examples from SCT are complete and independent stories that has short and meaningful sentence.\n[BOS] SCT is also different in that it requires the prediction of development of a story, which is not provided in the given hypothesis.\n[BOS] This novel task calls for stronger relation extraction and external inferential capability to identify the correct ending.\n[BOS] Our model paid attention on through structure and proved to be effective during experiments.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Transition", "Transition", "Transition", "Transition", "Reflection"], "span_citation_mapping": []}
{"id": "52011869_0", "paragraph": "[BOS] The issue of story ending prediction is related to several other research topics, such as reading comprehension and common sense learning, which will be briefly surveyed as follows.\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "52162885_4", "paragraph": "[BOS] Previous studies provide (indicative) support for the hypothesis that embeddings lack information people get from other modalities than language.\n[BOS] Fagarasan et al. (2015) present a method to ground embedding models in perceptual information by mapping distributional spaces to semantic spaces consisting of feature norms.\n[BOS] Several approaches to boosting distributional models with visual information show that the additional information improves the performance of word embedding vectors (Roller and Schulte im Walde, 2013; Lazaridou et al., 2014) .\n[BOS] Whereas this indicates that word embedding models lack visual information, it does not show to what extent different types of properties are encoded.\n[BOS] The method proposed in this paper is, to the best of our knowledge, the first approach specifically designed to identify what semantic knowledge is captured in word embeddings.\n[BOS] We are not aware of earlier work that provides explicit hypotheses about the kind of information we expect to learn from distributional vectors, making this the first attempt to confirm these hypotheses experimentally.\n\n", "discourse_tags": ["Transition", "Single_summ", "Narrative_cite", "Transition", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 25, "token_end": 57, "char_start": 158, "char_end": 332, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Fagarasan et al. (2015)": "16303338"}, "Reference": {}}}, {"token_start": 76, "token_end": 101, "char_start": 481, "char_end": 563, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Roller and Schulte im Walde, 2013;": "1172944", "Lazaridou et al., 2014)": "15152889"}}}]}
{"id": "52162885_3", "paragraph": "[BOS] A few other studies go beyond full vector comparisons, moving towards the interpretation of word embedding dimensions.\n[BOS] Tsvetkov et al. ( , 2016 evaluate word embeddings by measuring the correlation between word embedding vectors and count vectors representing cooccurrences of words with WordNet supersenses.\n[BOS] While they show that their results have a higher correlation with results obtained from extrinsic evaluations than standardly used intrinsic evaluations, they do not provide insights into what kind of semantic information is represented well.\n[BOS] Yaghoobzadeh and Schtze (2016) decompose distributional vectors into individual linguistic aspects by means of a supervised classification approach to test which linguistic phenomena are captured by embeddings.\n[BOS] They test their approach on an artificially created corpus and do not provide insights into specific semantic knowledge.\n[BOS] transform learned embedding matrices into sparse matrices to make them more interpretable, which is complementary to our approach.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 22, "token_end": 96, "char_start": 131, "char_end": 569, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Tsvetkov et al. ( , 2016": "6238748"}, "Reference": {}}}, {"token_start": 97, "token_end": 155, "char_start": 576, "char_end": 913, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Yaghoobzadeh and Sch\u00fctze (2016)": "13444726"}, "Reference": {}}}]}
{"id": "52162885_2", "paragraph": "[BOS] Several approaches have attempted to derive properties collected in property norm datasets from the distribution in naturally occurring texts (Kelly et al., 2014; Baroni et al., 2010; Barbu, 2008) .\n[BOS] Whereas these approaches yield indications about the potential of distributional models, they do not go beyond full-vector proximity on a low-dimensional SVD model or context words in a transparent, high-dimensional count model.\n[BOS] Their focus lies on detecting informative contexts.\n[BOS] We follow the idea behind this approach and make a human-elicited property dataset that is created in the same tradition, but larger.\n[BOS] Our approach goes beyond the previous work in two ways: first, we add gold negative examples which allows us to go beyond testing for salient properties.\n[BOS] Second, we compare full vector proximity to the outcome of a classifier which allows us to verify whether the property is captured for entities that share the property, but are not similar otherwise.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 18, "token_end": 42, "char_start": 122, "char_end": 202, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kelly et al., 2014;": null, "Baroni et al., 2010;": null, "Barbu, 2008)": null}}}]}
{"id": "52162885_1", "paragraph": "[BOS] On top of that, an embedding may capture specific semantic properties in ways that are not analogous to semantic properties of related categories.\n[BOS] Analogy methods assume that semantic properties stand in analogous relation to each other based on the information provided by the context, but there is no reason why (e.g.)\n[BOS] things made of wood and things made of plastic result in (combinations of) embedding dimensions that are similar enough to stand in a parallel relation to each other.\n[BOS] Our setup can determine whether the properties are represented without supposing such structures by targeting semantic properties directly rather than in relation to other concepts.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Reflection"], "span_citation_mapping": []}
{"id": "52162885_0", "paragraph": "[BOS] Intrinsic evaluation of word embeddings has primarily focused on two main tasks: identifying general semantic relatedness or similarity and the so-called analogy task, where word embeddings have been shown to be able to predict missing components of analogies of the type A is to B as C is to D (Mikolov et al., 2013; Turney, 2012) .\n[BOS] Furthermore, most intrinsic evaluation methods take full vectors into consideration.\n[BOS] The famous examples P aris  F rance + Italy  Rome or kingman+woman  queen evoke the suggestion that embeddings can capture semantic properties.\n[BOS] The task has, however, been criticized substantially (Linzen, 2016; Drozd et al., 2016, among others) .\n[BOS] follow an observation in Levy and Goldberg (2014) on the large differences in performance on different categories in the Google analogy set (Mikolov et al., 2013) .\n[BOS] They provide a new, more challenging, analogy dataset that improves existing sets on balance (capturing more semantic categories) and size.\n[BOS] Linzen (2016) points out more fundamental problems including the observation that the target vector in the analogy task can often be found by simply taking the vector closest to the source.\n[BOS] show that classifiers picking out the target word from a set of related terms outperform our experiments can be found at: https://cltl.github.io/semantic_space_navigation the standardly applied cosine addition or multiplication methods.\n[BOS] Though also boosted by the aforementioned proximity bias, these results indicate that standard methods of solving analogies miss information that is captured by embeddings.\n[BOS] Rogers et al. (2017) conclude that the analogy evaluation does not reveal if word embedding representations indeed capture specific semantic properties.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Narrative_cite", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Transition", "Single_summ"], "span_citation_mapping": [{"token_start": 43, "token_end": 72, "char_start": 256, "char_end": 337, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mikolov et al., 2013;": "7478738", "Turney, 2012)": "455112"}}}, {"token_start": 124, "token_end": 140, "char_start": 615, "char_end": 673, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Linzen, 2016;": "7906247"}}}, {"token_start": 146, "token_end": 207, "char_start": 697, "char_end": 1007, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Levy and Goldberg (2014)": "12730203", "(Mikolov et al., 2013)": "7478738"}}}, {"token_start": 208, "token_end": 243, "char_start": 1014, "char_end": 1203, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 324, "token_end": 349, "char_start": 1632, "char_end": 1784, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Rogers et al. (2017)": "28702383"}, "Reference": {}}}]}
{"id": "52943615_0", "paragraph": "[BOS] Abstractive Text Summarization Recent model architectures for abstractive text summarization basically use the sequence-tosequence (Sutskever et al., 2014) framework in combination with various novel mechanisms.\n[BOS] One popular mechanism is attention (Bahdanau et al., 2015) , which has been shown helpful for summarization (Nallapati et al., 2016; Rush et al., 2015; .\n[BOS] It is also possible to directly optimize evaluation metrics such as ROUGE (Lin, Figure 1: Proposed model.\n[BOS] Given long text, the generator produces a shorter text as a summary.\n[BOS] The generator is learned by minimizing the reconstruction loss together with the reconstructor and making discriminator regard its output as humanwritten text.\n[BOS] 2004) with reinforcement learning (Ranzato et al., 2016; Paulus et al., 2017; Bahdanau et al., 2016) .\n[BOS] The hybrid pointer-generator network (See et al., 2017) selects words from the original text with a pointer (Vinyals et al., 2015) or from the whole vocabulary with a trained weight.\n[BOS] In order to eliminate repetition, a coverage vector (Tu et al., 2016) can be used to keep track of attended words, and coverage loss (See et al., 2017) can be used to encourage model focus on diverse words.\n[BOS] While most papers focus on supervised learning with novel mechanisms, in this paper, we explore unsupervised training models.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Transition", "Transition", "Transition", "Multi_summ", "Multi_summ", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 19, "token_end": 33, "char_start": 117, "char_end": 161, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sutskever et al., 2014)": "7961699"}}}, {"token_start": 46, "token_end": 57, "char_start": 249, "char_end": 282, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bahdanau et al., 2015)": "11212020"}}}, {"token_start": 64, "token_end": 83, "char_start": 318, "char_end": 374, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Nallapati et al., 2016;": "8928715"}}}, {"token_start": 154, "token_end": 183, "char_start": 748, "char_end": 837, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ranzato et al., 2016;": "7147309", "Paulus et al., 2017;": "21850704", "Bahdanau et al., 2016)": "14096841"}}}, {"token_start": 187, "token_end": 228, "char_start": 857, "char_end": 1028, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(See et al., 2017)": null}, "Reference": {}}}, {"token_start": 236, "token_end": 246, "char_start": 1071, "char_end": 1104, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Tu et al., 2016)": "146843"}}}, {"token_start": 257, "token_end": 267, "char_start": 1154, "char_end": 1186, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(See et al., 2017)": null}}}]}
{"id": "52822214_4", "paragraph": "[BOS] Free-form answer-generation datasets.\n[BOS] MS MARCO (Nguyen et al., 2016) contains 100k user queries from Bing Search with human generated answers.\n[BOS] Systems generate free-form answers and are evaluated by automatic metrics such as ROUGE-L and BLEU-1.\n[BOS] However, the reliability of these metrics is questionable because they have been shown to correlate poorly with human judgement (Novikova et al., 2017) .\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 11, "token_end": 61, "char_start": 50, "char_end": 262, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Nguyen et al., 2016)": "1289517"}, "Reference": {}}}, {"token_start": 77, "token_end": 92, "char_start": 359, "char_end": 420, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Novikova et al., 2017)": "1929239"}}}]}
{"id": "52822214_3", "paragraph": "[BOS] KB-based multi-hop datasets.\n[BOS] Recent datasets like QAngaroo (Welbl et al., 2018) and COM-PLEXWEBQUESTIONS (Talmor and Berant, 2018) explore different approaches of using pre-existing knowledge bases (KB) with pre-defined logic rules to generate valid QA pairs, to test QA models' capability of performing multi-hop reasoning.\n[BOS] The diversity of questions and answers is largely limited by the fixed KB schemas or logical forms.\n[BOS] Furthermore, some of the questions might be answerable by one text sentence due to the incompleteness of KBs.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Multi_summ", "Multi_summ"], "span_citation_mapping": [{"token_start": 14, "token_end": 124, "char_start": 62, "char_end": 558, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Welbl et al., 2018)": "9192723", "(Talmor and Berant, 2018)": "3986974"}, "Reference": {}}}]}
{"id": "52822214_2", "paragraph": "[BOS] Multi-document datasets.\n[BOS] TriviaQA (Joshi et al., 2017) and SearchQA (Dunn et al., 2017) contain question answer pairs that are accompanied with more than one document as the context.\n[BOS] This further challenges QA systems' ability to accommodate longer contexts.\n[BOS] However, since the supporting documents are collected after the question answer pairs with information retrieval, the questions are not guaranteed to involve interesting reasoning between multiple documents.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Multi_summ", "Multi_summ"], "span_citation_mapping": [{"token_start": 8, "token_end": 91, "char_start": 37, "char_end": 490, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Joshi et al., 2017)": "26501419", "(Dunn et al., 2017)": "11606382"}, "Reference": {}}}]}
{"id": "52822214_1", "paragraph": "[BOS] Single-document datasets.\n[BOS] SQuAD (Rajpurkar et al., 2016 (Rajpurkar et al., , 2018 questions that are relatively simple because they usually require no more than one sentence in the paragraph to answer.\n\n", "discourse_tags": ["Transition", "Multi_summ"], "span_citation_mapping": [{"token_start": 8, "token_end": 49, "char_start": 38, "char_end": 213, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Rajpurkar et al., 2016": "11816014", "(Rajpurkar et al., , 2018": "47018994"}, "Reference": {}}}]}
{"id": "52822214_0", "paragraph": "[BOS] Various recently-proposed large-scale QA datasets can be categorized in four categories.\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "52282089_2", "paragraph": "[BOS] To our knowledge, there is no previous work exploring the use of and utility of stylistic selection for controlling stylistic variation in NLG from structured MRs.\n[BOS] This may be either because there have not been sufficiently large corpora in a particular domain, or because it is surprising, as we show, that relatively small corpora (2000 samples) whose style is controlled can be used to train a neural generator to achieve high semantic correctness while producing stylistic variation.\n\n", "discourse_tags": ["Transition", "Reflection"], "span_citation_mapping": []}
{"id": "52282089_1", "paragraph": "[BOS] While there is previous work on stylistic variation in NLG (Paiva and Evans, 2004; Mairesse and Walker, 2007) , this work did not use crowdsourced utterances for training.\n[BOS] More recent work in neural NLG that explores stylistic control has not needed to control semantic correctness, or examined the interaction between semantic correctness and stylistic variation (Sennrich et al., 2016; Ficler and Goldberg, 2017) .\n[BOS] Also related is the work of Niu and Carpuat (2017) that analyzes how dense word embeddings capture style variations, Kabbara and Cheung (2016) who explore the ability of neural NLG systems to transfer style without the need for parallel corpora, which are difficult to collect (Rao and Tetreault, 2018) , while Li et al. (2018) use a simple delete-and-retrieve method also without alignment to outperform adversarial methods in style transfer.\n[BOS] Finally, Oraby et al. (2018) propose two different methods that give neural generators control over the language style, corresponding to the Big Five personalities, while maintaining semantic fidelity of the generated utterances.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Multi_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 8, "token_end": 29, "char_start": 38, "char_end": 115, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Paiva and Evans, 2004;": null, "Mairesse and Walker, 2007)": "2817528"}}}, {"token_start": 68, "token_end": 91, "char_start": 331, "char_end": 426, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sennrich et al., 2016;": "845121", "Ficler and Goldberg, 2017)": "11054023"}}}, {"token_start": 99, "token_end": 117, "char_start": 463, "char_end": 550, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Niu and Carpuat (2017)": "26545229"}, "Reference": {}}}, {"token_start": 118, "token_end": 160, "char_start": 552, "char_end": 737, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kabbara and Cheung (2016)": "14553565"}, "Reference": {"(Rao and Tetreault, 2018)": "4859003"}}}, {"token_start": 162, "token_end": 190, "char_start": 746, "char_end": 878, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Li et al. (2018)": "4937880"}, "Reference": {}}}, {"token_start": 193, "token_end": 233, "char_start": 894, "char_end": 1114, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Oraby et al. (2018)": "46897209"}, "Reference": {}}}]}
{"id": "52282089_0", "paragraph": "[BOS] The restaurant domain has always been the domain of choice for NLG tasks in dialogue systems (Stent et al., 2004; Gai et al., 2008; Mairesse et al., 2010; Howcroft et al., 2013) , as it offers a good combination of structured information availability, expression complexity, and ease of incorporation into conversation.\n[BOS] Hence, even the more recent neural models for NLG continue to be tested primarily on data in this domain (Wen et al., 2015; Duek and Jurek, 2016; Nayak et al., 2017) .\n[BOS] These tend to focus solely on syntactic and semantic correctness of the generated utterances, nevertheless, there have also been re-cent efforts to collect training data for NLG with emphasis on stylistic variation (Nayak et al., 2017; Novikova et al., 2017a; Oraby et al., 2017) .\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 14, "token_end": 53, "char_start": 69, "char_end": 183, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Stent et al., 2004;": "1543141", "Ga\u0161i\u0107 et al., 2008;": "5731480", "Mairesse et al., 2010;": "6241225"}}}, {"token_start": 90, "token_end": 122, "char_start": 397, "char_end": 497, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wen et al., 2015;": "739696", "Du\u0161ek and Jur\u010d\u00ed\u010dek, 2016;": "6380915"}}}, {"token_start": 159, "token_end": 191, "char_start": 701, "char_end": 785, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Nayak et al., 2017;": "28464948", "Novikova et al., 2017a;": "19662556"}}}]}
{"id": "52228931_5", "paragraph": "[BOS] The idea of our baseline to improving the reading step by incorporating additional relation description spans is similar as Weissenborn et al. (2017) and Mihaylov and Frank (2018) , who integrate background commonsense knowledge into readingcomprehension systems.\n[BOS] Both rely on structured knowledge bases to extract information about semantic relations that hold between entities.\n[BOS] On the other hand, we extract text spans that mention each pair of entities and encoded them into vector representations of the relations between entities.\n\n", "discourse_tags": ["Multi_summ", "Multi_summ", "Reflection"], "span_citation_mapping": [{"token_start": 3, "token_end": 72, "char_start": 10, "char_end": 391, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Weissenborn et al. (2017)": null, "Mihaylov and Frank (2018)": "29151507"}, "Reference": {}}}]}
{"id": "52228931_4", "paragraph": "[BOS] In parallel to our work, Choi et al. (2018) and Reddy et al. (2018) introduce sequential question answering datasets (QuAC and CoQA) that focus on the reading comprehension setup (i.e., a single text snippet is pre-specified for answering the given questions).\n[BOS] QBLink is entirely naturally occurring (all questions and answers were authored independently from any knowledge sources) and is primarily designed to challenge human players.\n\n", "discourse_tags": ["Multi_summ", "Transition"], "span_citation_mapping": [{"token_start": 8, "token_end": 66, "char_start": 31, "char_end": 266, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Choi et al. (2018)": "52057510", "Reddy et al. (2018)": "52055325"}, "Reference": {}}}]}
{"id": "52228931_3", "paragraph": "[BOS] Both Iyyer et al. (2017) and Talmor and Berant (2018) answer complex questions by decomposing each into a sequence of simple questions.\n[BOS] Iyyer et al. (2017) adopt a semantic parsing approach to answer questions over semi-structured tables.\n[BOS] They construct a dataset of around 6,000 question sequences by asking humans to rewrite a set of 2,000 complex questions into simple sequences.\n[BOS] Talmor and Berant (2018) consider the setup of open-domain question answering over unstructured text, but their dataset is constructed synthetically (with human paraphrasing) by combining simple questions with a few rules.\n\n", "discourse_tags": ["Multi_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 3, "token_end": 34, "char_start": 11, "char_end": 141, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Iyyer et al. (2017)": "2623009", "Talmor and Berant (2018)": "3986974"}, "Reference": {}}}, {"token_start": 35, "token_end": 86, "char_start": 148, "char_end": 400, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Iyyer et al. (2017)": "2623009"}, "Reference": {}}}, {"token_start": 87, "token_end": 131, "char_start": 407, "char_end": 629, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Talmor and Berant (2018)": "3986974"}, "Reference": {}}}]}
{"id": "52228931_2", "paragraph": "[BOS] Several question answering datasets have been proposed (Berant et al., 2013; Joshi et al., 2017; Trischler et al., 2017; Rajpurkar et al., 2018, inter alia) .\n[BOS] However, all of them were limited to answering individual questions.\n[BOS] Saha et al. (2018) study the problem of sequential question answering, and introduce a dataset for the task.\n[BOS] However, we differ from them in two aspects: 1) They consider question-answering over structured knowledge-bases.\n[BOS] 2) Their dataset construction was overly synthetic: templates were collected by human annotators given knowledge-base predicates.\n[BOS] Further, sequences were constructed synthetically as well by grouping individual questions by predicate or subjects.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Single_summ", "Reflection", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 3, "token_end": 48, "char_start": 14, "char_end": 162, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Berant et al., 2013;": "6401679", "Joshi et al., 2017;": "26501419", "Trischler et al., 2017;": "1167588"}}}, {"token_start": 63, "token_end": 87, "char_start": 246, "char_end": 354, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Saha et al. (2018)": "19240019"}, "Reference": {}}}]}
{"id": "52228931_1", "paragraph": "[BOS] Aside from the open-domain setup, much of the recent work on question answering has focused on the sub-problem of reading-comprehension, where the gold answer to each question is assumed to exist in a given single paragraph for the model to read (Hermann et al., 2015; Rajpurkar et al., 2016; Seo et al., 2017) .\n[BOS] Another line of work on question answering is question answering over structured knowledge-bases (Berant et al., 2013; Berant and Liang, 2014; Yao and Van Durme, 2014; Gardner and Krishnamurthy, 2017) .\n[BOS] Although we focus on the more general open-domain setup, QBLink can be adapted to be usable in the readingcomprehension setup as well as the question answering over knowledge-bases setup.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 22, "token_end": 77, "char_start": 105, "char_end": 316, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hermann et al., 2015;": "6203757", "Rajpurkar et al., 2016;": "11816014", "Seo et al., 2017)": "8535316"}}}, {"token_start": 87, "token_end": 129, "char_start": 371, "char_end": 525, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Berant et al., 2013;": "6401679", "Berant and Liang, 2014;": "1336493", "Yao and Van Durme, 2014;": "2131938", "Gardner and Krishnamurthy, 2017)": "9060725"}}}]}
{"id": "52228931_0", "paragraph": "[BOS] We adopt the open-domain question answering framework (Wang et al., 2018; Chen et al., 2017) .\n[BOS] Previous work considers improving that base framework itself (Clark and Gardner, 2018; Swayamdipta et al., 2018, inter alia) .\n[BOS] But retains the assumption of answering individual questions.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 8, "token_end": 26, "char_start": 31, "char_end": 98, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wang et al., 2018;": "13764176", "Chen et al., 2017)": null}}}, {"token_start": 33, "token_end": 59, "char_start": 146, "char_end": 231, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Clark and Gardner, 2018;": "223637"}}}]}
{"id": "52155625_2", "paragraph": "[BOS] We proposed a general method to improve dialog response selection through manipulating existing data that can be applied to different models.\n[BOS] Our results show that for both open-domain and task-oriented dialogues, and for both English and Chinese languages, at least one of the proposed augmentation methods is effective, and the chance that they hurt is rare.\n[BOS] We have deliberately chosen a diverse set of domains and models to test this on to try to understand the contribution of data augmentation.\n[BOS] Thus even when working on new datasets, and new models, it seems data augmentation is still a valuable addition that will likely improve results.\n[BOS] Being more specific about when augmentation works is harder.\n[BOS] One future research direction would be to apply data transformation situationally based on the discourse structure of dialogs.\n[BOS] In our experiments, we tried combining permutation and flipping but found no advantage over using only one type of transformation.\n[BOS] We believe a more sophisticated method of combination could further improve the results, and leave it to future work.\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection", "Reflection", "Transition", "Transition", "Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "52155625_1", "paragraph": "[BOS] Recently, there has been a surging interest in adversarial training (Goodfellow et al., 2014) .\n[BOS] For text data, one class of methods generate adversarial examples by moving word embeddings along the opposite direction of the gradient of loss functions (Wu et al., 2017; Yasunaga et al., 2017) , hence small perturbation in the continuous space of word vectors.\n[BOS] Another class of methods aim to create genuinely new examples.\n[BOS] adds syntactic and semantic variations to training data based on grammar rules and thesaurus.\n[BOS] (Xie et al., 2017 ) add noises to data by blanking out or substituting words for language modeling.\n[BOS] (Yang et al., 2017 ) adopt a seq2seq model to generate questions based on paragraphs and answers into their generative adversarial framework.\n[BOS] One main difference between these methods and our approach is that, while adversarial training only manipulates training data, we in addition apply transformations to data at test time to help prediction.\n[BOS] This is closer to (Dong et al., 2017) in spirit.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Transition", "Transition", "Single_summ", "Single_summ", "Reflection", "Narrative_cite"], "span_citation_mapping": [{"token_start": 12, "token_end": 25, "char_start": 53, "char_end": 99, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Goodfellow et al., 2014)": "6706414"}}}, {"token_start": 45, "token_end": 70, "char_start": 210, "char_end": 303, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wu et al., 2017;": "34190303", "Yasunaga et al., 2017)": "5020278"}}}, {"token_start": 114, "token_end": 137, "char_start": 547, "char_end": 646, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Xie et al., 2017": "10635893"}, "Reference": {}}}, {"token_start": 138, "token_end": 167, "char_start": 653, "char_end": 794, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Yang et al., 2017": "15164488"}, "Reference": {}}}, {"token_start": 209, "token_end": 217, "char_start": 1030, "char_end": 1049, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dong et al., 2017)": "1282002"}}}]}
{"id": "52155625_0", "paragraph": "[BOS] Data augmentation has been widely adopted in computer vision and speech recognition (Krizhevsky et al., 2012; Ko et al., 2015) .\n[BOS] In image processing, label-preserving transformations such as tilting and flipping are used, but in NLP, finding such transformations that exactly preserve meanings is difficult.\n[BOS] Language data is discrete in nature, and minor perturbation may change the meaning.\n[BOS] Most commonly used techniques include word substitution (Fadaee et al., 2017) and paraphrasing (Dong et al., 2017) .\n[BOS] These methods may require heavy external resources, which can be difficult to apply across multiple languages and domains.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 9, "token_end": 32, "char_start": 51, "char_end": 132, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Krizhevsky et al., 2012;": "207763512", "Ko et al., 2015)": "7360763"}}}, {"token_start": 89, "token_end": 101, "char_start": 454, "char_end": 493, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Fadaee et al., 2017)": "3291104"}}}, {"token_start": 102, "token_end": 113, "char_start": 498, "char_end": 530, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dong et al., 2017)": "1282002"}}}]}
{"id": "52113103_0", "paragraph": "[BOS] Our approach is related to flow-based generative models, which are first described in NICE (Dinh et al., 2014) and have recently received more attention (Dinh et al., 2016; Jacobsen et al., 2018; Kingma and Dhariwal, 2018) .\n[BOS] This relevant work mostly adopts simple (e.g. Gaussian) and fixed priors and does not attempt to learn interpretable latent structures.\n[BOS] Another related generative model class is variational auto-encoders (VAEs) (Kingma and Welling, 2013) that optimize a lower bound on the marginal data likelihood, and can be extended to learn latent structures (Miao and Blunsom, 2016; Yin et al., 2018) .\n[BOS] Against the flow-based models, VAEs remove the invertibility constraint but sacrifice the merits of exact inference and exact log likelihood computation, which potentially results in optimization challenges (Kingma et al., 2016) .\n[BOS] Our approach can also be viewed in connection with generative adversarial networks (GANs) (Goodfellow et al., 2014) that is a likelihood-free framework to learn implicit generative models.\n[BOS] However, it is nontrivial for a gradient-based method like GANs to propagate gradients through discrete structures.\n\n", "discourse_tags": ["Reflection", "Transition", "Narrative_cite", "Narrative_cite", "Reflection", "Transition"], "span_citation_mapping": [{"token_start": 7, "token_end": 60, "char_start": 33, "char_end": 228, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dinh et al., 2014)": "13995862", "Jacobsen et al., 2018;": "3433237", "Kingma and Dhariwal, 2018)": "49657329"}}}, {"token_start": 96, "token_end": 124, "char_start": 421, "char_end": 540, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 130, "token_end": 150, "char_start": 565, "char_end": 631, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Miao and Blunsom, 2016;": "10480989", "Yin et al., 2018)": "49325612"}}}, {"token_start": 184, "token_end": 195, "char_start": 823, "char_end": 868, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kingma et al., 2016)": null}}}, {"token_start": 206, "token_end": 224, "char_start": 928, "char_end": 992, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Goodfellow et al., 2014)": "1033682"}}}]}
{"id": "52098907_3", "paragraph": "[BOS] Other work has focused on attributing network predictions.\n[BOS] Li et al. (2016) examined the impact of erasing portions of a network's representations on the output, Sundararajan et al. (2017) used a gradient based method to attribute predictions to inputs, and Murdoch et al. (2018) decomposed LSTMs to interpret classification predictions.\n[BOS] In contrast to these approaches, we explore the types of contextual information encoded in the biLM internal states instead of focusing on attributing this information to words in the input sentence.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Reflection"], "span_citation_mapping": [{"token_start": 13, "token_end": 36, "char_start": 71, "char_end": 172, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 37, "token_end": 57, "char_start": 174, "char_end": 264, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 59, "token_end": 77, "char_start": 270, "char_end": 349, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "52098907_2", "paragraph": "[BOS] Several prior studies have examined the learned representations in RNNs.\n[BOS] Karpathy et al. (2015) trained a character LSTM language model on source code and showed that individual neurons in the hidden state track the beginning and end of code blocks.\n[BOS] Linzen et al. (2016) assessed whether RNNs can learn number agreement in subject-verb dependencies.\n[BOS] Our analysis in Sec.\n[BOS] 5.1 showed that biLMs also learn number agreement for coreference.\n[BOS] Kdr et al. (2017) attributed the activation patters of RNNs to input tokens and showed that a RNN language model is strongly sensitive to tokens with syntactic functions.\n[BOS] Belinkov et al. (2017) used linear classifiers to determine whether neural machine translation systems learned morphology and POS tags.\n[BOS] Concurrent with our work, Khandelwal et al. (2018) studied the role of context in influencing language model predictions, Gaddy et al. (2018) analyzed neural constituency parsers, Blevins et al. (2018) explored whether RNNs trained with several different objectives can learn hierarchical syntax, and Conneau et al. (2018) examined to what extent sentence representations capture linguistic features.\n[BOS] Our intrinsic analysis is most similar to Belinkov et al. (2017) ; however, we probe span representations in addition to word representations, evaluate the transferability of the biLM representations to semantic tasks in addition to syntax tasks, and consider a wider variety of neural architectures in addition to RNNs.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Reflection", "Reflection", "Single_summ", "Single_summ", "Multi_summ", "Reflection"], "span_citation_mapping": [{"token_start": 15, "token_end": 52, "char_start": 85, "char_end": 261, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Karpathy et al. (2015)": "988348"}, "Reference": {}}}, {"token_start": 53, "token_end": 75, "char_start": 268, "char_end": 367, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 98, "token_end": 134, "char_start": 474, "char_end": 644, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"K\u00e1d\u00e1r et al. (2017)": "611341"}, "Reference": {}}}, {"token_start": 135, "token_end": 160, "char_start": 651, "char_end": 786, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Belinkov et al. (2017)": "7100502"}, "Reference": {}}}, {"token_start": 166, "token_end": 185, "char_start": 819, "char_end": 913, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Khandelwal et al. (2018)": "21700944"}, "Reference": {}}}, {"token_start": 186, "token_end": 200, "char_start": 915, "char_end": 971, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Gaddy et al. (2018)": "5040602"}, "Reference": {}}}, {"token_start": 201, "token_end": 223, "char_start": 973, "char_end": 1088, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Blevins et al. (2018)": "21663989"}, "Reference": {}}}, {"token_start": 225, "token_end": 244, "char_start": 1094, "char_end": 1193, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Conneau et al. (2018)": "24461982"}, "Reference": {}}}, {"token_start": 246, "token_end": 261, "char_start": 1204, "char_end": 1264, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "52098907_1", "paragraph": "[BOS] Liu et al. (2018) proposed using densely connected RNNs and layer pruning to speed up the use of context vectors for prediction.\n[BOS] As their method is applicable to other architectures, it could also be combined with our approach.\n\n", "discourse_tags": ["Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 48, "char_start": 6, "char_end": 239, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "52098907_0", "paragraph": "[BOS] In addition to biLM-based representations, McCann et al. (2017) learned contextualized vectors with a neural machine translation system (CoVe).\n[BOS] However, as showed the biLM based representations outperformed CoVe in all considered tasks, we focus exclusively on biLMs.\n\n", "discourse_tags": ["Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 3, "token_end": 34, "char_start": 9, "char_end": 149, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "52185177_3", "paragraph": "[BOS] Work in sentence simplification (see Shardlow (2014) for a survey) has some similarities with sentence compression, but it differs in that the key focus is on making sentences more easily understandable rather than shorter.\n[BOS] Though word deletion is used, sentence simplification methods feature sentence splitting and word simplification which are not usually present in sentence compression.\n[BOS] Furthermore, these methods often rely heavily on learned rules (e.g lexical simplification as in Biran et al. (2011) ), integer linear programming and sentence parse trees which makes them starkly different from our deep learning-based approach.\n[BOS] The exceptions that adopt end-to-end approaches, such as Filippova et al. (2015) , are usually supervised and focus on word deletion.\n\n", "discourse_tags": ["Single_summ", "Transition", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 4, "token_end": 14, "char_start": 14, "char_end": 58, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Shardlow (2014)": "6068649"}}}, {"token_start": 85, "token_end": 97, "char_start": 478, "char_end": 526, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Biran et al. (2011)": "11091984"}}}, {"token_start": 125, "token_end": 153, "char_start": 688, "char_end": 795, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Filippova et al. (2015)": "1992250"}}}]}
{"id": "52185177_2", "paragraph": "[BOS] Denoising auto-encoders (Vincent et al., 2008) have been successfully used in natural language processing for building sentence embeddings (Hill et al., 2016) , training unsupervised translation models (Artetxe et al., 2017) or for natural language generation in narrow domains (Freitag and Roy, 2018) .\n[BOS] In all those instances, the added noise takes the form of random deletion of words and word swapping or shuffling.\n[BOS] Although our noising mechanism relies on adding rather than removing words, we take some inspiration from these works.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 15, "char_start": 6, "char_end": 52, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vincent et al., 2008)": "207168299"}}}, {"token_start": 25, "token_end": 35, "char_start": 125, "char_end": 164, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hill et al., 2016)": "2937095"}}}, {"token_start": 37, "token_end": 50, "char_start": 176, "char_end": 230, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Artetxe et al., 2017)": "3515219"}}}, {"token_start": 56, "token_end": 67, "char_start": 269, "char_end": 307, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Freitag and Roy, 2018)": "5080441"}}}]}
{"id": "52185177_1", "paragraph": "[BOS] In machine translation, unsupervised methods for aligning word embeddings using only unmatched bilingual corpora, trained with only small seed dictionaries, (Mikolov et al., 2013; Lazaridou et al., 2015) , adversarial training on similar corpora (Zhang et al., 2017; Conneau et al., 2017b) or even on distant corpora and languages (Artetxe et al., 2018) have enabled the development of unsupervised machine translation (Artetxe et al., 2017; Lample et al., 2017) .\n[BOS] However, it is not clear how to adapt these methods for summarization where the task is to shorten the reference rather than translate it.\n[BOS] Wang and Lee (2018) train a generative adversarial network to encode references into a latent space and decode them in summaries using only unmatched documentsummary pairs.\n[BOS] However, in contrast with machine translation where monolingual data is plentiful and paired data scarce, summaries are paired with their respective documents when they exist, thus limiting the usefulness of such approaches.\n[BOS] In contrast, our method requires no summary corpora.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Single_summ", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 23, "token_end": 48, "char_start": 138, "char_end": 209, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mikolov et al., 2013;": "1966640", "Lazaridou et al., 2015)": "12187767"}}}, {"token_start": 49, "token_end": 73, "char_start": 212, "char_end": 295, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhang et al., 2017;": "26873455", "Conneau et al., 2017b)": "3470398"}}}, {"token_start": 76, "token_end": 90, "char_start": 307, "char_end": 359, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Artetxe et al., 2018)": "21728524"}}}, {"token_start": 95, "token_end": 116, "char_start": 392, "char_end": 468, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Artetxe et al., 2017;": "3515219", "Lample et al., 2017)": "3518190"}}}, {"token_start": 146, "token_end": 180, "char_start": 622, "char_end": 794, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "52185177_0", "paragraph": "[BOS] Early sentence compression approaches were extractive, focusing on deletion of uninformative words from sentences through learned rules (Knight and Marcu, 2002) or linguisticallymotivated heuristics (Dorr et al., 2003) .\n[BOS] The first abstractive approaches also relied on learned syntactic transformations (Cohn and Lapata, 2008) .\n[BOS] Recent work in automated text summarization has seen the application of sequence-to-sequence models to automatic summarization, including both extractive (Nallapati et al., 2017) and abstractive (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; Paulus et al., 2017; Fan et al., 2017) approaches, as well as hybrids of both (See et al., 2017) .\n[BOS] Although these methods have achieved state-of-the-art results, they are constrained by their need for large amounts paired document-summary data.\n[BOS] Miao and Blunsom (2016) seek to overcome this shortcoming by training separate compressor and reconstruction models, allowing for training based on both paired (supervised) and unlabeled (unsupervised) data.\n[BOS] For their compressor, they train a discrete variational auto-encoder for sentence compression and use the REINFORCE algorithm to allow end-to-end training.\n[BOS] They further use a pre-trained language model as a prior for their compression model to induce their compressed output to be grammatical.\n[BOS] However, their reported results are still based on models trained on at least 500k instances of paired data.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Narrative_cite", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 21, "token_end": 32, "char_start": 128, "char_end": 166, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Knight and Marcu, 2002)": "7793213"}}}, {"token_start": 33, "token_end": 48, "char_start": 170, "char_end": 224, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dorr et al., 2003)": "1729177"}}}, {"token_start": 58, "token_end": 70, "char_start": 281, "char_end": 338, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cohn and Lapata, 2008)": "2411338"}}}, {"token_start": 97, "token_end": 109, "char_start": 490, "char_end": 525, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Nallapati et al., 2017)": "6405271"}}}, {"token_start": 110, "token_end": 153, "char_start": 530, "char_end": 645, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rush et al., 2015;": "1918428", "Chopra et al., 2016;": "133195", "Nallapati et al., 2016;": "8928715", "Paulus et al., 2017;": "21850704", "Fan et al., 2017)": "22716243"}}}, {"token_start": 158, "token_end": 169, "char_start": 669, "char_end": 703, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(See et al., 2017)": null}}}, {"token_start": 201, "token_end": 321, "char_start": 864, "char_end": 1492, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Miao and Blunsom (2016)": "10480989"}, "Reference": {}}}]}
{"id": "52009825_2", "paragraph": "[BOS] There have been many studies using CQA data, but most of them are different from our task, i.e., dealing with answering questions (Surdeanu et al., 2008; Celikyilmaz et al., 2009; Bhaskar, 2013; Nakov et al., 2017) , retrieving similar questions (Lei et al., 2016; Romeo et al., 2016; Nakov et al., 2017) , and generating questions (Heilman and Smith, 2010) .\n[BOS] Tamura et al. (2005) focused on extracting a core sentence and identifying the question type as classification tasks for answering multiple-sentence questions.\n[BOS] Although their method is useful to retrieve important information, we cannot directly use it since our task requires shorter expressions for headlines than sentences.\n[BOS] In addition, they used a support vector machine as a classifier, which is almost the same as SVM in Section 5.2, and it is not expected to be suitable for our task, as shown in Section 5.4.\n[BOS] The work of Ishigaki et al. (2017) is the most related one in that they summarized lengthy questions by using both abstractive and extractive approaches.\n[BOS] Their work is promising because our task is regarded as the construction of short summaries, but the training of their models requires a lot of paired data consisting of questions and their headlines, which means that their method cannot be used to our task.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 27, "token_end": 67, "char_start": 103, "char_end": 220, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Surdeanu et al., 2008;": "7831164", "Celikyilmaz et al., 2009;": null, "Bhaskar, 2013;": "15127084", "Nakov et al., 2017)": "3063394"}}}, {"token_start": 68, "token_end": 95, "char_start": 223, "char_end": 310, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lei et al., 2016;": "6468765", "Romeo et al., 2016;": "14491346", "Nakov et al., 2017)": "3063394"}}}, {"token_start": 97, "token_end": 107, "char_start": 317, "char_end": 363, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Heilman and Smith, 2010)": "1809816"}}}, {"token_start": 109, "token_end": 204, "char_start": 372, "char_end": 874, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Tamura et al. (2005)": "17166178"}, "Reference": {}}}, {"token_start": 217, "token_end": 297, "char_start": 919, "char_end": 1325, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ishigaki et al. (2017)": "32480590"}, "Reference": {}}}]}
{"id": "52009825_1", "paragraph": "[BOS] After Rush et al. (2015) proposed a neural headline generation model, there have been many studies on the same headline generation task (Takase et al., 2016; Chopra et al., 2016; Kiyono et al., 2017; Ayana et al., 2017; Raffel et al., 2017) .\n[BOS] However, all of them are abstractive methods that can yield erroneous output, and the training for them requires a lot of paired data, i.e., news articles and headlines.\n[BOS] There have also been several classical studies based on nonneural approaches to headline generation (Woodsend et al., 2010; Alfonseca et al., 2013; Colmenares et al., 2015) , but they basically addressed sentence compression after extracting important linguistic units such as phrases.\n[BOS] In other words, their methods can still yield erroneous output, although they would be more controllable than neural models.\n[BOS] One exception is the work of Alotaiby (2011) , where fixed-sized substrings were considered for headline generation.\n[BOS] Although that approach is similar to ours, Alotaiby only considered an unsupervised method based on similarity to the original text (almost the same as SimTfidf in Section 5.2), in contrast to our proposal based on learning to rank.\n[BOS] This implies that Alotaiby's method will also not perform well for our task, as shown in Section 5.4.\n[BOS] There have been several studies on extractive summarization (Kobayashi et al., 2015; Yogatama et al., 2015) based on sentence embeddings, but they were basically developed for extracting multiple sentences, which means that these methods are almost the same as SimEmb in Section 5.2 for our purpose, i.e., extraction of the best candidate.\n[BOS] This also implies that they will not be suitable for our task.\n[BOS] Furthermore, recent sophisticated neural models for extractive summarization (Cheng and Lapata, 2016; Nallapati et al., 2017) basically require large-scale paired data (e.g., article-headline) to automatically label candidates, as manual annotation is very costly.\n[BOS] However, such paired data do not always exist for real applications, as in our task described in Section 1.\n\n", "discourse_tags": ["Multi_summ", "Transition", "Narrative_cite", "Transition", "Single_summ", "Single_summ", "Reflection", "Narrative_cite", "Transition", "Multi_summ", "Reflection"], "span_citation_mapping": [{"token_start": 3, "token_end": 18, "char_start": 12, "char_end": 74, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Rush et al. (2015)": "1918428"}, "Reference": {}}}, {"token_start": 26, "token_end": 73, "char_start": 112, "char_end": 246, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Takase et al., 2016;": "5450302", "Chopra et al., 2016;": "133195", "Kiyono et al., 2017;": "23971423", "Ayana et al., 2017;": "22324032", "Raffel et al., 2017)": "14345813"}}}, {"token_start": 123, "token_end": 158, "char_start": 487, "char_end": 603, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Woodsend et al., 2010;": "2292300", "Alfonseca et al., 2013;": "16997286", "Colmenares et al., 2015)": "16992492"}}}, {"token_start": 204, "token_end": 277, "char_start": 883, "char_end": 1209, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Alotaiby (2011)": "3244487"}, "Reference": {}}}, {"token_start": 312, "token_end": 333, "char_start": 1359, "char_end": 1431, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kobayashi et al., 2015;": "15448388", "Yogatama et al., 2015)": "12194143"}}}, {"token_start": 400, "token_end": 424, "char_start": 1773, "char_end": 1864, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cheng and Lapata, 2016;": "1499080", "Nallapati et al., 2017)": "6405271"}}}]}
{"id": "52009825_0", "paragraph": "[BOS] In this section, we briefly explain several related studies from two aspects: headline generation task and CQA data.\n[BOS] As discussed below, our work is the first attempt to address an extractive headline generation task for a CQA service based on learning to rank the substrings of a question.\n\n", "discourse_tags": ["Transition", "Reflection"], "span_citation_mapping": []}
{"id": "42197507_4", "paragraph": "[BOS] Attention based methods have been successful in many application domains, such as image classification (Stollenga et al., 2014) , image caption generation , machine translation Luong et al., 2015) , and question answering (Shih et al., 2016; Chen et al., 2015; Yang et al., 2016) .\n[BOS] However, in the field of sentiment analysis, the attention is applied to only aspect-based sentiment classification (Yanase et al., 2016) .\n[BOS] To the best knowledge of ours, there is no attention-based model for a general sentiment analysis task.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 15, "token_end": 27, "char_start": 88, "char_end": 133, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Stollenga et al., 2014)": "7875983"}}}, {"token_start": 28, "token_end": 43, "char_start": 136, "char_end": 202, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Luong et al., 2015)": "1998416"}}}, {"token_start": 45, "token_end": 70, "char_start": 209, "char_end": 285, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Shih et al., 2016;": "11923637", "Chen et al., 2015;": "16566944", "Yang et al., 2016)": "8849206"}}}, {"token_start": 87, "token_end": 101, "char_start": 372, "char_end": 431, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yanase et al., 2016)": "17844515"}}}]}
{"id": "42197507_3", "paragraph": "[BOS] Two notable pioneers in using lexicon for sentiment analysis are Mohammad et al. (2013a) ; Kalchbrenner et al. (2014b) generated scores with other manually generated sentiment lexicon scores to achieved the state-of-the-art result in SemEval-2013 Twitter sentiment analysis task.\n[BOS] In general domain, Hu and Liu (2004) generated a user review lexicon that showed promising result in capturing sentiment in customer product reviews.\n\n", "discourse_tags": ["Multi_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 66, "char_start": 6, "char_end": 285, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Mohammad et al. (2013a)": "13845267", "Kalchbrenner et al. (2014b)": "1306065"}, "Reference": {}}}, {"token_start": 67, "token_end": 94, "char_start": 292, "char_end": 441, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hu and Liu (2004)": "207155218"}, "Reference": {}}}]}
{"id": "42197507_2", "paragraph": "[BOS] Endeavors to capture n-gram information bore fruits with CNN, max pooling, and softmax (Collobert et al., 2011; Kim, 2014) , which is regarded as the standard methods of the document classification problem these days.\n[BOS] Kalchbrenner et al. (2014a) extended this standard CNN model with dynamic k-max pooling, which served as an input layer to another stacked convolution layer.\n[BOS] Multichannel CNN methods (Kim, 2014; Yin and Schtze, 2015) are another branch of CNN, where assorted embeddings are considered together when convolving the input.\n[BOS] Unlike Kim (2014) 's model that relies on a single type of embedding with different mutability characteristics of the weights of embedding layer, Yin and Schtze (2015) incorporates diverse sort of embedding types using multichannel CNN.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Multi_summ", "Multi_summ"], "span_citation_mapping": [{"token_start": 14, "token_end": 35, "char_start": 63, "char_end": 128, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Collobert et al., 2011;": "351666", "Kim, 2014)": null}}}, {"token_start": 52, "token_end": 88, "char_start": 230, "char_end": 387, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kalchbrenner et al. (2014a)": "1306065"}, "Reference": {}}}, {"token_start": 89, "token_end": 176, "char_start": 394, "char_end": 799, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Kim, 2014;": null, "Yin and Sch\u00fctze, 2015)": "7146903", "Kim (2014)": null, "Yin and Sch\u00fctze (2015)": "7146903"}, "Reference": {}}}]}
{"id": "42197507_1", "paragraph": "[BOS] Since the emergence of the Convolutional Neural Networks (CNN; Collobert et al. (2011) ), conventional methods have become gradually obsolete because of the outstanding performance from the CNN variants.\n[BOS] CNN based models are distinguished from earlier methods because they do not rely on laborious feature engineering.\n[BOS] The first success of CNN in sentiment analysis was triggered by document classification research (Kim, 2014) , where CNN showed state-of-the-art results in numerous document classification datasets.\n[BOS] This success has engendered an upsurge in deep neural network research for sentiment analysis.\n[BOS] Various modified models have been proposed in the literature.\n[BOS] One of the famous deep learning methods that models a document is the generalized phrase proposed by Yin and Schtze (2014) , which represents a sentence using element-wise addition, multiplication, or recursive auto-encoder.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Single_summ", "Single_summ", "Transition", "Single_summ"], "span_citation_mapping": [{"token_start": 7, "token_end": 21, "char_start": 33, "char_end": 92, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Collobert et al. (2011)": "351666"}}}, {"token_start": 62, "token_end": 119, "char_start": 337, "char_end": 636, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Kim, 2014)": null}, "Reference": {}}}, {"token_start": 131, "token_end": 175, "char_start": 711, "char_end": 935, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Yin and Sch\u00fctze (2014)": "14956855"}, "Reference": {}}}]}
{"id": "42197507_0", "paragraph": "[BOS] The first attempt of sentiment analysis on text was initiated by Pang et al. (2002) who pioneered this field by using bag-of-word features.\n[BOS] This work mostly hinged on feature engineering; since then, many kinds of feature learning methods had been introduced to increase the performance (Pang and Lee, 2008; Liu, 2012; Gimpel et al., 2011; Feldman, 2013; Mohammad et al., 2013b) .\n[BOS] Aside from pure machine learning approaches, lexicon based approaches had been another trend, which relied on the manual or algorithmic creation of word sentiment scores (Hu and Liu, 2004; Kim and Hovy, 2004; Ding et al., 2008; Taboada et al., 2011) .\n\n", "discourse_tags": ["Single_summ", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 2, "token_end": 96, "char_start": 6, "char_end": 390, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Pang et al. (2002)": "7105713"}, "Reference": {"(Pang and Lee, 2008;": null, "Liu, 2012;": "114955219", "Gimpel et al., 2011;": "14113765", "Feldman, 2013;": "18397233", "Mohammad et al., 2013b)": "13845267"}}}, {"token_start": 117, "token_end": 155, "char_start": 513, "char_end": 648, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hu and Liu, 2004;": "207155218", "Kim and Hovy, 2004;": "5690545", "Ding et al., 2008;": "12442299", "Taboada et al., 2011)": "3181362"}}}]}
{"id": "384520_3", "paragraph": "[BOS] Finally, recent work on Automatic Speech Recognition (ASR) uses a similar sequence-to-sequence LSTM framework to produce letter sequences directly from acoustic frame sequences (Chan et al., 2015; Bahdanau et al., 2015) .\n[BOS] Just as we are discarding the usual intermediate representations used for text processing, their models make no use of phonetic alignments, clustered triphones, or pronunciation dictionaries.\n[BOS] This line of work -discarding intermediate representations in speech -was pioneered by Graves and Jaitly (2014) and earlier, by Eyben et al. (2009) .\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 31, "token_end": 51, "char_start": 158, "char_end": 225, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chan et al., 2015;": "12444004", "Bahdanau et al., 2015)": null}}}, {"token_start": 95, "token_end": 127, "char_start": 451, "char_end": 579, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Eyben et al. (2009)": "2687308"}}}]}
{"id": "384520_2", "paragraph": "[BOS] Our work is philosophically similar to Collobert et al. 's (2011) experiments with \"almost from scratch\" language processing.\n[BOS] They avoid taskspecific feature engineering, instead relying on a multilayer feedforward (or convolutional) Neural Network to combine word embeddings to produce features useful for each task.\n[BOS] In the Results section, below, we compare NER performance on the same dataset they used.\n[BOS] The \"almost\" in the title actually refers to the use of preprocessed (lowercased) tokens as input instead of raw sequences of letters.\n[BOS] Our byte-level models can be seen as a realization of their comment: \"A completely from scratch approach would presumably not know anything about words at all and would work from letters only.\"\n[BOS] Recent work with convolutional neural networks that read character-level inputs (Zhang et al., 2015) shows some interesting results on a variety of classification tasks, but because their models need very large training sets, they do not present comparisons to established baselines on standard tasks.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Reflection", "Transition", "Reflection", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 61, "char_start": 6, "char_end": 329, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 153, "token_end": 207, "char_start": 772, "char_end": 1073, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Zhang et al., 2015)": null}, "Reference": {}}}]}
{"id": "384520_1", "paragraph": "[BOS] Recent work has shown that modeling the sequence of characters in each token with an LSTM can more effectively handle rare and unknown words than independent word embeddings (Ling et al., 2015; Ballesteros et al., 2015) .\n[BOS] Similarly, language modeling, especially for morphologically complex languages, benefits from a Convolutional Neural Network (CNN) over characters to generate word embeddings (Kim et al., 2015) .\n[BOS] Rather than decompose words into characters, Rohan and Denero (2015) encode rare words with Huffman codes, allowing a neural translation model to learn something about word subcomponents.\n[BOS] In contrast to this line of research, our work has no explicit notion of tokens and operates on bytes rather than characters.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 28, "token_end": 48, "char_start": 152, "char_end": 225, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ling et al., 2015;": "1689426"}}}, {"token_start": 64, "token_end": 84, "char_start": 330, "char_end": 427, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kim et al., 2015)": "1080545"}}}, {"token_start": 86, "token_end": 125, "char_start": 436, "char_end": 623, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "384520_0", "paragraph": "[BOS] One important feature of our work is the use of byte inputs.\n[BOS] Character-level inputs have been used with some success for tasks like NER (Klein et al., 2003) , parallel text alignment (Church, 1993) , and authorship attribution (Peng et al., 2003) as an effective way to deal with n-gram sparsity while still capturing some aspects of word choice and morphology.\n[BOS] Such approaches often combine character and word features and have been especially useful for handling languages with large character sets (Nakagawa, 2004) .\n[BOS] However, there is almost no work that explicitly uses bytes -one exception uses byte n-grams to identify source code authorship (Frantzeskou et al., 2006 ) -but there is nothing, to the best of our knowledge, that exploits bytes as a cross-lingual representation of language.\n[BOS] Work on multilingual parsing using Neural Networks that share some subset of the parameters across languages (Duong et al., 2015) seems to benefit the low-resource languages; however, we are sharing all the parameters among all languages.\n\n", "discourse_tags": ["Reflection", "Narrative_cite", "Narrative_cite", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 29, "token_end": 38, "char_start": 144, "char_end": 168, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Klein et al., 2003)": "1080545"}}}, {"token_start": 39, "token_end": 47, "char_start": 171, "char_end": 209, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Church, 1993)": "5703760"}}}, {"token_start": 49, "token_end": 59, "char_start": 216, "char_end": 258, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Peng et al., 2003)": "58507384"}}}, {"token_start": 97, "token_end": 108, "char_start": 483, "char_end": 535, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Nakagawa, 2004)": "2988891"}}}, {"token_start": 125, "token_end": 145, "char_start": 624, "char_end": 697, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 187, "token_end": 199, "char_start": 907, "char_end": 955, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Duong et al., 2015)": "17263016"}}}]}
{"id": "28108713_1", "paragraph": "[BOS] Compared to vision, where augmentation is common, little work has been done on augmenting text for classification problems.\n[BOS] A notable exception is Zhang et al. (2015) , where a thesaurus was used to replace synonymous words to create more training data for text classification.\n[BOS] However, this augmentation produced little improvement and sometimes even hurt performance.\n[BOS] The authors' argued that because large quantities of real data are available, models generalize properly without augmentation.\n[BOS] Although training using augmented text data is rare, generating new questions about images has been studied.\n[BOS] The COCO-QA dataset (Ren et al., 2015) for VQA was created by parsing COCO captions with a syntactic parser, and then used this to create QA pairs for four kinds of questions using hand-crafted rules.\n[BOS] However, due to inability of the algorithm to cope with complex sentence structures, a significant portion of COCO-QA questions have grammatical errors or are oddly phrased.\n[BOS] Visual question generation was also studied in (Mostafazadeh et al., 2016) , with an emphasis on generating questions about images that are beyond the literal visual content of the image.\n[BOS] They endeavored to avoid simple questions such as counting and color, which were emphasized in COCO-QA.\n[BOS] Unlike our work, their objective was not data augmentation and they did not try to answer the generated questions.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Transition", "Transition", "Single_summ", "Transition", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 25, "token_end": 72, "char_start": 136, "char_end": 387, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhang et al. (2015)": "368182"}, "Reference": {}}}, {"token_start": 113, "token_end": 164, "char_start": 642, "char_end": 842, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Ren et al., 2015)": "2950705"}, "Reference": {}}}, {"token_start": 200, "token_end": 285, "char_start": 1029, "char_end": 1447, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Mostafazadeh et al., 2016)": "16227864"}, "Reference": {}}}]}
{"id": "28108713_0", "paragraph": "[BOS] For supervised computer vision problems, e.g., image recognition, labels are scarcer than images.\n[BOS] This is especially a problem with deep convolutional neural networks (CNNs) that have millions of parameters.\n[BOS] Although more human labeled data would be ideal, it is easier to exploit the training dataset to generate new examples.\n[BOS] For image classification, common ways to exploit training images to create more labeled examples include mirror reflection, random crops etc.\n[BOS] Many of these methods were used in training the seminal AlexNet (Krizhevsky et al., 2012) , which increased the training data by more than ten folds and produced relative improvement of over 4% for image classification.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Transition", "Single_summ"], "span_citation_mapping": [{"token_start": 92, "token_end": 138, "char_start": 500, "char_end": 719, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Krizhevsky et al., 2012)": "207763512"}, "Reference": {}}}]}
{"id": "3544821_1", "paragraph": "[BOS] A third difficulty with Och et al. 's study was that it used MERT, which is not an ideal vehicle for feature exploration because it is observed not to perform well with large feature sets.\n[BOS] Others have introduced alternative discriminative training methods (Tillmann and Zhang, 2006; Turian et al., 2007; Blunsom et al., 2008; Macherey et al., 2008) , in which a recurring challenge is scalability: to train many features, we need many train-ing examples, and to train discriminatively, we need to search through all possible translations of each training example.\n[BOS] Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset.\n[BOS] We follow this approach here.\n\n", "discourse_tags": ["Single_summ", "Narrative_cite", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 42, "char_start": 6, "char_end": 194, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 47, "token_end": 84, "char_start": 236, "char_end": 360, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Tillmann and Zhang, 2006;": "6913385", "Turian et al., 2007;": "1467337", "Blunsom et al., 2008;": "6677774", "Macherey et al., 2008)": "7305992"}}}, {"token_start": 127, "token_end": 162, "char_start": 582, "char_end": 730, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Watanabe et al., 2007;": "2665828", "Chiang et al., 2008)": "3506035"}}}]}
{"id": "3544821_0", "paragraph": "[BOS] The work of Och et al (2004) is perhaps the bestknown study of new features and their impact on translation quality.\n[BOS] However, it had a few shortcomings.\n[BOS] First, it used the features for reranking n-best lists of translations, rather than for decoding or forest reranking (Huang, 2008) .\n[BOS] Second, it attempted to incorporate syntax by applying off-the-shelf part-ofspeech taggers and parsers to MT output, a task these tools were never designed for.\n[BOS] By contrast, we incorporate features directly into hierarchical and syntaxbased decoders.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 106, "char_start": 6, "char_end": 470, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Och et al (2004)": "6244213"}, "Reference": {"(Huang, 2008)": "1131864"}}}]}
{"id": "4812047_5", "paragraph": "[BOS] Functionality Separation.\n[BOS] Recent work has revealed that the overloaded use of representations makes model training difficult, and such problems can be alleviated by explicitly separating these functions (Reed and Freitas, 2015; Ba et al., 2016; Miller et al., 2016; Gulcehre et al., 2016; Rocktschel et al., 2017) .\n[BOS] For example, Miller et al. (2016) separate the functionality of look-up keys and memory contents in memory networks (Sukhbaatar et al., 2015) .\n[BOS] Rocktschel et al. (2017) propose a keyvalue-predict attention model, which outputs three vectors at each step: the first is used to predict the next-word distribution; the second serves as the key for decoding; and the third is used for the attention mechanism.\n[BOS] In this work, we further separate PAST and FUTURE functionalities from the decoder's hidden representations.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 18, "token_end": 75, "char_start": 112, "char_end": 325, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Reed and Freitas, 2015;": "7034786", "Ba et al., 2016;": "568305", "Miller et al., 2016;": "2711679", "Gulcehre et al., 2016;": "1399676", "Rockt\u00e4schel et al., 2017)": "3000562"}}}, {"token_start": 80, "token_end": 112, "char_start": 347, "char_end": 475, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Miller et al. (2016)": "2711679"}, "Reference": {"(Sukhbaatar et al., 2015)": "1399322"}}}, {"token_start": 114, "token_end": 171, "char_start": 484, "char_end": 745, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Rockt\u00e4schel et al. (2017)": "3000562"}, "Reference": {}}}]}
{"id": "4812047_4", "paragraph": "[BOS] Future Modeling.\n[BOS] Standard neural sequence decoders generate target sentences from left to right, thus failing to estimate some desired properties in the future (e.g., the length of target sentence).\n[BOS] To address this problem, actor-critic algorithms are employed to predict future properties Bahdanau et al., 2017) , in their models, an interpolation of the actor (the standard generation policy) and the critic (a value function that estimates the future values) is used for decision making.\n[BOS] Concerning the future generation at each decoding step, Weng et al. (2017) guide the decoder's hidden states to not only generate the current target word, but also predict the target words that remain untranslated.\n[BOS] Along the direction of future modeling, we introduce a FUTURE layer to maintain the untranslated source contents, which is updated at each decoding step by subtracting the source content being translated (i.e., attention vector) from the last state (i.e., the untranslated source content so far).\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 43, "token_end": 102, "char_start": 217, "char_end": 508, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Bahdanau et al., 2017)": "14096841"}, "Reference": {}}}, {"token_start": 103, "token_end": 148, "char_start": 515, "char_end": 729, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Weng et al. (2017)": "11229482"}, "Reference": {}}}]}
{"id": "4812047_3", "paragraph": "[BOS] In the context of semantic-level coverage, and Meng et al. (2016) propose a memory-enhanced attention model.\n[BOS] Both implement the memory with a Neural Turing Machine (Graves et al., 2014) , in which the reading and writing operations are expected to erase translated contents and highlight untranslated contents.\n[BOS] However, their models lack an explicit objective to guide such intuition, which is one of the key ingredients for the success in this work.\n[BOS] In addition, we use two separate layers to explicitly model translated and untranslated contents, which is another distinguishing feature of the proposed approach.\n\n", "discourse_tags": ["Single_summ", "Narrative_cite", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 12, "token_end": 27, "char_start": 53, "char_end": 114, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Meng et al. (2016)": "416723"}, "Reference": {}}}, {"token_start": 34, "token_end": 46, "char_start": 154, "char_end": 197, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Graves et al., 2014)": "15299054"}}}]}
{"id": "4812047_2", "paragraph": "[BOS] In addition, we explicitly model both translated (with PAST-RNN) and untranslated (with FUTURE-RNN) instead of using a single coverage vector to indicate translated source words.\n[BOS] The difference with Tu et al. (2016) is that the PAST and FUTURE contents in our model are fed not only to the attention mechanism but also the decoder's states.\n\n", "discourse_tags": ["Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 42, "token_end": 52, "char_start": 191, "char_end": 227, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Tu et al. (2016)": "146843"}}}]}
{"id": "4812047_1", "paragraph": "[BOS] Coverage Modeling.\n[BOS] Tu et al. (2016) and Mi et al. (2016) maintain a coverage vector to indicate which source words have been translated and which source words have not.\n[BOS] These vectors are updated by accumulating attention probabilities at each decoding step, which provides an opportunity for the attention model to distinguish translated source words from untranslated ones.\n[BOS] Viewing coverage vectors as a (soft) indicator of translated source contents, following this idea, we take one step further.\n[BOS] We model translated and untranslated source contents by directly manipulating the attention vector (i.e., the source contents that are being translated) instead of attention probability (i.e., the probability of a source word being translated).\n\n", "discourse_tags": ["Transition", "Multi_summ", "Multi_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 6, "token_end": 73, "char_start": 31, "char_end": 392, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Tu et al. (2016)": "146843", "Mi et al. (2016)": "14900221"}, "Reference": {}}}]}
{"id": "4812047_0", "paragraph": "[BOS] Our research is built upon an attention-based sequence-to-sequence model (Bahdanau et al., 2015) , but is also related to coverage modeling, future modeling, and functionality separation.\n[BOS] We discuss these topics in the following.\n\n", "discourse_tags": ["Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 8, "token_end": 27, "char_start": 36, "char_end": 102, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bahdanau et al., 2015)": "11212020"}}}]}
{"id": "294054_2", "paragraph": "[BOS] Our motivation for generation of material for language education exists in work such as Sumita et al. (2005) and Mostow and Jang (2012) , which deal with automatic generation of classic fill in the blank questions.\n[BOS] Our work is naturally complementary to these efforts, as their methods require a corpus of in-vocab text to serve as seed sentences.\n\n", "discourse_tags": ["Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 9, "token_end": 33, "char_start": 52, "char_end": 141, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Sumita et al. (2005)": "14952993", "Mostow and Jang (2012)": "10920633"}}}]}
{"id": "294054_1", "paragraph": "[BOS] Computational creativity is another subfield of NLG that often does not fix an a priori meaning in its output.\n[BOS] Examples such aszbal et al. (2013) and Valitutti et al. (2013) use template filling techniques guided by quantified notions of humor or how catchy a phrase is.\n\n", "discourse_tags": ["Transition", "Multi_summ"], "span_citation_mapping": [{"token_start": 25, "token_end": 66, "char_start": 123, "char_end": 282, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Valitutti et al. (2013)": "10951045"}, "Reference": {}}}]}
{"id": "294054_0", "paragraph": "[BOS] The majority of NLG focuses on the satisfaction of a communicative goal, with examples such as Belz (2008) which produces weather reports from structured data or Mitchell et al. (2013) which generates descriptions of objects from images.\n[BOS] Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry (Greene et al., 2010) (Colton et al., 2012 ) (Jiang and Zhou, 2008) or song lyrics (Wu et al., 2013) (Ramakrishnan A et al., 2009) , where specified meter or rhyme schemes are enforced.\n[BOS] In these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric.\n\n", "discourse_tags": ["Multi_summ", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 20, "token_end": 32, "char_start": 101, "char_end": 164, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Belz (2008)": null}, "Reference": {}}}, {"token_start": 33, "token_end": 48, "char_start": 168, "char_end": 243, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Mitchell et al. (2013)": "6034383"}, "Reference": {}}}, {"token_start": 65, "token_end": 93, "char_start": 339, "char_end": 424, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Greene et al., 2010)": "8090830", "(Colton et al., 2012": "2218980", "(Jiang and Zhou, 2008)": "8773022"}}}, {"token_start": 94, "token_end": 117, "char_start": 428, "char_end": 487, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wu et al., 2013)": "7467552", "(Ramakrishnan A et al., 2009)": null}}}]}
{"id": "464513_3", "paragraph": "[BOS] A closely related task is aspect-based opinion mining (Zhao et al., 2010; Yu et al., 2011; .\n[BOS] Instead of locating the opinion expressions, aspect-based opinion mining directly analyzes polarities of different opinion targets.\n[BOS] The targets are usually constrained to be some predefined set.\n[BOS] Shared tasks (SemEval2014, SemEval2015) have been held on the task, and various systems are proposed and evaluated (Pontiki et al., 2014; Pontiki et al., 2015) .\n[BOS] Comparing with aspect-based opinion mining, we will extract opinion expressions which are more informative, and we won't constrain opinion target types which helps us to handle open domain texts.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 26, "char_start": 6, "char_end": 95, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhao et al., 2010;": "5235435"}}}, {"token_start": 86, "token_end": 111, "char_start": 384, "char_end": 471, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Pontiki et al., 2014;": "1021411", "Pontiki et al., 2015)": "61874237"}}}]}
{"id": "464513_2", "paragraph": "[BOS] Our model is inspired by previous distantly supervised algorithms (Snow et al., 2004; Mintz et al., 2009) .\n[BOS] They use relations from WordNet or knowledge bases as distant supervision.\n[BOS] Since we don't have similar resources for opinion relation extraction, we use patterns to generate relations.\n[BOS] Neural network classifiers are popular for relation extraction recently.\n[BOS] Many of them focus on fully supervised settings, recurrent neural networks (RNN) and convolutional neural networks (CNN) (Vu et al., 2016; Zeng et al., 2015; Xu et al., 2015a; Xu et al., 2015b; Zhang and Wang, 2015) , sequence models and tree models are investigated (Li et al., 2015; dos Santos et al., 2015) .\n[BOS] One similar network structure to our model is proposed in (Miwa and Bansal, 2016) .\n[BOS] They jointly extract entities and relations using two LSTM models.\n[BOS] Another recent work (Jebbara and Cimiano, 2016) uses stacked RNNs and CNNs for aspect and opinion detection.\n[BOS] Different from models there, we will learn representations for different lexical and syntactic features explicitly.\n[BOS] Our formulation follows the features in traditional relation classifiers, which helps to interpret the learned vectors.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Reflection", "Transition", "Narrative_cite", "Single_summ", "Single_summ", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 8, "token_end": 28, "char_start": 40, "char_end": 111, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Snow et al., 2004;": "1854720", "Mintz et al., 2009)": "10910955"}}}, {"token_start": 85, "token_end": 136, "char_start": 445, "char_end": 611, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vu et al., 2016;": "17297069", "Zeng et al., 2015;": "2778800", "Xu et al., 2015a;": "12203896", "Xu et al., 2015b;": "5403702", "Zhang and Wang, 2015)": "11717703"}}}, {"token_start": 137, "token_end": 160, "char_start": 614, "char_end": 705, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Li et al., 2015;": "9283982", "dos Santos et al., 2015)": "15620570"}}}, {"token_start": 162, "token_end": 195, "char_start": 714, "char_end": 870, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Miwa and Bansal, 2016)": "2476229"}, "Reference": {}}}, {"token_start": 199, "token_end": 222, "char_start": 897, "char_end": 985, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "464513_1", "paragraph": "[BOS] Semi-supervised and unsupervised models are also applied for extracting opinion relations.\n[BOS] Approaches include rule-based bootstrapping (Qiu et al., 2011) , graph propagation algorithms (Xu et al., 2013; Liu et al., 2014; Brody and Elhadad, 2010) , integer programming (Lu et al., 2011) , and probabilistic topic models (Titov and McDonald, 2008; Mukherjee and Liu, 2012) .\n\n", "discourse_tags": ["Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 19, "token_end": 33, "char_start": 122, "char_end": 165, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Qiu et al., 2011)": "1578481"}}}, {"token_start": 34, "token_end": 61, "char_start": 168, "char_end": 257, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Xu et al., 2013;": "9874999", "Liu et al., 2014;": "14142685", "Brody and Elhadad, 2010)": "16669252"}}}, {"token_start": 62, "token_end": 72, "char_start": 260, "char_end": 297, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lu et al., 2011)": "111092"}}}, {"token_start": 74, "token_end": 93, "char_start": 304, "char_end": 382, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Titov and McDonald, 2008;": "1599046", "Mukherjee and Liu, 2012)": "2466315"}}}]}
{"id": "464513_0", "paragraph": "[BOS] Opinion relation extraction is an important task for fine-grained sentiment analysis.\n[BOS] If human annotations are provided (e.g. MPQA corpus (Deng and Wiebe, 2015) ), we could formulate the task into a supervised relation extraction problem as (Kobayashi et al., 2007; Johansson and Moschitti, 2013) .\n[BOS] Two types of models have been applied: pipeline models which first extract candidates of opinion expressions and targets then identify correct relations (Wu et al., 2009; Yang and Cardie, 2012) , and joint models which extract opinion expressions, targets and relations using a unified joint model (Yang and Cardie, 2013; Yang and Cardie, 2014) .\n[BOS] One consideration of applying supervised methods is their dependencies on the domains and human annotations.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 28, "token_end": 39, "char_start": 138, "char_end": 172, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Deng and Wiebe, 2015)": "12355873"}}}, {"token_start": 48, "token_end": 71, "char_start": 211, "char_end": 308, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kobayashi et al., 2007;": "17068090", "Johansson and Moschitti, 2013)": "8525297"}}}, {"token_start": 94, "token_end": 111, "char_start": 452, "char_end": 510, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wu et al., 2009;": "13573624", "Yang and Cardie, 2012)": "11176977"}}}, {"token_start": 125, "token_end": 143, "char_start": 595, "char_end": 661, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yang and Cardie, 2013;": "1594813", "Yang and Cardie, 2014)": "2280062"}}}]}
{"id": "28929215_1", "paragraph": "[BOS] Paradigm completion.\n[BOS] SIGMORPHON hosted two shared tasks on paradigm completion (Cotterell et al., 2016 (Cotterell et al., , 2017 , in order to encourage the development of systems for the task.\n[BOS] One approach is to treat it as a string transduction problem by applying an alignment model with a semi-Markov model (Durrett and DeNero, 2013; Nicolai et al., 2015) .\n[BOS] Recently, neural sequenceto-sequence models are also widely used (Faruqui et al., 2016; Kann and Schtze, 2016; Aharoni and Goldberg, 2017; Zhou and Neubig, 2017) .\n[BOS] All the above mentioned work were designed for one single language.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 14, "token_end": 35, "char_start": 71, "char_end": 140, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cotterell et al., , 2017": "519236"}}}, {"token_start": 67, "token_end": 89, "char_start": 311, "char_end": 377, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Durrett and DeNero, 2013;": "6023976", "Nicolai et al., 2015)": "14929030"}}}, {"token_start": 93, "token_end": 137, "char_start": 396, "char_end": 547, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Faruqui et al., 2016;": "3089175", "Kann and Sch\u00fctze, 2016;": "6387118", "Aharoni and Goldberg, 2017;": "18487032"}}}]}
{"id": "28929215_0", "paragraph": "[BOS] Transfer learning with encoder-decoder networks.\n[BOS] Encoder-decoder RNNs were introduced by Cho et al. (2014) and Sutskever et al. (2014) and extended by an attention mechanism by Bahdanau et al. (2015) .\n[BOS] Lately, much work was done on multi-task learning and transfer learning with encoder-decoder RNNs.\n[BOS] Luong et al. (2015) investigated multi-task setups for sequence-to-sequence learning, combining multiple encoders and decoders.\n[BOS] In contrast, in our experiments, we use only one encoder and one decoder.\n[BOS] There exists much work on multi-task learning with encoderdecoder RNNs for machine translation (Johnson et al., 2016; Dong et al., 2015; Firat et al., 2016; Ha et al., 2016) .\n[BOS] Alonso and Plank (2016) explored multi-task learning empirically, analyzing when it improves performance.\n[BOS] Here, we focus on how transfer via multi-task learning works.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Single_summ", "Reflection", "Narrative_cite", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 11, "token_end": 36, "char_start": 61, "char_end": 146, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Cho et al. (2014)": null, "Sutskever et al. (2014)": "7961699"}}}, {"token_start": 40, "token_end": 52, "char_start": 166, "char_end": 211, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Bahdanau et al. (2015)": "11212020"}}}, {"token_start": 77, "token_end": 107, "char_start": 325, "char_end": 452, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Luong et al. (2015)": "6954272"}, "Reference": {}}}, {"token_start": 130, "token_end": 173, "char_start": 565, "char_end": 712, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Johnson et al., 2016;": "6053988", "Dong et al., 2015;": "3666937", "Firat et al., 2016;": "6359641", "Ha et al., 2016)": "5234044"}}}, {"token_start": 175, "token_end": 195, "char_start": 721, "char_end": 826, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "3830451_0", "paragraph": "[BOS] Hypernym detection from text is one of the most popular hierarchical relation extraction tasks in ontology learning for which research work dates back to at least 1984 (Calzolari, 1984) .\n[BOS] Hypernym can be described as a linguistic term for a word whose meaning includes the meanings of other words, which are known as hyponyms.\n[BOS] For instance, flower is a hypernym of daisy and rose.\n[BOS] On the other hand, daisy and rose are some of the hyponyms of flower.\n[BOS] In simple words, this relation deals with identifying the concepts and finding the particular superclass they fit in.\n[BOS] Manually constructing these kind of relations from text is a time-consuming and labourintensive procedure.\n[BOS] Hence, the researchers felt the need to make this process automatic.\n[BOS] The methods proposed can broadly be categorized into two: supervised and unsupervised.\n[BOS] While the unsupervised methods can identify and extract semantic relations from plain text without the need of any preannotated text corpora, the supervised methods often find it difficult to find an annotated corpora in similar domain.\n[BOS] A major part of the previous researches on automatic semantic classification of words was developed based on the method first proposed by Hearst, that the presence of certain lexico-syntactic patterns can indicate a particular semantic relationship between two noun phrases (Hearst, 1992) .\n[BOS] This paper introduced six basic lexical patterns.\n[BOS] This rule based approach was further extended in subsequent works bringing out more valid patterns, either handcrafted or learned from training corpus for semantic relation extraction (Berland and Charniak, 1999 ) (Kozareva et al., 2008 ) (Widdows and Dorow, 2002) .\n[BOS] Pattern based results are effective and reliable, scores high on precision measure.\n[BOS] However, these methods suffer in terms of recall.\n[BOS] Later, a few distributional approaches were proposed by different authors making use of the large corpora present in (Guido Boella, 2014) (Navigli and Velardi, 2010) .\n[BOS] Machine learning based methods make use of features like term co-occurrence, semantic similarity or other syntantic information from text collection.\n[BOS] Precision of these machine learning based approaches is lower compared to pattern based approach.\n[BOS] The simple morpho-syntactic approach also proved to yield a decent result (Lefever, 2015) (Sang et al., 2011) .\n[BOS] In recent years, several researches are being carried out to extract semantic relations from texts in other languages.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Transition", "Transition", "Transition", "Transition", "Transition", "Transition", "Narrative_cite", "Transition", "Narrative_cite", "Transition", "Transition", "Narrative_cite", "Transition", "Transition", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 19, "token_end": 38, "char_start": 104, "char_end": 191, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Calzolari, 1984)": "1500098"}}}, {"token_start": 258, "token_end": 270, "char_start": 1356, "char_end": 1417, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hearst, 1992)": "15763200"}}}, {"token_start": 307, "token_end": 341, "char_start": 1637, "char_end": 1746, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Berland and Charniak, 1999": "17767129", "(Kozareva et al., 2008": "1560925", "(Widdows and Dorow, 2002)": "11473223"}}}, {"token_start": 384, "token_end": 405, "char_start": 1993, "char_end": 2066, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Navigli and Velardi, 2010)": "9372965"}}}, {"token_start": 449, "token_end": 479, "char_start": 2335, "char_end": 2444, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lefever, 2015)": "12730435", "(Sang et al., 2011)": "1236074"}}}]}
{"id": "44158400_1", "paragraph": "[BOS] The works mentioned above improve the attention models by access auxiliary information, and thus they modify the structure of attention models in both inference and learning.\n[BOS] In contrast, ; Liu et al. (2016b) ; Chen et al. (2016) maintain the structure of the attention models in inference but utilize some external signals to supervise the outputs of attention models during the learning.\n[BOS] They improve the generalization abilities of attention models by use of the external aligners as the signals, which typically yield alignment results accurate enough to guide the learning of attention.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Multi_summ"], "span_citation_mapping": [{"token_start": 35, "token_end": 111, "char_start": 202, "char_end": 609, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Liu et al. (2016b)": null, "Chen et al. (2016)": "17078659"}, "Reference": {}}}]}
{"id": "44158400_0", "paragraph": "[BOS] Attention model becomes a standard component for many applications due to its ability of dynamically selecting the informative context from sequential representations.\n[BOS] For example, Xu et al. (2015) propose an attention based neural network for image caption task and advance the state-of-the-art results; Yin et al. (2015) put the attention structure between a pair of convolution networks for answer selection, paraphrase identification and textual entailment tasks.\n[BOS] In the context of machine translation, the idea of attention based neural networks has been pioneered by Bahdanau et al. (2014) ; Luong et al. (2015b) improve the attention with a gated operator for encoding states and a decoding state, and and Dutil et al. (2017) enhance attention through a planning mechanism.\n[BOS] Furthermore, Feng et al. (2016) adopt a recurrent structure for attention to take longterm dependencies into account, propose a look-ahead attention by additionally modeling the translation history, and Cohn et al. (2016) incorporate structural biases into attention models.\n[BOS] Recently introduce the syntactic knowledge into attention models.\n[BOS] These works are essentially similar to the propose approach, since we introduce auxiliary information from a target foresight word into the attention model.\n[BOS] However, there is a significant difference between our approach and their approaches.\n[BOS] Our auxiliary information biases to the word to be translated at next timestep while theirs biases to the information available so far at the current timestep, and thereby our approach is orthogonal to theirs.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Multi_summ", "Multi_summ", "Transition", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 29, "token_end": 58, "char_start": 193, "char_end": 315, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Xu et al. (2015)": "1055111"}, "Reference": {}}}, {"token_start": 59, "token_end": 90, "char_start": 317, "char_end": 479, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Yin et al. (2015)": "5535381"}, "Reference": {}}}, {"token_start": 98, "token_end": 143, "char_start": 525, "char_end": 721, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Bahdanau et al. (2014)": "11212020", "Luong et al. (2015b)": "1998416"}, "Reference": {}}}, {"token_start": 146, "token_end": 161, "char_start": 731, "char_end": 798, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Dutil et al. (2017)": "31174170"}, "Reference": {}}}, {"token_start": 164, "token_end": 196, "char_start": 818, "char_end": 1002, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 198, "token_end": 213, "char_start": 1008, "char_end": 1079, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Cohn et al. (2016)": "1964946"}, "Reference": {}}}]}
{"id": "2851973_0", "paragraph": "[BOS] Some earlier work (Lappin and Leass, 1994; Kennedy and Boguraev, 1996) use heuristic to determine whether a phrase is anaphoric or not.\n[BOS] Bean and Riloff (1999) extracts rules from non-anaphoric noun phrases and noun phrases patterns, which are then applied to test data to identify existential noun phrases.\n[BOS] It is intended as as pre-filtering step before a coreference resolution system is run.\n[BOS] Ng and Cardie (2002a) trains a separate anaphoricity classifier in addition to a coreference model.\n[BOS] The anaphoricity classifier is applied as a filter and only anaphoric mentions are later considered by the coreference model.\n[BOS] Ng (2004) studies what is the best way to make use of anaphoricity information and concludes that the constrained-based and globallyoptimized approach works the best.\n[BOS] Poesio et al. (2004) contains a good summary of recent research work on discourse new or anaphoricity.\n[BOS] Luo et al. (2004) uses a start model to determine whether a mention is the first one in a coreference chain, but it is computed ad hoc without training.\n[BOS] Nicolae and Nicolae (2006) constructs a graph where mentions are nodes and an edge represents the likelihood two mentions are in an entity, and then a graph-cut algorithm is employed to produce final coreference results.\n[BOS] We take the view that determining whether an anaphor is coreferential with any candidate antecedent is part of the coreference process.\n[BOS] But we do recognize that the disparity between the two types of events: while a coreferential relationship can be resolved by examining the local context of the anaphor and its antecedent, it is necessary to compare the anaphor with all the preceding candidates before it can be declared that it is not coreferential with any.\n[BOS] Thus, a creation component P c (|E t , m t ) is needed to model the second type of events.\n[BOS] A problem arising from the adoption of the creation model is that it is very expensive to have a conditional model depending on all preceding entities E t .\n[BOS] To solve this problem, we adopt the MaxEnt model and impose some reasonable constraints on the feature functions, which makes it possible to synthesize features in the creation model from those of the link model.\n[BOS] The twin model components are intimately trained and used simultaneously in our coreference system.\n\n", "discourse_tags": ["Multi_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection", "Reflection", "Reflection", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 35, "char_start": 6, "char_end": 141, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Lappin and Leass, 1994;": "11500985", "Kennedy and Boguraev, 1996)": "5462334"}, "Reference": {}}}, {"token_start": 36, "token_end": 90, "char_start": 148, "char_end": 411, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 91, "token_end": 140, "char_start": 418, "char_end": 649, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ng and Cardie (2002a)": null}, "Reference": {}}}, {"token_start": 141, "token_end": 175, "char_start": 656, "char_end": 822, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ng (2004)": "5692502"}, "Reference": {}}}, {"token_start": 176, "token_end": 201, "char_start": 829, "char_end": 931, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Poesio et al. (2004)": "14642384"}, "Reference": {}}}, {"token_start": 202, "token_end": 237, "char_start": 938, "char_end": 1090, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Luo et al. (2004)": "8810581"}, "Reference": {}}}, {"token_start": 238, "token_end": 286, "char_start": 1097, "char_end": 1317, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Nicolae and Nicolae (2006)": null}, "Reference": {}}}]}
{"id": "44087711_1", "paragraph": "[BOS] There is a huge body of research on mining bitexts, e.g. by analyzing the name of WEB pages or links (Resnik and Smith, 2003) .\n[BOS] Another direction of research is to use cross-lingual information retrieval, e.g. (Utiyama and Isahara, 2003; Munteanu and Marcu, 2005; Rauf and Schwenk, 2009 ).\n[BOS] There are some works which use joint embeddings in the process of filtering or mining bitexts.\n[BOS] For instance, Grgoire and Langlais (2017) first embed sentences into two separate spaces.\n[BOS] Then, a classifier is learned on labeled data to decide whether sentences are parallel or not.\n[BOS] Our approach clearly outperforms this technique on the BUCC corpus (cf.\n[BOS] section 4).\n[BOS] Bouamor and Sajjad (2018) use averaged multilingual word embeddings to calculate a joint embedding of all sen-tences.\n[BOS] However, distances between all sentences are only used to extract a set of potential mutual translation.\n[BOS] The decision is based on a different system.\n[BOS] In NMT systems for Zh  En are learned using a joint encoder.\n[BOS] A sentence representation is obtained as the mean of the last encoder states.\n[BOS] Noisy bitexts are filtered based on the distance.\n[BOS] In all these works, embeddings are learned for two languages only, while we learn one joint embedding for up to nine languages.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Transition", "Single_summ", "Single_summ", "Reflection", "Transition", "Single_summ", "Single_summ", "Single_summ", "Transition", "Transition", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 19, "token_end": 36, "char_start": 63, "char_end": 131, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Resnik and Smith, 2003)": "41263"}}}, {"token_start": 45, "token_end": 84, "char_start": 180, "char_end": 298, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Utiyama and Isahara, 2003;": "62276184", "Munteanu and Marcu, 2005;": "15289038", "Rauf and Schwenk, 2009": null}}}, {"token_start": 107, "token_end": 147, "char_start": 409, "char_end": 599, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Gr\u00e9goire and Langlais (2017)": "7450499"}, "Reference": {}}}, {"token_start": 168, "token_end": 225, "char_start": 702, "char_end": 981, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Bouamor and Sajjad (2018)": null}, "Reference": {}}}]}
{"id": "44087711_0", "paragraph": "[BOS] The problem of how to select parts of bitexts has been addressed before, but mainly from the aspect of domain adaptation (Axelrod et al., 2011; Santamara and Axelrod, 2017) .\n[BOS] It was successfully used in many phrase-based MT systems, but it was reported to be less successful for NMT (van der Wees et al., 2017) .\n[BOS] It should be stressed that domain adaptation is different from filtering noisy training data.\n[BOS] Data selection extracts the most relevant bitexts for the test set domain, but does not necessarily remove wrong translations, e.g. source and target sentences are both in-domain and well formed, but they are not mutual translations.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 24, "token_end": 46, "char_start": 109, "char_end": 178, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Axelrod et al., 2011;": "10766958", "Santamar\u00eda and Axelrod, 2017)": "131773987"}}}, {"token_start": 69, "token_end": 82, "char_start": 291, "char_end": 322, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(van der Wees et al., 2017)": "7921428"}}}]}
{"id": "4410027_2", "paragraph": "[BOS] Within the framework of Neural MT, there is one work that has similar motivation to ours.\n[BOS] Choi et al. (2017) leverage the NBOW as context and gate the word-embedding on both encoder and decoder side.\n[BOS] However, their work does not distinguish context vectors for words in the same sequence, in contrast to the method in this paper, and our results demonstrate that this is an important feature of methods that handle homographs in NMT.\n[BOS] In addition, our quantitative analysis of the problems that homographs pose to NMT and evaluation of how context-aware models fix them was not covered in this previous work.\n\n", "discourse_tags": ["Transition", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 21, "token_end": 47, "char_start": 102, "char_end": 211, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Choi et al. (2017)": "7037110"}, "Reference": {}}}]}
{"id": "4410027_1", "paragraph": "[BOS] In contrast to the standard WSD formulation, Vickrey et al. (2005) reformulated the task of WSD for Statistical Machine Translation (SMT) as predicting possible target translations which directly improves the accuracy of machine translation.\n[BOS] Following this reformulation, Chan et al. (2007) ; Carpuat and Wu (2007a,b) integrated WSD systems into phrase-based systems.\n[BOS] Xiong and Zhang (2014) breaks the process into two stages.\n[BOS] First predicts the sense of the ambiguous source word.\n[BOS] The predicted word senses together with other context features are then used to predict possible target translation.\n\n", "discourse_tags": ["Single_summ", "Multi_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 49, "char_start": 6, "char_end": 247, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Vickrey et al. (2005)": "7241107"}, "Reference": {}}}, {"token_start": 50, "token_end": 84, "char_start": 254, "char_end": 379, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Chan et al. (2007)": "14598745"}, "Reference": {}}}, {"token_start": 85, "token_end": 129, "char_start": 386, "char_end": 628, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Xiong and Zhang (2014)": "1690435"}, "Reference": {}}}]}
{"id": "4410027_0", "paragraph": "[BOS] Word sense disambiguation (WSD), the task of determining the correct meaning or sense of a word in context is a long standing task in NLP (Yarowsky, 1995; Ng and Lee, 1996; Mihalcea and Faruque, 2004; Navigli, 2009; Zhong and Ng, 2010; Di Marco and Navigli, 2013; Chen et al., 2014; Camacho-Collados et al., 2015) .\n[BOS] Recent research on tackling WSD and capturing multi-senses includes work leveraging LSTM (Kgebck and Salomonsson, 2016; Yuan et al., 2016) , which we extended as a context network in our paper and predicting senses with word embeddings that capture context (Kgebck and Sa- (ref), the translation generated by our best context-aware model (best), and the translation generated by baseline model (base).\n[BOS] We also highlight the word with multiple senses in source language in bold, the corresponding correctly translated words in blue and wrongly translated words in red.\n[BOS] The definitions of words in blue or red are in parenthesis.\n[BOS] lomonsson, 2016; Yuan et al., 2016) .\n[BOS] uster et al. (2016) ; Kawakami and Dyer (2016) also showed that bilingual data improves WSD.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Reflection", "Transition", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 2, "token_end": 96, "char_start": 6, "char_end": 319, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yarowsky, 1995;": "1487550", "Ng and Lee, 1996;": "11202365", "Mihalcea and Faruque, 2004;": "15037844", "Navigli, 2009;": "461624", "Zhong and Ng, 2010;": "11174540", "Di Marco and Navigli, 2013;": "1775181", "Chen et al., 2014;": "2434362", "Camacho-Collados et al., 2015)": "3080112"}}}, {"token_start": 103, "token_end": 134, "char_start": 356, "char_end": 466, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(K\u00e5geb\u00e4ck and Salomonsson, 2016;": "14748840"}}}, {"token_start": 235, "token_end": 248, "char_start": 974, "char_end": 1009, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 250, "token_end": 277, "char_start": 1018, "char_end": 1110, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Kawakami and Dyer (2016)": "11706860"}}}]}
{"id": "2485577_0", "paragraph": "[BOS] Recently, discriminative methods for alignment have rivaled the quality of IBM Model 4 alignments (Liu et al., 2005; Ittycheriah and Roukos, 2005; Taskar et al., 2005; Moore et al., 2006; Fraser and Marcu, 2007b) .\n[BOS] However, except for (Fraser and Marcu, 2007b) , none of these advances in alignment quality has improved translation quality of a state-of-the-art system.\n[BOS] We use a discriminatively trained model to identify and delete incorrect links, and demonstrate that these gains in alignment quality lead to gains in translation quality in a stateof-the-art syntax-based MT system.\n[BOS] In contrast to the semi-supervised LEAF alignment algorithm of (Fraser and Marcu, 2007b) , which requires 1,500-2,000 CPU days per iteration to align 8.4M ChineseEnglish sentences (anonymous, p.c.\n[BOS] ), link deletion requires only 450 CPU hours to re-align such a corpus (after initial alignment by GIZA++, which requires 20-24 CPU days).\n[BOS] Several recent works incorporate syntactic features into alignment.\n[BOS] (May and Knight, 2007) use syntactic constraints to re-align a parallel corpus that has been aligned by GIZA++ as follows: they extract string-to-tree transducer rules from the corpus, the target parse trees, and the alignment; discard the initial alignment; use the extracted rules to construct a forest of possible string-to-tree derivations for each string/tree pair in the corpus; use EM to select the Viterbi derivation tree for each pair; and finally, induce a new alignment from the Viterbi derivations, using the re-aligned corpus to train a syntax-based MT system.\n[BOS] (May and Knight, 2007) differs from our approach in two ways: first, the set of possible re-alignments they consider for each sentence pair is limited by the initial GIZA++ alignments seen over the training corpus, while we consider all alignments that can be reached by deleting links from the initial GIZA++ alignment for that sentence pair.\n[BOS] Second, (May and Knight, 2007) use a time-intensive training algorithm to select the best re-alignment for each sentence pair, while we use a fast greedy search to determine which links to delete; in contrast to (May and Knight, 2007) , who require 400 CPU hours to re-align 330k Chinese-English sentence pairs (anonymous, p.c), link deletion requires only 18 CPU hours to re-align such a corpus.\n[BOS] (Lopez and Resnik, 2005) and (Denero and Klein, 2007 ) modify the distortion model of the HMM alignment model (Vogel et al., 1996) to reflect tree distance rather than string distance; (Cherry and Lin, 2006 ) modify an ITG aligner by introducing a penalty for induced parses that violate syntactic bracketing constraints.\n[BOS] Similarly to these approaches, we use syntactic bracketing to constrain alignment, but our work extends beyond improving alignment quality to improve translation quality as well.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Reflection", "Single_summ", "Single_summ", "Transition", "Single_summ", "Single_summ", "Single_summ", "Multi_summ", "Reflection"], "span_citation_mapping": [{"token_start": 4, "token_end": 62, "char_start": 16, "char_end": 218, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Liu et al., 2005;": "15319550", "Taskar et al., 2005;": "2379886", "Moore et al., 2006;": null}}}, {"token_start": 64, "token_end": 78, "char_start": 227, "char_end": 272, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 145, "token_end": 232, "char_start": 610, "char_end": 951, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 243, "token_end": 369, "char_start": 1032, "char_end": 1605, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 370, "token_end": 442, "char_start": 1612, "char_end": 1955, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 443, "token_end": 537, "char_start": 1962, "char_end": 2358, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(May and Knight, 2007)": "14328976"}, "Reference": {}}}, {"token_start": 538, "token_end": 580, "char_start": 2365, "char_end": 2548, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Denero and Klein, 2007": "9882011"}, "Reference": {"(Vogel et al., 1996)": null}}}, {"token_start": 581, "token_end": 610, "char_start": 2550, "char_end": 2686, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Cherry and Lin, 2006": "2787289"}, "Reference": {}}}]}
{"id": "2941631_2", "paragraph": "[BOS] Our work is conceptually close to the recent CoNLL shared tasks on semantic role labeling, where the predicate frames were converted to se-1627 Figure 2 : Overview of the approach.\n[BOS] Rounded rectangles indicate domain-independent components; regular rectangles mark domain-specific modules; blocks in dashed lines surround components not necessary for the domain presented in this paper.\n[BOS] mantic dependencies between predicates and their arguments (Surdeanu et al., 2008; Hajic et al., 2009) .\n[BOS] In this representation the dependency structure is a directed acyclic graph (DAG), i.e., the same node can be an argument to multiple predicates, and there are no explicit dependencies between predicates.\n[BOS] Due to this representation, all joint models proposed for semantic role labeling handle semantic frames independently.\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 72, "token_end": 100, "char_start": 404, "char_end": 506, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Surdeanu et al., 2008;": null}}}]}
{"id": "2941631_1", "paragraph": "[BOS] In the biomedical domain, two recent papers proposed joint models for event extraction based on Markov logic networks (MLN) (Riedel et al., 2009; Poon and Vanderwende, 2010) .\n[BOS] Both works propose elegant frameworks where event anchors and arguments are jointly predicted for all events in the same sentence.\n[BOS] One disadvantage of MLN models is the requirement that a human expert develop domainspecific predicates and formulas, which can be a cumbersome process because it requires thorough domain understanding.\n[BOS] On the other hand, our approach maintains the joint modeling advantage, but our model is built over simple, domain-independent features.\n[BOS] We also propose and analyze a richer feature space that captures more information on the global event structure in a sentence.\n[BOS] Furthermore, since our approach is agnostic to the parsing model used, it could easily be tuned for various scenarios, e.g., models with lower inference overhead such as shift-reduce parsers.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 18, "token_end": 43, "char_start": 102, "char_end": 179, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Riedel et al., 2009;": "475213", "Poon and Vanderwende, 2010)": "1160159"}}}]}
{"id": "2941631_0", "paragraph": "[BOS] The pioneering work of Miller et al. (1997) was the first, to our knowledge, to propose parsing as a framework for information extraction.\n[BOS] They extended the syntactic annotations of the Penn Treebank corpus (Marcus et al., 1993) with entity and relation mentions specific to the MUC-7 evaluation (Chinchor et al., 1997 ) -e.g., EMPLOYEE OF relations that hold between person and organization named entities -and then trained a generative parsing model over this combined syntactic and semantic representation.\n[BOS] In the same spirit, Finkel and Manning (2009) merged the syntactic annotations and the named entity annotations of the OntoNotes corpus (Hovy et al., 2006) and trained a discriminative parsing model for the joint problem of syntactic parsing and named entity recognition.\n[BOS] However, both these works require a unified annotation of syntactic and semantic elements, which is not always feasible, and focused only on named entities and binary relations.\n[BOS] On the other hand, our approach focuses on event structures that are nested and have an arbitrary number of arguments.\n[BOS] We do not need a unified syntactic and semantic representation (but we can and do extract features from the underlying syntactic structure of the text).\n[BOS] Finkel and Manning (2009b) also proposed a parsing model for the extraction of nested named entity mentions, which, like this work, parses just the corresponding semantic annotations.\n[BOS] In this work, we focus on more complex structures (events instead of named entities) and we explore more global features through our reranking layer.\n\n", "discourse_tags": ["Single_summ", "Narrative_cite", "Single_summ", "Transition", "Reflection", "Reflection", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 32, "char_start": 6, "char_end": 144, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Miller et al. (1997)": null}, "Reference": {}}}, {"token_start": 40, "token_end": 53, "char_start": 198, "char_end": 240, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Marcus et al., 1993)": "252796"}}}, {"token_start": 54, "token_end": 75, "char_start": 246, "char_end": 330, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chinchor et al., 1997": null}}}, {"token_start": 110, "token_end": 165, "char_start": 528, "char_end": 799, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Finkel and Manning (2009)": null}, "Reference": {"(Hovy et al., 2006)": "8508974"}}}, {"token_start": 250, "token_end": 288, "char_start": 1274, "char_end": 1457, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Finkel and Manning (2009b)": "10573012"}, "Reference": {}}}]}
{"id": "44075965_0", "paragraph": "[BOS] Multi-task Learning Neural networks make multi-task learning via (hard) parameter sharing particularly easy (Caruana, 1993) and has shown to be successful for a variety of NLP tasks, such as machine translation (Dong et al., 2015; Luong et al., 2016) , keyphrase boundary classification (Augenstein and Sgaard, 2018) , tagging (Martnez Alonso and Plank, 2017; Bjerva et al., 2016) , complex word identification (Bingel and Bjerva, 2018) , and natural language understanding .\n[BOS] For sequence labelling, many combinations of tasks have been explored, e.g., by Martnez Alonso and Plank (2017); Bjerva (2017a,b) .\n[BOS] An analysis of different task combinations was performed by Sgaard and Goldberg (2016) ; .\n[BOS] presented a more flexible architecture, which learned what to share between the main and auxiliary tasks, and might require further investigation in future work.\n[BOS] combine multi-task learning with semi-supervised learning for strongly related tasks with different output spaces.\n[BOS] For this shared task, we opt for a simple hard parameter sharing strategy, though we would expect to see improvements with more involved architectures.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Narrative_cite", "Transition", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 14, "token_end": 28, "char_start": 71, "char_end": 129, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Caruana, 1993)": null}}}, {"token_start": 44, "token_end": 62, "char_start": 197, "char_end": 256, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dong et al., 2015;": "3666937", "Luong et al., 2016)": "6954272"}}}, {"token_start": 63, "token_end": 80, "char_start": 259, "char_end": 322, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Augenstein and S\u00f8gaard, 2018)": null}}}, {"token_start": 81, "token_end": 101, "char_start": 325, "char_end": 386, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mart\u00ednez Alonso and Plank, 2017;": "2418468"}}}, {"token_start": 102, "token_end": 115, "char_start": 389, "char_end": 442, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 122, "token_end": 159, "char_start": 488, "char_end": 617, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 161, "token_end": 178, "char_start": 626, "char_end": 712, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "27930067_3", "paragraph": "[BOS] The relevance vector we used is significantly different from the attention matrix (Bahdanau et al., 2015) .\n[BOS] While attention only demonstrates the association degree between source and target words, relevance can be used to calculate the association degree between two arbitrary neurons in neural networks.\n[BOS] In addition, relevance is effective in analyzing the effect of source and target contexts on generating target words.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 12, "token_end": 24, "char_start": 71, "char_end": 111, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bahdanau et al., 2015)": "11212020"}}}]}
{"id": "27930067_2", "paragraph": "[BOS] Our work is closely related to previous visualization approaches that compute the contribution of a unit at the input layer to the final decision at the output layer (Simonyan et al., 2014; Mahendran and Vedaldi, 2015; Nguyen et al., 2015; Girshick et al., 2014; Bach et al., 2015; .\n[BOS] Among them, our approach bears most resemblance to (Bach et al., 2015) since we adapt layer-wise relevance propagation to neural machine translation.\n[BOS] The major difference is that word vectors rather than single pixels are the basic units in NMT.\n[BOS] Therefore, we propose vectorlevel relevance based on neuron-level relevance for NMT.\n[BOS] Calculating weight ratios has also been carefully designed for the operators in NMT.\n[BOS] The proposed approach also differs from in that we use relevance rather than partial derivative to quantify the contributions of contextual words.\n[BOS] A major advantage of using relevance is that it does not require neural activations to be differentiable or smooth (Bach et al., 2015) .\n\n", "discourse_tags": ["Narrative_cite", "Reflection", "Reflection", "Reflection", "Reflection", "Reflection", "Narrative_cite"], "span_citation_mapping": [{"token_start": 9, "token_end": 71, "char_start": 46, "char_end": 286, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Simonyan et al., 2014;": "1450294", "Mahendran and Vedaldi, 2015;": "206593185", "Nguyen et al., 2015;": "206592585", "Girshick et al., 2014;": "1241368"}}}, {"token_start": 77, "token_end": 92, "char_start": 308, "char_end": 366, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bach et al., 2015)": "215192424"}}}, {"token_start": 196, "token_end": 211, "char_start": 954, "char_end": 1023, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bach et al., 2015)": "215192424"}}}]}
{"id": "27930067_1", "paragraph": "[BOS] We observe that unrelated words are more likely to occur if multiple contextual words have high values in the relevance vector of the target word being generated.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "27930067_0", "paragraph": "[BOS] Given a source sentence \"ci ci huiyi de yi ge zhongyao yiti shi kuadaxiyang guanxi\" (one the the top agendas of the meeting is to discuss the cross-atlantic relations), the model prediction is \"a key topic of the meeting is to forge ahead\".\n[BOS] One translation error is that the 9-th English word \"forge\" is totally unrelated to the source sentence.\n[BOS] Figure 9 visualizes the hidden states of the 9-th target word \"forge\".\n[BOS] We find that while the attention identifies the 10-th source word \"kuadaxiyang\" (cross-atlantic) to be most relevant, the relevance vector of the target word R y 9 finds that multiple source and target words should contribute to the generation of the next target word.\n\n", "discourse_tags": ["Other", "Other", "Other", "Other"], "span_citation_mapping": []}
{"id": "31779941_1", "paragraph": "[BOS] Some other methods also do not have such requirement.\n[BOS] Plank and Moschitti (2013) designed the semantic syntactic tree kernel (SSTK) to learn cross-domain patterns.\n[BOS] Nguyen et al. (2015b) constructed a case study comparing feature-based methods and kernel-based models.\n[BOS] They presented some effective features and kernels (e.g. word embedding).We share the same intuition of finding those cross-domain features, but our work differs from such previous work in that they manually designed those features and kernels while we automatically learn cross-domain features from unlabeled target-domain examples with neural networks.\n[BOS] To our best knowledge, this is the first work on neural networks for domain adaptation of relation extraction.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 13, "token_end": 39, "char_start": 66, "char_end": 175, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Plank and Moschitti (2013)": "3011134"}, "Reference": {}}}, {"token_start": 40, "token_end": 63, "char_start": 182, "char_end": 285, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Nguyen et al. (2015b)": "7333692"}, "Reference": {}}}]}
{"id": "31779941_0", "paragraph": "[BOS] There has been a lot of research on domain adaptation in natural language processing (Blitzer et al., 2006; Daume, 2007; Jing and Zhai, 2007; Glorot et al., 2011; Ajakan et al., 2014; Ganin and Lempitsky, 2015) .\n[BOS] Most of the existing domain adaptation methods are based on discrete feature representations and linear classifiers.\n[BOS] There is also recent work on domain adaptation for relation extraction including feature-based systems (Nguyen and Grishman, 2014; and kernelbased system (Plank and Moschitti, 2013) .\n[BOS] Nguyen and Grishman (2014) and both require a few labels in the target domain.\n[BOS] Our proposed method can perform domain adaptation without target labels.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Narrative_cite", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 13, "token_end": 63, "char_start": 63, "char_end": 216, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Blitzer et al., 2006;": "15978939", "Daume, 2007;": "5360764", "Jing and Zhai, 2007;": "15036406", "Glorot et al., 2011;": "18235792", "Ajakan et al., 2014;": "18126905", "Ganin and Lempitsky, 2015)": "6755881"}}}, {"token_start": 95, "token_end": 107, "char_start": 429, "char_end": 477, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 109, "token_end": 122, "char_start": 483, "char_end": 529, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Plank and Moschitti, 2013)": "3011134"}}}, {"token_start": 124, "token_end": 143, "char_start": 538, "char_end": 616, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Nguyen and Grishman (2014)": "18895336"}}}]}
{"id": "2468773_3", "paragraph": "[BOS] Older work on morphological inflection includes Ahlberg et al. (2014) ; Durrett and DeNero (2013) ; Nicolai et al. (2015) ; Faruqui et al. (2016) , inter alia.\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 5, "token_end": 48, "char_start": 20, "char_end": 151, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Ahlberg et al. (2014)": null, "Durrett and DeNero (2013)": "6023976", "Nicolai et al. (2015)": "14929030", "Faruqui et al. (2016)": "3089175"}}}]}
{"id": "2468773_2", "paragraph": "[BOS] Research not immediately done for the shared tasks included papers on multi-source reinflection Kann et al., 2017a) , cross-lingual transfer for reinflection (Kann et al., 2017b) , or first intents of neural inflection systems which make use of context for lemmatization (Bergmanis and Goldwater, 2018) .\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 13, "token_end": 27, "char_start": 76, "char_end": 121, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Kann et al., 2017a)": "482862"}}}, {"token_start": 28, "token_end": 46, "char_start": 124, "char_end": 184, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kann et al., 2017b)": "519236"}}}, {"token_start": 48, "token_end": 74, "char_start": 190, "char_end": 308, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bergmanis and Goldwater, 2018)": "44063762"}}}]}
{"id": "2468773_1", "paragraph": "[BOS] The first edition of the shared task in 2016 (Cotterell et al., 2016) resulted in 3 different types of systems: \"pipeline approaches\" (unsupervised alignment algorithms applied to the source-target pairs, followed by a model which predicts edit operations), \"neural approaches\", and \"linguistically inspired systems\".\n[BOS] The winning system was a neural network, namely a character-based RNN encoder-decoder model with attention, similar to the one we use here (Kann and Schtze, 2016) .\n[BOS] Hence, neural models gained popularity in the 2017 edition of the shared task (Cotterell et al., 2017a) .\n[BOS] In 2017, explicit low-resource settings were first introduced to the shared task.\n[BOS] These settings demonstrated the effectiveness of hard attention in neural sequence-to-sequence models if training data are limited (Makarov et al., 2017) .\n\n", "discourse_tags": ["Single_summ", "Narrative_cite", "Narrative_cite", "Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 2, "token_end": 66, "char_start": 6, "char_end": 321, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 79, "token_end": 107, "char_start": 380, "char_end": 492, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kann and Sch\u00fctze, 2016)": "16406435"}}}, {"token_start": 111, "token_end": 134, "char_start": 508, "char_end": 604, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cotterell et al., 2017a)": "482862"}}}, {"token_start": 159, "token_end": 184, "char_start": 750, "char_end": 854, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Makarov et al., 2017)": "28704517"}}}]}
{"id": "2468773_0", "paragraph": "[BOS] Most recent work on morphological reinflection was done in the context of the SIGMOR-PHON 2016 and the CoNLL-SIGMORPHON 2017 shared tasks.\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "2445030_1", "paragraph": "[BOS] Our work is closely related to recent work on injecting prior knowledge into NMT (Arthur et al., 2016; Cohn et al., 2016; Tang et al., 2016; Feng et al., 2016; .\n[BOS] The major difference is that our approach aims to provide a general framework for incorporating arbitrary prior knowledge sources while keeping the neural translation model unchanged.\n[BOS] also propose to combine the strengths of neural networks on learning representations and log-linear models on encoding prior knowledge.\n[BOS] But they treat neural translation models as a feature in the log-linear model.\n[BOS] In contrast, we connect the two models via KL divergence to keep the transparency of our approach to model architectures.\n[BOS] This enables our approach to be easily applied to other neural models in NLP.\n\n", "discourse_tags": ["Narrative_cite", "Reflection", "Transition", "Transition", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 13, "token_end": 47, "char_start": 68, "char_end": 164, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Arthur et al., 2016;": "10086161", "Cohn et al., 2016;": "1964946", "Tang et al., 2016;": "18192067"}}}]}
{"id": "2445030_0", "paragraph": "[BOS] Our work is directly inspired by posterior regularization (Ganchev et al., 2010) .\n[BOS] The major difference is that we use a log-linear model to represent the desired distribution rather than a constrained posterior set.\n[BOS] Using log-linear models not only enables our approach to incorporate arbitrary knowledge sources as real-valued features, but also is differentiable to be jointly trained with neural translation models efficiently.\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 8, "token_end": 20, "char_start": 39, "char_end": 86, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ganchev et al., 2010)": "6589999"}}}]}
{"id": "27970287_3", "paragraph": "[BOS] On the stochastic front, Xu et al. (2015) demonstrate the effectiveness of \"hard\" attention.\n[BOS] While standard \"soft\" attention averages the representations of where the model attends to, hard attention discretely selects a single location.\n[BOS] Hard attention has been successfully applied in various computer vision tasks (Mnih et al., 2014; , but so far has limited usage in NLP.\n[BOS] We will apply hard attention to the document summarization task by sparsifying our reading of the source text.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 50, "char_start": 6, "char_end": 249, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Xu et al. (2015)": "1055111"}, "Reference": {}}}, {"token_start": 59, "token_end": 70, "char_start": 312, "char_end": 352, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "27970287_2", "paragraph": "[BOS] Several methods, some stochastic and some deterministic, have been explored in the vein of conditional computation.\n[BOS] In this work, we will focus on stochastic methods, although deterministic methods are worth considering as future work (Rae et al., 2016; Shazeer et al., 2017; Miller et al., 2016; Martins and Astudillo, 2016) .\n\n", "discourse_tags": ["Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 34, "token_end": 77, "char_start": 188, "char_end": 337, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rae et al., 2016;": null, "Shazeer et al., 2017;": null, "Miller et al., 2016;": "2711679", "Martins and Astudillo, 2016)": "16432551"}}}]}
{"id": "27970287_1", "paragraph": "[BOS] Many techniques have been proposed in the literature to efficiently handle the problem of large inputs to deep neural networks.\n[BOS] One particular framework is that of \"conditional computation\", as coined by Bengio et al. (2013) -the idea is to only compute a subset of a network's units for a given input by gating different parts of the network.\n\n", "discourse_tags": ["Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 31, "token_end": 47, "char_start": 177, "char_end": 236, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Bengio et al. (2013)": "18406556"}}}]}
{"id": "27970287_0", "paragraph": "[BOS] In summarization, neural attention models were first applied by Rush et al. (2015) to do headline generation, i.e. produce a title for a news article given only the first sentence.\n[BOS] Nallapati et al. (2016) and See et al. (2017) apply attention models to summarize full documents, achieving stateof-the-art results on the CNN/Dailymail dataset.\n[BOS] All of these models, however, suffer from the inherent complexity of attention over the full document.\n[BOS] Indeed, See et al. (2017) report that a single model takes over 3 days to train.\n\n", "discourse_tags": ["Single_summ", "Multi_summ", "Transition", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 44, "char_start": 6, "char_end": 186, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Rush et al. (2015)": "1918428"}, "Reference": {}}}, {"token_start": 45, "token_end": 86, "char_start": 193, "char_end": 354, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Nallapati et al. (2016)": "8928715", "See et al. (2017)": null}, "Reference": {}}}, {"token_start": 107, "token_end": 128, "char_start": 470, "char_end": 550, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"See et al. (2017)": null}, "Reference": {}}}]}
{"id": "28044867_3", "paragraph": "[BOS] Compatibility As generic semantic similarity judgements are known to be \"fuzzy\" (Faruqui et al., 2016) , we also evaluate on Kruszewski and Baroni (2015) 's benchmark on semantic compatibility.\n[BOS] They define two words as being semantically compatible \"if they can potentially refer to the same thing\".\n[BOS] We expect our denotational and visual embeddings to be highly useful for this task.\n[BOS] We report unsupervised results obtained from cosine similarities between word embeddings.\n\n", "discourse_tags": ["Narrative_cite", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 14, "token_end": 27, "char_start": 79, "char_end": 108, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Faruqui et al., 2016)": "7392978"}}}, {"token_start": 28, "token_end": 41, "char_start": 111, "char_end": 159, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Kruszewski and Baroni (2015)": "17863073"}}}]}
{"id": "28044867_2", "paragraph": "[BOS] Silberer and Lapata (2014)'s data with semantic (SemSim) and visual similarity (VisSim) ratings.\n\n", "discourse_tags": ["Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 29, "char_start": 6, "char_end": 102, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "28044867_1", "paragraph": "[BOS] Similarity We evaluate on some similarity data sets, reporting Spearman  correlations between human ratings and cosine similarities for word vectors.\n[BOS] We use the MEN (Bruni et al., 2012) and\n\n", "discourse_tags": ["Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 25, "token_end": 38, "char_start": 162, "char_end": 197, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bruni et al., 2012)": "8712237"}}}]}
{"id": "28044867_0", "paragraph": "[BOS] We now have four different continuous representations for words; in the following, we evaluate them for how well they predict semantic relations.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "46920380_1", "paragraph": "[BOS] Self-attention has been successfully applied in various NLP applications including neural machine translation (Vaswani et al., 2017) , abstractive summarization (Paulus et al., 2017) and sentence embedding (Lin et al., 2017) .\n[BOS] Self-attention links different positions of a sequence to generate a structural representation for the sequence.\n[BOS] In reading comprehension literature, self-attention has been investigated.\n[BOS] (Wang et al., 2017b) proposed a Gated Self-Matching mechanism which produced context-enhanced token encodings in a document.\n[BOS] In this paper, we have a different angle for applying self-attention.\n[BOS] We employ selfattention to weight and propagate evidences from different positions of a query to the cloze position to enhance reading comprehension performance.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 15, "token_end": 28, "char_start": 89, "char_end": 138, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vaswani et al., 2017)": "13756489"}}}, {"token_start": 29, "token_end": 42, "char_start": 141, "char_end": 188, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Paulus et al., 2017)": "21850704"}}}, {"token_start": 43, "token_end": 53, "char_start": 193, "char_end": 230, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 87, "token_end": 115, "char_start": 439, "char_end": 563, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Wang et al., 2017b)": "12501880"}, "Reference": {}}}]}
{"id": "46920380_0", "paragraph": "[BOS] The cloze-style reading comprehension task can be formulated as: Given a document-query pair (d, q), select cC that answers the cloze position in q where C is the candidate set.\n[BOS] Each candidate answer c appears at least once in the document d. Below are related approaches to address reading comprehension problem.\n[BOS] Hermann et al. (2015) employed Attentive Reader that computes a document vector via attention using q, giving a joint representation g(d(q), q).\n[BOS] In some sense, d(q) becomes a queryaware representation of a document.\n[BOS] Impatient Reader was proposed in the same paper to model the joint representation but in a incremental fashion.\n[BOS] Stanford Reader (Chen et al., 2016) further simplified Attentive Reader with shallower recurrent units and a bilinear attention.\n[BOS] Attention-Sum (AS) Reader introduced a bias towards frequently occurred entity candidates via summation of the probabilities of the same entity instances in a document (Kadlec et al., 2016) .\n[BOS] Cui et al. (2017) proposed Attention-over-Attention (AoA) Reader that employed a two-way attention for reading comprehension.\n[BOS] Multi-hop architecture for text comprehension was also investigated in (Hill et al., 2016; Sordoni et al., 2016; Shen et al., 2017; Munkhdalai and Yu, 2017; Dhingra et al., 2017) .\n[BOS] Kobayashi et al. (2016) and built dynamic representations for candidate answers while reading the document, sharing the same spirit to GA Reader (Dhingra et al., 2017) where token encodings of a document become query-aware.\n[BOS] Brarda et al. (2017) proposed sequential attention to make the alignment of query and document tokens context-aware.\n[BOS] Wang et al. (2017a) showed that additional linguistic features improve reading comprehension.\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ", "Transition", "Transition", "Single_summ", "Single_summ", "Single_summ", "Narrative_cite", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 69, "token_end": 105, "char_start": 332, "char_end": 476, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 146, "token_end": 171, "char_start": 678, "char_end": 806, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 172, "token_end": 209, "char_start": 813, "char_end": 1002, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 211, "token_end": 240, "char_start": 1011, "char_end": 1136, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 241, "token_end": 294, "char_start": 1143, "char_end": 1321, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Sordoni et al., 2016;": "14500125", "Shen et al., 2017;": "6300274"}}}, {"token_start": 296, "token_end": 344, "char_start": 1330, "char_end": 1553, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 345, "token_end": 370, "char_start": 1560, "char_end": 1676, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 371, "token_end": 388, "char_start": 1683, "char_end": 1776, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wang et al. (2017a)": "27246259"}, "Reference": {}}}]}
{"id": "34166175_4", "paragraph": "[BOS] In the last few years, several important efforts have broken new ground with more comprehensive event schema induction.\n[BOS] These efforts discover new event types in unfiltered text, and identify verb argument positions associated with overall event roles.\n[BOS] Chambers and Jurafsky (2011) used a pipeline approach, first discovering related event patterns, then clustering arguments into event slots.\n[BOS] For the first step, they tested both LDA and agglomerative clustering, based on event terms' co-occurrence.\n[BOS] They used the MUC-4 data set, but relied on an additional external corpus for role induction, due to data limitations when clustering roles separately in each event category.\n[BOS] Chambers (2013 ), Cheung et al (2013 , Nguyen et al (2015) , and Sha et al (2016) all use probabilistic generative models that jointly model the assignment of predicates to event schemas and arguments to event roles.\n[BOS] Chambers uses an entitydriven model, linking coreferring arguments to the same event role.\n[BOS] Cheung et al focus on event clauses and model transitions between them, using a pair of HMMs.\n[BOS] Nguyen et al (2015) add phrase modifiers to argument similarity scores, and Sha et al (2016) add a normalized cut approach to maximize intra-class similarity within slots.\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ", "Single_summ", "Single_summ", "Multi_summ", "Multi_summ", "Multi_summ", "Multi_summ"], "span_citation_mapping": [{"token_start": 47, "token_end": 132, "char_start": 271, "char_end": 706, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Chambers and Jurafsky (2011)": "12808163"}, "Reference": {}}}, {"token_start": 133, "token_end": 259, "char_start": 713, "char_end": 1304, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Chambers (2013": "6341459", "), Cheung et al (2013": "278288", "Nguyen et al (2015)": "1017257", "Sha et al (2016)": "5741899"}, "Reference": {}}}]}
{"id": "34166175_3", "paragraph": "[BOS] There has been growing work over the past decade on purely unsupervised role induction.\n[BOS] Most of these efforts begin with a set of documents known to cover a type of event or domain, then cluster verb arguments to determine each verb's role slots within that domain (Filatova et al., 2006; Sekine, 2006) .\n[BOS] These approaches typically learn verb-specific roles, rather than multi-verb event schemas.\n[BOS] Other recent work models multiple verb roles in combination, in various forms of subject-verb-object relational triples (O'Connor et al., 2013; Balasubramanian et al., 2013) .\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 39, "token_end": 68, "char_start": 199, "char_end": 314, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Filatova et al., 2006;": "2225746", "Sekine, 2006)": "648239"}}}, {"token_start": 102, "token_end": 131, "char_start": 502, "char_end": 594, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(O'Connor et al., 2013;": "4989386", "Balasubramanian et al., 2013)": "2578382"}}}]}
{"id": "34166175_2", "paragraph": "[BOS] Open IE systems (Banko et al., 2007; Angeli et al., 2015) extract general relational patterns between entity pairs, based on domain-independent patterns or heuristics.\n[BOS] Similar efforts have emerged to extract more complex event frames by bootstrapping from seed event patterns (Huang and Riloff, 2012; Surdeanu et al., 2006; Yangarber et al., 2000; Patwardhan and Riloff, 2007) .\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 2, "token_end": 39, "char_start": 6, "char_end": 173, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Banko et al., 2007;": null, "Angeli et al., 2015)": "6015236"}}}, {"token_start": 54, "token_end": 95, "char_start": 268, "char_end": 388, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Huang and Riloff, 2012;": "2213149", "Surdeanu et al., 2006;": "7419156", "Yangarber et al., 2000;": "2344397", "Patwardhan and Riloff, 2007)": "5749336"}}}]}
{"id": "34166175_1", "paragraph": "[BOS] Semi-supervised approaches have been used to identify relations between pairs of entities, using seed pairs with known relations (Brin, 1998; Culotta and Sorensen, 2004; Mintz et al., 2009 ).\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 18, "token_end": 45, "char_start": 103, "char_end": 194, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Brin, 1998;": "6075461", "Culotta and Sorensen, 2004;": "7395989", "Mintz et al., 2009": "10910955"}}}]}
{"id": "34166175_0", "paragraph": "[BOS] Early automation of event extraction relied on rule-based pattern matching, using hand-written templates (Chinchor et al., 1993; Schrodt et al., 1994) .\n[BOS] Modern efforts have focused on supervised machine learning, using annotated corpora for training data, again with pre-defined event types and roles (Miyao et al., 2008; Bjorne and Salakoski, 2011; Bunescu and Mooney, 2004) .\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 16, "token_end": 39, "char_start": 88, "char_end": 156, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chinchor et al., 1993;": "16857799", "Schrodt et al., 1994)": "61672326"}}}, {"token_start": 51, "token_end": 94, "char_start": 231, "char_end": 387, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Miyao et al., 2008;": null, "Bjorne and Salakoski, 2011;": "18361255", "Bunescu and Mooney, 2004)": null}}}]}
{"id": "4829361_2", "paragraph": "[BOS] Degeneracy of Variational Autoencoders.\n[BOS] For sequence modeling, VAEs are often merged with the RNN encoder-decoder structure (Bowman et al., 2016; Serban et al., 2017; Zhao et al., 2017) where the encoder predicts the posterior distribution of a latent variable z, and the decoder models the output distributions conditioned on z.\n[BOS] However, Bowman et al. (2016) report that a VAE with a RNN decoder easily degenerates; that is, it learns to ignore the latent variable z and falls back to a vanilla RNN.\n[BOS] They propose two techniques to alleviate this issue: KL annealing and word drop.\n[BOS] Chen et al. (2017) interpret this degeneracy in the context of bits-back coding and show that a VAE equipped with autoregressive models such as RNNs often ignores the latent variable to minimize the code length needed for describing data.\n[BOS] They propose to constrain the decoder to selectively encode the information of interest in the latent variable.\n[BOS] However, their empirical results are limited to an image domain.\n[BOS] Zhao et al. (2017) use an auxiliary bag-of-words loss on the latent variable to force the model to use z.\n[BOS] That is, they train an auxiliary network that predicts bag-of-words representation of the target utterance based on z.\n[BOS] Yet this loss works in an opposite di-rection to the original objective of VAEs that minimizes the minimum description length.\n[BOS] Thus, it may be in danger of forcibly moving the information that is better modeled in the decoder to the latent variable.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 11, "token_end": 76, "char_start": 52, "char_end": 341, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Bowman et al., 2016;": "748227", "Serban et al., 2017;": "14857825", "Zhao et al., 2017)": "14688760"}, "Reference": {}}}, {"token_start": 79, "token_end": 138, "char_start": 357, "char_end": 605, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Bowman et al. (2016)": "748227"}, "Reference": {}}}, {"token_start": 139, "token_end": 221, "char_start": 612, "char_end": 1039, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Chen et al. (2017)": "15534684"}, "Reference": {}}}, {"token_start": 222, "token_end": 301, "char_start": 1046, "char_end": 1409, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhao et al. (2017)": "14688760"}, "Reference": {}}}]}
{"id": "4829361_1", "paragraph": "[BOS] Recently, latent variable models such as VAEs have been adopted in language modeling (Bowman et al., 2016; Zhang et al., 2016; Serban et al., 2017) .\n[BOS] The VHRED model (Serban et al., 2017) integrates the VAE with the HRED to model Twitter and Ubuntu IRC conversations by introducing an utterance latent variable.\n[BOS] This makes a conditional VAE where the generation process is conditioned on the context of conversation.\n[BOS] Zhao et al. (2017) further make use of discourse act labels to capture the diversity of conversations.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 15, "token_end": 41, "char_start": 73, "char_end": 153, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bowman et al., 2016;": "748227", "Zhang et al., 2016;": "9134916", "Serban et al., 2017)": "14857825"}}}, {"token_start": 43, "token_end": 100, "char_start": 162, "char_end": 434, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Serban et al., 2017)": "14857825"}, "Reference": {}}}, {"token_start": 101, "token_end": 122, "char_start": 441, "char_end": 543, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhao et al. (2017)": "14688760"}, "Reference": {}}}]}
{"id": "4829361_0", "paragraph": "[BOS] Conversation Modeling.\n[BOS] One popular approach for conversation modeling is to use RNN-based encoders and decoders, such as (Vinyals and Le, 2015; Sordoni et al., 2015b; Shang et al., 2015) .\n[BOS] Hierarchical recurrent encoder-decoder (HRED) models (Sordoni et al., 2015a; Serban et al., , 2017 consist of utterance encoder and decoder, and a context RNN which runs over utterance representations to model long-term temporal structure of conversation.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 15, "token_end": 54, "char_start": 92, "char_end": 198, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vinyals and Le, 2015;": "12300158", "Sordoni et al., 2015b;": "94285", "Shang et al., 2015)": "7356547"}}}, {"token_start": 56, "token_end": 85, "char_start": 207, "char_end": 305, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sordoni et al., 2015a;": null, "Serban et al., , 2017": "14857825"}}}]}
{"id": "3438497_1", "paragraph": "[BOS] Parallel WaveNet Simultaneously with Gu et al. (2017) , Oord et al. (2017) presented a nonautoregressive sequence model for speech generation.\n[BOS] They use inverse autoregressive flow (IAF, Kingma et al., 2016) to map a sequence of independent random variables to a target sequence.\n[BOS] They apply the IAF multiple times, similarly to our iterative refinement strategy.\n[BOS] Their approach is however restricted to continuous target variables, while the proposed approach in principle could be applied to both discrete and continuous variables.\n[BOS] Novak et al. (2016) proposed a convolutional neural network that iteratively predicts and applies token substitutions given a translation from a phasebased translation system.\n[BOS] Unlike their system, our approach can edit an intermediate translation with a higher degree of freedom.\n[BOS] QuickEdit (Grangier and Auli, 2017) and deliberation network (Xia et al., 2017) incorporate the idea of refinement into neural machine translation.\n[BOS] Both systems consist of two autoregressive decoders.\n[BOS] The second decoder takes into account the translation generated by the first decoder.\n[BOS] We extend these earlier efforts by incorporating more than one refinement steps without necessitating extra annotations.\n[BOS] Bordes et al. (2017) proposed an unconditional generative model for images based on iterative refinement.\n[BOS] At each step l of iterative refinement, the model is trained to maximize the log-likelihood of target Y given the weighted mixture of generated samples from the previous iteration l1 and a corrupted target .\n[BOS] That is, the corrupted version of target is \"infused\" into generated samples during training.\n[BOS] In the domain of text, however, computing a weighted mixture of two sequences of discrete tokens is not well defined, and we propose to stochastically mix denoising and lowerbound maximization objectives.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Multi_summ", "Multi_summ", "Multi_summ", "Reflection", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 14, "char_start": 6, "char_end": 59, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Gu et al. (2017)": null}}}, {"token_start": 15, "token_end": 110, "char_start": 62, "char_end": 555, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Oord et al. (2017)": "27706557"}, "Reference": {}}}, {"token_start": 111, "token_end": 160, "char_start": 562, "char_end": 847, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Novak et al. (2016)": "4863328"}, "Reference": {}}}, {"token_start": 161, "token_end": 222, "char_start": 854, "char_end": 1152, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Xia et al., 2017)": "13481571"}, "Reference": {}}}, {"token_start": 242, "token_end": 321, "char_start": 1286, "char_end": 1705, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Bordes et al. (2017)": "3073252"}, "Reference": {}}}]}
{"id": "3438497_0", "paragraph": "[BOS] Non-Autoregressive Neural Machine Translation Schwenk (2012) proposed a continuousspace translation model to estimate the conditional distribution over a target phrase given a source phrase, while dropping the conditional dependencies among target tokens.\n[BOS] The evaluation was however limited to reranking and to short phrase pairs (up to 7 words on each side) only.\n[BOS] Kaiser and Bengio (2016) investigated neural GPU (Kaiser and Sutskever, 2015) , for machine translation.\n[BOS] They evaluated both non-autoregressive and autoregressive approaches, and found that the nonautoregressive approach significantly lags behind the autoregressive variants.\n[BOS] It however differs from our approach that each iteration does not output a refined version from the previous iteration.\n[BOS] The recent paper by Gu et al. (2017) is most relevant to the proposed work.\n[BOS] They similarly introduced a sequence of discrete latent variables.\n[BOS] They however use supervised learning for inference, using the word alignment tool (Dyer et al., 2013) .\n[BOS] To achieve the best result, Gu et al. (2017) stochastically sample the latent variables and rerank the corresponding target sequences with an external, autoregressive model.\n[BOS] This is in contrast to the proposed approach which is fully deterministic during decoding and does not rely on any extra reranking mechanism.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 69, "char_start": 6, "char_end": 376, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 70, "token_end": 145, "char_start": 383, "char_end": 790, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kaiser and Bengio (2016)": null}, "Reference": {"(Kaiser and Sutskever, 2015)": "2009318"}}}, {"token_start": 146, "token_end": 199, "char_start": 797, "char_end": 1053, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Gu et al. (2017)": null}, "Reference": {"(Dyer et al., 2013)": "8476273"}}}, {"token_start": 201, "token_end": 262, "char_start": 1062, "char_end": 1383, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Gu et al. (2017)": null}, "Reference": {}}}]}
{"id": "46936631_1", "paragraph": "[BOS] Natural language generation (NLG)-based abstractive summarization (Carenini and Cheung, 2008; Gerani et al., 2014; Fabbrizio et al., 2014; Liu et al., 2015; Takase et al., 2016 ) also makes extensive use of structural information, including syntactic/semantic parse trees, discourse structures, and domainspecific templates built using a text planner or an OpenIE system (Pighin et al., 2014) .\n[BOS] In particular, Cao et al. (2018) leverage OpenIE and dependency parsing to extract fact tuples from the source text and use those to improve the faithfulness of summaries.\n[BOS] Different from the above approaches, this paper seeks to directly incorporate source-side syntactic structure in the copy mechanism of an abstractive sentence summarization system.\n[BOS] It learns to recognize important source words and relations during training, while striving to preserve them in the summaries at test time to aid reproduction of factual details.\n[BOS] Our intent of incorporating source syntax in summarization is different from that of neural machine translation (NMT) (Li et al., 2017a; Chen et al., 2017) , in part because NMT does not handle the information loss from source to target.\n[BOS] In contrast, a summarization system must selectively preserve source content to render concise and grammatical summaries.\n[BOS] We specifically focus on sentence summarization, where the goal is to reduce the first sentence of an article to a title-like summary.\n[BOS] We believe even for this reasonably simple task there remains issues unsolved.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Reflection", "Reflection", "Narrative_cite", "Transition", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 57, "char_start": 6, "char_end": 182, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Carenini and Cheung, 2008;": "8660413", "Gerani et al., 2014;": "2767900", "Fabbrizio et al., 2014;": "12682781", "Liu et al., 2015;": "5001921", "Takase et al., 2016": "5450302"}}}, {"token_start": 87, "token_end": 99, "char_start": 363, "char_end": 398, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Pighin et al., 2014)": "14959"}}}, {"token_start": 104, "token_end": 136, "char_start": 422, "char_end": 578, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Cao et al. (2018)": "19198109"}, "Reference": {}}}, {"token_start": 215, "token_end": 238, "char_start": 1042, "char_end": 1112, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Li et al., 2017a;": "12637374", "Chen et al., 2017)": "3504277"}}}]}
{"id": "46936631_0", "paragraph": "[BOS] Prior to the deep learning era, sentence syntactic structure has been utilized to generate summaries with an \"extract-and-compress\" framework.\n[BOS] Compressed summaries are generated using a joint model to extract sentences and drop non-important syntactic constituents (Daume III and Marcu, 2002; BergKirkpatrick et al., 2011; Thadani and McKeown, 2013; Durrett et al., 2016) , or a pipeline approach that combines generic sentence compression (McDonald, 2006; Clarke and Lapata, 2008; Filippova et al., 2015) with a sentence pre-selection or post-selection process (Zajic et al., 2007; Galanis and Androutsopoulos, 2010; Wang et al., 2013; Li et al., 2013; Li et al., 2014) .\n[BOS] Although syntactic information is helpful for summarization, there has been little prior work investigating how best to combine sentence syntactic structure with the neural abstractive summarization systems.\n[BOS] Existing neural summarization systems handle syntactic structure only implicitly (Kikuchi et al., 2016; Chen et al., 2016; Zhou et al., 2017; Tan et al., 2017; Paulus et al., 2017) .\n[BOS] Most systems adopt a \"cut-andstitch\" scheme that picks words either from the vocabulary or the source text and stitch them together using a recurrent language model.\n[BOS] However, there lacks a mechanism to ensure structurally salient words and relations in source sentences are preserved in the summaries.\n[BOS] The resulting summary sentences can contain misleading information (e.g., \"mozambican man arrested for murder\" flips the meaning of the original) or grammatical errors (e.g., verbless, as in \"alaska father who was too drunk to drive\").\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Narrative_cite", "Transition", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 39, "token_end": 87, "char_start": 213, "char_end": 383, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Daume III and Marcu, 2002;": "189898", "Thadani and McKeown, 2013;": "13458891", "Durrett et al., 2016)": "5125975"}}}, {"token_start": 90, "token_end": 118, "char_start": 391, "char_end": 517, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(McDonald, 2006;": "3014198", "Clarke and Lapata, 2008;": "3004447", "Filippova et al., 2015)": "1992250"}}}, {"token_start": 120, "token_end": 170, "char_start": 525, "char_end": 682, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zajic et al., 2007;": null, "Wang et al., 2013;": "1260503", "Li et al., 2013;": "8928513", "Li et al., 2014)": "10112929"}}}, {"token_start": 206, "token_end": 254, "char_start": 914, "char_end": 1085, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kikuchi et al., 2016;": "11157751", "Chen et al., 2016;": "12755643", "Zhou et al., 2017;": "1770102", "Tan et al., 2017;": "26698484", "Paulus et al., 2017)": "21850704"}}}]}
{"id": "416723_1", "paragraph": "[BOS] Our work is inspired by recent efforts on attaching an external memory to neural networks, such as neural turing machines (Graves et al., 2014) , memory networks (Weston et al., 2014; Meng et al., 2015) and exploiting an external memory (Tang et al., 2016; during translation.\n[BOS] Tang et al. (2016) exploited a phrase memory for NMT, which stores phrase pairs in symbolic form.\n[BOS] They let the decoder utilize a mixture of word-generating and phrase-generating component, to generate a sequence of multiple words all at once.\n[BOS] extended the NMT decoder by maintaining an external memory, which is operated by reading and writing operations of neural turing machines (Graves et al., 2014) , while keeping a read-only copy of the original source annotations along side the \"read-write\" memory.\n[BOS] These powerful extensions have been verified on Chinese-English translation tasks.\n[BOS] Our INTERACTIVE ATTENTION is different from previous works.\n[BOS] We take the annotations of source sentence as a memory instead of using an external memory, and we design a mechanism to directly read from and write to it during translation.\n[BOS] Therefore, the original source annotations are not accessible in later steps.\n[BOS] More specially, our model inherited the notation and some simple operations for writing from (Graves et al., 2014) , while NMT IA extends it to \"unbounded\" memory for representing the source.\n[BOS] In addition, although the read-write operations in INTERACTIVE ATTENTION are not exactly the same with those in (Graves et al., 2014; , our model can also achieve good performance.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ", "Narrative_cite", "Transition", "Reflection", "Reflection", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 21, "token_end": 33, "char_start": 105, "char_end": 149, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Graves et al., 2014)": "15299054"}}}, {"token_start": 34, "token_end": 52, "char_start": 152, "char_end": 208, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Meng et al., 2015)": "15937983"}}}, {"token_start": 55, "token_end": 64, "char_start": 227, "char_end": 261, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 69, "token_end": 121, "char_start": 289, "char_end": 537, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Tang et al. (2016)": "18192067"}, "Reference": {}}}, {"token_start": 142, "token_end": 154, "char_start": 659, "char_end": 703, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Graves et al., 2014)": "15299054"}}}, {"token_start": 257, "token_end": 274, "char_start": 1275, "char_end": 1349, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Graves et al., 2014)": "15299054"}}}, {"token_start": 297, "token_end": 320, "char_start": 1459, "char_end": 1565, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "416723_0", "paragraph": "[BOS] Our work is related to recent works that focus on improving attention models (Luong et al., 2015a; Cohn et al., 2016; Feng et al., 2016) .\n[BOS] Luong et al. (2015a) proposed to use global and local attention models to improve translation performance.\n[BOS] They use a global one to attend to all source words and a local one to look at a subset of source words at a time.\n[BOS] Cohn et al. (2016) extended the attentionbased NMT to include structural biases from word-based alignment models, which achieved improvements across several language pairs.\n[BOS] Feng et al. (2016) added implicit distortion and fertility models to attention-based NMT to achieve translation improvements.\n[BOS] These works are different with our IN-TERACTIVE ATTENTION approach, as we use a rather generic attentive reading while at the same time performing attentive writing.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 13, "token_end": 40, "char_start": 66, "char_end": 142, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Luong et al., 2015a;": "1998416", "Cohn et al., 2016;": "1964946", "Feng et al., 2016)": "8063399"}}}, {"token_start": 42, "token_end": 92, "char_start": 151, "char_end": 378, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Luong et al. (2015a)": "1998416"}, "Reference": {}}}, {"token_start": 93, "token_end": 126, "char_start": 385, "char_end": 557, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Cohn et al. (2016)": "1964946"}, "Reference": {}}}, {"token_start": 127, "token_end": 151, "char_start": 564, "char_end": 689, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Feng et al. (2016)": "8063399"}, "Reference": {}}}]}
{"id": "24544277_1", "paragraph": "[BOS] ishahi et al., 2017) and language processing models (Khn, 2015; Qian et al., 2016a,b; Adi et al., 2016; Linzen et al., 2016) .\n[BOS] Methodologically, our work is most similar to Shi et al. (2016) and , who also used hidden vectors from neural MT models to predict linguistic properties.\n[BOS] However, they focused on relatively lowlevel tasks (syntax and morphology, respectively), while we apply the approach to a semantic task and compare the results with a POS tagging task.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 10, "char_start": 6, "char_end": 26, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 11, "token_end": 47, "char_start": 31, "char_end": 130, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(K\u00f6hn, 2015;": null, "Adi et al., 2016;": "6771196", "Linzen et al., 2016)": "14091946"}}}, {"token_start": 58, "token_end": 81, "char_start": 185, "char_end": 293, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Shi et al. (2016)": "7197724"}, "Reference": {}}}]}
{"id": "24544277_0", "paragraph": "[BOS] Techniques for analyzing neural network models include visualization of hidden units (Elman, 1991; Karpathy et al., 2015; Kdr et al., 2016; Qian et al., 2016a) , which provide illuminating, but often anecdotal information on how the network works.\n[BOS] A number of studies aim to obtain quantitative correlations between parts of the neural network and linguistic properties, in both speech (Wang et al., 2017; Wu and King, 2016; Table 6 : POS and SEM tagging accuracy with features from different layers of 2/3/4-layer encoders, averaged over all non-English target languages.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 9, "token_end": 45, "char_start": 61, "char_end": 165, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Elman, 1991;": "7069311", "Karpathy et al., 2015;": "988348", "K\u00e1d\u00e1r et al., 2016;": "611341"}}}, {"token_start": 85, "token_end": 99, "char_start": 391, "char_end": 435, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wang et al., 2017;": "14589038"}}}]}
{"id": "44064978_1", "paragraph": "[BOS] Reinforcement learning (RL) has been recently applied to a number of NLP applications, including dialog generation (Li et al., 2017) , machine translation (MT) (Ranzato et al., 2016; Gu et al., 2018) , question answering (Choi et al., 2017) , and summarization and sentence simplification (Zhang and Lapata, 2017; Paulus et al., 2017; Chen and Bansal, 2018; Narayan et al., 2018) .\n[BOS] This study leverages RL to explore the space of possible extractive summaries.\n[BOS] The summaries are encouraged to preserve salient source content useful for answering questions as well as sharing common words with the abstracts.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 20, "token_end": 30, "char_start": 103, "char_end": 138, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Li et al., 2017)": "98180"}}}, {"token_start": 31, "token_end": 53, "char_start": 141, "char_end": 205, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ranzato et al., 2016;": "7147309", "Gu et al., 2018)": null}}}, {"token_start": 54, "token_end": 64, "char_start": 208, "char_end": 246, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Choi et al., 2017)": null}}}, {"token_start": 66, "token_end": 102, "char_start": 253, "char_end": 385, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhang and Lapata, 2017;": "7473831", "Paulus et al., 2017;": "21850704", "Chen and Bansal, 2018;": "44129061", "Narayan et al., 2018)": "3510042"}}}]}
{"id": "44064978_0", "paragraph": "[BOS] This study focuses on generic summarization.\n[BOS] It is different from the query-based summarization (Daum III and Marcu, 2006; Dang and Owczarzak, 2008) , where systems are trained to select text pieces related to predefined queries.\n[BOS] In this work we have no predefined queries but the system carefully generates questions from human abstracts and learns to produce generic summaries that are capable of answering all questions.\n[BOS] Cloze questions have been used in reading comprehension (Richardson et al., 2013; Weston et al., 2016; Mostafazadeh et al., 2016; Rajpurkar et al., 2016) to test the system's ability to perform reasoning and language understanding.\n[BOS] Hermann et al. (2015) describe an approach to extract (context, question, answer) triples from news articles.\n[BOS] Our work draws on this approach to automatically create questions from human abstracts.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Reflection", "Narrative_cite", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 16, "token_end": 42, "char_start": 82, "char_end": 160, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Daum\u00e9 III and Marcu, 2006;": "6241932", "Dang and Owczarzak, 2008)": null}}}, {"token_start": 89, "token_end": 134, "char_start": 448, "char_end": 601, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Richardson et al., 2013;": "2100831", "Weston et al., 2016;": "3178759", "Mostafazadeh et al., 2016;": null, "Rajpurkar et al., 2016)": "11816014"}}}, {"token_start": 149, "token_end": 174, "char_start": 686, "char_end": 795, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hermann et al. (2015)": "6203757"}, "Reference": {}}}]}
{"id": "4406182_1", "paragraph": "[BOS] Attention mechanisms have shown to be crucial for summarization as well (Rush et al., 2015; Zeng et al., 2016; Nallapati et al., 2017) , and pointer networks (Vinyals et al., 2015a) , in particular, help address redundancy and saliency in generated summaries (Cheng and Lapata, 2016; See et al., 2017; Paulus et al., 2018; Fan et al., 2017) .\n[BOS] While we share the same motivation as these works, our work uniquely presents an approach based on CommNet, the deep communicating agent framework (Sukhbaatar et al., 2016) .\n[BOS] Compared to prior multi-agent works on logic puzzles (Foerster et al., 2017) , language learning (Lazaridou et al., 2016; Mordatch and Abbeel, 2017) and starcraft games (Vinyals et al., 2017) , we present the first study in using this framework for long text generation.\n[BOS] Finally, our model is related to prior works that address repetitions in generating long text.\n[BOS] See et al. (2017) introduce a post-trained coverage network to penalize repeated attentions over the same regions in the input, while Paulus et al. (2018) use intra-decoder attention to punish generating the same words.\n[BOS] In contrast, we propose a new semantic coherence loss and intermediate sentencebased rewards for reinforcement learning to discourage semantically similar generations ( 3).\n\n", "discourse_tags": ["Narrative_cite", "Reflection", "Narrative_cite", "Reflection", "Multi_summ", "Reflection"], "span_citation_mapping": [{"token_start": 10, "token_end": 39, "char_start": 56, "char_end": 140, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rush et al., 2015;": "1918428", "Zeng et al., 2016;": null, "Nallapati et al., 2017)": "6405271"}}}, {"token_start": 41, "token_end": 54, "char_start": 147, "char_end": 187, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 64, "token_end": 96, "char_start": 245, "char_end": 346, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cheng and Lapata, 2016;": "1499080", "See et al., 2017;": null, "Paulus et al., 2018;": "21850704", "Fan et al., 2017)": "22716243"}}}, {"token_start": 116, "token_end": 135, "char_start": 454, "char_end": 527, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sukhbaatar et al., 2016)": null}}}, {"token_start": 145, "token_end": 158, "char_start": 575, "char_end": 612, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Foerster et al., 2017)": "19141434"}}}, {"token_start": 159, "token_end": 182, "char_start": 615, "char_end": 684, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lazaridou et al., 2016;": null, "Mordatch and Abbeel, 2017)": "13548281"}}}, {"token_start": 183, "token_end": 196, "char_start": 689, "char_end": 727, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vinyals et al., 2017)": "28808621"}}}, {"token_start": 230, "token_end": 257, "char_start": 914, "char_end": 1040, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"See et al. (2017)": null}, "Reference": {}}}, {"token_start": 259, "token_end": 280, "char_start": 1048, "char_end": 1133, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Paulus et al. (2018)": "21850704"}, "Reference": {}}}]}
{"id": "4406182_0", "paragraph": "[BOS] Several recent works investigate attention mechanisms for encoder-decoder models to sharpen the context that the decoder should focus on within the input encoding (Luong et al., 2015; Vinyals et al., 2015b; Bahdanau et al., 2015) .\n[BOS] For example, Luong et al. (2015) proposes global and local attention networks for machine translation, while others investigate hierarchical attention networks for document classification (Yang et al., 2016) , sentiment classification , and dialog response selection (Zhou et al., 2016) .\n\n", "discourse_tags": ["Narrative_cite", "Single_summ"], "span_citation_mapping": [{"token_start": 26, "token_end": 56, "char_start": 154, "char_end": 235, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Luong et al., 2015;": "1998416", "Vinyals et al., 2015b;": "14223", "Bahdanau et al., 2015)": "61556494"}}}, {"token_start": 61, "token_end": 112, "char_start": 257, "char_end": 530, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Luong et al. (2015)": "1998416"}, "Reference": {"(Zhou et al., 2016)": null}}}]}
{"id": "25113027_2", "paragraph": "[BOS] In the respect of speeding up the decoding of the neural Transformer, Gu et al. (2018) change the auto-regressive architecture to speed up translation by directly generating target words without relying on any previous predictions.\n[BOS] However, compared with our work, their model achieves the improvement in decoding speed at the cost of the drop in translation quality.\n[BOS] Our model, instead, not only achieves a remarkable gain in terms of decoding speed, but also preserves the translation performance.\n[BOS] Developing fast and efficient attention module for the Transformer, to the best of our knowledge, has never been investigated before.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 72, "char_start": 6, "char_end": 379, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Gu et al. (2018)": "3480671"}, "Reference": {}}}]}
{"id": "25113027_1", "paragraph": "[BOS] The attention mechanism is originally proposed to induce translation-relevant source information for predicting next target word in NMT.\n[BOS] It contributes a lot to make NMT outperform SMT.\n[BOS] Recently, a variety of efforts are made to further improve its accuracy and capability.\n[BOS] Luong et al. (2015) explore several attention formulations and distinguish local attention from global attention.\n[BOS] Zhang et al. (2016) treat RNN as an alternative to the attention to improve model's capability in dealing with long-range dependencies.\n[BOS] Yang et al. (2017) introduce a recurrent cycle on the attention layer to enhance the model's memorization of previous translated source words.\n[BOS] Zhang et al. (2017a) observe the weak discrimination ability of the attention-generated context vectors and propose a GRU-gated attention network.\n[BOS] Kim et al. (2017) further model intrinsic structures inside attention through graphical models.\n[BOS] Shen et al. (2017) introduce a direction structure into a selfattention network to integrate both long-range dependencies and temporal order information.\n[BOS] Mi et al. (2016) and Liu et al. (2016) employ standard word alignment to supervise the automatically generated attention weights.\n[BOS] Our work also focus on the evolution of attention network, but unlike previous work, we seek to simplify the selfattention network so as to accelerate the decoding procedure.\n[BOS] The design of our model is partially inspired by the highway network (Srivastava et al., 2015) and the residual network (He et al., 2015) .\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Multi_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 55, "token_end": 75, "char_start": 298, "char_end": 411, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Luong et al. (2015)": "1998416"}, "Reference": {}}}, {"token_start": 76, "token_end": 106, "char_start": 418, "char_end": 553, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhang et al. (2016)": "15886238"}, "Reference": {}}}, {"token_start": 107, "token_end": 136, "char_start": 560, "char_end": 702, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Yang et al. (2017)": "14909391"}, "Reference": {}}}, {"token_start": 137, "token_end": 167, "char_start": 709, "char_end": 855, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhang et al. (2017a)": "1106874"}, "Reference": {}}}, {"token_start": 168, "token_end": 185, "char_start": 862, "char_end": 957, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kim et al. (2017)": null}, "Reference": {}}}, {"token_start": 186, "token_end": 215, "char_start": 964, "char_end": 1117, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Shen et al. (2017)": "19152001"}, "Reference": {}}}, {"token_start": 216, "token_end": 244, "char_start": 1124, "char_end": 1253, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Mi et al. (2016)": "18193214", "Liu et al. (2016)": "13292366"}, "Reference": {}}}, {"token_start": 289, "token_end": 302, "char_start": 1494, "char_end": 1535, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 304, "token_end": 314, "char_start": 1544, "char_end": 1578, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(He et al., 2015)": "206594692"}}}]}
{"id": "25113027_0", "paragraph": "[BOS] GRU (Chung et al., 2014) or LSTM (Hochreiter and Schmidhuber, 1997) RNNs are widely used for neural machine translation to deal with longrange dependencies as well as the gradient vanishing issue.\n[BOS] A major weakness of RNNs lies at its sequential architecture that completely disables parallel computation.\n[BOS] To cope with this problem, Gehring et al. (2017a) propose to use CNN-based encoder as an alternative to RNN, and Gehring et al. (2017b) further develop a completely CNNbased NMT system.\n[BOS] However, shallow CNN can only capture local dependencies.\n[BOS] Hence, CNNbased NMT normally develops deep archictures to model long-distance dependencies.\n[BOS] Different from these studies, Vaswani et al. (2017) propose the Transformer, a neural architecture that abandons recurrence and convolution.\n[BOS] It fully relies on attention networks to model translation.\n[BOS] The properties of parallelization and short dependency path significantly improve the training speed as well as model performance for the Transformer.\n[BOS] Unfortunately, as we have mentioned in Section 1, it suffers from decoding inefficiency.\n\n", "discourse_tags": ["Multi_summ", "Transition", "Multi_summ", "Transition", "Transition", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 50, "char_start": 6, "char_end": 202, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Chung et al., 2014)": null, "(Hochreiter and Schmidhuber, 1997)": "1915014"}, "Reference": {}}}, {"token_start": 76, "token_end": 99, "char_start": 350, "char_end": 430, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Gehring et al. (2017a)": "6728280"}, "Reference": {}}}, {"token_start": 101, "token_end": 121, "char_start": 436, "char_end": 508, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 158, "token_end": 216, "char_start": 707, "char_end": 1040, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Vaswani et al. (2017)": "13756489"}, "Reference": {}}}]}
{"id": "37605145_2", "paragraph": "[BOS] Integrating Morphology Some work has also been done in injecting morphological or more general linguistic knowledge into an NMT system.\n[BOS] Sennrich and Haddow (2016) proposed a factored model that incorporates linguistic features on the source side as additional factors.\n[BOS] An embedding is learned for each factor, just like a source word, and then the word and factor embeddings are combined before being passed on to the encoder.\n[BOS] Aharoni and Goldberg (2017) proposed a method to predict the target sentence along with its syntactic tree.\n[BOS] They linearize the tree in order to use the existing sequence-to-sequence model.\n[BOS] Nadejde et al. (2017) also evaluated several methods of incorporating syntactic knowledge on both the source and target.\n[BOS] While they used factors on the source side, their best method for the target side was to linearize the information and interleave it between the target words.\n[BOS] Garca-Martnez et al. (2016) used a neural MT model with multiple outputs, like in our case of Multi-task learning.\n[BOS] Their model predicts two properties at every step, the lemma of the target word and its morphological information.\n[BOS] They then use an external tool to use this information to generate the actual target word.\n[BOS] Dong et al. (2015) presented multi-task learning to translate a language into multiple target languages, and Luong et al. (2015) did experiments involving several levels of source and target language information.\n[BOS] There have been previous efforts to integrate morphology into MT systems by learning factored models Durrani et al., 2014b) over POS and morphological tags.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Multi_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 25, "token_end": 83, "char_start": 148, "char_end": 444, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Sennrich and Haddow (2016)": "16126936"}, "Reference": {}}}, {"token_start": 84, "token_end": 125, "char_start": 451, "char_end": 645, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 126, "token_end": 182, "char_start": 652, "char_end": 937, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Nadejde et al. (2017)": "7037582"}, "Reference": {}}}, {"token_start": 183, "token_end": 252, "char_start": 944, "char_end": 1276, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Garc\u00eda-Mart\u00ednez et al. (2016)": "6949337"}, "Reference": {}}}, {"token_start": 253, "token_end": 273, "char_start": 1283, "char_end": 1386, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Dong et al. (2015)": "3666937"}, "Reference": {}}}, {"token_start": 275, "token_end": 295, "char_start": 1392, "char_end": 1495, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 305, "token_end": 322, "char_start": 1564, "char_end": 1625, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "37605145_1", "paragraph": "[BOS] Analysis Several approaches have been devised to analyze MT models and the linguistic properties that are learned during training.\n[BOS] A common approach has been to use activations from a trained model to train an external classifier to predict some relevant information about the input.\n[BOS] Khn (2015) and Qian et al. (2016b) analyzed linguistic information learned in word embeddings, while Qian et al. (2016a) went further and analyzed linguistic properties in the hidden states of a recurrent neural network.\n[BOS] Adi et al. (2016) looked at the overall information learned in a sentence summary vector generated by an RNN using a similar approach.\n[BOS] Our approach closely aligns with that of Shi et al. (2016) and Belinkov et al. (2017a) , where the activations from various layers in a trained NMT system are used to predict linguistic properties.\n\n", "discourse_tags": ["Transition", "Transition", "Multi_summ", "Single_summ", "Multi_summ"], "span_citation_mapping": [{"token_start": 50, "token_end": 72, "char_start": 302, "char_end": 395, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"K\u00f6hn (2015)": null, "Qian et al. (2016b)": "3104544"}, "Reference": {}}}, {"token_start": 74, "token_end": 99, "char_start": 403, "char_end": 522, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 100, "token_end": 129, "char_start": 529, "char_end": 663, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Adi et al. (2016)": "6771196"}, "Reference": {}}}, {"token_start": 130, "token_end": 176, "char_start": 670, "char_end": 867, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Shi et al. (2016)": "7197724", "Belinkov et al. (2017a)": "7100502"}, "Reference": {}}}]}
{"id": "37605145_0", "paragraph": "[BOS] The related work to this paper can be broken into two groups:\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "42452608_0", "paragraph": "[BOS] The first success of sentiment analysis based on convolutional neural networks (CNN) was triggered by text classification (Kim, 2014) .\n[BOS] This work provided simple and effective In order to consider local n-gram features and long term dependency, various models which combined both CNN and LSTM were proposed (Zhang, 2017) .\n[BOS] Our model improves this approach.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 10, "token_end": 26, "char_start": 55, "char_end": 139, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kim, 2014)": null}}}, {"token_start": 53, "token_end": 64, "char_start": 292, "char_end": 332, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhang, 2017)": "8261321"}}}]}
{"id": "32626383_0", "paragraph": "[BOS] Research on parsing has mostly concentrated on parsing the traditional treebanks.\n[BOS] Therefore most parsers have statistical models that are optimized for the syntactic annotations in these treebanks and more generally for their language.\n[BOS] This means that such parsers will show a degradation in performance when used for parsing data from another domain.\n[BOS] Thus research has started on adapting parsers to new domains.\n[BOS] One of the first venues at which domain adaptation was targeted was the 2007 CoNLL shared task on dependency parsing, see (Nivre et al., 2007) .\n[BOS] One of the challenges in domain adaptation for parsing is the lack of annotated data in the target domain that could be used for evaluation.\n[BOS] Focusing on the domain of learner texts and their parsing, the great majority of works concern texts of English learners.\n[BOS] We support this fact with a list of learner corpora in Table 1 where their basic characteristics are provided.\n[BOS] Dickinson and Ragheb (2015) consider very carefully the SALLE annotation scheme for syntactically annotating learner English.\n[BOS] 2 Napoles et al. (2016) studied the effect of grammatical errors on the dependency parse.\n[BOS] As the source of the data, they used the NUCLE corpus.\n[BOS] Berzak et al. (2016) benchmarked POS tagging and dependency parsing performance on the TLE dataset and measure the effect of grammatical errors on parsing accuracy.\n[BOS] Cahill et al. (2014) used selftraining parsing technique with both native and non-native training texts.\n[BOS] They found that both training sets performed at about the same level, but that both significantly outperformed the baseline parser trained on traditional labeled data.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Transition", "Narrative_cite", "Transition", "Transition", "Reflection", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 93, "token_end": 112, "char_start": 516, "char_end": 586, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Nivre et al., 2007)": "1585700"}}}, {"token_start": 185, "token_end": 209, "char_start": 987, "char_end": 1112, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Dickinson and Ragheb (2015)": "12077579"}, "Reference": {}}}, {"token_start": 211, "token_end": 244, "char_start": 1121, "char_end": 1269, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Napoles et al. (2016)": "14102170"}, "Reference": {}}}, {"token_start": 245, "token_end": 278, "char_start": 1276, "char_end": 1440, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Berzak et al. (2016)": "8462706"}, "Reference": {}}}, {"token_start": 279, "token_end": 331, "char_start": 1447, "char_end": 1725, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Cahill et al. (2014)": "16644681"}, "Reference": {}}}]}
{"id": "2982769_9", "paragraph": "[BOS] Integer Linear Programming and NLP.\n[BOS] Finally, there has been much work in recent years on using ILP for natural language processing.\n[BOS] In particular, (Kuznetsova et al., 2012) proposes an ILP formulation for the generation of natural image descriptions from visual and text data and (Filippova and Strube, 2008 ) uses ILP to model sentence compression.\n[BOS] The ILP formulation of our content selection method is most similar to that proposed for sentence compression in (Filippova and Strube, 2008) .\n[BOS] One important difference though is both the application (content selection rather than sentence compression) and the way in which relevance is computed.\n[BOS] While (Filippova and Strube, 2008) uses weights derived from a treebank to determine the relative importance of an edge, we use bigram models over DBpedia properties to estimate the relative importance of DBpedia triples.\n\n", "discourse_tags": ["Transition", "Transition", "Multi_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 33, "token_end": 61, "char_start": 165, "char_end": 293, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Kuznetsova et al., 2012)": "10315654"}, "Reference": {}}}, {"token_start": 62, "token_end": 80, "char_start": 298, "char_end": 367, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Filippova and Strube, 2008": "17477341"}, "Reference": {}}}, {"token_start": 81, "token_end": 184, "char_start": 374, "char_end": 904, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Filippova and Strube, 2008)": "17477341"}, "Reference": {}}}]}
{"id": "2982769_8", "paragraph": "[BOS] Finally, the work presented here is closely related to a simpler proposal we introduced in (Mohammed et al., 2016) .\n[BOS] It differs from it in that it defines the notions of chain, sibling and mixed models for ngrams of DBpedia properties; relate them to the notion of topic-and discourse-coherence; and provide a comparative evaluation of their impact on content selection.\n\n", "discourse_tags": ["Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 4, "token_end": 28, "char_start": 15, "char_end": 120, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mohammed et al., 2016)": "9292550"}}}]}
{"id": "2982769_7", "paragraph": "[BOS] Our approach differs from these proposals in that it focuses on content selection from typed RDF data.\n[BOS] Using bigram models whose basic units are DBpedia triples, we maximise global coherence by favouring content where DBpedia properties that often co-occur are selected together.\n[BOS] In contrast, (Lampouras and Androutsopoulos, 2013) assumes that the relevance scores are given.\n[BOS] Moreover, while they focus on selecting content that leads to maximally aggregated content, we focus on selecting content that is discourse coherent.\n[BOS] Like us, (Biran and McKeown, 2015) focus on DBpedia data and use bigram models.\n[BOS] However their approach investigate discourse planning not content selection and relatedly, the basic units of their bigram models are discourse relations rather than triples.\n[BOS] Our approach also differs from (Barzilay and Lapata, 2005) in that it is unsupervised and does not require an aligned data-text corpus.\n\n", "discourse_tags": ["Reflection", "Reflection", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 56, "token_end": 104, "char_start": 298, "char_end": 549, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 108, "token_end": 158, "char_start": 565, "char_end": 816, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Biran and McKeown, 2015)": "14678425"}, "Reference": {}}}, {"token_start": 164, "token_end": 190, "char_start": 854, "char_end": 958, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "2982769_6", "paragraph": "[BOS] (Bouayad-Agha et al., 2011) introduces an ontology driven content selection procedure in which a base domain ontology is used to infer new facts.\n[BOS] For instance, given the numerical scores of two teams playing in the same game, a result event will be inferred between the winner and the loser and a causal relation will be inferred between the number of goals of a given team and this result event.\n[BOS] Content selection proceeds in three steps.\n[BOS] First, a set of hand written rules is used to select a subset of the knowledge base.\n[BOS] Second, relevance scores learned from a parallel data/text corpus are used to select the most relevant individual and relation instances.\n[BOS] Third, hand-written templates are used to determine the content to be included in the generated text.\n\n", "discourse_tags": ["Single_summ", "Transition", "Transition", "Transition", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 2, "token_end": 36, "char_start": 6, "char_end": 151, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "2982769_5", "paragraph": "[BOS] (Lampouras and Androutsopoulos, 2013) present an Integer Linear Programming model of content selection, lexicalisation and aggregation for generating text from OWL ontologies.\n[BOS] The objective function used in their ILP model maximises the total importance of selected facts and minimizes the number of distinct elements mentioned in each sentence thereby favouring aggregated sentences i.e., sentences where repeated elements are avoided through e.g., ellipsis or coordination.\n\n", "discourse_tags": ["Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 90, "char_start": 6, "char_end": 487, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "2982769_4", "paragraph": "[BOS] Content Planning (Biran and McKeown, 2015) describes a discourse planning approach applied to the generation of comparison stories from DBpedia data.\n[BOS] Given two DBpedia entity e 1 and e 2 , they first select all DBpedia triples whose subject is either e 1 or e 2 .\n[BOS] Based on the shape of the triples (shared entities or predicates) and on the property they include, they then enrich this set of DBpedia triples with discourse relations.\n[BOS] For instance, if two triples share the same predicate and object, an expansion relation is added between the two triples (e.g., \"John has a ball.\n[BOS] Mary also has a ball\").\n[BOS] Discourse planning then consists in finding a path through the resulting multigraphs of potential relations between DBpedia triples using an bigram model over discourse relations.\n[BOS] Good discourse plans are those which maximise the probability of a sequence of discourse relations.\n[BOS] In this way, the proposed approach determines both the order of the events and the discourse relation holding between them.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Transition", "Transition", "Transition", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 2, "token_end": 94, "char_start": 6, "char_end": 452, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Biran and McKeown, 2015)": "14678425"}, "Reference": {}}}]}
{"id": "2982769_3", "paragraph": "[BOS] Our approach also departs from (Cheng et al., 2015) 's in that the methods used are very different.\n[BOS] While we use Integer Linear Programming and language models to select DBpedia subgraphs that are both discourse-and topic-coherent, (Cheng et al., 2015) use a random surfer model, pointwise mutual information and probabilistic estimates to measure relatedness and informativeness.\n[BOS] Generally, the two methods are complementary using different resources, algorithms and metrics thereby opening interesting possibilities for combination.\n[BOS] It would be interesting for instance, to investigate how modifying our ILP formulation to integrate the relatedness metrics used by (Cheng et al., 2015) would impact results.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 2, "token_end": 27, "char_start": 6, "char_end": 105, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cheng et al., 2015)": "2699851"}}}, {"token_start": 52, "token_end": 103, "char_start": 244, "char_end": 552, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Cheng et al., 2015)": "2699851"}, "Reference": {}}}, {"token_start": 116, "token_end": 134, "char_start": 630, "char_end": 711, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cheng et al., 2015)": "2699851"}}}]}
{"id": "2982769_2", "paragraph": "[BOS] We depart from (Cheng et al., 2015) both in terms of goals and of methods.\n[BOS] In terms of goals, while (Cheng et al., 2015) aim to produce entity summaries, our goal is to produce a large set of content units that are varied both in terms of content and in terms of structure.\n[BOS] In particular, one important difference is that we produce trees of varying shapes and depths while the graphs produced by (Cheng et al., 2015) are restricted to trees of depth one i.e., set of DBpedia triples whose subject is the entity to be described.\n[BOS] As discussed in Section 5.1, this allows us to produce knowledge trees which, because they vary in shape, will give rise to different linguistic structures and will therefore better support the creation of a linguistically varied benchmark for Natural Language Generation.\n\n", "discourse_tags": ["Reflection", "Reflection", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 22, "char_start": 6, "char_end": 80, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cheng et al., 2015)": "2699851"}}}, {"token_start": 29, "token_end": 42, "char_start": 112, "char_end": 164, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cheng et al., 2015)": "2699851"}}}, {"token_start": 87, "token_end": 98, "char_start": 396, "char_end": 435, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cheng et al., 2015)": "2699851"}}}]}
{"id": "2982769_1", "paragraph": "[BOS] Entity Summarisation (Cheng et al., 2015) presents an approach which focuses on a task very similar to ours, namely the task of selecting, for a given entity e, a subgraph of the knowledge graph whose root is e. The goal is to generate entity summaries that is, sets of facts which adequately summarise a given entity.\n[BOS] The method used extends a standard random surfer model navigating the knowledge graph based on metrics indicating (i) the informativeness of a fact and (ii) the relatedness between two facts.\n[BOS] In this way, the selected subgraphs are both coherent (solutions which maximise relatedness are preferred) and informative (facts that helps distinguishing the entity to be summarised from others are preferred).\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 2, "token_end": 109, "char_start": 6, "char_end": 522, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Cheng et al., 2015)": "2699851"}, "Reference": {}}}]}
{"id": "2982769_0", "paragraph": "[BOS] Our approach has similarity with approaches on entity summarisation, content planning from DBpedia data and ILP (Integer Linear Programming) approaches for content planning.\n[BOS] There is also a vast literature on using ILP for natural language processing.\n\n", "discourse_tags": ["Reflection", "Transition"], "span_citation_mapping": []}
{"id": "24787645_0", "paragraph": "[BOS] Some recent work has also focused on lowresource name tagging (Tsai et al., 2016; Littell et al., 2016; Zhang et al., 2016; Yang et al., 2017) and cross-lingual entity linking (McNamee et al., 2011; Spitkovsky and Chang, 2011; Sil and Florian, 2016) , but the system demonstrated in this paper is the first publicly available end-to-end system to perform both tasks and all of the 282 Wikipedia languages.\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 9, "token_end": 44, "char_start": 43, "char_end": 148, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Tsai et al., 2016;": "2889848", "Littell et al., 2016;": "10625626", "Zhang et al., 2016;": "2226854", "Yang et al., 2017)": "17984798"}}}, {"token_start": 45, "token_end": 76, "char_start": 153, "char_end": 255, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(McNamee et al., 2011;": null, "Spitkovsky and Chang, 2011;": null, "Sil and Florian, 2016)": "14792663"}}}]}
{"id": "2578649_2", "paragraph": "[BOS] There were also some works that handled lexical preference for Chinese parsing in other ways.\n[BOS] For example, Cheng et al. (2006) and Hall et al. (2007) applied shift-reduce deterministic parsing to Chinese.\n[BOS] Sagae and Tsujii (2007) generalized the standard deterministic framework to probabilistic parsing by using a best-first search strategy.\n[BOS] In these works, lexical preferences were introduced as features for predicting parsing action.\n[BOS] Besides, Bikel and Chiang (2000) applied two lexicalized parsing models developed for English to Penn Chinese Treebank.\n[BOS] Wang et al. (2005) proposed a completely lexicalized bottom-up generative parsing model to parse Chinese, in which a word-similarity-based smoothing was introduced to replace part-of-speech smoothing.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Multi_summ", "Transition", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 22, "token_end": 46, "char_start": 119, "char_end": 216, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 47, "token_end": 73, "char_start": 223, "char_end": 359, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 90, "token_end": 115, "char_start": 467, "char_end": 586, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 116, "token_end": 158, "char_start": 593, "char_end": 793, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "2578649_1", "paragraph": "[BOS] Parsing history has been used to improve parsing accuracy by many researchers (Yamada and Matsumoto, 2003; McDonald and Pereira, 2006) .\n[BOS] Yamada and Matsumoto (2003) showed that keeping a small amount of parsing history was useful to improve parsing performance in a shiftreduce parser.\n[BOS] McDonald and Pereira (2006) expanded their first-order spanning tree model to be second-order by factoring the score of the tree into the sum of adjacent edge pair scores.\n[BOS] In our proposed approach, the case patterns remember the neighboring modifiers for a head node like McDonald and Pereira's work.\n[BOS] But it keeps all the parsing histories of a head, which is different from only keeping adjacent two modifiers in (McDonald and Pereira, 2006) .\n[BOS] Besides, to use the parsing histories in CKY decoding, our approach applies horizontal Markovization during case pattern construction.\n[BOS] In general, the success of using case patterns in Chinese parsing in his paper proves again that keeping parsing history is crucial to improve parsing performance, no matter in which way and to which parsing model it is applied.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ", "Reflection", "Narrative_cite", "Reflection", "Transition"], "span_citation_mapping": [{"token_start": 2, "token_end": 32, "char_start": 6, "char_end": 140, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 34, "token_end": 64, "char_start": 149, "char_end": 297, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 65, "token_end": 103, "char_start": 304, "char_end": 475, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 144, "token_end": 165, "char_start": 667, "char_end": 758, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "2578649_0", "paragraph": "[BOS] To our current knowledge, there were few works about using case structures in Chinese parsing, except for the work of Wu (2003) and Han et al. (2004) .\n[BOS] Compared with them, our proposed approach presents a new type of case structures for all kinds of head-modifier pairs, which not only recognizes bi-lexical dependency but also remembers the parsing history of a head node.\n\n", "discourse_tags": ["Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 36, "char_start": 6, "char_end": 155, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "35844178_1", "paragraph": "[BOS] Our work is related to the work (Luong et al., 2015a; Feng et al., 2016; Tu et al., 2016; Cohn et al., 2016; Meng et al., 2016; that concentrate on the improvement of the attention mechanism.\n[BOS] To remit the computing cost of the attention mechanism when dealing with long sentences, Luong et al. (2015a) proposed the local attention mechanism by just focusing on a subscope of source positions.\n[BOS] Cohn et al. (2016) incorporated structural alignment biases into the attention mechanism and obtained improvements across several challenging language pairs in low-resource settings.\n[BOS] Feng et al. (2016) passed the previous attention context to the attention mechanism by adding recurrent connections as the implicit distortion model.\n[BOS] Tu et al. (2016) maintained a coverage vector for keeping the attention history to acquire accurate translations.\n[BOS] Meng et al. (2016) proposed the interactive attention with the attentive read and attentive write operation to keep track of the interaction history.\n[BOS] utilized an external memory to store additional information for guiding the attention computation.\n[BOS] These works are different from ours, as our distortion models explicitly capture word reordering knowledge through estimating the probability distribution of relative jump distances on source words to incorporate word reordering knowledge into the attention-based NMT.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Other", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 47, "char_start": 6, "char_end": 132, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Luong et al., 2015a;": "1998416", "Feng et al., 2016;": "8063399", "Tu et al., 2016;": "146843", "Cohn et al., 2016;": "1964946"}}}, {"token_start": 59, "token_end": 100, "char_start": 204, "char_end": 404, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Luong et al. (2015a)": "1998416"}, "Reference": {}}}, {"token_start": 101, "token_end": 131, "char_start": 411, "char_end": 593, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Cohn et al. (2016)": "1964946"}, "Reference": {}}}, {"token_start": 132, "token_end": 158, "char_start": 600, "char_end": 749, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Feng et al. (2016)": "8063399"}, "Reference": {}}}, {"token_start": 159, "token_end": 180, "char_start": 756, "char_end": 869, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Tu et al. (2016)": "146843"}, "Reference": {}}}, {"token_start": 181, "token_end": 210, "char_start": 876, "char_end": 1025, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Meng et al. (2016)": "416723"}, "Reference": {}}}]}
{"id": "35844178_0", "paragraph": "[BOS] Our work is inspired by the distortion models that widely used in SMT.\n[BOS] The most related work in SMT is the distortion model proposed by Yaser and Papineni (2006) .\n[BOS] Their model is identical to our S-Distortion model that captures the relative jump distance knowledge on source words.\n[BOS] However, our approach is deliberately designed for the attention-based NMT system and is capable of exploiting variant context information to predict the relative jump distances.\n\n", "discourse_tags": ["Reflection", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 18, "token_end": 63, "char_start": 83, "char_end": 300, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Yaser and Papineni (2006)": "10119880"}, "Reference": {}}}]}
{"id": "38244965_2", "paragraph": "[BOS] Recently, the morphological inflection has been also addressed at SIGMORPHON 2016 Shared Task (Cotterell et al., 2016) where, given a lemma with its part-of-speech, a target inflected form had to be generated (task 1).\n[BOS] This task was addressed through several approaches, including align and transduce (Alegria and Etxeberria, 2016; Nicolai et al., 2016; Liu and Mao, 2016) ; recurrent neural networks (Kann and Schtze, 2016; Aharoni et al., 2016; stling, 2016) ; and, linguisticinspired heuristics approaches (Taji et al., 2016; Sorokin, 2016) .\n[BOS] Overall, recurrent neural networks approaches performed better, being (Kann and Schtze, 2016) the best performing system in the shared task, obtaining around 98%.\n[BOS] Furthermore, the work described here differs from existing statistical surface realisation methods which use phrase-based learning (e.g., (Konstas and Lapata, 2012)) since they do not usually include morphological inflection.\n[BOS] In this respect, our work is more similar to (Duek and Jurek, 2013) , where the inflected word forms are learnt through multi-class logistic regression by predicting edit scripts.\n[BOS] The aforementioned data-driven methods achieve high accuracy in predicting the appropriate inflection by learning from huge datasets.\n[BOS] For example, Durret and DeNero (2013) use 11400 amount of data (i.e. the total number of instances or rules used to predict the inflections of a verb).\n[BOS] In contrast, we use almost half to train our system (4556 instances), and we achieve comparable or better results for Spanish.\n[BOS] Finally, the work presented here relies on ensembles of classifiers which has been proved successful for content selection in data-to-text systems (Gkatzia et al., 2014) .\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Narrative_cite", "Narrative_cite", "Narrative_cite", "Transition", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 13, "token_end": 29, "char_start": 72, "char_end": 124, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cotterell et al., 2016)": null}}}, {"token_start": 66, "token_end": 97, "char_start": 293, "char_end": 384, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Alegria and Etxeberria, 2016;": "4367293"}}}, {"token_start": 98, "token_end": 124, "char_start": 387, "char_end": 472, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Aharoni et al., 2016;": "14229358"}}}, {"token_start": 127, "token_end": 147, "char_start": 480, "char_end": 555, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 151, "token_end": 180, "char_start": 573, "char_end": 724, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 199, "token_end": 220, "char_start": 842, "char_end": 898, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 235, "token_end": 250, "char_start": 982, "char_end": 1032, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 297, "token_end": 334, "char_start": 1304, "char_end": 1442, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 383, "token_end": 399, "char_start": 1708, "char_end": 1751, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "38244965_1", "paragraph": "[BOS] Previous work has used supervised or semisupervised learning (Durrett and DeNero, 2013; Ahlberg et al., 2014; Nicolai et al., 2015; Faruqui et al., 2016) to learn from large datasets of morphological rules on word forms in order to apply them to inflect the desired words.\n[BOS] Other approaches have relied on linguistic information, such as morphemes and phonology (Cotterell et al., 2016) ; morphosyntactic disambiguation rules (Surez et al., 2005) ; and, graphical models (Dreyer and Eisner, 2009 ).\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 6, "token_end": 48, "char_start": 29, "char_end": 159, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Durrett and DeNero, 2013;": "6023976", "Ahlberg et al., 2014;": null, "Faruqui et al., 2016)": "3089175"}}}, {"token_start": 82, "token_end": 97, "char_start": 349, "char_end": 397, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cotterell et al., 2016)": null}}}, {"token_start": 98, "token_end": 114, "char_start": 400, "char_end": 457, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 117, "token_end": 127, "char_start": 465, "char_end": 506, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dreyer and Eisner, 2009": "9829521"}}}]}
{"id": "38244965_0", "paragraph": "[BOS] Morphological inflection has been addressed from different perspectives within the area of Compu-tational Linguistics, commonly for morphological rich languages, such as German, Spanish, Finnish or Arabic, as well as less morphological rich languages such as English.\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "2592133_1", "paragraph": "[BOS] Previous question answering datasets such as MCTest (Richardson et al., 2013) and TREC-QA (Dang et al., 2007) were too small to successfully train end-to-end neural architectures such as the models discussed in 4 and required different approaches.\n[BOS] Traditional statistical QA systems (e.g., Ferrucci (2012) ) relied on linguistic pre-processing pipelines and extensive exploitation of external resources, such as knowledge bases for feature-engineering.\n[BOS] Other paradigms include template matching or passage retrieval (Andrenucci and Sneiders, 2005) .\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 8, "token_end": 18, "char_start": 51, "char_end": 83, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Richardson et al., 2013)": "2100831"}}}, {"token_start": 19, "token_end": 32, "char_start": 88, "char_end": 115, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dang et al., 2007)": "208030833"}}}, {"token_start": 58, "token_end": 98, "char_start": 260, "char_end": 464, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ferrucci (2012)": null}, "Reference": {}}}, {"token_start": 102, "token_end": 119, "char_start": 495, "char_end": 565, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Andrenucci and Sneiders, 2005)": "7229163"}}}]}
{"id": "2592133_0", "paragraph": "[BOS] The creation of large scale cloze datasets such the DailyMail/CNN dataset (Hermann et al., 2015) or the Children's Book Corpus (Hill et al., 2016) paved the way for the construction of end-to-end neural architectures for reading comprehension.\n[BOS] A thorough analysis by , however, revealed that the DailyMail/CNN was too easy and still quite noisy.\n[BOS] New datasets were constructed to eliminate these problems including SQuAD (Rajpurkar et al., 2016) , NewsQA (Trischler et al., 2017) and MsMARCO (Nguyen et al., 2016) .\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 12, "token_end": 26, "char_start": 58, "char_end": 102, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hermann et al., 2015)": "6203757"}}}, {"token_start": 28, "token_end": 41, "char_start": 110, "char_end": 152, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hill et al., 2016)": "14915449"}}}, {"token_start": 93, "token_end": 105, "char_start": 432, "char_end": 462, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rajpurkar et al., 2016)": "11816014"}}}, {"token_start": 106, "token_end": 118, "char_start": 465, "char_end": 496, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Trischler et al., 2017)": "1167588"}}}, {"token_start": 119, "token_end": 130, "char_start": 501, "char_end": 530, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Nguyen et al., 2016)": "1289517"}}}]}
{"id": "46937503_2", "paragraph": "[BOS] While previous works counted the word frequency in corpora such as Wikipedia, which is written by native speakers, we used corpora written by language learners.\n[BOS] As anticipated, the word frequency in the learner corpus proved to be a vital feature in the CWI task.\n\n", "discourse_tags": ["Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "46937503_1", "paragraph": "[BOS] In the first CWI shared task 2016, numerous systems (Brooke et al., 2016; Davoodi and Kosseim, 2016; Mukherjee et al., 2016; Ronzano et al., 2016) used random forest classifiers.\n[BOS] The length (Wrbel, 2016; Paetzold and Specia, 2016b; Ronzano et al., 2016; Palakurthi and Mamidi, 2016; Quijada and Medero, 2016; Konkol, 2016) and frequency (Wrbel, 2016; Paetzold and Specia, 2016b; Brooke et al., 2016; Ronzano et al., 2016; Palakurthi and Mamidi, 2016; Quijada and Medero, 2016; Konkol, 2016; Kauchak, 2016) features of the CWI shared task 2016.\n[BOS] These are used as baselines, and a majority of the systems use them as part of their features.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 2, "token_end": 50, "char_start": 6, "char_end": 152, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 57, "token_end": 107, "char_start": 195, "char_end": 334, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wr\u00f3bel, 2016;": "7104239", "Paetzold and Specia, 2016b;": "10433592", "Palakurthi and Mamidi, 2016;": "13331088", "Quijada and Medero, 2016;": "2360421", "Konkol, 2016)": "16536883"}}}, {"token_start": 108, "token_end": 172, "char_start": 339, "char_end": 517, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wr\u00f3bel, 2016;": "7104239", "Paetzold and Specia, 2016b;": "10433592", "Palakurthi and Mamidi, 2016;": "13331088", "Quijada and Medero, 2016;": "2360421", "Konkol, 2016;": "16536883"}}}]}
{"id": "46937503_0", "paragraph": "[BOS] Although our systems (random forest with length and frequency of the target word) are simple, they achieve competitive results.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "2883449_3", "paragraph": "[BOS] ILP has seen widespread use in natural language problems involving formulations which cannot be decoded efficiently with dynamic programming but can be expressed as relatively compact linear programs.\n[BOS] DeNero and Klein (2008) and Thadani and McKeown (2011) proposed ILP approaches to finding phrase-based alignments in a multilingual and monolingual context respectively.\n[BOS] Chang et al. (2010) describe a joint token-based and arc-based alignment technique using ILP to ensure consistency between the two alignment representations.\n[BOS] Our proposed joint phrasal and arc-based aligner generalizes over both these alignment techniques.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 33, "token_end": 71, "char_start": 213, "char_end": 382, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"DeNero and Klein (2008)": "6810336", "Thadani and McKeown (2011)": "9456888"}, "Reference": {}}}, {"token_start": 72, "token_end": 103, "char_start": 389, "char_end": 546, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Chang et al. (2010)": "11265565"}, "Reference": {}}}]}
{"id": "2883449_2", "paragraph": "[BOS] Monolingual token-based alignment has been used for many natural language processing applications such as paraphrase generation (Barzilay and Lee, 2003; Quirk et al., 2004) .\n[BOS] Dependency arc-based alignment has seen similar widespread use in applications such as sentence fusion (Barzilay and McKeown, 2005; Marsi and Krahmer, 2005) , redundancy removal (Thadani and McKeown, 2008) and textual entailment recognition (Dagan et al., 2005) .\n[BOS] Furthermore, joint aligners that simultaneously account for the similarity of tokens and dependency arcs have also been explored (Chambers et al., 2007; Chang et al., 2010) .\n[BOS] Monolingual phrasebased alignment was first tackled by the MANLI system of MacCartney et al. (2008) and was subsequently expanded upon by Thadani and McKeown (2011) to incorporate exact inference.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 19, "token_end": 40, "char_start": 112, "char_end": 178, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Barzilay and Lee, 2003;": "6387310", "Quirk et al., 2004)": "13043395"}}}, {"token_start": 56, "token_end": 78, "char_start": 274, "char_end": 343, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Barzilay and McKeown, 2005;": "16188305", "Marsi and Krahmer, 2005)": "2293515"}}}, {"token_start": 79, "token_end": 92, "char_start": 346, "char_end": 392, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Thadani and McKeown, 2008)": "15084553"}}}, {"token_start": 93, "token_end": 106, "char_start": 397, "char_end": 448, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dagan et al., 2005)": null}}}, {"token_start": 120, "token_end": 143, "char_start": 535, "char_end": 629, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chambers et al., 2007;": "808347", "Chang et al., 2010)": "11265565"}}}, {"token_start": 156, "token_end": 170, "char_start": 697, "char_end": 737, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"MacCartney et al. (2008)": "1922162"}}}, {"token_start": 171, "token_end": 186, "char_start": 742, "char_end": 802, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Thadani and McKeown (2011)": "9456888"}}}]}
{"id": "2883449_1", "paragraph": "[BOS] 1 Nevertheless, modern MT evaluation metrics have recently been found to be remarkably effective for tasks requiring monolingual alignments (Bouamor et al., 2011; Madnani et al., 2012; Heilman and Madnani, 2012 )-even used off-the-shelf with their default parameter settings-and for this reason we use Meteor as a baseline in this paper.\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 20, "token_end": 50, "char_start": 123, "char_end": 216, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bouamor et al., 2011;": "856102", "Madnani et al., 2012;": "2503536", "Heilman and Madnani, 2012": "1570953"}}}]}
{"id": "2883449_0", "paragraph": "[BOS] Text alignment is a crucial component of machine translation (MT) systems (Vogel et al., 1996; Och and Ney, 2003; Liang et al., 2006; DeNero and Klein, 2008) ; however, the general goal of multilingual aligners is the production of wide-coverage phrase tables for translation.\n[BOS] In contrast, monolingual alignment is often consumed directly in applications like paraphrasing and textual entailment recognition; this task therefore involves substantially different challenges and tradeoffs.\n\n", "discourse_tags": ["Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 9, "token_end": 44, "char_start": 47, "char_end": 163, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vogel et al., 1996;": "18973811", "Och and Ney, 2003;": "5219389", "Liang et al., 2006;": "618683", "DeNero and Klein, 2008)": "6810336"}}}]}
{"id": "298740_2", "paragraph": "[BOS] Although character-based tagging became mainstream approach in the last two Bakeoffs, it does not mean that word information is valueless in Chinese word segmentation.\n[BOS] A word-based perceptron algorithm is proposed recently (Zhang and Clark, 2007) , which views Chinese word segmentation task from a new angle instead of character-based tagging and gets comparable results with the best results of Bakeoff.\n[BOS] Table 6 Results of different approach used in our experiments (White background lines are the results we repeat Zhang\"s methods and they have some trivial difference with Table 1. )\n[BOS] Therefore, the most important thing worth to pay attention in future study is how to integrate linguistic information into the statistical model effectively, no matter character or word information.\n\n", "discourse_tags": ["Transition", "Single_summ", "Reflection", "Transition"], "span_citation_mapping": [{"token_start": 34, "token_end": 81, "char_start": 180, "char_end": 417, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Zhang and Clark, 2007)": "2687347"}, "Reference": {}}}]}
{"id": "298740_1", "paragraph": "[BOS] In Table 6 , the lines about \"pure CT\" provide the results generated by pure CT with 6-tag set.\n[BOS] We can see from the Table 6 this pure CT approach achieves the state-of-the-art results on all the corpora.\n[BOS] On three of the four corpora (AS, MSRA and PKU) this pure CT method gets the best result.\n[BOS] Even on IV word, this pure CT approach outperforms Zhang\"s CT method and produces comparable results with combination with EIV tags, which shows that pure CT method can perform well on IV words too.\n[BOS] Moreover, this character-based tagging approach is more clear and simple than the confidence measure method.\n\n", "discourse_tags": ["Other", "Other", "Other", "Other", "Other"], "span_citation_mapping": []}
{"id": "298740_0", "paragraph": "[BOS] Although the method such as confidence measure can be helpful at some circumstance, our experiment shows that pure character-based tagging (pure CT) can work well with reasonable features and tag set.\n[BOS] In (Zhao et al., 2006) , an enhanced CRF tag set is proposed to distinguish different positions in the multi-character words when the word length is less than 6.\n[BOS] In this method, feature templates are almost the same as shown in Table 3 with a 3-character window and a 6-tag set {B, B2, B3, M, E, O} is used.\n[BOS] Here, tag B and E stand for the first and the last position in a multi-character word, respectively.\n[BOS] S stands up a single-character word.\n[BOS] B2 and B3 stand for the second and the third position in a multi-character word, whose length is larger than two-character or three-character.\n[BOS] M stands for the fourth or more rear position in a multicharacter word, whose length is larger than fourcharacter.\n\n", "discourse_tags": ["Reflection", "Single_summ", "Other", "Other", "Other", "Other", "Other"], "span_citation_mapping": [{"token_start": 41, "token_end": 77, "char_start": 213, "char_end": 374, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Zhao et al., 2006)": "18371469"}, "Reference": {}}}]}
{"id": "399489_1", "paragraph": "[BOS] Generally, active learning for sequential labeling has received less attention than for classification (Settles and Craven, 2008; Marcheggiani and Artieres, 2014) .\n[BOS] Our experiments were simple, and several things can be done to improve results, i.e., by reducing sampling bias.\n[BOS] In particular, several techniques have been introduced for improving out-of-domain performance using active learning.\n[BOS] Rai et al. (2010) perform target-domain AL with a seed of source-domain data.\n[BOS] Among other things, they propose to use the source and target unlabeled data to train a classifier to learn what target domain data points are similar to the source domain, in a way similar to Plank et al. (2014) .\n[BOS] For more work along these lines, see Chan and Ng (2007) and Xiao and Guo (2013) .\n\n", "discourse_tags": ["Narrative_cite", "Reflection", "Transition", "Single_summ", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 12, "token_end": 36, "char_start": 75, "char_end": 168, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Settles and Craven, 2008;": "8197231", "Marcheggiani and Artieres, 2014)": "13642704"}}}, {"token_start": 85, "token_end": 107, "char_start": 420, "char_end": 497, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Rai et al. (2010)": "5594021"}, "Reference": {}}}, {"token_start": 117, "token_end": 152, "char_start": 548, "char_end": 716, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Plank et al. (2014)": "8509375"}}}, {"token_start": 154, "token_end": 175, "char_start": 725, "char_end": 804, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Chan and Ng (2007)": "15391473", "Xiao and Guo (2013)": "1635392"}}}]}
{"id": "399489_0", "paragraph": "[BOS] The AL models considered here are very standard.\n[BOS] We take a small seed of data points, train a sequential labeling, and iterate over an unlabeled pool of data, selecting the data points our labeler is least confident about.\n[BOS] In the AL literature, the selected data points are often those close to a decision boundary or those most likely to decrease overall uncertainty.\n[BOS] This obviously leads to biased sampling, which can sometimes be avoided using different techniques, e.g., by exploiting cluster structure in the data.\n\n", "discourse_tags": ["Reflection", "Reflection", "Transition", "Transition"], "span_citation_mapping": []}
{"id": "2626026_6", "paragraph": "[BOS] Our work has made use of dictionaries that are automatically extracted from bilingual corpora.\n[BOS] An alternative approach would be to use hand-crafted translation lexicons, for example, PanLex (Baldwin et al., 2010) (e.g. see Duong et al. (2015b) ), which covers 1253 language varieties, Google translate (e.g., see Ammar et al. (2016c) ), or Wiktionary (e.g., see Durrett et al. (2012) for an approach that uses Wiktionary for cross-lingual transfer).\n[BOS] These resources are potentially very rich sources of information.\n[BOS] Future work should investigate whether they can give improvements in performance.\n\n", "discourse_tags": ["Reflection", "Narrative_cite", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 38, "token_end": 64, "char_start": 195, "char_end": 255, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Duong et al. (2015b)": "755490"}}}, {"token_start": 73, "token_end": 91, "char_start": 297, "char_end": 345, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Ammar et al. (2016c)": "1227830"}}}, {"token_start": 94, "token_end": 112, "char_start": 352, "char_end": 395, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Durrett et al. (2012)": "2895378"}}}]}
{"id": "2626026_5", "paragraph": "[BOS] pus (see Figure 1 ) is closely related to Duong et al. (2015a) ; Gouws and Sgaard (2015) ; and Wick et al. (2015) .\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 2, "token_end": 41, "char_start": 6, "char_end": 119, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Duong et al. (2015a)": "7295553", "Gouws and S\u00f8gaard (2015)": null, "Wick et al. (2015)": "35494"}}}]}
{"id": "2626026_4", "paragraph": "[BOS] A number of authors (Tckstrm et al., 2012; Guo et al., 2015; Guo et al., 2016) have introduced methods that learn cross-lingual representations that are then used in syntactic transfer.\n[BOS] Most of these approaches introduce constraints to a clustering or embedding algorithm that encourage words that are translations of each other to have similar representations.\n[BOS] Our method of deriving a cross-lingual cor- Table 12 : Precision, recall and f-score of unlabeled dependency attachment for different POS tags as head for three groups of languages for the universal dependencies experiments in Table 9 : G1 (languages with UAS  80), G2 (languages with 70  UAS < 80), G3 (languages with UAS < 70).\n[BOS] The rows are sorted by frequency in the G1 languages.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Other", "Other"], "span_citation_mapping": [{"token_start": 2, "token_end": 49, "char_start": 6, "char_end": 191, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(T\u00e4ckstr\u00f6m et al., 2012;": "891605", "Guo et al., 2015;": "18634877", "Guo et al., 2016)": "2937031"}}}]}
{"id": "2626026_3", "paragraph": "[BOS] approach described in this paper is a simple form of treebank translation, where we use a word-to-word translation model.\n[BOS] In spite of its simplicity, it is an effective approach.\n[BOS] A number of authors have considered incorporating universal syntactic properties, such as dependency order, by selectively learning syntactic attributes from similar source languages (Naseem et al., 2012; Tckstrm et al., 2013; Zhang and Barzilay, 2015; Ammar et al., 2016a) .\n[BOS] Selective sharing of syntactic properties is complementary to our work.\n[BOS] We used a very limited form of selective sharing, through the WALS properties, in our baseline approach.\n[BOS] More recently, Wang and Eisner (2016) have developed a synthetic treebank as a universal treebank to help learn parsers for new languages.\n[BOS] Martnez Alonso et al. (2017) try a very different approach in cross-lingual transfer by using a ranking approach.\n\n", "discourse_tags": ["Reflection", "Reflection", "Narrative_cite", "Reflection", "Reflection", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 58, "token_end": 104, "char_start": 305, "char_end": 470, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Naseem et al., 2012;": "3143538", "T\u00e4ckstr\u00f6m et al., 2013;": "2037646", "Zhang and Barzilay, 2015;": "9555772"}}}, {"token_start": 140, "token_end": 170, "char_start": 668, "char_end": 806, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wang and Eisner (2016)": "10817864"}, "Reference": {}}}, {"token_start": 171, "token_end": 198, "char_start": 813, "char_end": 926, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "2626026_2", "paragraph": "[BOS] Other recent work (Tiedemann et al., 2014; Tiedemann, 2015; Tiedemann and Agi, 2016) has considered treebank translation, where a statistical machine translation system (e.g., MOSES (Koehn et al., 2007) ) is used to translate a source language treebank into the target language, complete with reordering of the input sentence.\n[BOS] The lexicalization Table 11 : Accuracy of unlabeled dependencies by POS of the modifier word, for three groups of languages for the universal dependencies experiments in Table 9 : G1 (languages with UAS  80), G2 (languages with 70  UAS < 80), G3 (languages with UAS < 70).\n[BOS] The rows are sorted by frequency in the G1 languages.\n\n", "discourse_tags": ["Multi_summ", "Other", "Other"], "span_citation_mapping": [{"token_start": 2, "token_end": 81, "char_start": 6, "char_end": 332, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Tiedemann et al., 2014;": "14049482", "Tiedemann, 2015;": "6335141", "Tiedemann and Agi\u0107, 2016)": "7805664"}, "Reference": {"(Koehn et al., 2007)": "18431103"}}}]}
{"id": "2626026_1", "paragraph": "[BOS] The annotation projection approach, where dependencies from one language are transferred through translation alignments to another language, has been considered by several authors (Hwa et al., 2005; Ganchev et al., 2009; McDonald et al., 2011; Ma and Xia, 2014; Rasooli and Collins, 2015;  Table 10 : Precision, recall and f-score of different dependency relations on the English development data of the Google universal treebank.\n[BOS] The major columns show the dependency labels (\"dep.\n[BOS] \"), frequency (\"freq.\n[BOS] \"), the baseline delexicalized model (\"delex\"), and our method using the Bible and Europarl (\"EU\") as translation data.\n[BOS] The rows are sorted by frequency.\n[BOS] Lacroix et al., 2016; Schlichtkrull and Sgaard, 2017) .\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Transition", "Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 2, "token_end": 65, "char_start": 6, "char_end": 293, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hwa et al., 2005;": "157167", "Ganchev et al., 2009;": "11681086", "McDonald et al., 2011;": "6698104", "Ma and Xia, 2014;": "15371205"}}}, {"token_start": 161, "token_end": 181, "char_start": 695, "char_end": 748, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Schlichtkrull and S\u00f8gaard, 2017)": null}}}]}
{"id": "2626026_0", "paragraph": "[BOS] There has recently been a great deal of work on syntactic transfer.\n[BOS] A number of methods (Zeman and Resnik, 2008; McDonald et al., 2011; Cohen et al., 2011; Naseem et al., 2012; Tckstrm et al., 2013; Rosa and Zabokrtsky, 2015) directly learn delexicalized models that can be trained on universal treebank data from one or more source languages, then applied to the target language.\n[BOS] More recent work has introduced cross-lingual representationsfor example cross-lingual word-embeddings-that can be used to improve performance (Zhang and Barzilay, 2015; Guo et al., 2015; Duong et al., 2015a; Duong et al., 2015b; Guo et al., 2016; Ammar et al., 2016b) .\n[BOS] These cross-lingual representations are usually learned from parallel translation data.\n[BOS] We show results of several methods (Zhang and Barzilay, 2015; Guo et al., 2016; Ammar et al., 2016b) in Table 7 of this paper.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite", "Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 16, "token_end": 103, "char_start": 80, "char_end": 392, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zeman and Resnik, 2008;": "10674977", "McDonald et al., 2011;": "6698104", "Cohen et al., 2011;": "14287962", "Naseem et al., 2012;": "3143538", "T\u00e4ckstr\u00f6m et al., 2013;": "2037646", "Rosa and Zabokrtsky, 2015)": "9398597"}}}, {"token_start": 116, "token_end": 181, "char_start": 472, "char_end": 667, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhang and Barzilay, 2015;": "9555772", "Guo et al., 2015;": "18634877", "Duong et al., 2015a;": "7295553", "Duong et al., 2015b;": "755490", "Guo et al., 2016;": "2937031"}}}, {"token_start": 198, "token_end": 236, "char_start": 770, "char_end": 896, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhang and Barzilay, 2015;": "9555772", "Guo et al., 2016;": "2937031", "Ammar et al., 2016b)": "2436803"}}}]}
{"id": "46940396_2", "paragraph": "[BOS] Other systems at the SemEval 2016 shared task used SVM (Kuru, 2016; Choubey and Pateria, 2016; S P et al., 2016; , Random Forest (Davoodi and Kosseim, 2016; Mukherjee et al., 2016; Brooke et al., 2016; Ronzano et al., 2016) , Neural Networks (Bingel et al., 2016; Nat, 2016) , Decision Trees (Quijada and Medero, 2016; , Nearest Centroid classifier (Palakurthi and Mamidi, 2016) , Naive Bayes (Mukherjee et al., 2016) , threshold bagged classifiers (Kauchak, 2016) and Entropy classifiers (Konkol, 2016; Martnez Martnez and Tan, 2016) .\n[BOS] The features used in most of the systems were common, such as length-based features (like target word length), presence in a corpus (like presence of the target word in Simple English Wikipedia), PoS features of the target word, position features (position of the target word in the sentence), etc.\n[BOS] However, a few of the systems used some innovative features.\n[BOS] One of them was the MRC Psycholinguistic database (Wilson, 1988) used by Davoodi and Kosseim (2016) .\n[BOS] Another system by Konkol (2016) used a single feature namely document frequency of the word in Wikipedia, for classifying using a maximum entropy classifier.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Narrative_cite", "Single_summ"], "span_citation_mapping": [{"token_start": 13, "token_end": 37, "char_start": 57, "char_end": 117, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kuru, 2016;": "16801581", "Choubey and Pateria, 2016;": "14268359"}}}, {"token_start": 39, "token_end": 78, "char_start": 121, "char_end": 229, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Davoodi and Kosseim, 2016;": "7257700", "Mukherjee et al., 2016;": "2225895", "Brooke et al., 2016;": "5243955"}}}, {"token_start": 79, "token_end": 94, "char_start": 232, "char_end": 280, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bingel et al., 2016;": "12058938", "Nat, 2016)": "20414377"}}}, {"token_start": 95, "token_end": 106, "char_start": 283, "char_end": 323, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 108, "token_end": 122, "char_start": 327, "char_end": 384, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Palakurthi and Mamidi, 2016)": "13331088"}}}, {"token_start": 123, "token_end": 135, "char_start": 387, "char_end": 423, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mukherjee et al., 2016)": "2225895"}}}, {"token_start": 136, "token_end": 147, "char_start": 426, "char_end": 470, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kauchak, 2016)": "335885"}}}, {"token_start": 148, "token_end": 165, "char_start": 475, "char_end": 540, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Konkol, 2016;": "16536883"}}}, {"token_start": 249, "token_end": 259, "char_start": 941, "char_end": 985, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 259, "token_end": 271, "char_start": 986, "char_end": 1020, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 273, "token_end": 302, "char_start": 1029, "char_end": 1186, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Konkol (2016)": "16536883"}, "Reference": {}}}]}
{"id": "46940396_1", "paragraph": "[BOS] In this task, the winning team used a soft votingbased approach from the outputs of 21 predictors (either classifiers, threshold-based, or lexical) (Paetzold and Specia, 2016b) .\n[BOS] This system was the best system according to the G-Score -an evaluation metric designed specifically for this task at SemEval 2016 (Paetzold and Specia, 2016a) .\n[BOS] The system with the best F1-Score made use of a threshold-based approach that marked a word as complex if its frequency in Simple Wikipedia is above a threshold (Wrbel, 2016) .\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 11, "token_end": 43, "char_start": 44, "char_end": 182, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Paetzold and Specia, 2016b)": "10433592"}}}, {"token_start": 67, "token_end": 82, "char_start": 309, "char_end": 350, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Paetzold and Specia, 2016a)": "14776729"}}}, {"token_start": 111, "token_end": 123, "char_start": 482, "char_end": 533, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wr\u00f3bel, 2016)": "7104239"}}}]}
{"id": "46940396_0", "paragraph": "[BOS] In SemEval 2016, 21 teams participated in a shared task on complex word identification (Paetzold and Specia, 2016a) .\n[BOS] The competition involved finding out whether a given word in a sentence was complex or not for a non-native speaker.\n[BOS] The dataset used was completely in English.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 16, "token_end": 30, "char_start": 65, "char_end": 121, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Paetzold and Specia, 2016a)": "14776729"}}}]}
{"id": "44072041_1", "paragraph": "[BOS] The 2017 scientific publications.\n[BOS] Here, however, there were only 2 relation types, HYPONYM-OF and SYNONYM-OF.\n[BOS] One successful model on this task utilized a convolutional network operating on word, tag, position, and part-of-speech features (Lee et al., 2017) , and found that restricting network focus to only the words between the requisite entities offered a notable performance improvement.\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 43, "token_end": 64, "char_start": 208, "char_end": 275, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lee et al., 2017)": "5420466"}}}]}
{"id": "44072041_0", "paragraph": "[BOS] Previous SemEval challenges have explored relation identification and extraction.\n[BOS] The 2010 SemEval Task 8 (Hendrickx et al., 2010 ) explored classification of natural language relations, such as CONTENT-CONTAINER or ENTITY-ORIGIN.\n[BOS] This challenge differs from ours in its generalizability; our relations are specific to ACL papers (e.g. MODEL-FEATURE) whereas the 2010 relations are more general, and may necessitate more common-sense knowledge than the 2018 relations.\n[BOS] The 2010 data has been extensively studied and has offered significant opportunity for other researchers to test their model.\n[BOS] Rink and Harabagiu (2010) produced a strong SVM/LR model to attack this challenge.\n[BOS] Several deep architectures have also been proposed for this task, including the work of Cai et al. (2016) , which demonstrated a novel approach merging ideas from recurrent networks and convolutional networks based on shortest dependency path (SDP).\n[BOS] Xu et al. (2015a) and Santos et al. (2015) both used convolutional architectures along with negative sampling to pursue this task.\n[BOS] More recently, used two levels of attention, one for input selection and the other for output pooling, to boost the performance of their model to state of the art.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Multi_summ", "Transition"], "span_citation_mapping": [{"token_start": 15, "token_end": 119, "char_start": 94, "char_end": 618, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Hendrickx et al., 2010": "436023"}, "Reference": {}}}, {"token_start": 120, "token_end": 142, "char_start": 625, "char_end": 707, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Rink and Harabagiu (2010)": "12390391"}, "Reference": {}}}, {"token_start": 143, "token_end": 189, "char_start": 714, "char_end": 963, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Cai et al. (2016)": "1774259"}, "Reference": {}}}, {"token_start": 190, "token_end": 219, "char_start": 970, "char_end": 1100, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Xu et al. (2015a)": "12203896", "Santos et al. (2015)": "15620570"}, "Reference": {}}}]}
{"id": "3295641_0", "paragraph": "[BOS] Multi-lingual NMT has been extensively studied in a number of papers such as Lee et al. (2017) , Johnson et al. (2017) , Zoph et al. (2016) and Firat et al. (2016) .\n[BOS] As we discussed, these approaches have significant limitations with zero-resource cases.\n[BOS] Johnson et al. (2017) is more closely related to our current approach, our work is extending it to overcome the limitations with very low-resource languages and enable sharing of lexical and sentence representation across multiple languages.\n[BOS] Two recent related works are targeting the same problem of minimally supervised or totally unsupervised NMT.\n[BOS] Artetxe et al. (2018) proposed a totally unsupervised approach depending on multi-lingual embedding similar to ours and duallearning and reconstruction techniques to train the model from mono-lingual data only.\n[BOS] also proposed a quite similar approach while utilizing adversarial learning.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Single_summ", "Transition", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 2, "token_end": 52, "char_start": 6, "char_end": 169, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Lee et al. (2017)": "10509498", "Johnson et al. (2017)": "6053988", "Zoph et al. (2016)": "16631020", "Firat et al. (2016)": "6359641"}}}, {"token_start": 70, "token_end": 113, "char_start": 273, "char_end": 514, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Johnson et al. (2017)": "6053988"}, "Reference": {}}}, {"token_start": 133, "token_end": 175, "char_start": 636, "char_end": 846, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Artetxe et al. (2018)": "3515219"}, "Reference": {}}}]}
{"id": "2712419_1", "paragraph": "[BOS] Our learning and decoding algorithms are also different from Kruengkrai et al. (2009) .\n[BOS] While Kruengkrai et al. (2009) perform dynamic programming and MIRA learning, we use beam-search to perform incremental decoding, and the early-update version of the perceptron algorithm to train the model.\n[BOS] Dynamic programming is exact inference, for which the time complexity is decided by the locality of feature templates.\n[BOS] In contrast, beam-search is approximate and can run in linear time.\n[BOS] The parameter updating for our algorithm is conceptually and computationally simpler than MIRA, though its performance can be slightly lower.\n[BOS] However, the earlyupdate mechanism we use is consistent with our incremental approach, and improves the learning of the beam-search process.\n\n", "discourse_tags": ["Reflection", "Single_summ", "Transition", "Transition", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 22, "char_start": 6, "char_end": 91, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Kruengkrai et al. (2009)": "769547"}}}, {"token_start": 24, "token_end": 70, "char_start": 100, "char_end": 306, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "2712419_0", "paragraph": "[BOS] The effectiveness of our beam-search decoder showed that the joint segmentation and tagging problem may be less complex than previously perceived (Zhang and Clark, 2008; Jiang et al., 2008a) .\n[BOS] At the very least, the single model approach with a simple decoder achieved competitive accuracies to what has been achieved so far by the reranking (Shi and Wang, 2007; Jiang et al., 2008b) models and an ensemble model using machine-translation techniques (Jiang et al., 2008a) .\n[BOS] This may shed new light on joint segmentation and POS-tagging methods.\n[BOS] Kruengkrai et al. (2009) and Zhang and Clark (2008) are the most similar to our system among related work.\n[BOS] Both systems use a discriminatively trained linear model to score candidate outputs.\n[BOS] The work of Kruengkrai et al. (2009) is based on Nakagawa and Uchimoto (2007), which separates the processing of known words and unknown words, and uses a set of segmentation tags to represent the segmentation of characters.\n[BOS] In contrast, our model is conceptually simpler, and does not differentiate known words and unknown words.\n[BOS] Moreover, our model is based on our previous work, in line with Zhang and Clark (2007) , which does not treat word segmentation as character sequence labeling.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Transition", "Multi_summ", "Multi_summ", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 13, "token_end": 40, "char_start": 67, "char_end": 196, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhang and Clark, 2008;": "105219", "Jiang et al., 2008a)": "9285364"}}}, {"token_start": 67, "token_end": 85, "char_start": 344, "char_end": 395, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Shi and Wang, 2007;": null, "Jiang et al., 2008b)": "12138087"}}}, {"token_start": 88, "token_end": 104, "char_start": 410, "char_end": 483, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Jiang et al., 2008a)": "9285364"}}}, {"token_start": 121, "token_end": 150, "char_start": 569, "char_end": 675, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kruengkrai et al. (2009)": "769547", "Zhang and Clark (2008)": "105219"}, "Reference": {}}}, {"token_start": 166, "token_end": 217, "char_start": 773, "char_end": 997, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kruengkrai et al. (2009)": "769547"}, "Reference": {}}}, {"token_start": 240, "token_end": 258, "char_start": 1126, "char_end": 1202, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Zhang and Clark (2007)": "2687347"}}}]}
{"id": "46940236_7", "paragraph": "[BOS] Prediction For prediction, we take a query with each Chengyu definition (q, d j ), 1  j  m as input, and predict a probability matrix M  R mm , where m is the number of candidates.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "46940236_6", "paragraph": "[BOS] , where W  maps the final representation of the query into R m , and m is the number of classes.\n[BOS] Then we optimize the log likelihood: L = m j=1 y j log(p j ), where y j is 0 or 1 depending on if the truth is Chengyu d j or not.\n\n", "discourse_tags": ["Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "46940236_5", "paragraph": "[BOS] Training With the weighted sum vector representation of the query R, we apply a softmax function to compute the probability of each candidate Chengyu d j to be filled into the slot.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "46940236_4", "paragraph": "[BOS] and e i = d T W  h i , where W  is a parameter to capture the relevance between a query and a definition flexibly .\n[BOS] d T is the last hidden hidden state of the Bi-LSTM encoding the definition.\n\n", "discourse_tags": ["Other", "Other"], "span_citation_mapping": []}
{"id": "46940236_3", "paragraph": "[BOS] Given the hidden states H = h 0 , h 1 , ..., h n of the Bi-LSTM encoding the query sentence, where h i denotes the concatenation of the hidden states of word w i with forward and backward LSTMs, the attention layer sum over h i with learnable weight : R = n i=1  i  h i , where R is the weighted sum vector representation of the query.\n[BOS]  i is a learnable weight which is computed by\n\n", "discourse_tags": ["Other", "Other"], "span_citation_mapping": []}
{"id": "46940236_2", "paragraph": "[BOS] Attention To better capture the correlation between a query and each Chengyu definition, we use an attention mechanism (Bahdanau et al., 2014; Sutskever et al., 2014) to compare the semantic relatedness of each word in the query sentence with the meaning of each Chengyu definition.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": [{"token_start": 20, "token_end": 41, "char_start": 105, "char_end": 172, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bahdanau et al., 2014;": "11212020", "Sutskever et al., 2014)": "7961699"}}}]}
{"id": "46940236_1", "paragraph": "[BOS] 3 Approach Figure 1 shows the overall architecture of our approach.\n[BOS] For a query and the definition of a candidate Chengyu, we first apply a word segmentation tool jieba 2 to segment query and definition into words, and apply a Bi-LSTM network to encode each word with a contextual embedding.\n[BOS] In order to better capture the correlation between a query and a Chengyu, we further compare the representations of the Chengyu definition and the contextual embedding of each word in the query, and Encoding Given a query q and a Chenyu definition d j from the target Chengyu database D = {d 1 , d 2 , ..., d m }, we apply two Bi-LSTM networks to encode them separately.\n[BOS] Each Bi-LSTM network leverages long distance features from the whole sentence to capture the context information by using a memory cell (Hochreiter and Schmidhuber, 1997) .\n[BOS] Each word in q and d j is assigned a contextual embedding.\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection", "Narrative_cite", "Other"], "span_citation_mapping": [{"token_start": 173, "token_end": 187, "char_start": 811, "char_end": 857, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hochreiter and Schmidhuber, 1997)": "1915014"}}}]}
{"id": "46940236_0", "paragraph": "[BOS] Our Chengyu cloze test task is similar to reading comprehension (Hermann et al., 2015; Cui et al., 2016; Kadlec et al., 2016; Seo et al., 2016 (Xu et al., 2010) and improve Chinese word segmentation (Chan and Chong, 2008; Sun and Xu, 2011; Wang and Xu, 2017) .\n[BOS] Chengyus differ from metaphors in other languages (Tsvetkov et al., 2014; Shutova, 2010) because they do not follow the grammatical structure and syntax of the modern Chinese.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 12, "token_end": 53, "char_start": 48, "char_end": 166, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hermann et al., 2015;": "6203757", "Cui et al., 2016;": "3933075", "Kadlec et al., 2016;": "11022639", "Seo et al., 2016": "8535316", "(Xu et al., 2010)": "1250256"}}}, {"token_start": 55, "token_end": 77, "char_start": 179, "char_end": 264, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chan and Chong, 2008;": null, "Sun and Xu, 2011;": "7490669", "Wang and Xu, 2017)": null}}}, {"token_start": 79, "token_end": 104, "char_start": 273, "char_end": 361, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Tsvetkov et al., 2014;": "950358", "Shutova, 2010)": "7578946"}}}]}
{"id": "3504277_1", "paragraph": "[BOS] In NMT systems, the attention model (Bahdanau et al., 2015) becomes a crucial part of the decoder model.\n[BOS] Cohn et al. (2016) and Feng et al. (2016) extend the attentional model to include structural biases from word based alignment models.\n[BOS] Kim et al. (2017) incorporate richer structural distributions within deep networks to extend the attention model.\n[BOS] Our contribution to the decoder model is to directly exploit structural information in the attention model combined with a coverage mechanism.\n\n", "discourse_tags": ["Narrative_cite", "Multi_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 7, "token_end": 20, "char_start": 22, "char_end": 65, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bahdanau et al., 2015)": "11212020"}}}, {"token_start": 30, "token_end": 60, "char_start": 117, "char_end": 250, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Cohn et al. (2016) and": "1964946", "Feng et al. (2016)": "14410825"}, "Reference": {}}}, {"token_start": 61, "token_end": 81, "char_start": 257, "char_end": 370, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kim et al. (2017)": "6961760"}, "Reference": {}}}]}
{"id": "3504277_0", "paragraph": "[BOS] Recently, many studies have focused on using explicit syntactic tree structure to help learn sentence representations for various sentence classification tasks.\n[BOS] For example, Teng and Zhang (2016) and Kokkinos and Potamianos (2017) extend the bottom-up model to a bidirectional model for classification tasks, using Tree-LSTMs with head lexicalization and Tree-GRUs, respectively.\n[BOS] We draw on some of these ideas and apply them to machine translation.\n[BOS] We use the representation learnt from tree structures to enhance the original sequential model, and make use of these syntactic information during the generation phase.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 26, "token_end": 80, "char_start": 173, "char_end": 391, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Teng and Zhang (2016)": "16497082", "Kokkinos and Potamianos (2017)": "5099151"}, "Reference": {}}}]}
{"id": "30164212_2", "paragraph": "[BOS] In this push toward complexity, we do not believe that researchers have adequately explored baselines, and thus it is unclear how much various NN techniques actually help.\n[BOS] To this end, our work builds on Ture and Jojic (2017) , who adopted a straightforward problem decomposition with simple NN models to argue that attentionbased mechanisms don't really help.\n[BOS] We take this one step further and examine techniques that do not involve neural networks.\n[BOS] Establishing strong baselines allows us to objectively quantify the contribution of various deep learning techniques.\n\n", "discourse_tags": ["Transition", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 34, "token_end": 73, "char_start": 184, "char_end": 372, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ture and Jojic (2017)": "8382317"}, "Reference": {}}}]}
{"id": "30164212_1", "paragraph": "[BOS] The original solution of Bordes et al. (2015) featured memory networks, but over the past several years, researchers have applied many NN architectures for tackling this problem: Golub and He (2016) proposed a characterlevel attention-based encoder-decoder framework; Dai et al. (2016) proposed a conditional probabilistic framework using BiGRUs.\n[BOS] Lukovnikov et al. (2017) used a hierarchical word/character-level question encoder and trained a neural network in an end-to-end manner.\n[BOS] Yin et al. (2016) applied a character-level CNN for entity linking and a separate word-level CNN with attentive max-pooling for fact selection.\n[BOS] Yu et al. (2017) used a hierarchical residual Bi-LSTM for relation detection, the results of which are combined with entity linking output.\n[BOS] These approaches can be characterized as exploiting increasingly sophisticated modeling techniques (e.g., attention, residual learning, etc.\n[BOS] ).\n\n", "discourse_tags": ["Multi_summ", "Single_summ", "Single_summ", "Single_summ", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 2, "token_end": 18, "char_start": 6, "char_end": 76, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Bordes et al. (2015)": "9605730"}}}, {"token_start": 38, "token_end": 56, "char_start": 185, "char_end": 272, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Golub and He (2016)": null}, "Reference": {}}}, {"token_start": 57, "token_end": 73, "char_start": 274, "char_end": 352, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Dai et al. (2016)": "2887257"}, "Reference": {}}}, {"token_start": 74, "token_end": 108, "char_start": 359, "char_end": 495, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Lukovnikov et al. (2017)": "12983389"}, "Reference": {}}}, {"token_start": 143, "token_end": 173, "char_start": 652, "char_end": 791, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Yu et al. (2017)": "7752968"}, "Reference": {}}}]}
{"id": "30164212_0", "paragraph": "[BOS] The problem of question answering on knowledge graphs dates back at least a decade, but the most relevant recent work in the NLP community comes from Berant et al. (2013) .\n[BOS] This thread of work focuses on semantic parsing, where a question is mapped to its logical form and then translated to a structured query, cf.\n[BOS] (Berant and Liang, 2014; Reddy et al., 2014) .\n[BOS] However, the more recent SIMPLEQUESTIONS dataset (Bordes et al., 2015) has emerged as the de facto benchmark for evaluating simple QA over knowledge graphs.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Narrative_cite", "Multi_summ"], "span_citation_mapping": [{"token_start": 25, "token_end": 38, "char_start": 131, "char_end": 176, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Berant et al. (2013)": "6401679"}}}, {"token_start": 69, "token_end": 85, "char_start": 334, "char_end": 378, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Berant and Liang, 2014;": "1336493", "Reddy et al., 2014)": null}}}, {"token_start": 92, "token_end": 105, "char_start": 412, "char_end": 457, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Bordes et al., 2015)": "9605730"}, "Reference": {}}}]}
{"id": "2949011_2", "paragraph": "[BOS] A recent approach by Hermann et al. (2015) uses attention-based recurrent neural networks to attack the problem of machine comprehension.\n[BOS] In this work, the authors show how to generate large amounts of data for machine comprehension exploiting news websites, and how to use novel architectures in deep learning to solve the task.\n[BOS] However, due to the need for a large dataset for training, and the focus only on questions that take entities as answers, this approach has not been applied to MCTest.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 101, "char_start": 6, "char_end": 515, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hermann et al. (2015)": "6203757"}, "Reference": {}}}]}
{"id": "2949011_1", "paragraph": "[BOS] The recently proposed class of methods called Memory Network (Weston et al., 2014) , uses neural networks and external memory to answer a simpler comprehension task.\n[BOS] Though quite successful on toy tasks, those methods cannot yet be applied to MCTest as they require much larger training datasets than the ones available for this task.\n\n", "discourse_tags": ["Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 66, "char_start": 6, "char_end": 346, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "2949011_0", "paragraph": "[BOS] The use of shallow methods for machine comprehension has been explored in previous work, for example Hirschman et al. (1999) used a bag-ofwords to match question-answer pairs to sentences in the text, and choose the best pair with the best matching sentence.\n[BOS] As discussed in our analysis, such systems cannot handle well questions involving negation and quantification.\n[BOS] Numerical questions, which we found to be particularly challenging, have been the focus of recent work on algebra word problems (Kushman et al., 2014) for which dedicated systems have been developed.\n[BOS] MacCartney et al. (2006) demonstrated that a large set of rules can be used to recognize valid textual entailments.\n[BOS] These consider phenomena such as polarity and quantification, similar to those we used in our analysis of the MCTest datasets.\n[BOS] More complex methods, which attempt deeper modeling of text include Natural Logic (Angeli and Manning, 2014) and Combinatorial Categorial Grammars (Lewis and Steedman, 2013) combined with distributional models.\n[BOS] While promising, these approaches have been developed primarily on sentence-level tasks, thus the stories in MCTest are likely to present additional challenges.\n\n", "discourse_tags": ["Single_summ", "Transition", "Narrative_cite", "Single_summ", "Single_summ", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 19, "token_end": 56, "char_start": 107, "char_end": 264, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 94, "token_end": 107, "char_start": 494, "char_end": 538, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kushman et al., 2014)": "12451537"}}}, {"token_start": 116, "token_end": 167, "char_start": 594, "char_end": 842, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"MacCartney et al. (2006)": "5898972"}, "Reference": {}}}, {"token_start": 179, "token_end": 190, "char_start": 917, "char_end": 957, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Angeli and Manning, 2014)": "2854390"}}}, {"token_start": 191, "token_end": 204, "char_start": 962, "char_end": 1022, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lewis and Steedman, 2013)": "2816192"}}}]}
{"id": "3144258_3", "paragraph": "[BOS] All the above methods rely on a multilingual lexicon or a word/pharse alignment, usually from a machine translation (MT) system.\n[BOS] (Blunsom et al., 2014) proposed a novel approach based on a joint optimization method for word alignments and the embedding.\n[BOS] A simplified version of this approach is proposed in (Hermann and Blunsom, 2014) , where a sentence is represented by the mean vector of the words involved.\n[BOS] Multilingual learning is then reduced to maximizing the overall distance of the parallel sentences in the training corpus, with the distance computed upon the sentence vectors.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 32, "token_end": 59, "char_start": 141, "char_end": 265, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Blunsom et al., 2014)": "5809776"}, "Reference": {}}}, {"token_start": 60, "token_end": 124, "char_start": 272, "char_end": 611, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Hermann and Blunsom, 2014)": "5809776"}, "Reference": {}}}]}
{"id": "3144258_2", "paragraph": "[BOS] The regularization-based approaches involve the multilingual constraint in the objective function for learning the embedding.\n[BOS] For example, (Zou et al., 2013) adds an extra term that reflects the distances of some pairs of semantically related words from different languages into the objective funtion.\n[BOS] A similar approach is proposed in (Klementiev et al., 2012) , which casts multilingual learning as a multitask learning and encodes the multilingual information in the interaction matrix.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 22, "token_end": 58, "char_start": 138, "char_end": 313, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Zou et al., 2013)": "931054"}, "Reference": {}}}, {"token_start": 59, "token_end": 99, "char_start": 320, "char_end": 507, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Klementiev et al., 2012)": "6758088"}, "Reference": {}}}]}
{"id": "3144258_1", "paragraph": "[BOS] Multilingual learning can be categorized into projection-based approaches and regularizationbased approaches.\n[BOS] In the projection-based approaches, the embedding is performed for each language individually with monolingual data, and then one or several projections are learned using multilingual data to represent the relation between languages.\n[BOS] Our method in this paper and the linear projection method in (Mikolov et al., 2013b ) both belong to this category.\n[BOS] Another interesting work proposed by (Faruqui and Dyer, 2014) learns linear transforms that project word vectors of all languages to a common low-dimensional space, where the correlation of the multilingual word pairs is maximized with the canonical correlation analysis (CCA).\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite", "Single_summ"], "span_citation_mapping": [{"token_start": 66, "token_end": 80, "char_start": 395, "char_end": 445, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mikolov et al., 2013b": "1966640"}}}, {"token_start": 88, "token_end": 142, "char_start": 484, "char_end": 761, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Faruqui and Dyer, 2014)": "3792324"}, "Reference": {}}}]}
{"id": "3144258_0", "paragraph": "[BOS] This work largely follows the methodology and experimental settings of (Mikolov et al., 2013b) , while we normalize the embedding and use an orthogonal transform to conduct bilingual translation.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": [{"token_start": 7, "token_end": 23, "char_start": 36, "char_end": 100, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mikolov et al., 2013b)": "1966640"}}}]}
{"id": "2796457_0", "paragraph": "[BOS] Klein and Manning presented an unlexicalized PCFG parser that eliminated all the lexicalized parameters (Klein and Manning, 2003 splitting tags (Matsuzaki et al., 2005; Petrov et al., 2006) .\n[BOS] In particular, Petrov et al. reported an F 1 of 90.2%, which is equivalent to that of state-of-the-art lexicalized parsers.\n[BOS] Dependency parsing has been actively studied in recent years (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Isozaki et al., 2004; McDonald et al., 2005; McDonald and Pereira, 2006; Corston-Oliver et al., 2006) .\n[BOS] For instance, Nivre and Scholz presented a deterministic dependency parser trained by memory-based learning (Nivre and Scholz, 2004) .\n[BOS] McDonald et al. proposed an online large-margin method for training dependency parsers (McDonald et al., 2005) .\n[BOS] All of them performed experiments using section 23 of the Penn Treebank.\n[BOS] Table 2 summarizes their dependency accuracies based on three evaluation criteria shown in Table 1 .\n[BOS] These parsers believed in the generalization ability of machine learners and did not pay attention to the issue of lexicalization.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Narrative_cite", "Single_summ", "Single_summ", "Single_summ", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 2, "token_end": 49, "char_start": 6, "char_end": 195, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Klein and Manning, 2003": "11495042", "(Matsuzaki et al., 2005;": "8008954", "Petrov et al., 2006)": "6684426"}}}, {"token_start": 51, "token_end": 87, "char_start": 204, "char_end": 327, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 88, "token_end": 149, "char_start": 334, "char_end": 549, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yamada and Matsumoto, 2003;": "13163488", "Nivre and Scholz, 2004;": "643522", "Isozaki et al., 2004;": "4150437", "McDonald et al., 2005;": "12926517", "McDonald and Pereira, 2006;": "802998", "Corston-Oliver et al., 2006)": "5989172"}}}, {"token_start": 151, "token_end": 179, "char_start": 558, "char_end": 690, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Nivre and Scholz, 2004)": "643522"}}}, {"token_start": 181, "token_end": 205, "char_start": 699, "char_end": 809, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {"(McDonald et al., 2005)": "12926517"}}}]}
{"id": "3666937_4", "paragraph": "[BOS] In the natural language processing field, a notable work related with multi-task learning was proposed by Collobert et al. (2011) which shared common representation for input words and solve different traditional NLP tasks such as part-of-Speech tagging, name entity recognition and semantic role labeling within one framework, where the convolutional neural network model was used.\n[BOS] Hatori et al. (2012) proposed to jointly train word segmentation, POS tagging and dependency parsing, which can also be seen as a multi-task learning approach.\n[BOS] Similar idea has also been proposed by in Chinese dependency parsing.\n[BOS] Most of multi-task learning or joint training frameworks can be summarized as parameter sharing approaches proposed by Ando and Zhang (2005) where they jointly trained models and shared center parameters in NLP tasks.\n[BOS] Researchers have also explored similar approaches (Sennrich et al., 2013; Cui et al., 2013) in statistical machine translation which are often refered as domain adaption.\n[BOS] Our work explores the possibility of machine translation under the multitask framework by using the recurrent neural networks.\n[BOS] To the best of our knowledge, this is the first trial of end to end machine translation under multi-task learning framework.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Transition", "Single_summ", "Narrative_cite", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 72, "char_start": 6, "char_end": 388, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Collobert et al. (2011)": "351666"}, "Reference": {}}}, {"token_start": 73, "token_end": 107, "char_start": 395, "char_end": 554, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hatori et al. (2012)": "10011032"}, "Reference": {}}}, {"token_start": 121, "token_end": 161, "char_start": 637, "char_end": 854, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ando and Zhang (2005)": null}, "Reference": {}}}, {"token_start": 162, "token_end": 185, "char_start": 861, "char_end": 952, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sennrich et al., 2013;": "16292943", "Cui et al., 2013)": "16379628"}}}]}
{"id": "3666937_3", "paragraph": "[BOS] Different from traditional statistical machine translation, neural machine translation encodes a variable-length source sentence with a recurrent neural network into a fixed-length vector representation and decodes it with another recurrent neural network from a fixed-length vector into variable-length target sentence.\n[BOS] A typical model is the RNN encoder-decoder approach proposed by , which utilizes a bidirectional recurrent neural network to compress the source sentence information and fits the conditional probability of words in target languages with a recurrent manner.\n[BOS] Moreover, soft alignment parameters are considered in this model.\n[BOS] As a specific example model in this paper, we adopt a RNN encoder-decoder neural machine translation model for multi-task learning, though all neural network based model can be adapted in our framework.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Reflection"], "span_citation_mapping": []}
{"id": "3666937_2", "paragraph": "[BOS] Neural Machine translation is a emerging new field in machine translation, proposed by several work recently (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; , aiming at end-to-end machine translation without phrase table extraction and language model training.\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 2, "token_end": 40, "char_start": 6, "char_end": 170, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kalchbrenner and Blunsom, 2013;": "12639289"}}}]}
{"id": "3666937_1", "paragraph": "[BOS] On the problem of how to translate one source language to many target languages within one model, few work has been done in statistical machine translation.\n[BOS] A related work in SMT is the pivot language approach for statistical machine translation which uses a commonly used language as a \"bridge\" to generate source-target translation for language pair with few training corpus.\n[BOS] Pivot based statistical machine translation is crucial in machine translation for resource-poor language pairs, such as Spanish to Chinese.\n[BOS] Considering the problem of translating one source language to many target languages, pivot based SMT approaches does work well given a large-scale source language to pivot language bilingual corpus and large-scale pivot language to target languages corpus.\n[BOS] However, in reality, language pairs between English and many other target languages may not be large enough, and pivot-based SMT sometimes fails to handle this problem.\n[BOS] Our approach handles one to many target language translation in a different way that we directly learn an end to multi-end translation system that does not need a pivot language based on the idea of neural machine translation.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Transition", "Transition", "Reflection"], "span_citation_mapping": []}
{"id": "3666937_0", "paragraph": "[BOS] Statistical machine translation systems often rely on large-scale parallel and monolingual training corpora to generate translations of high quality.\n[BOS] Unfortunately, statistical machine translation system often suffers from data sparsity problem due to the fact that phrase tables are extracted from the limited bilingual corpus.\n[BOS] Much work has been done to address the data sparsity problem such as the pivot language approach (Wu and Wang, 2007; Cohn and Lapata, 2007) and deep learning techniques (Devlin et al., 2014; Gao et al., 2014; Sundermeyer et al., 2014; .\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 68, "token_end": 87, "char_start": 420, "char_end": 486, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wu and Wang, 2007;": "3681367", "Cohn and Lapata, 2007)": "9334744"}}}, {"token_start": 88, "token_end": 116, "char_start": 491, "char_end": 580, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Devlin et al., 2014;": "7417943", "Gao et al., 2014;": "10473972"}}}]}
{"id": "3701984_3", "paragraph": "[BOS] Under a non-AL framework, Mejer and Crammer (2012) propose an interesting light feedback scheme for dependency parsing by letting annotators decide the better one from top-2 parse trees produced by the current parsing model.\n[BOS] Hwa (1999) pioneers the idea of using PA to reduce manual labeling effort for constituent grammar induction.\n[BOS] She uses a variant InsideOutside re-estimation algorithm (Pereira and Schabes, 1992) to induce a grammar from PA. Clark and Curran (2006) propose to train a Combinatorial Categorial Grammar parser using partially labeled data only containing predicate-argument dependencies.\n[BOS] Tsuboi et al. (2008) extend CRFbased sequence labeling models to learn from incomplete annotations, which is the same with Marcheggiani and Artires (2014) .\n[BOS] Li et al. (2014) propose a CRF-based dependency parser that can learn from partial tree projected from sourcelanguage structures in the cross-lingual parsing scenario.\n[BOS] Mielens et al. (2015) propose to impute missing dependencies based on Gibbs sampling in order to enable traditional parsers to learn from partial trees.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 49, "char_start": 6, "char_end": 230, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Mejer and Crammer (2012)": "5679981"}, "Reference": {}}}, {"token_start": 50, "token_end": 101, "char_start": 237, "char_end": 465, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hwa (1999)": "7117045"}, "Reference": {"(Pereira and Schabes, 1992)": "696805"}}}, {"token_start": 101, "token_end": 128, "char_start": 466, "char_end": 626, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Clark and Curran (2006)": "2547341"}, "Reference": {}}}, {"token_start": 129, "token_end": 165, "char_start": 633, "char_end": 787, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Tsuboi et al. (2008)": "18083801"}, "Reference": {"Marcheggiani and Arti\u00e8res (2014)": "13642704"}}}, {"token_start": 167, "token_end": 201, "char_start": 796, "char_end": 963, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Li et al. (2014)": "7527306"}, "Reference": {}}}, {"token_start": 202, "token_end": 234, "char_start": 970, "char_end": 1122, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Mielens et al. (2015)": "17978489"}, "Reference": {}}}]}
{"id": "3701984_2", "paragraph": "[BOS] Most previous works on AL with PA only conduct simulation experiments.\n[BOS] Flannery and Mori (2015) perform human annotation to measure true annotation time.\n[BOS] A single annotator is employed to annotate for two hours alternating FA and PA (33% batch) every fifteen minutes.\n[BOS] Beyond their initial expectation, they find that the annotation time per dependency is nearly the same for FA and PA (different from our findings) and gives a few interesting explanations.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 15, "token_end": 93, "char_start": 83, "char_end": 480, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Flannery and Mori (2015)": "18357997"}, "Reference": {}}}]}
{"id": "3701984_1", "paragraph": "[BOS] In parsing community, Sassano and Kurohashi (2010) select bunsetsu (similar to phrases) pairs with smallest scores from a local classifier, and let annotators decide whether the pair composes a dependency.\n[BOS] They convert partially annotated instances into local dependency/non-dependency classification instances to help a simple shiftreduce parser.\n[BOS] Mirroshandel and Nasr (2011) select most uncertain words based on votes of nbest parsers, and convert partial trees into full trees by letting a baseline parser perform constrained decoding in order to preserve partial annotation.\n[BOS] Under a different query-by-committee AL framework, Majidi and Crane (2013) select most uncertain words using a committee of diverse parsers, and convert partial trees into full trees by letting the parsers of committee to decide the heads of remaining tokens.\n[BOS] Based on a first-order (pointwise) Japanese parser, Flannery and Mori (2015) use scores of a local classifier for task selection, and treat PA as dependency/non-dependency instances (Flannery et al., 2011) .\n[BOS] Different from above works, this work adopts a state-of-the-art probabilistic dependency parser, uses more principled tree probabilities and dependency marginal probabilities for uncertainty measurement, and learns from PA based on a forest-based training objective which is more theoretically sound.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 2, "token_end": 70, "char_start": 6, "char_end": 359, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Sassano and Kurohashi (2010)": "14925259"}, "Reference": {}}}, {"token_start": 71, "token_end": 116, "char_start": 366, "char_end": 596, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 117, "token_end": 170, "char_start": 603, "char_end": 862, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Majidi and Crane (2013)": "150387488"}, "Reference": {}}}, {"token_start": 171, "token_end": 222, "char_start": 869, "char_end": 1074, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Flannery and Mori (2015)": "18357997"}, "Reference": {"(Flannery et al., 2011)": "11364094"}}}]}
{"id": "3701984_0", "paragraph": "[BOS] Recently, AL with PA attracts much attention in sentence-wise natural language processing such as sequence labeling and parsing.\n[BOS] For sequence labeling, Marcheggiani and Artires (2014) systematically compare a dozen uncertainty metrics in token-wise AL with PA (without comparison with FA), whereas Settles and Craven (2008) investigate different uncertainty metrics in AL with FA.\n[BOS] Li et al. (2012) propose to only annotate the most uncertain word boundaries in a sentence for Chinese word segmentation and show promising results on both simulation and human annotation experiments.\n[BOS] All above works are based on CRFs and make extensive use of sequence probabilities and token marginal probability.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 30, "token_end": 60, "char_start": 164, "char_end": 300, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Marcheggiani and Arti\u00e8res (2014)": "13642704"}, "Reference": {}}}, {"token_start": 62, "token_end": 79, "char_start": 310, "char_end": 392, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Settles and Craven (2008)": "8197231"}, "Reference": {}}}, {"token_start": 80, "token_end": 116, "char_start": 399, "char_end": 599, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Li et al. (2012)": "13160411"}, "Reference": {}}}]}
{"id": "29050992_2", "paragraph": "[BOS] Our local-global SEQ2SEQ model is closely related to a many-to-many multi-task SEQ2SEQ proposed by Luong et al. (2016) .\n[BOS] The critical difference is in that their model assumes only local tasks, while our model assumes many local tasks (situation-specific dialogue modeling) and one global task (general dialogue modeling).\n\n", "discourse_tags": ["Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 20, "token_end": 36, "char_start": 74, "char_end": 124, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Luong et al. (2016)": "6954272"}}}]}
{"id": "29050992_1", "paragraph": "[BOS] Recently, Xing et al. (2017) proposed to explicitly consider topics of utterances to generate topiccoherent responses.\n[BOS] Although they used latent Dirichlet allocation while we use k-means clustering, both methods confirmed the importance of utterance situations.\n[BOS] The way to obtain specific situations is still an open research problem.\n[BOS] As demonstrated in this study, our primary contribution is the invention of neural mechanisms that can consider various conversational situations.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 50, "char_start": 6, "char_end": 273, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Xing et al. (2017)": "9514751"}, "Reference": {}}}]}
{"id": "29050992_0", "paragraph": "[BOS] Conversational situations have been implicitly addressed by preparing datasets specific to the target situations and by solving the problem as a taskoriented conversation task (Williams and Young, 2007) ; examples include troubleshooting (Vinyals and Le, 2015) , navigation (Wen et al., 2015) , interviewing (Kobori et al., 2016) , and restaurant search (Wen et al., 2017) .\n[BOS] In what follows, we introduce non-task-oriented conversational models that explicitly consider conversational situations.\n[BOS] Hasegawa et al. (2013) presented a conversational model that generates a response so that it elicits a certain emotion (e.g., joy) in the addressee mind.\n[BOS] Their model is based on statistical machine translation and linearly interpolates two conversational models that are trained from a small emotion-labeled dialogue corpus and a large nonlabeled dialogue corpus, respectively.\n[BOS] This model is similar to our local-global SEQ2SEQ but differs in that it has hyperparameters for the interpolation, whereas our local-global SEQ2SEQ automatically learns W G and W L from the training data.\n[BOS] Li et al. (2016b) proposed a neural conversational model that generates responses taking into consideration speakers' personalities such as gender or living place.\n[BOS] Because they fed a specific speaker ID to their model and represent individual (known) speakers with embeddings, Their model cannot handle unknown speakers.\n[BOS] In contrast, our model can consider any speakers with profiles because we represent each cluster of profiles with an embedding and find an appropriate profile type for the given profile by nearest-neighbor search.\n[BOS] Sordoni et al. (2015) encoded a given utterance and the past dialogue exchanges, and combined the resulting representations for RNN to decode a response.\n[BOS] Zhao et al. (2017) used a conditional variational autoencoder and automaticallyinduced dialogue acts to handle discourse-level diversity in the encoder.\n[BOS] While these sophisticated architectures are designed to take dialogue histories into consideration, our simple models can easily exploit various situations.\n\n", "discourse_tags": ["Narrative_cite", "Reflection", "Single_summ", "Single_summ", "Reflection", "Single_summ", "Single_summ", "Reflection", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 24, "token_end": 35, "char_start": 151, "char_end": 208, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Williams and Young, 2007)": "13903063"}}}, {"token_start": 38, "token_end": 50, "char_start": 228, "char_end": 266, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vinyals and Le, 2015)": "12300158"}}}, {"token_start": 51, "token_end": 60, "char_start": 269, "char_end": 298, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wen et al., 2015)": "739696"}}}, {"token_start": 61, "token_end": 72, "char_start": 301, "char_end": 335, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kobori et al., 2016)": "17719456"}}}, {"token_start": 74, "token_end": 85, "char_start": 342, "char_end": 378, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wen et al., 2017)": null}}}, {"token_start": 109, "token_end": 187, "char_start": 515, "char_end": 898, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hasegawa et al. (2013)": "7585105"}, "Reference": {}}}, {"token_start": 233, "token_end": 292, "char_start": 1117, "char_end": 1443, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Li et al. (2016b)": "2955580"}, "Reference": {}}}, {"token_start": 331, "token_end": 363, "char_start": 1670, "char_end": 1823, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Sordoni et al. (2015)": "94285"}, "Reference": {}}}, {"token_start": 364, "token_end": 393, "char_start": 1830, "char_end": 1982, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhao et al. (2017)": "14688760"}, "Reference": {}}}]}
{"id": "2412277_0", "paragraph": "[BOS] Because recurrent networks are such a natural fit for modeling languages (given the sequential nature of the latter), bi-directional LSTM networks are becoming increasingly common in all sorts of linguistic tasks, for example event detection in Ghaeini et al. (2016) .\n[BOS] In fact, we discovered after submission that Kiperwasser and Goldberg (2016) have concurrently developed an extremely similar approach to our dependency parser.\n[BOS] Instead of extending it to constituency parsing, they also apply the same idea to graph-based dependency parsing.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 42, "token_end": 54, "char_start": 232, "char_end": 272, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Ghaeini et al. (2016)": "3608203"}}}, {"token_start": 64, "token_end": 108, "char_start": 326, "char_end": 561, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kiperwasser and Goldberg (2016)": "1642392"}, "Reference": {}}}]}
{"id": "33693785_3", "paragraph": "[BOS] The CAEVO architecture is itself inspired by the sieve-based architectures that have been successfully applied to event and entity coreference as well as spatial relation extraction tasks (Lee et al., 2012 (Lee et al., , 2013 D'Souza and Ng, 2015) .\n[BOS] Years since CAEVO's introduction, a coreference sieve architecture still achieves top performance (Lee et al., 2017) .\n[BOS] The key idea behind these architectures is to combine information from several classifiers by assigning precedence to predictions according to the reliability of the classifier from which they originate.\n[BOS] A precision-ranked series of \"sieve\" classifiers generate predictions, and predictions from the more reliable sieves earlier in the series inform the predictions of the less reliable sieves later in the series.\n[BOS] Generally, the predictions from a highly-ranked sieve can inform a lowranked sieve in several ways, but within CAEVO, predictions from early classifiers are coupled via transitive inference rules to generate an expanding set of predictions that override output from less reliable classifiers later on in the series.\n[BOS] In the next section, we describe a more generic view of this architecture which will motivate alternative methods for assigning precedence to predictions from the collection of classifiers.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Transition", "Transition", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 23, "token_end": 59, "char_start": 120, "char_end": 253, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lee et al., 2012": "2294115", "(Lee et al., , 2013": "13475584", "D'Souza and Ng, 2015)": "15926842"}}}, {"token_start": 71, "token_end": 88, "char_start": 298, "char_end": 378, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lee et al., 2017)": "14576942"}}}]}
{"id": "33693785_2", "paragraph": "[BOS] Recent work has focused on the construction of timelines of related events, using SRL annotations to determine which events are related through common actors (Laparra et al., 2015) .\n[BOS] In addition, other work has outperformed the original CAEVO with a 2.2% relative F1 gain on TimeBank-Dense using word embedding features within a stacked ensemble of event-event, eventtime, and event-creation-time logistic regression classifiers (Mirza and Tonelli, 2016) .\n[BOS] We draw inspiration from this recent work by incorporating SRL and word embedding features into the machine-learned CAEVO sieves.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 27, "token_end": 39, "char_start": 150, "char_end": 186, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Laparra et al., 2015)": "7226753"}}}, {"token_start": 75, "token_end": 104, "char_start": 341, "char_end": 466, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mirza and Tonelli, 2016)": "14374843"}}}]}
{"id": "33693785_1", "paragraph": "[BOS] The TimeBank-Dense corpus provides a significantly more dense and complete set of annotations compared to previous corpora.\n[BOS] 1 TimeBank-Dense extends a subset of the original TimeBank corpus with annotations for (almost) all event-time, time-time, and event-time pairs across consecutive sentences, as well as relations to the document creation time.\n[BOS] This dense corpus facilitated the evaluation of CAEVO-a sieve-based architecture which maintains transitivity constraints across independent predictions from several specialized classifiers.\n\n", "discourse_tags": ["Transition", "Transition", "Transition"], "span_citation_mapping": []}
{"id": "33693785_0", "paragraph": "[BOS] Early work on event ordering focused on developing machine-learned classifiers that label the temporal relations between small subsets of pairs of events within documents using lexical and syntactical features (Bethard and Martin, 2007; Cheng et al., 2007; UzZaman and Allen, 2010; Llorens et al., 2010; Bethard, 2013) .\n[BOS] Later work leveraged information across pairwise predictions by imposing transitivity constraints using techniques like integer linear programming and Markov logic networks (Bramsen et al., 2006; Chambers and Jurafsky, 2008; Tatu and Srikanth, 2008; Yoshikawa et al., 2009) .\n[BOS] CAEVO followed these and other hybrid rule-based approaches (D'Souza and Ng, 2013) , but with the transitivity constraints yielding larger gains in performance for the more complete temporal graph constructed on the TimeBankDense corpus (Cassidy et al., 2014; Chambers et al., 2014) .\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 29, "token_end": 70, "char_start": 183, "char_end": 324, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"UzZaman and Allen, 2010;": "1213407", "Llorens et al., 2010;": "9513775", "Bethard, 2013)": "41372474"}}}, {"token_start": 88, "token_end": 130, "char_start": 453, "char_end": 606, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Tatu and Srikanth, 2008;": "15847806", "Yoshikawa et al., 2009)": "6945139"}}}, {"token_start": 139, "token_end": 154, "char_start": 646, "char_end": 697, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(D'Souza and Ng, 2013)": "5881896"}}}, {"token_start": 175, "token_end": 196, "char_start": 831, "char_end": 897, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "2815754_2", "paragraph": "[BOS] Hypergraphs have been successfully used in parsing (Klein and Manning., 2001; Huang and Chiang, 2005; Huang, 2008) and machine translation (Huang and Chiang, 2007; .\n[BOS] Both and use a translation hypergraph to represent search space.\n[BOS] The difference is that their hypergraphs are specifically designed for the forest-based tree-to-string model and the hierarchical phrase-based model, respectively, while ours is more general and can be applied to arbitrary models.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 9, "token_end": 30, "char_start": 49, "char_end": 120, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Klein and Manning., 2001;": "6481971", "Huang and Chiang, 2005;": "3598758", "Huang, 2008)": "1131864"}}}, {"token_start": 31, "token_end": 40, "char_start": 125, "char_end": 168, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "2815754_1", "paragraph": "[BOS] In machine translation, confusion-network based combination techniques (e.g., (Rosti et al., 2007; He et al., 2008) ) have achieved the state-of-theart performance in MT evaluations.\n[BOS] From a dif- ferent perspective, we try to combine different approaches directly in decoding phase by using hypergraphs.\n[BOS] While system combination techniques manipulate only the final translations of each system, our method opens the possibility of exploiting much more information.\n[BOS] first distinguish between max-derivation decoding and max-translation decoding explicitly.\n[BOS] They show that max-translation decoding outperforms max-derivation decoding for the latent variable model.\n[BOS] While they train the parameters using a maximum a posteriori estimator, we extend the MERT algorithm (Och, 2003) to take the evaluation metric into account.\n\n", "discourse_tags": ["Narrative_cite", "Reflection", "Reflection", "Transition", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 6, "token_end": 34, "char_start": 30, "char_end": 121, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rosti et al., 2007;": "12146323", "He et al., 2008)": "2822831"}}}, {"token_start": 148, "token_end": 156, "char_start": 784, "char_end": 810, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Och, 2003)": "5474833"}}}]}
{"id": "2815754_0", "paragraph": "[BOS] System combination has benefited various NLP tasks in recent years, such as products-of-experts (e.g., (Smith and Eisner, 2005) ) and ensemblebased parsing (e.g., (Henderson and Brill, 1999) ).\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 17, "token_end": 36, "char_start": 82, "char_end": 133, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Smith and Eisner, 2005)": "259144"}}}, {"token_start": 38, "token_end": 56, "char_start": 140, "char_end": 196, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Henderson and Brill, 1999)": "5666926"}}}]}
{"id": "35362759_0", "paragraph": "[BOS] Recent research that achieves state-of-the-art results is primarily based on deep learning techniques.\n[BOS] Chen et al. (2015) propose a dynamic multi-pooling convolutional neural network (DM-CNN), which automatically induces lexical-level and sentence-level features from text, achieving state-of-the-art results.\n[BOS] Nguyen and Grishman (2015)'s work focuses on CNNs using word embeddings in order to achieve a more generalizable event detection system.\n[BOS] Other approaches include Ghaeini et al. (2016) 's FBRNN, which is a modification of RNNs using word and branch embeddings, and Liu et al. (2016) 's ANN & Random ANN, which exploits the direct relationship between the FrameNet and the ACE Ontology in order to construct an out-domain ANN model.\n[BOS] Peng et al. (2016) showed that it is feasible to achieve state-of-the-art results with minimal supervision.\n[BOS] In their approach, they use only a few examples and the SRL of a candidate event in order to construct a structured vector representation, which maps the event to an ontology.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Multi_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 23, "token_end": 69, "char_start": 115, "char_end": 321, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Chen et al. (2015)": "14339673"}, "Reference": {}}}, {"token_start": 70, "token_end": 100, "char_start": 328, "char_end": 464, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 104, "token_end": 131, "char_start": 496, "char_end": 592, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ghaeini et al. (2016)": "3608203"}, "Reference": {}}}, {"token_start": 133, "token_end": 171, "char_start": 598, "char_end": 764, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Liu et al. (2016)": "16884406"}, "Reference": {}}}, {"token_start": 172, "token_end": 234, "char_start": 771, "char_end": 1060, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Peng et al. (2016)": "9834233"}, "Reference": {}}}]}
{"id": "47017117_2", "paragraph": "[BOS] Our work is also related to several papers which model an agent that navigates in an environment to find objects in an image (Ba et al., 2015) , relations in a knowledge-base (Das et al., 2018) , or documents on the web (Nogueira and Cho, 2016) .\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 25, "token_end": 34, "char_start": 125, "char_end": 148, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ba et al., 2015)": "14814581"}}}, {"token_start": 38, "token_end": 49, "char_start": 166, "char_end": 199, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Das et al., 2018)": "13206339"}}}, {"token_start": 51, "token_end": 64, "char_start": 205, "char_end": 250, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Nogueira and Cho, 2016)": null}}}]}
{"id": "47017117_1", "paragraph": "[BOS] In this work we use TRIVIAQA-NOP for evaluation of our navigation based approach and comparison to an IR baseline.\n[BOS] While there are various aspects to consider in such evaluation setup, our choice of data was derived mainly by the requirements for long and structured context.\n[BOS] Recently, several new datasets such as WIKIHOP and NARRATIVEQA were published.\n[BOS] These datasets try to focus on the tendency of RC models to match local context patterns, and are designed for multi-step reasoning.\n[BOS] (Welbl et al., 2017; Wadhwa et al., 2018; Koisky et al., 2017) .\n\n", "discourse_tags": ["Reflection", "Reflection", "Transition", "Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 101, "token_end": 128, "char_start": 518, "char_end": 580, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Welbl et al., 2017;": "9192723", "Wadhwa et al., 2018;": "13699041", "Ko\u010disky et al., 2017)": "2593903"}}}]}
{"id": "47017117_0", "paragraph": "[BOS] Handling the challenges of reasoning over multiple long documents is gaining fast momentum recently (Shen et al., 2017) .\n[BOS] As mentioned, some approaches use IR for reducing the amount of processed text Clark and Gardner, 2017) , while others use cheap or parallelizable models to handle long documents Swayamdipta et al., 2018; Wang et al., 2018a) .\n[BOS] Searching for answers while using a trained RC model as a black-box was also implemented recently in Wang et al. (2018b) , for open-domain questions and multiple short evidence texts from the Web.\n[BOS] Another thrust has focused on skimming text in a sequential manner (Yu et al., 2017) , or designing recurrent architectures that can consume text quickly (Bradbury et al., 2017; Seo et al., 2018; Campos et al., 2018; Yu et al., 2018) .\n[BOS] However, to the best of our knowledge no work has previously applied these methods to long documents such as Wikipedia pages.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Narrative_cite", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 13, "token_end": 24, "char_start": 83, "char_end": 125, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Shen et al., 2017)": "6300274"}}}, {"token_start": 38, "token_end": 47, "char_start": 198, "char_end": 237, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Clark and Gardner, 2017)": "223637"}}}, {"token_start": 51, "token_end": 79, "char_start": 257, "char_end": 358, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Swayamdipta et al., 2018;": "3474156", "Wang et al., 2018a)": "19178620"}}}, {"token_start": 87, "token_end": 108, "char_start": 403, "char_end": 487, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Wang et al. (2018b)": "13764176"}}}, {"token_start": 134, "token_end": 144, "char_start": 619, "char_end": 654, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yu et al., 2017)": "1762731"}}}, {"token_start": 147, "token_end": 186, "char_start": 670, "char_end": 803, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bradbury et al., 2017;": "51559", "Seo et al., 2018;": null, "Campos et al., 2018;": null, "Yu et al., 2018)": null}}}]}
{"id": "44176293_2", "paragraph": "[BOS] The other line of research attempted to combine character-level information with word-level information in neural machine translation models, which is more similar with our work.\n[BOS] Ling et al. (2015a) employed a bidirectional LSTM to compose character embeddings to form the word-level information with the help of word boundary information.\n[BOS] Costa-juss and Fonollosa (2016) replaced the word-lookup table with a convolutional network followed by a highway network (Srivastava et al., 2015) , which learned the word-level representation by its constituent characters.\n[BOS] Zhao and Zhang (2016) designed a decimator for their encoder, which effectively uses a RNN to compute a word representation from the characters of the word.\n[BOS] These approaches only consider word boundary information and ignore the word-level meaning information itself.\n[BOS] In contrast, our model can make use of both character-level and wordlevel information.\n[BOS] Luong and Manning (2016) proposed a hybrid scheme that consults character-level information whenever the model encounters an OOV word.\n[BOS] Wu et al. (2016) converted the OOV words in the word-based model into the sequence of its constituent characters.\n[BOS] These methods only focus on dealing with OOV words by augmenting the character-level information.\n[BOS] In our work, we augment the character information to all the words.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Transition", "Reflection", "Single_summ", "Single_summ", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 34, "token_end": 66, "char_start": 191, "char_end": 351, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ling et al. (2015a)": "5799549"}, "Reference": {}}}, {"token_start": 67, "token_end": 117, "char_start": 358, "char_end": 582, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Costa-juss\u00e0 and Fonollosa (2016)": "1712853"}, "Reference": {"(Srivastava et al., 2015)": "2722012"}}}, {"token_start": 118, "token_end": 151, "char_start": 589, "char_end": 745, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhao and Zhang (2016)": "6461746"}, "Reference": {}}}, {"token_start": 189, "token_end": 217, "char_start": 962, "char_end": 1096, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Luong and Manning (2016)": "13972671"}, "Reference": {}}}, {"token_start": 218, "token_end": 244, "char_start": 1103, "char_end": 1216, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wu et al. (2016)": "3603249"}, "Reference": {}}}]}
{"id": "44176293_1", "paragraph": "[BOS] The first line of research attempted to build neural machine translation models purely on characters without explicit segmentation.\n[BOS] Lee et al. (2017) proposed to directly learn the segmentation from characters by using convolution and pooling layers.\n[BOS] Yang et al. (2016) composed the high-level representation by the character embedding and its surrounding character-level context with a bidirectional and concatenated row convolution network.\n[BOS] Different from their models, our model aims to use characters to enhance words representation instead of depending on characters solely; our model is also much simpler.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 22, "token_end": 44, "char_start": 144, "char_end": 262, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Lee et al. (2017)": "10509498"}, "Reference": {}}}, {"token_start": 45, "token_end": 78, "char_start": 269, "char_end": 460, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Yang et al. (2016)": "16307400"}, "Reference": {}}}]}
{"id": "44176293_0", "paragraph": "[BOS] Many recent studies have focused on using character-level information in neural machine translation systems.\n[BOS] These efforts could be roughly divided into the following two categories.\n\n", "discourse_tags": ["Transition", "Transition"], "span_citation_mapping": []}
{"id": "44160625_3", "paragraph": "[BOS] However, all these works rely on identifying sentence level features to compute topic affinities that are leveraged for choosing topic specific sentences for the summary.\n[BOS] Since sequence-tosequence frameworks generate text in a word-byword fashion, incorporating sentence level statistics is not feasible in this framework.\n[BOS] Therefore, we modify the attention of the network to focus on topic-specific parts of the input text to generate the tuned summaries.\n\n", "discourse_tags": ["Transition", "Transition", "Reflection"], "span_citation_mapping": []}
{"id": "44160625_2", "paragraph": "[BOS] There have been some works on extending extractive summarization towards topical tuning.\n[BOS] Lin and Hovy (2000) proposed the idea of extracting topic-based signature terms for summarization.\n[BOS] Given a topic and a corpus of documents relevant and not relevant to the topic, a set of words characterizing each topic is extracted using a log-likelihood based measure.\n[BOS] Sentences which contain these chosen words are assigned more importance while summarizing.\n[BOS] Conroy et al. (2006) further extended the method for querybased multi-document summarization by considering sentence overlap with both query terms and topic signature words.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 18, "token_end": 87, "char_start": 101, "char_end": 474, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Lin and Hovy (2000)": "8598694"}, "Reference": {}}}, {"token_start": 88, "token_end": 121, "char_start": 481, "char_end": 654, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Conroy et al. (2006)": "7096616"}, "Reference": {}}}]}
{"id": "44160625_1", "paragraph": "[BOS] With the advent of deep sequence to sequence models which generated text word-byword (Sutskever et al., 2014) , attention based neural network models have been proposed for summarizing long sentences.\n[BOS] Rush et al. (2015) first demonstrated the use of neural networks to generate shorter forms of long sentences.\n[BOS] proposed a neural approach for abstractive summarization of large articles by applying the sequence to sequence model.\n[BOS] See et al. (2017) further improved the performance on abstractive summarization of articles by introducing the ability to copy words from the source article (Gulcehre et al., 2016 ) using a pointer network (Vinyals et al., 2015) , in addition to generating new words.\n[BOS] However, all these frameworks focus on generating a single summary, and do not support topic-tuned summary generation.\n[BOS] We use the architecture by See et al. as the starting point for our work and develop a method to generate topictuned summaries.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Other", "Single_summ", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 13, "token_end": 28, "char_start": 74, "char_end": 115, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sutskever et al., 2014)": "7961699"}}}, {"token_start": 43, "token_end": 66, "char_start": 213, "char_end": 322, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Rush et al. (2015)": "1918428"}, "Reference": {}}}, {"token_start": 88, "token_end": 150, "char_start": 454, "char_end": 721, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"See et al. (2017)": null}, "Reference": {"(Gulcehre et al., 2016": "969555", "(Vinyals et al., 2015)": "5692837"}}}]}
{"id": "44160625_0", "paragraph": "[BOS] Traditional methods for summarization (Nenkova and McKeown, 2011) extract key sentences from the source text to construct the summary.\n[BOS] Early works on abstractive summarization were focused on sentence compression based approaches (Filippova, 2010; Berg-Kirkpatrick et al., 2011; Banerjee et al., 2015) which connected fragments from multiple sentences to generate novel sentences for the summary or template based approaches that generated summaries by fitting content into a template (Wang and Cardie, 2013; Genest and Lapalme, 2011) .\n\n", "discourse_tags": ["Single_summ", "Multi_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 30, "char_start": 6, "char_end": 140, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Nenkova and McKeown, 2011)": null}, "Reference": {}}}, {"token_start": 34, "token_end": 116, "char_start": 162, "char_end": 546, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Filippova, 2010;": "14750088", "Berg-Kirkpatrick et al., 2011;": "15467396", "Banerjee et al., 2015)": "15795297"}, "Reference": {"(Wang and Cardie, 2013;": "1030812", "Genest and Lapalme, 2011)": "4942873"}}}]}
{"id": "462553_1", "paragraph": "[BOS] Generating phrases for NMT In these studies, the generated NMT multi-word phrases are either from an SMT model or a bilingual dictionary.\n[BOS] In syntactically guided neural machine translation (SGNMT), the NMT decoder uses phrase translations produced by the hierarchical phrasebased SMT system Hiero, as hard decoding constraints.\n[BOS] In this way, syntactic phrases are generated by the NMT decoder (Stahlberg et al., 2016b) .\n[BOS] Zhang and Zong (2016) use an SMT translation system, which is integrated an additional bilingual dictionary, to synthesize pseudo-parallel sentences and feed the sentences into the training of NMT in order to translate low-frequency words or phrases.\n[BOS] Tang et al. (2016) propose an external phrase memory that stores phrase pairs in symbolic forms for NMT.\n[BOS] During decoding, the NMT decoder enquires the phrase memory and properly generates phrase translations.\n[BOS] The significant differences between these efforts and ours are 1) that we dynamically generate phrase translations via an SMT model, and 2) that at the same time we modify the encoder to incorporate structural information to enhance the capability of NMT in phrase translation.\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 73, "token_end": 93, "char_start": 359, "char_end": 435, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Stahlberg et al., 2016b)": "11642690"}}}, {"token_start": 95, "token_end": 144, "char_start": 444, "char_end": 694, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhang and Zong (2016)": "8551820"}, "Reference": {}}}, {"token_start": 145, "token_end": 188, "char_start": 701, "char_end": 915, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Tang et al. (2016)": "18192067"}, "Reference": {}}}]}
{"id": "462553_0", "paragraph": "[BOS] Our work is related to the following research topics on NMT:\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "38898903_3", "paragraph": "[BOS] Our practical goal is thus a development of a robust argument critique analysis system for essays.\n[BOS] Our theoretical goal is the investigation of the extent that it is at all possible to capture aspects of argument content in a fashion that would generalize across various essay topics.\n\n", "discourse_tags": ["Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "38898903_2", "paragraph": "[BOS] In practical large-scale automated scoring contexts, new essay prompts are often introduced without rebuilding the scoring system, which is typically subject to a periodic release schedule.\n[BOS] Therefore, the assumption that the system will have seen essays responding to each of the prompts it could encounter at deployment time is often unwarranted.\n[BOS] Further, not only should a system be able to handle responses to an unseen prompt, it must do it gracefully, since a large disparity in the system's performance across different prompts might raise fairness concerns.\n\n", "discourse_tags": ["Transition", "Transition", "Transition"], "span_citation_mapping": []}
{"id": "38898903_1", "paragraph": "[BOS] Automated analysis of argumentative writing has mostly concentrated on argument structurenamely, presence of claims and premises, and relationships between them (Ghosh et al., 2016; Nguyen and Litman, 2016; Persing and Ng, 2016; Ong et al., 2014; Stab and Gurevych, 2014) .\n[BOS] Addressing the content of arguments in on-line debates, Habernal and Gurevych (2016) ranked arguments on the same topic by convincingness; they showed that convincingness can be automatically predicted, to an extent, in a cross-topics fashion, as they trained their systems on 31 debates and tested on a new one.\n[BOS] Swanson et al. (2015) reported that annotation of argument quality is challenging, with inter-annotator agreement (ICC) around 0.40.\n[BOS] They also showed that automated acrosstopics prediction is very hard; for some topics, no effective prediction was achieved.\n[BOS] Song et al. (2014) developed an annotation protocol for analyzing argument critiques in students' essays, drawing on the theory of argumentation schemes (Walton et al., 2008; Walton, 1996) .\n[BOS] According to this theory, different types of arguments invite specific types of critiques.\n[BOS] For example, an argument from authority made in the promptAccording to X, Y is the case -avails critiques along the lines of whether X has the necessary knowledge and is an unbiased source of information about Y. Analyzing prompts used in an assessment of argument critique skills, Song et al. (2014) identified a number of common schemes, such as arguments from policy, sample, example, and used the argumentation schemes theory to specify what critiques would count as \"good\" for arguments from the given scheme.\n[BOS] Once a prompt is associated with a specific set of argumentation schemes, it follows that those critiques that count as good under one of the schemes used in the prompt would be considered as good critiques in essays responding to that prompt.\n[BOS] The goal of the annotation was to identify all sentences in an essay that participate in making a good critique, according to the above definition.\n[BOS] Every sentence in an essay is annotated with the label of the critique that it raises, or \"generic\" if none.\n[BOS] In the current paper, we build upon this earlier work.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 17, "token_end": 67, "char_start": 103, "char_end": 277, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ghosh et al., 2016;": "8557718", "Nguyen and Litman, 2016;": null, "Persing and Ng, 2016;": "1045043", "Ong et al., 2014;": "16985582", "Stab and Gurevych, 2014)": "71907"}}}, {"token_start": 69, "token_end": 136, "char_start": 286, "char_end": 598, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Habernal and Gurevych (2016)": "3083231"}, "Reference": {}}}, {"token_start": 137, "token_end": 192, "char_start": 605, "char_end": 868, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Swanson et al. (2015)": "5359682"}, "Reference": {}}}, {"token_start": 193, "token_end": 461, "char_start": 875, "char_end": 2202, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Song et al. (2014)": "18890727"}, "Reference": {"(Walton et al., 2008;": "36339677", "Walton, 1996)": "123477268"}}}]}
{"id": "38898903_0", "paragraph": "[BOS] Argumentation is an important skill in higher education and the workplace; students are expected to show sound reasoning and use relevant evidence (Council of Chief State School Officers & National Governors Association, 2010).\n[BOS] The increase in argumentative writing tasks, in both instructional and assessment contexts, results in a high demand for automated feedback on and scoring of arguments.\n\n", "discourse_tags": ["Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 2, "token_end": 41, "char_start": 6, "char_end": 233, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "2936869_3", "paragraph": "[BOS] The semi-supervised approach of initializing the weights of an RNN encoder with those of a recurrent autoencoder was first studied by Dai et al. (2015) in the context of document classification and further studied by Ramachandran et al. (2016) for traditional sequence-to-sequence tasks such as machine translation.\n[BOS] Our baseline semisupervised model can be viewed as an extension of these approaches to a reading comprehension setting.\n[BOS] Dai et al. (2015) also explore initialization from a language model, but find that the recurrent autoencoder is superior, which is why we do not consider language models in this work.\n\n", "discourse_tags": ["Narrative_cite", "Reflection", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 36, "char_start": 6, "char_end": 157, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Dai et al. (2015)": "7138078"}}}, {"token_start": 40, "token_end": 56, "char_start": 176, "char_end": 249, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Ramachandran et al. (2016)": "3488076"}}}, {"token_start": 91, "token_end": 130, "char_start": 454, "char_end": 637, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Dai et al. (2015)": "7138078"}, "Reference": {}}}]}
{"id": "2936869_2", "paragraph": "[BOS] The basic recurrent autoencoder was first introduced by Dai et al. (2015) , a standard seq2seq model with the same input and output.\n[BOS] Fabius et al. (2014) expanded this model into the Variational Recurrent Autoencoder (VRAE), which we describe in Section 4.1.1.\n[BOS] VRAE is an application of the general idea of variational autoencoding, which applies variational approximation to the posterior to reconstruct the input (Kingma and Welling, 2013 ).\n[BOS] While we train window autoencoders, an alternative approach is hierarchical document autoencoders (Li et al., 2015) .\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 2, "token_end": 33, "char_start": 6, "char_end": 138, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Dai et al. (2015)": "7138078"}, "Reference": {}}}, {"token_start": 34, "token_end": 56, "char_start": 145, "char_end": 235, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Fabius et al. (2014)": null}, "Reference": {}}}, {"token_start": 85, "token_end": 102, "char_start": 365, "char_end": 458, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kingma and Welling, 2013": null}}}, {"token_start": 117, "token_end": 130, "char_start": 531, "char_end": 583, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Li et al., 2015)": "207468"}}}]}
{"id": "2936869_1", "paragraph": "[BOS] Our reviewer approach is inspired by \"Encode, Review, Decode\" approach introduced by Yang et al. (2016) , which showed the value of introducing additional computation steps between the encoder and decoder in a seq2seq model.\n\n", "discourse_tags": ["Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 47, "char_start": 6, "char_end": 230, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Yang et al. (2016)": "9256367"}, "Reference": {}}}]}
{"id": "2936869_0", "paragraph": "[BOS] Our model architecture is one of many hierarchical models for documents proposed in the literature.\n[BOS] The most similar is proposed by Choi et al. (2017) , which uses a coarse-to-fine approach of first encoding each sentence with a cheap BoW or Conv model, then selecting the top k sentences to form a mini-document which is then processed by a standard seq2seq model.\n[BOS] While they also evaluate their approach on WikiReading, their emphasis is on efficiency rather than model accuracy, with the resulting model performing slightly worse than the full seq2seq model but taking much less time to execute.\n[BOS] SWEAR also requires fewer sequential steps than the document length but still computes at least as many recurrent steps in parallel.\n[BOS] Our model can also be viewed as containing a Memory Network (MemNet) built from a document (Weston et al., 2014; Sukhbaatar et al., 2015) , where the memories are the window encodings.\n[BOS] The core MemNet operation consists of attention over a set of vectors (memories) based on a query encoding, and then reduction of a second set of vectors by weighted sum based on the attention weights.\n[BOS] In particular, Miller et al. (2016) introduce the Key-Value MemNet where the two sets of memories are computed from the keys and values of a map, respectively: In their QA task, each memory entry consists of a potential answer (the value) and its context bag of words (the key).\n\n", "discourse_tags": ["Reflection", "Single_summ", "Single_summ", "Transition", "Narrative_cite", "Transition", "Single_summ"], "span_citation_mapping": [{"token_start": 19, "token_end": 123, "char_start": 112, "char_end": 616, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Choi et al. (2017)": null}, "Reference": {}}}, {"token_start": 157, "token_end": 186, "char_start": 807, "char_end": 899, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Sukhbaatar et al., 2015)": null}}}, {"token_start": 238, "token_end": 302, "char_start": 1161, "char_end": 1439, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "25111673_4", "paragraph": "[BOS] In addition to NMT (Gulcehre et al., 2016) , the copy mechanism has been shown effective on tasks such as dialogue (Gu et al., 2016) , summarization (See et al., 2017) and question generation (Song et al., 2018) .\n[BOS] We investigate the copy mechanism on AMR-to-text generation.\n\n", "discourse_tags": ["Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 5, "token_end": 18, "char_start": 21, "char_end": 48, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Gulcehre et al., 2016)": "969555"}}}, {"token_start": 30, "token_end": 39, "char_start": 112, "char_end": 138, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Gu et al., 2016)": "8174613"}}}, {"token_start": 40, "token_end": 50, "char_start": 141, "char_end": 173, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(See et al., 2017)": null}}}, {"token_start": 51, "token_end": 61, "char_start": 178, "char_end": 217, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Song et al., 2018)": "44178763"}}}]}
{"id": "25111673_3", "paragraph": "[BOS] The recurrent information exchange mechanism in our state transition process is remotely related to the idea of loopy belief propagation (LBP) (Murphy et al., 1999) .\n[BOS] However, there are two major differences.\n[BOS] First, messages between LSTM states are gated neural node values, rather than probabilities in LBP.\n[BOS] Second, while the goal of LBP is to estimate marginal probabilities, the goal of information exchange between graph states in our LSTM is to find neural representation features, which are directly optimized by a task objective.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 19, "token_end": 34, "char_start": 118, "char_end": 170, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Murphy et al., 1999)": "16462148"}}}]}
{"id": "25111673_2", "paragraph": "[BOS] Closest to our work, modeled syntactic and discourse structures using DAG LSTM, which can be viewed as extensions to tree LSTMs (Tai et al., 2015) .\n[BOS] The state update follows the sentence order for each node, and has sequential nature.\n[BOS] Our state update is in parallel.\n[BOS] In addition, split input graphs into separate DAGs before their method can be used.\n[BOS] To our knowledge, we are the first to apply an LSTM structure to encode AMR graphs.\n\n", "discourse_tags": ["Narrative_cite", "Reflection", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 22, "token_end": 36, "char_start": 109, "char_end": 152, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Tai et al., 2015)": "3033526"}}}]}
{"id": "25111673_1", "paragraph": "[BOS] Prior work using graph neural networks for NLP include the use graph convolutional networks (GCN) (Kipf and Welling, 2017) for semantic role labeling , neural machine translation (Bastings et al., 2017) and graph-to-sequence learning (Xu et al., 2018) .\n[BOS] Both GCN and the graph LSTM update node states by exchanging information between neighboring nodes within each iteration.\n[BOS] However, our graph state LSTM adopts gated operations for making updates, while GCN uses a linear transformation.\n[BOS] Intuitively, the former has better learning power than the later.\n[BOS] Another major difference is that our graph state LSTM keeps a cell vector for each node to remember all history.\n[BOS] The contrast between our model with GCN is reminiscent of the contrast between RNN and CNN.\n[BOS] We leave empirical comparison of their effectiveness to future work.\n[BOS] In this work our main goal is to show that graph LSTM encoding of AMR is superior compared with sequence LSTM.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Reflection", "Reflection", "Reflection", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 14, "token_end": 30, "char_start": 69, "char_end": 128, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 35, "token_end": 48, "char_start": 158, "char_end": 208, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bastings et al., 2017)": "6206777"}}}, {"token_start": 49, "token_end": 63, "char_start": 213, "char_end": 257, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Xu et al., 2018)": "4590511"}}}]}
{"id": "25111673_0", "paragraph": "[BOS] Among early statistical methods for AMR-to-text generation, Flanigan et al. (2016b) convert input graphs to trees by splitting re-entrances, and then translate the trees into sentences with a tree-tostring transducer.\n[BOS] Song et al. (2017) use a synchronous node replacement grammar to parse input AMRs and generate sentences at the same time.\n[BOS] Pourdamghani et al. (2016) linearize input (p / possible-01 :polarity -:ARG1 (l / look-over-06 :ARG0 (w / we) :ARG1 (a / account-01 :ARG1 (w2 / war-01 :ARG1 (c2 / country :wiki \"Japan\" :name (n2 / name :op1 \"Japan\")) :time (p2 / previous) :ARG1-of (c / call-01 :mod (s / so))) :mod (o / old)))) Lin: possible :polarity -:arg1 ( look-over :arg0 we :arg1 ( account :arg1 ( war :arg1 ( country :wiki japan :name ( name :op1 japan ) ) :time previous :arg1-of ( call :mod so ) ) :mod old ) ) Ref: we can n't look over the old accounts of the previous so-called anti-japanese war .\n[BOS] S2S: we can n't be able to account the past drawn out of japan 's entire war .\n[BOS] G2S: we can n't be able to do old accounts of the previous and so called japan war.\n[BOS] G2S+CP: we can n't look-over the old accounts of the previous so called war on japan .\n[BOS] graphs by breadth-first traversal, and then use a phrase-based machine translation system 3 to generate results by translating linearized sequences.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Other", "Other", "Other", "Other", "Transition"], "span_citation_mapping": [{"token_start": 2, "token_end": 52, "char_start": 6, "char_end": 223, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Flanigan et al. (2016b)": "9135033"}, "Reference": {}}}, {"token_start": 53, "token_end": 79, "char_start": 230, "char_end": 352, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Song et al. (2017)": "9573708"}, "Reference": {}}}]}
{"id": "35814904_5", "paragraph": "[BOS] Thus, we propose automatically extracting normalization candidates from unlabeled data and present a method for incorporating these candidates into Japanese morphological analysis and normalization.\n[BOS] Our method can extract new variant patterns from real text.\n\n", "discourse_tags": ["Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "35814904_4", "paragraph": "[BOS] Han et al. (2012) and Hassan and Menezes (2013) developed the method of extracting variant-normalization pairs automatically for English.\n[BOS] Yang and Eisenstein (2013) introduced a highly accurate unsupervised normalization model using log-linear model.\n[BOS] In these studies, clear word segmentations were assumed to exist.\n[BOS] However, since Japanese is unsegmented, the normalization problem needs to be treated as a joint normalization, word segmentation, and POS tagging problem.\n\n", "discourse_tags": ["Multi_summ", "Single_summ", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 2, "token_end": 32, "char_start": 6, "char_end": 143, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hassan and Menezes (2013)": "9600472"}, "Reference": {}}}, {"token_start": 33, "token_end": 53, "char_start": 150, "char_end": 262, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "35814904_3", "paragraph": "[BOS] Many studies have been done on text normalization for English; for example, Han and Baldwin (2011) classifies whether or not OOVs are non-standard tokens and estimates standard forms on the basis of contextual, string, and phonetic similarities.\n\n", "discourse_tags": ["Single_summ"], "span_citation_mapping": [{"token_start": 16, "token_end": 52, "char_start": 82, "char_end": 251, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Han and Baldwin (2011)": "2577850"}, "Reference": {}}}]}
{"id": "35814904_2", "paragraph": "[BOS] Several studies have applied a character-based approach.\n[BOS] For example, Sasaki et al. (2013) proposed a character-level sequential labeling method for normalization.\n[BOS] However, it handles only oneto-one character transformations and does not take the word-level context into account.\n[BOS] The proposed method can handle many-to-many character transformations and takes word-level context into account, so it has a wider scope for handling nonstandard tokens.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 16, "token_end": 95, "char_start": 82, "char_end": 473, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Sasaki et al. (2013)": null}, "Reference": {}}}]}
{"id": "35814904_1", "paragraph": "[BOS] (totemo, such)\" and \" (tanoshii, fun)\" and their POS tags.\n[BOS] If we can generate sufficiently appropriate rules, these approaches seem to be effective.\n[BOS] However, there are many types of derivational patterns in SNS text, and they are difficult to all cover manually.\n[BOS] Moreover, how to set the path score for appropriately Figure 2 : Overview of proposed system ranking the word lattice when the number of candidates increases becomes a serious problem.\n[BOS] Saito et al. (2014) proposed supervised extraction of derivational patterns (we call them transformation patterns), incorporated these patterns into a word lattice, and formulated morphological analysis and normalization using a discriminate model.\n[BOS] Although this approach can generate broad-coverage normalization candidates, it needs a large amount of annotation data of variant words and their normalization.\n[BOS] Kaji and Kitsuregawa (2014) also proposed morphological analysis and normalization based on a discriminative model and created variant words on the basis of handmade rules.\n[BOS] As far as we know, automatic extraction of variant-normalization pairs has not been researched.\n[BOS] If we can extract variant-normalization pairs automatically, we can decrease the annotation cost and possibly increase accuracy by combining our method with other conventional methods.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Transition", "Single_summ", "Single_summ", "Single_summ", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 98, "token_end": 167, "char_start": 478, "char_end": 894, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Saito et al. (2014)": "7545702"}, "Reference": {}}}, {"token_start": 168, "token_end": 201, "char_start": 901, "char_end": 1073, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kaji and Kitsuregawa (2014)": "429482"}, "Reference": {}}}]}
{"id": "35814904_0", "paragraph": "[BOS] Several studies have been conducted on Japanese morphological analysis and normalization.\n[BOS] The approach proposed by Sasano et al. (2013) developed heuristics to flexibly search by using a simple, manually created derivational rule.\n[BOS] Their system generates a normalized character sequence based on derivational rules and adds new nodes when generating the word lattice using dictionary lookup.\n[BOS] Figure 1 presents an example of this approach.\n[BOS] If the non-standard written sentence \" (totemo tanoshii, It is such fun)\" is input, the traditional dictionary-based system generates nodes that are described using solid lines, as shown in Figure 1 .\n[BOS] Since \" (totemo, such)\" and \" (tanoshii, fun)\" are Out Of Vocabulary (OOVs), the traditional system cannot generate the correct word segments or POS tags.\n[BOS] However, their system generates additional nodes for the OOVs, shown as broken line rectangles in Figure 1 .\n[BOS] In this case, derivational rules are used that substitute \" \" with \"null\" and \" (i)\" with \" (i)\", and the system can generate the standard forms \"\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Other", "Other", "Other", "Single_summ", "Other"], "span_citation_mapping": [{"token_start": 15, "token_end": 70, "char_start": 102, "char_end": 408, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Sasano et al. (2013)": "14926846"}, "Reference": {}}}]}
{"id": "44108216_2", "paragraph": "[BOS] There is also a study (Zhao et al., 2017) shares a similar name with this work, i.e. bag-of-word loss, our work has significant difference with this study.\n[BOS] First, the methods are very different.\n[BOS] The previous work uses the bag-of-word to constraint the latent variable, and the latent variable is the output of the encoder.\n[BOS] However, we use the bag-of-word to supervise the distribution of the generated words, which is the output of the decoder.\n[BOS] Compared with the previous work, our method directly supervises the predicted distribution to improve the whole model, including the encoder, the decoder and the output layer.\n[BOS] On the contrary, the previous work only supervises the output of the encoder, and only the encoder is trained.\n[BOS] Second, the motivations are quite different.\n[BOS] The bag-of-word loss in the previous work is an assistant component, while the bag of word in this paper is a direct target.\n[BOS] For example, in the paper you mentioned, the bag-of-word loss is a component of variational autoencoder to tackle the vanishing latent variable problem.\n[BOS] In our paper, the bag of word is the representation of the unseen correct translations to tackle the data sparseness problem.\n\n", "discourse_tags": ["Single_summ", "Transition", "Transition", "Reflection", "Reflection", "Reflection", "Transition", "Transition", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 43, "char_start": 6, "char_end": 161, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Zhao et al., 2017)": "14688760"}, "Reference": {}}}]}
{"id": "44108216_1", "paragraph": "[BOS] There are also some effective neural networks other RNN.\n[BOS] Gehring et al. (2017) turned the RNN-based model into CNN-based model, which greatly improves the computation speed.\n[BOS] Vaswani et al. (2017) only used attention mechanism to build the model and showed outstanding performance.\n[BOS] Also, some researches incorporated external knowledge and also achieved obvious improvement (Li et al., 2017; Chen et al., 2017) .\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 14, "token_end": 43, "char_start": 69, "char_end": 185, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Gehring et al. (2017)": "3648736"}, "Reference": {}}}, {"token_start": 44, "token_end": 66, "char_start": 192, "char_end": 298, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 69, "token_end": 94, "char_start": 311, "char_end": 433, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Li et al., 2017;": "12637374", "Chen et al., 2017)": "3504277"}}}]}
{"id": "44108216_0", "paragraph": "[BOS] The studies of encoder-decoder framework (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014) for this task launched the Neural Machine Translation.\n[BOS] To improve the focus on the information in the encoder, proposed the attention mechanism, which greatly improved the performance of the Seq2Seq model on NMT.\n[BOS] Most of the existing NMT systems are based on the Seq2Seq model and the attention mechanism.\n[BOS] Some of them have variant architectures to capture more information from the inputs (Su et al., 2016; Tu et al., 2016) , and some improve the attention mechanism Meng et al., 2016; Mi et al., 2016; Jean et al., 2015; Feng et al., 2016; Calixto et al., 2017) , which also enhanced the performance of the NMT model.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 5, "token_end": 31, "char_start": 21, "char_end": 103, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kalchbrenner and Blunsom, 2013;": "12639289", "Sutskever et al., 2014)": "7961699"}}}, {"token_start": 103, "token_end": 122, "char_start": 484, "char_end": 546, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Su et al., 2016;": "8299428", "Tu et al., 2016)": "146843"}}}, {"token_start": 127, "token_end": 166, "char_start": 570, "char_end": 685, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Meng et al., 2016;": "416723", "Mi et al., 2016;": "18193214", "Jean et al., 2015;": "2863491", "Feng et al., 2016;": "14410825", "Calixto et al., 2017)": "6881637"}}}]}
{"id": "4559639_0", "paragraph": "[BOS] The first works on NLG were mostly focused on rule-based language generation (Dale et al., 1998; Reiter et al., 2005; Green, 2006) .\n[BOS] NLG systems typically perform three different steps: content selection, where a subset of relevant slot/value pairs are selected, followed by sentence planning, where these selected pairs are realized into their respective linguistic variations, and finally surface realization, where these linguistic structures are combined to generate text.\n[BOS] Our use case differs from the above in that there is no selection done on the slot/value pairs, but all of them undergo the sentence planning step.\n[BOS] In rule-based systems, all of the above steps rely on hand-crafted rules.\n[BOS] The recent work focuses on generating texts from structured data as input by performing selective generation, i.e. they run a selection step that determines the slot/value pairs which will be included in the realization (Mei et al., 2015; Lebret et al., 2016; Duma and Klein, 2013; Chisholm et al., 2017) .\n[BOS] In our use case, all slot/value pairs are relevant and need to be realized.\n[BOS] Serban et al. (2016) generate questions from facts (structured input) by leveraging fact embeddings and then employing placeholders for handling rare words.\n[BOS] In their work, the placeholders are heuristically mapped to the facts, however, we map our placeholders depending on the neural attention (for details, see Section 4).\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Reflection", "Transition", "Narrative_cite", "Reflection", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 12, "token_end": 38, "char_start": 52, "char_end": 136, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dale et al., 1998;": null, "Reiter et al., 2005;": "13461687", "Green, 2006)": "14175735"}}}, {"token_start": 171, "token_end": 224, "char_start": 844, "char_end": 1033, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mei et al., 2015;": "1354459", "Lebret et al., 2016;": "15443255", "Duma and Klein, 2013;": "772286", "Chisholm et al., 2017)": "14633379"}}}, {"token_start": 245, "token_end": 276, "char_start": 1124, "char_end": 1280, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Serban et al. (2016)": "12241221"}, "Reference": {}}}]}
{"id": "46938675_2", "paragraph": "[BOS] Different from the attention based aggregation methods, aggregation via dynamic routing is iteratively deciding that what and how much information need be transfer to the final encoding of each word.\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "46938675_1", "paragraph": "[BOS] The attention based aggregation collects information in a bottom-up way, without considering the state of the final encoding.\n[BOS] It is hard to avoid the problems of information redundancy or information lost.\n[BOS] An improved idea is to use multi-hop attention, like memory network (Sukhbaatar et al., 2015; Kumar et al., 2015) , to iterative aggregate information.\n[BOS] This idea is equivalent to our proposed reversed dynamic routing mechanism.\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 52, "token_end": 72, "char_start": 277, "char_end": 337, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sukhbaatar et al., 2015;": null, "Kumar et al., 2015)": "2319779"}}}]}
{"id": "46938675_0", "paragraph": "[BOS] Currently, much attention has been paid to how developing a sophisticated encoding models to capture the long and short term dependency information in a sequence.\n[BOS] Specific to text classification task, most of the models cannot deal with the texts of several sentences (paragraphs, documents), such as MV-RNN (Socher et al., 2012) , RNTN (Socher et al., 2013) , CNN (Kim, 2014) , AdaSent (Zhao et al., 2015) , and so on.\n[BOS] The simple neural bag-of-words model can deal with long texts, but it loses the word order information.\n[BOS] PV (Le and Mikolov, 2014) works in an unsupervised way, and the learned vector cannot be fine-tuned on the specific task.\n[BOS] There are also many works Xu et al., 2016; Cheng et al., 2016) to improve LSTM's ability to carrying information for a long distance.\n[BOS] A line of orthogonal researches (Lin et al., 2017; Yang et al., 2016; Shen et al., 2018a; Shen et al., 2018b) is to introduce attention mechanism (Vaswani et al., 2017) to weighted average the outputs of CNN/RNN layer.\n[BOS] The attention mechanism can effectively reduce the burden of CNN/RNN.\n[BOS] The CNN/RNN encoding layer is only expected to extract local context information for each word, while the global semantics of text sequence can be aggregated from the local encoding vectors.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Single_summ", "Narrative_cite", "Narrative_cite", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 56, "token_end": 69, "char_start": 313, "char_end": 341, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Socher et al., 2012)": "806709"}}}, {"token_start": 70, "token_end": 81, "char_start": 344, "char_end": 370, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Socher et al., 2013)": "990233"}}}, {"token_start": 82, "token_end": 88, "char_start": 373, "char_end": 388, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kim, 2014)": null}}}, {"token_start": 89, "token_end": 100, "char_start": 391, "char_end": 418, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhao et al., 2015)": "14182215"}}}, {"token_start": 130, "token_end": 160, "char_start": 548, "char_end": 669, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Le and Mikolov, 2014)": "2407601"}, "Reference": {}}}, {"token_start": 161, "token_end": 180, "char_start": 676, "char_end": 738, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Xu et al., 2016;": "15124020", "Cheng et al., 2016)": "6506243"}}}, {"token_start": 199, "token_end": 232, "char_start": 826, "char_end": 925, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lin et al., 2017;": "15280949", "Yang et al., 2016;": "6857205", "Shen et al., 2018a;": "19152001", "Shen et al., 2018b)": "4564356"}}}, {"token_start": 235, "token_end": 247, "char_start": 942, "char_end": 984, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vaswani et al., 2017)": "13756489"}}}]}
{"id": "35796099_2", "paragraph": "[BOS] Recursive neural networks (Socher et al., 2011 (Socher et al., , 2013 have proved useful for tasks involving longdistance relations, such as semantic relation extraction (Hashimoto et al., 2013; .\n[BOS] Convolutional networks have also been used (dos Santos et al., 2015; Zeng et al., 2014) and more recently, recurrent networks such as LSTM showed to be more robust for learning long-distance semantic information (Miwa and Bansal, 2016; Xu et al., 2015; Zhou et al., 2016) .\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 2, "token_end": 22, "char_start": 6, "char_end": 75, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Socher et al., 2011": "3116311", "(Socher et al., , 2013": "990233"}}}, {"token_start": 34, "token_end": 45, "char_start": 147, "char_end": 199, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 48, "token_end": 70, "char_start": 209, "char_end": 296, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(dos Santos et al., 2015;": "15620570", "Zeng et al., 2014)": "12873739"}}}, {"token_start": 87, "token_end": 115, "char_start": 386, "char_end": 480, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Miwa and Bansal, 2016;": "2476229", "Xu et al., 2015;": "5403702", "Zhou et al., 2016)": "9870160"}}}]}
{"id": "35796099_1", "paragraph": "[BOS] From a more general perspective, relation extraction and classification is a task explored by many approaches, from fully unsupervised to fully supervised.\n[BOS] Recent years have seen an increasing interest for the use of neural approaches.\n\n", "discourse_tags": ["Transition", "Transition"], "span_citation_mapping": []}
{"id": "35796099_0", "paragraph": "[BOS] SemEval has been offering a shared task related to temporal relation extraction from clinical narratives over the past two years (Bethard et al., 2015 .\n[BOS] Relying on the THYME corpus, the task challenged participants to extract EVENT and TIMEX3 entities and then to extract narrative container relations and document creation time relations.\n[BOS] Herein, we focus on the second part of the challenge, temporal relation extraction and more specifically the narrative container relations.\n[BOS] Different approaches have been implemented by the participants, including Support Vector Machine (SVM) classifiers (AAl Abdulsalam et al., 2016; Cohan et al., 2016; Lee et al., 2016; Tourille et al., 2016) , Conditional Random Fields (CRF) and convolutional neural networks (CNNs) (Chikka, 2016) .\n[BOS] Beyond the challenges, Leeuwenberg and Moens (2017) propose a model based on a structured perceptron to jointly predict both types of temporal relations.\n[BOS] Lin et al. (2016) performs training instance augmentation to increase the number of training examples and implement a SVM based model for containment relation extraction.\n[BOS] Dligach et al. (2017) implement models based on CNNs and Long Short-Term Memory Networks (LSTMs) (Hochreiter and Schmidhuber, 1997) to extract containment relations from the THYME corpus.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Reflection", "Narrative_cite", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 32, "char_start": 6, "char_end": 156, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bethard et al., 2015": "209538"}}}, {"token_start": 101, "token_end": 144, "char_start": 578, "char_end": 709, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(AAl Abdulsalam et al., 2016;": "11794147", "Cohan et al., 2016;": "196391", "Lee et al., 2016;": "3810317", "Tourille et al., 2016)": "17274860"}}}, {"token_start": 145, "token_end": 166, "char_start": 712, "char_end": 799, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chikka, 2016)": "17416995"}}}, {"token_start": 168, "token_end": 199, "char_start": 808, "char_end": 961, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Leeuwenberg and Moens (2017)": "17894632"}, "Reference": {}}}, {"token_start": 200, "token_end": 229, "char_start": 968, "char_end": 1138, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Lin et al. (2016)": "1793754"}, "Reference": {}}}, {"token_start": 230, "token_end": 279, "char_start": 1145, "char_end": 1332, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Dligach et al. (2017)": "1338096"}, "Reference": {"(Hochreiter and Schmidhuber, 1997)": "1915014"}}}]}
{"id": "339492_0", "paragraph": "[BOS] To the best of our knowledge, the only work to have addressed the stance classification in tweets is that of Rajadesingan and Liu (2014) , who specifically tackle the data sparseness problem using a semi-supervised approach based on label propagation.\n[BOS] Wiebe (2009) and (2010) address stance detection in two-sided debates using supervised models with opinion-target pairs, as well as sentiment and argumentation trigger words as features.\n[BOS] Anand et al. (2011) address the same domain, but also consider the dialogical properties of debates by identifying the rebuttals between posts, while Sridhar et al. (2015) consider the joint stance classification of posts and relations among them.\n[BOS] Hasan and Ng (2014) combine stance classification with reason classification in a joint learning framework.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Multi_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 52, "char_start": 6, "char_end": 257, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Rajadesingan and Liu (2014)": "10607738"}, "Reference": {}}}, {"token_start": 53, "token_end": 91, "char_start": 264, "char_end": 450, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wiebe (2009) and": "2845337", "(2010)": "927208"}, "Reference": {}}}, {"token_start": 92, "token_end": 123, "char_start": 457, "char_end": 599, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Anand et al. (2011)": "2300698"}, "Reference": {}}}, {"token_start": 125, "token_end": 146, "char_start": 607, "char_end": 704, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Sridhar et al. (2015)": "7562971"}, "Reference": {}}}, {"token_start": 147, "token_end": 166, "char_start": 711, "char_end": 818, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "35618061_1", "paragraph": "[BOS] Neural networks based multi-task learning has recently proven effective in many NLP problems (Liu et al., 2015 (Liu et al., , 2016 Firat et al., 2016; .\n[BOS] Aiming at single document summarization with relatively small amounts of reference summaries, we demonstrated document summarization in the framework of multi-task learning with curriculum learning for sentence extraction and document classification.\n[BOS] This enabled us to obtain better feature representations to extract sentences from documents.\n\n", "discourse_tags": ["Narrative_cite", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 15, "token_end": 40, "char_start": 86, "char_end": 155, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Liu et al., 2015": "11754890", "(Liu et al., , 2016": "16017905"}}}]}
{"id": "35618061_0", "paragraph": "[BOS] Based on the recent advances of neural networkbased approaches (Kgebck et al., 2014; Cao et al., 2015; Yin and Pei, 2015; Cao et al., 2016) , an attentional encoder-decoder for extractive single-document summarization and its application to the news corpus was proposed (Cheng and Lapata, 2016; Nallapati et al., 2017) .\n[BOS] Although we employ an encoder-decoder architecture in the predictor component of our summarization framework, the framework can be applied to all models of sentence extraction using distributed representation as inputs, including recently advanced other attention-based encoder-decoder networks (Wang et al., 2016; Yang et al., 2016 ) (Cheng and Lapata, 2016; Nallapati et al., 2017) argue that a stumbling block to applying neural network models to extractive summarization is the lack of training data and documents with sentences labeled as summary-worthy.\n[BOS] To overcome this, several studies have used artificial reference summaries (Sun et al., 2005; Svore et al., 2007; Woodsend and Lapata, 2010; Cheng and Lapata, 2016) compiled by collecting documents and corresponding highlights from other sources.\n[BOS] However, preparing such a parallel corpus often requires domain-specific or expert knowledge depending on the domain (Filippova et al., 2009; Parveen et al., 2016) .\n[BOS] Our summarization uses document-associated information as pseudo rough reference summaries, which enables us to learn feature representations for both document classification and sentence identification with smaller amounts of actual reference summaries.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Narrative_cite", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 8, "token_end": 42, "char_start": 38, "char_end": 145, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(K\u00e5geb\u00e4ck et al., 2014;": "17394382", "Cao et al., 2015;": "10675728", "Yin and Pei, 2015;": "6026194"}}}, {"token_start": 61, "token_end": 82, "char_start": 251, "char_end": 324, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cheng and Lapata, 2016;": "1499080"}}}, {"token_start": 123, "token_end": 162, "char_start": 587, "char_end": 716, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wang et al., 2016;": null, "Yang et al., 2016": "6857205", "(Cheng and Lapata, 2016;": "1499080"}}}, {"token_start": 204, "token_end": 238, "char_start": 943, "char_end": 1063, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sun et al., 2005;": "5754331", "Svore et al., 2007;": "7290594", "Woodsend and Lapata, 2010;": "8015669"}}}, {"token_start": 259, "token_end": 287, "char_start": 1209, "char_end": 1315, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Filippova et al., 2009;": null}}}]}
{"id": "26754970_4", "paragraph": "[BOS] In contrast to those, we apply this concept to answer selection, we directly compare vector representations of questions and candidate answers, and we use a separate BiLSTM for importance weighting.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "26754970_3", "paragraph": "[BOS] In this work, we use a different method for importance weighting that determines the importance of segments in the texts while assuming the independence of questions and candidate answers.\n[BOS] This is related to previous work in other areas of NLP that incorporate self-attention mechanisms.\n[BOS] Within natural language inference, Liu et al. (2016) derive the importance of each segment in a short text based on the comparison to a average-pooled representation of the text itself.\n[BOS] Parikh et al. (2016) determine intra-attention with a feedforward component and combine the importance of nearby segments.\n[BOS] And Lin et al. (2017) propose a model that derives multiple attention vectors with matrix multiplications.\n[BOS] Within factoid QA, Li et al. (2016) weight the importance of each token in a question with a feedforward network and perform sequence labeling.\n\n", "discourse_tags": ["Reflection", "Reflection", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 54, "token_end": 91, "char_start": 306, "char_end": 491, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Liu et al. (2016)": "12305768"}, "Reference": {}}}, {"token_start": 92, "token_end": 116, "char_start": 498, "char_end": 620, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Parikh et al. (2016)": "8495258"}, "Reference": {}}}, {"token_start": 118, "token_end": 138, "char_start": 631, "char_end": 733, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 139, "token_end": 169, "char_start": 740, "char_end": 883, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Li et al. (2016)": "6901603"}, "Reference": {}}}]}
{"id": "26754970_2", "paragraph": "[BOS] Answer selection can also be formulated as a ranking task where we learn dense vector representations of questions and candidate answers and measure the distance between them for scoring.\n[BOS] Feng et al. (2015) use such an approach and compare different models based on CNN with different similarity measures.\n[BOS] Based on that, models with attention mechanisms were proposed.\n[BOS] Tan et al. (2016) apply an attentive BiLSTM component that performs importance weighting before pooling based on the relatedness of segments in the candidate answer to the question.\n[BOS] Dos introduce a two-way attention mechanism based on a learned measure of similarity between questions and candidate answers.\n[BOS] And Wang et al. (2016) propose novel ways to integrate attention inside and before a GRU.\n\n", "discourse_tags": ["Transition", "Single_summ", "Transition", "Single_summ", "Transition", "Single_summ"], "span_citation_mapping": [{"token_start": 33, "token_end": 56, "char_start": 200, "char_end": 317, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Feng et al. (2015)": "3477924"}, "Reference": {}}}, {"token_start": 69, "token_end": 103, "char_start": 393, "char_end": 574, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Tan et al. (2016)": "12320170"}, "Reference": {}}}, {"token_start": 127, "token_end": 147, "char_start": 717, "char_end": 802, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wang et al. (2016)": "17464854"}, "Reference": {}}}]}
{"id": "26754970_1", "paragraph": "[BOS] More recently, researchers started using deep neural networks for answer selection.\n[BOS] Yu et al. (2014) , for example, propose a convolutional bigram model to classify a candidate answer as correct or incorrect.\n[BOS] Similar but more enhanced, Severyn and Moschitti (2015) use a CNN with additional dense layers to capture interactions between questions and candidate answers, a model that is also part of a combined approach with tree kernels (Tymoshenko et al., 2016) .\n[BOS] And Wang and Nyberg (2015) incorporate stacked BiLSTMs to learn a joint feature vector of a question and a candidate answer for classification.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 16, "token_end": 43, "char_start": 96, "char_end": 220, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Yu et al. (2014)": "12211448"}, "Reference": {}}}, {"token_start": 49, "token_end": 99, "char_start": 254, "char_end": 479, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Severyn and Moschitti (2015)": "3356807"}, "Reference": {"(Tymoshenko et al., 2016)": "6751470"}}}, {"token_start": 102, "token_end": 130, "char_start": 492, "char_end": 631, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "26754970_0", "paragraph": "[BOS] Earlier work in answer selection relies on handcrafted features based on semantic role annotations (Shen and Lapata, 2007; Surdeanu et al., 2011) , parse trees (Wang and Manning, 2010; Heilman and Smith, 2010) , tree kernels (Moschitti et al., 2007; Severyn and Moschitti, 2012) , discourse structures (Jansen et al., 2014) , and external resources (Yih et al., 2013) .\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 15, "token_end": 36, "char_start": 79, "char_end": 151, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Shen and Lapata, 2007;": "402181", "Surdeanu et al., 2011)": "1699647"}}}, {"token_start": 37, "token_end": 54, "char_start": 154, "char_end": 215, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wang and Manning, 2010;": "16725676", "Heilman and Smith, 2010)": "279533"}}}, {"token_start": 55, "token_end": 78, "char_start": 218, "char_end": 284, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Moschitti et al., 2007;": "14848907", "Severyn and Moschitti, 2012)": "14492994"}}}, {"token_start": 79, "token_end": 90, "char_start": 287, "char_end": 329, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Jansen et al., 2014)": "6049812"}}}, {"token_start": 92, "token_end": 103, "char_start": 336, "char_end": 373, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yih et al., 2013)": "10402642"}}}]}
{"id": "46938116_4", "paragraph": "[BOS] Number of vowels, number of syllables and number of characters (word length) together with word frequencies in corpora were investigated in (Yimam et al., 2017b) , but no experiments with character n-grams were conducted.\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 2, "token_end": 37, "char_start": 6, "char_end": 167, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yimam et al., 2017b)": null}}}]}
{"id": "46938116_3", "paragraph": "[BOS] Another system (Zampieri et al., 2016 ) used probabilities of word character trigrams and sentence character trigrams together with word length and sentence length to measure orthographic difficulty.\n[BOS] These features together with the word frequency features are used for three classifiers: Random Forest, Nearest neighbour and SVM.\n[BOS] Nevertheless, no results regarding the contribution of character trigram features were reported.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 81, "char_start": 6, "char_end": 445, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Zampieri et al., 2016": "4666955"}, "Reference": {}}}]}
{"id": "46938116_2", "paragraph": "[BOS] One of the submitted systems (Mukherjee et al., 2016) used Naive Bayes classifier with morphological, semantic and lexical features, however no character n-grams were investigated.\n\n", "discourse_tags": ["Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 38, "char_start": 6, "char_end": 186, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "46938116_1", "paragraph": "[BOS] The first CWI shared task (Paetzold and Specia, 2016 ) featured 42 systems based on different techniques and using different features such as semantic, morphological, lexical, as well as word frequencies which are reported to be a very important factor for CWI.\n\n", "discourse_tags": ["Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 56, "char_start": 6, "char_end": 267, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Paetzold and Specia, 2016": "14776729"}, "Reference": {}}}]}
{"id": "46938116_0", "paragraph": "[BOS] Several different techniques for identifying complex words were investigated by (Shardlow, 2013) which include word frequency, word length and syllable counts among others, but no character sequences.\n\n", "discourse_tags": ["Single_summ"], "span_citation_mapping": [{"token_start": 6, "token_end": 19, "char_start": 39, "char_end": 102, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Shardlow, 2013)": "17679719"}, "Reference": {}}}]}
{"id": "2760413_3", "paragraph": "[BOS] ILP has been used extensively for text-to-text generation problems in recent years (Clarke and Lapata, 2008; Filippova and Strube, 2008b; Woodsend et al., 2010) , including techniques which incorporate syntax directly into the decoding to imporove the fluency of the resulting text.\n[BOS] In this paper, we focus on generating valid intersections and do not incorporate syntactic and semantic constraints into our ILP models; these are areas we intend to explore in the future.\n\n", "discourse_tags": ["Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 9, "token_end": 45, "char_start": 40, "char_end": 166, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Clarke and Lapata, 2008;": "3004447", "Filippova and Strube, 2008b;": "14909308", "Woodsend et al., 2010)": "2292300"}}}]}
{"id": "2760413_2", "paragraph": "[BOS] Additionally, in this work, we frequently consider the sentence intersection task from the perspective of textual entailment (cf.\n[BOS] 5.1).\n[BOS] The textual entailment task involves automatically determining whether a given hypothesis can be inferred from a textual premise (Dagan et al., 2005; Bar-Haim et al., 2006) .\n[BOS] Automatic construction of positive and negative entailment examples has been explored in the past (Bensley and Hickl, 2008) to provide training data for entailment systems; however the production of text that is simultaneously entailed by two (or more) sentences is a far more constrained and difficult challenge.\n\n", "discourse_tags": ["Reflection", "Reflection", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 49, "token_end": 70, "char_start": 267, "char_end": 326, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dagan et al., 2005;": null, "Bar-Haim et al., 2006)": "13385138"}}}, {"token_start": 72, "token_end": 98, "char_start": 335, "char_end": 458, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bensley and Hickl, 2008)": "17396787"}}}]}
{"id": "2760413_1", "paragraph": "[BOS] This task is also related to the field of sentence compression which has received much attention in recent years (Turner and Charniak, 2005; McDonald, 2006; Clarke and Lapata, 2008; Filippova and Strube, 2008a; Cohn and Lapata, 2009; Marsi et al., 2010) .\n[BOS] Intersections can be viewed as guided com-pressions in which the redundancy of information content across input sentences in a multidocument setting is assumed to directly indicate its salience, thereby consigning it to the output.\n\n", "discourse_tags": ["Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 9, "token_end": 67, "char_start": 39, "char_end": 259, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Turner and Charniak, 2005;": "15519576", "McDonald, 2006;": "3014198", "Clarke and Lapata, 2008;": "3004447", "Filippova and Strube, 2008a;": "17477341", "Cohn and Lapata, 2009;": "6429026", "Marsi et al., 2010)": "8177501"}}}]}
{"id": "2760413_0", "paragraph": "[BOS] The distinction between intersection and union of text was introduced in the context of sentence fusion (Krahmer et al., 2008; Marsi and Krahmer, 2005) in order to distinguish between traditional fusion strategies that attempted to include only common content and fusions that attempted to include all non-redundant content from the input.\n[BOS] We focus here on strict sentence intersection, explicitly incorporating a constraint that requires that a produced fusion must not contain information that is not present in all input sentences.\n[BOS] This distinguishes our approach from traditional sentence fusion approaches (Jing and McKeown, 2000; Barzilay and McKeown, 2005; Filippova and Strube, 2008b) which generally attempt to retain common information but are typically evaluated in an abstractive summarization context in which additional information in the fusion output does not negatively impact judgments.\n\n", "discourse_tags": ["Narrative_cite", "Reflection", "Narrative_cite"], "span_citation_mapping": [{"token_start": 16, "token_end": 37, "char_start": 94, "char_end": 157, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Krahmer et al., 2008;": "17303811", "Marsi and Krahmer, 2005)": "2293515"}}}, {"token_start": 106, "token_end": 139, "char_start": 590, "char_end": 710, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Jing and McKeown, 2000;": "800331", "Barzilay and McKeown, 2005;": "16188305", "Filippova and Strube, 2008b)": "14909308"}}}]}
{"id": "33867255_2", "paragraph": "[BOS] Our method still follows the RNN encoder-decoder framework which gives the full play of the advantages of RNNs to transfer information through words bidirectionally.\n[BOS] In additional, this method also captures relationship between source words without any external knowledge injection, so is easy to use.\n[BOS] It employs relation networks to connect source words explicitly so that the model can learn the relationship itself.\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "33867255_1", "paragraph": "[BOS] Another line is to change the structure of the encoder totally.\n[BOS] Gehring et al. (2016) and Gehring et al. (2017) present to substitute the conventional RNN encoder with the CNN encoder in order to train faster.\n[BOS] They employ stacked CNNs to capture the relationship between source words which can be calculated simultaneously, not like RNNs the computation of which is constrained by temporal dependencies.\n[BOS] The attention scores are also computed based on the output of the CNNs and the decoder is still the RNN decoder.\n[BOS] Vaswani et al. (2017) is another work to eschew the recurrence.\n[BOS] It instead relies entirely on the attention mechanism to draw the global dependencies between input and output.\n[BOS] Although temporal dependency inherent in RNNs hinders the parallelization, it can pass messages through the history and assists the current decision, so retaining the RNNs in the model has more advantages than disadvantages.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Multi_summ", "Multi_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 15, "token_end": 110, "char_start": 76, "char_end": 540, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Gehring et al. (2017)": "3648736"}, "Reference": {}}}, {"token_start": 111, "token_end": 191, "char_start": 547, "char_end": 959, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "33867255_0", "paragraph": "[BOS] Many researchers have worked at learning the relationship of source words to improve the translation performance.\n[BOS] One line is to refine the presentations of the source by adding the relationship between source words or between source and target words, remaining the main architecture as RNN encoder-decoder (Bahdanau, Cho, and Bengio 2014).\n[BOS] Meng et al. (2016) introduces a new attention mechanism to the encoder-decoder architecture.\n[BOS] It defines reading and writing operations between the decoder and the representation of the source sentence to introduce interaction which is a form of relationship between source and target.\n[BOS] Bastings et al. (2017) employ graph convolutional networks to capture the relationship between the source word pairs which has a dependent relation in the source dependency tree, so this method needs the supervision of external dependency syntax.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 49, "token_end": 67, "char_start": 299, "char_end": 352, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 68, "token_end": 118, "char_start": 359, "char_end": 649, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 119, "token_end": 163, "char_start": 656, "char_end": 902, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "3708647_1", "paragraph": "[BOS] (2012) presents a method for text simplification using phrase based machine translation with reranking the outputs.\n[BOS] Kauchak (2013) proposes a text simplification corpus, and evaluates language modeling for text simplification on the proposed corpus.\n[BOS] Narayan and Gardent (2014) propose a hybrid approach to sentence simplification which combines deep semantics and monolingual machine translation.\n[BOS] Hwang et al. (2015) introduces a parallel simplification corpus by evaluating the similarity between the source text and the simplified text based on WordNet.\n[BOS] Glava andtajner (2015) propose an unsupervised approach to lexical simplification that makes use of word vectors and require only regular corpora.\n[BOS] Xu et al. (2016) design automatic metrics for text simplification.\n[BOS] Recently, most works focus on the neural sequence-to-sequence model.\n[BOS] Nisioi et al. (2017) present a sequence-to-sequence model, and re-ranks the predictions with BLEU and SARI.\n[BOS] Zhang and Lapata (2017) propose a deep reinforcement learning model to improve the simplicity, fluency and adequacy of the simplified texts.\n[BOS] Cao et al. (2017) introduce a novel sequence-tosequence model to join copying and restricted generation for text simplification.\n[BOS] Rush et al. (2015) first used an attention-based encoder to compress texts and a neural network language decoder to generate summaries.\n[BOS] Following this work, recurrent encoder was introduced to text summarization, and gained better performance (Lopyrev, 2015; Chopra et al., 2016) .\n[BOS] Towards Chinese texts, Hu et al. (2015) built a large corpus of Chinese short text summarization.\n[BOS] To deal with unknown word problem, Nallapati et al. (2016) proposed a generator-pointer model so that the decoder is able to generate words in source texts.\n[BOS] Gu et al. (2016) also solved this issue by incorporating copying mechanism.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Narrative_cite", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 23, "char_start": 6, "char_end": 121, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 24, "token_end": 48, "char_start": 128, "char_end": 261, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kauchak (2013)": "9516661"}, "Reference": {}}}, {"token_start": 49, "token_end": 74, "char_start": 268, "char_end": 414, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Narayan and Gardent (2014)": "15489071"}, "Reference": {}}}, {"token_start": 75, "token_end": 104, "char_start": 421, "char_end": 579, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hwang et al. (2015)": "767485"}, "Reference": {}}}, {"token_start": 105, "token_end": 133, "char_start": 586, "char_end": 732, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 134, "token_end": 148, "char_start": 739, "char_end": 805, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Xu et al. (2016)": "2177849"}, "Reference": {}}}, {"token_start": 165, "token_end": 196, "char_start": 887, "char_end": 994, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Nisioi et al. (2017)": "36364048"}, "Reference": {}}}, {"token_start": 197, "token_end": 223, "char_start": 1001, "char_end": 1141, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhang and Lapata (2017)": "7473831"}, "Reference": {}}}, {"token_start": 224, "token_end": 249, "char_start": 1148, "char_end": 1276, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Cao et al. (2017)": "13531903"}, "Reference": {}}}, {"token_start": 250, "token_end": 278, "char_start": 1283, "char_end": 1418, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Rush et al. (2015)": "1918428"}, "Reference": {}}}, {"token_start": 288, "token_end": 311, "char_start": 1482, "char_end": 1568, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lopyrev, 2015;": "9816245"}}}, {"token_start": 313, "token_end": 335, "char_start": 1577, "char_end": 1674, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hu et al. (2015)": "11597846"}, "Reference": {}}}, {"token_start": 336, "token_end": 371, "char_start": 1681, "char_end": 1837, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Nallapati et al. (2016)": "8928715"}, "Reference": {}}}, {"token_start": 372, "token_end": 388, "char_start": 1844, "char_end": 1919, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Gu et al. (2016)": "8174613"}, "Reference": {}}}]}
{"id": "3708647_0", "paragraph": "[BOS] Our work is related to the encoder-decoder framework and the attention mechanism .\n[BOS] Encoderdecoder framework, like sequence-to-sequence model, has achieved success in machine translation Jean et al., 2015; Luong et al., 2015; ), text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; , and other natural language processing tasks .\n[BOS] There are many other methods to improve neural attention model (Jean et al., 2015; Luong et al., 2015) .\n[BOS] Zhu et al. (2010) constructs a wikipedia dataset, and proposes a tree-based simplification model.\n[BOS] Woodsend and Lapata (2011) introduces a data-driven model based on quasi-synchronous grammar, which captures structural mismatches and complex rewrite operations.\n[BOS] Wubben et al.\n\n", "discourse_tags": ["Reflection", "Narrative_cite", "Narrative_cite", "Single_summ", "Single_summ", "Other"], "span_citation_mapping": [{"token_start": 35, "token_end": 51, "char_start": 178, "char_end": 235, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Jean et al., 2015;": "2863491"}}}, {"token_start": 54, "token_end": 82, "char_start": 240, "char_end": 322, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rush et al., 2015;": "1918428", "Chopra et al., 2016;": "133195"}}}, {"token_start": 99, "token_end": 118, "char_start": 418, "char_end": 480, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Jean et al., 2015;": "2863491", "Luong et al., 2015)": "1998416"}}}, {"token_start": 120, "token_end": 141, "char_start": 489, "char_end": 586, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhu et al. (2010)": "15636533"}, "Reference": {}}}, {"token_start": 142, "token_end": 172, "char_start": 593, "char_end": 755, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Woodsend and Lapata (2011)": "9945908"}, "Reference": {}}}]}
{"id": "44092258_2", "paragraph": "[BOS] Due to loss of states across time steps, the decoder may generate duplicate outputs.\n[BOS] Attempts have been made to address this problem.\n[BOS] Some architectures try to utilize History attention records.\n[BOS] See et al. (2017) introduce a coverage mechanism, and Paulus et al. (2017) use history attention weights to normalize new attention.\n[BOS] Others are featured in network modules.\n[BOS] Suzuki and Nagata (2017) estimate the frequency of target words and record the occurrence.\n[BOS] Our model shows that simply attending to history decoder states can reduce redundancy.\n[BOS] Then we use the context vector of attention to history decoder states to perform attention to the memory.\n[BOS] Doing this enables the decoder to correctly decide what to say at memory addressing time, rather than decoding time, thus increasing answer coverage and information enrichment.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Multi_summ", "Transition", "Single_summ", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 39, "token_end": 50, "char_start": 219, "char_end": 267, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"See et al. (2017)": null}, "Reference": {}}}, {"token_start": 52, "token_end": 69, "char_start": 273, "char_end": 351, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Paulus et al. (2017)": "21850704"}, "Reference": {}}}, {"token_start": 79, "token_end": 97, "char_start": 404, "char_end": 494, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Suzuki and Nagata (2017)": "1207251"}, "Reference": {}}}]}
{"id": "44092258_1", "paragraph": "[BOS] Conditional Sentence Generation: Controllable sentence generation with external information is wildly studied from different views.\n[BOS] From the task perspective, Fan et al. (2017) utilize label information for generation, and tackle information coverage in a summarization task.\n[BOS] He et al. (2017a) use recursive Network to represent knowledge base, and Bordes and Weston (2016) track generation states and provide information enrichment, both are in a dialog setting.\n[BOS] In terms of network architecture, Wen et al. (2015) equip LSTM with a semantic control cell to improve informativeness of generated sentence.\n[BOS] Kiddon et al. (2016) propose the neural checklist model to explicitly track what has been mentioned and what left to say by splitting these two into different lists.\n[BOS] Our model is related to these models with respect to information representation and challenges from coverage and redundancy.\n[BOS] The most closely related one is the checklist model.\n[BOS] But it does not explicitly study information redundancy.\n[BOS] Also, the information we track is heterogeneous, and we track it in a different way, i.e. using Cumulative attention.\n\n", "discourse_tags": ["Transition", "Single_summ", "Multi_summ", "Single_summ", "Single_summ", "Reflection", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 21, "token_end": 49, "char_start": 144, "char_end": 287, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Fan et al. (2017)": "22716243"}, "Reference": {}}}, {"token_start": 50, "token_end": 65, "char_start": 294, "char_end": 361, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"He et al. (2017a)": "3051772"}, "Reference": {}}}, {"token_start": 67, "token_end": 90, "char_start": 367, "char_end": 481, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Bordes and Weston (2016)": "2129889"}, "Reference": {}}}, {"token_start": 91, "token_end": 120, "char_start": 488, "char_end": 629, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wen et al. (2015)": "739696"}, "Reference": {}}}, {"token_start": 121, "token_end": 154, "char_start": 636, "char_end": 801, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kiddon et al. (2016)": "9818013"}, "Reference": {}}}]}
{"id": "44092258_0", "paragraph": "[BOS] Natural Answer Generation with Sequence to Sequence Learning: Sequence to sequence models (with attention) have achieved successful results in many NLP tasks Bahdanau et al., 2014; Vinyals et al., 2015; See et al., 2017) .\n[BOS] Memory is an effective way to equip seq2seq systems with external information (Weston et al., 2014; Sukhbaatar et al., 2015; Miller et al., 2016; Kumar et al., 2015) .\n[BOS] GenQA (Yin et al., 2015 ) applies a seq2seq model to generate natural answer sentences from a knowledge base, and CoreQA (He et al., 2017b) extends it with copying mechanism (Gu et al., 2016) .\n[BOS] But they do not consider the heterogeneity of the memory, only tackle questions with one single answer word, and do not study information enrichment.\n[BOS] Memory and Attention: There are also increasing works focusing on different memory representations and the interaction between the decoder and memory, i.e., attention.\n[BOS] Miller et al. (2016) propose the Key-Value style memory to explore textual knowledge (both structured and unstructured) from different sources, but they still utilize them separately, without a uniform addressing and attention mechanism.\n[BOS] Daniluk et al. (2017) split the decoder states into key and value representation, and increase language modeling performance.\n[BOS] Multiple variants of attention mechanism have also been studied.\n[BOS] Sukhbaatar et al. (2015) introduce multi-hop attention, and extend it to convolutional sequence to sequence learning (Gehring et al., 2017) .\n[BOS] Kumar et al. (2015) further extend it by using a Gated Recurrent Unit (Chung et al., 2014) between hops.\n[BOS] These models show that multiple hops may increase the model's ability to reason.\n[BOS] These multi-hop attention is performed within a single homogeneous memory.\n[BOS] Our Cumulative Attention is inspired by them, but we utilize it cross different memory, hence can explicitly reason over different memory components.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Multi_summ", "Transition", "Transition", "Single_summ", "Single_summ", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 25, "token_end": 53, "char_start": 154, "char_end": 226, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Bahdanau et al., 2014;": "11212020", "Vinyals et al., 2015;": "5692837", "See et al., 2017)": null}}}, {"token_start": 55, "token_end": 102, "char_start": 235, "char_end": 400, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Sukhbaatar et al., 2015;": "1399322", "Miller et al., 2016;": "2711679", "Kumar et al., 2015)": "2319779"}}}, {"token_start": 104, "token_end": 129, "char_start": 409, "char_end": 517, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Yin et al., 2015": "14039866"}, "Reference": {}}}, {"token_start": 131, "token_end": 155, "char_start": 523, "char_end": 600, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(He et al., 2017b)": "43225062"}, "Reference": {"(Gu et al., 2016)": "8174613"}}}, {"token_start": 216, "token_end": 259, "char_start": 939, "char_end": 1176, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Miller et al. (2016)": "2711679"}, "Reference": {}}}, {"token_start": 260, "token_end": 285, "char_start": 1183, "char_end": 1308, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Daniluk et al. (2017)": "3000562"}, "Reference": {}}}, {"token_start": 297, "token_end": 332, "char_start": 1386, "char_end": 1525, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Sukhbaatar et al. (2015)": "1399322"}, "Reference": {"(Gehring et al., 2017)": "3648736"}}}, {"token_start": 334, "token_end": 392, "char_start": 1534, "char_end": 1806, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kumar et al. (2015)": "2319779"}, "Reference": {"(Chung et al., 2014)": "5201925"}}}]}
{"id": "26375631_5", "paragraph": "[BOS] We use bilingual translations from the source to the target language, English, obtained from Wikipedia page titles with interlanguage links.\n[BOS] Since Wikipedia pages in the source language may be linked to pages in languages other than English, we also use high accuracy, crowdsourced translations (Pavlick et al., 2014 ) from these third languages to English as additional bilingual translations.\n[BOS] To alleviate the cold start issue, when a source word has no existing known translation to English or other third languages, our model backsoff to additional signals of translation equivalence estimated based on its word embedding and visual representations.\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 51, "token_end": 63, "char_start": 281, "char_end": 328, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Pavlick et al., 2014": "72276"}}}]}
{"id": "26375631_4", "paragraph": "[BOS] However, due to the sparsity of existing bilingual dictionaries (for some language pairs such dictionaries may not exist), the traditional formulation of MF with BPR suffers from the \"cold start\" issue (Gantner et al., 2010; He and McAuley, 2016; Verga et al., 2016) .\n[BOS] In our case, these are situations in which some source words have no translations to any word in the target or related languages.\n[BOS] For these words, additional information, e.g., monolingual signals of translation equivalence or language-independent representations such as visual representations, must be used.\n\n", "discourse_tags": ["Narrative_cite", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 26, "token_end": 68, "char_start": 129, "char_end": 272, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Gantner et al., 2010;": "8955774", "He and McAuley, 2016;": "3099285", "Verga et al., 2016)": "9206785"}}}]}
{"id": "26375631_3", "paragraph": "[BOS] Since we do not observe false translations in the seed dictionary, the training data in the matrix consists only of positive translations.\n[BOS] The absence of values in the matrix does not imply that the corresponding words are not translations.\n[BOS] In fact, we seek to predict which of these missing values are true.\n[BOS] The BPR approach to MF (Rendle et al., 2009) formulates the task of predicting missing values as a ranking task.\n[BOS] With the assumption that observed true translations should be given higher values than unobserved translations, BPR learns to optimize the difference between values assigned to the observed translations and values assigned to the unobserved translations.\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 62, "token_end": 90, "char_start": 333, "char_end": 445, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Rendle et al., 2009)": "10795036"}, "Reference": {}}}]}
{"id": "26375631_2", "paragraph": "[BOS] Bayesian Personalized Ranking (BPR) Our approach is based on extensions to the probabilis-tic model of MF in collaborative filtering (Koren et al., 2009; Rendle et al., 2009) .\n[BOS] We represent our translation task as a matrix with source words in the columns and target words in the rows (Figure 1) .\n[BOS] Based on some observed translations in the matrix found in a seed dictionary, our model learns low-dimensional feature vectors that encode the latent properties of the words in the row and the words in the column.\n[BOS] The dot product of these vectors, which indicate how \"aligned\" the source and the target word properties are, captures how likely they are to be translations.\n\n", "discourse_tags": ["Narrative_cite", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 17, "token_end": 44, "char_start": 85, "char_end": 180, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Koren et al., 2009;": "58370896", "Rendle et al., 2009)": "10795036"}}}]}
{"id": "26375631_1", "paragraph": "[BOS] Most prior work on BLI however, either makes use of only one monolingual signal or uses unsupervised methods (e.g., rank combination) to aggregate the signals.\n[BOS] Irvine and Callison-Burch (2016) show that combining monolingual signals in a supervised logistic regression model produces higher accuracy word translations than unsupervised models.\n[BOS] More recently, show that their multi-modal model that employs a simple weighted-sum of word embeddings and visual similarities can improve translation accuracy.\n[BOS] These works show that there is a need for combining diverse, multi-modal monolingual signals of translations.\n[BOS] In this paper, we take this step further by combining the monolingual signals with bilingual signals of translations from existing bilingual dictionaries of related, \"third\" languages.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 38, "token_end": 121, "char_start": 172, "char_end": 638, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Irvine and Callison-Burch (2016)": "13357713"}, "Reference": {}}}]}
{"id": "26375631_0", "paragraph": "[BOS] Bilingual Lexicon Induction Previous research has used different sources for estimating transla-1 http://www.cis.upenn.edu/%7Ederry/translations.html tions from monolingual corpora.\n[BOS] Signals such as contextual, temporal, topical, and ortographic similarities between words are used to measure their translation equivalence (Schafer and Yarowsky, 2002; Klementiev and Roth, 2006; Callison-Burch, 2013, 2017) .\n[BOS] With the increasing popularity of word embeddings, many recent works approximate similarities between words in different languages by constructing a shared bilingual embedding space (Klementiev et al., 2012b; Zou et al., 2013; Vuli and Moens, 2013; Mikolov et al., 2013a; Faruqui and Dyer, 2014; Chandar A P et al., 2014; Gouws et al., 2015; Luong et al., 2015; Lu et al., 2015; Upadhyay et al., 2016) .\n[BOS] In the shared space, words from different languages are represented in a language-independent manner such that similar words, regardless of language, have similar representations.\n[BOS] Similarities between words can then be measured in the shared space.\n[BOS] One approach to induce this shared space is to learn a mapping function between the languages' monolingual semantic spaces (Mikolov et al., 2013a; Dinu et al., 2014) .\n[BOS] The mapping relies on seed translations which can be from existing dictionaries or be reliably chosen from pseudo-bilingual corpora of comparable texts e.g., Wikipedia with interlanguage links.\n[BOS] Vuli and Moens (2015) show that by learning a linear function with a reliably chosen seed lexicon, they outperform other models with more expensive bilingual signals for training on benchmark data.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite", "Transition", "Transition", "Narrative_cite", "Transition", "Single_summ"], "span_citation_mapping": [{"token_start": 65, "token_end": 96, "char_start": 310, "char_end": 417, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Schafer and Yarowsky, 2002;": "28882385", "Klementiev and Roth, 2006;": "669616"}}}, {"token_start": 120, "token_end": 215, "char_start": 582, "char_end": 827, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Klementiev et al., 2012b;": "6758088", "Zou et al., 2013;": "931054", "Vuli\u0107 and Moens, 2013;": "10068440", "Mikolov et al., 2013a;": "1966640", "Faruqui and Dyer, 2014;": "3792324", "Chandar A P et al., 2014;": "217774", "Gouws et al., 2015;": "7021865", "Luong et al., 2015;": "13603998", "Lu et al., 2015;": "874413", "Upadhyay et al., 2016)": "5357629"}}}, {"token_start": 278, "token_end": 301, "char_start": 1192, "char_end": 1262, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mikolov et al., 2013a;": "1966640", "Dinu et al., 2014)": "17910711"}}}, {"token_start": 340, "token_end": 377, "char_start": 1471, "char_end": 1668, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Vuli\u0107 and Moens (2015)": "14183678"}, "Reference": {}}}]}
{"id": "3146611_1", "paragraph": "[BOS] Strict left-to-right ordering is also prevalent in sequence tagging.\n[BOS] Indeed, one major influence on our work is Shen et.al.\n[BOS] 's bi-directional POS-tagging algorithm (Shen et al., 2007) , which combines a perceptron learning procedure similar to our own with beam search to produce a state-of-the-art POStagger, which does not rely on left-to-right processing.\n[BOS] Shen and Joshi (2008) extends the bidirectional tagging algorithm to LTAG parsing, with good results.\n[BOS] We build on top of that work and present a concrete and efficient greedy non-directional dependency parsing algorithm.\n[BOS] Eisner and Smith (2005) propose to improve the efficiency of a globally optimized parser by posing hard constraints on the lengths of arcs it can produce.\n[BOS] Such constraints pose an explicit upper bound on parser accuracy.\n[BOS] 10 Our parsing model does not pose such restrictions.\n[BOS] Shorter edges are arguably easier to predict, and our parses builds them early in time.\n[BOS] However, it is also capable of producing long dependencies at later stages in the parsing process.\n[BOS] Indeed, the distribution of arc lengths produced by our parser is similar to those produced by the MALT and MST parsers.\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite", "Single_summ", "Reflection", "Single_summ", "Single_summ", "Reflection", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 34, "token_end": 49, "char_start": 145, "char_end": 201, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Shen et al., 2007)": "15876808"}}}, {"token_start": 90, "token_end": 111, "char_start": 383, "char_end": 484, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 134, "token_end": 165, "char_start": 616, "char_end": 770, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "3146611_0", "paragraph": "[BOS] Deterministic shift-reduce parsers are restricted by a strict left-to-right processing order.\n[BOS] Such parsers can rely on rich syntactic information on the left, but not on the right, of the decision point.\n[BOS] They are forced to commit early, and suffer from error propagation.\n[BOS] Our non-directional parser addresses these deficiencies by discarding the strict left-to-right processing order, and attempting to make easier decisions before harder ones.\n[BOS] Other methods of dealing with these deficiencies were proposed over the years: Several Passes Yamada and Matsumoto's (2003) pioneering work introduces a shift-reduce parser which makes several left-to-right passes over a sentence.\n[BOS] Each pass adds structure, which can then be used in subsequent passes.\n[BOS] Sagae and Lavie (2006b) extend this model to alternate between left-to-right and right-to-left passes.\n[BOS] This model is similar to ours, in that it attempts to defer harder decisions to later passes over the sentence, and allows late decisions to make use of rich syntactic information (built in earlier passes) on both sides of the decision point.\n[BOS] However, the model is not explicitly trained to optimize attachment ordering, has an O(n 2 ) runtime complexity, and produces results which are inferior to current single-pass shift-reduce parsers.\n[BOS] Beam Search Several researchers dealt with the early-commitment and error propagation of deterministic parsers by extending the greedy decisions with various flavors of beam-search (Sagae and Lavie, 2006a; Zhang and Clark, 2008; Titov and Henderson, 2007) .\n[BOS] This approach works well and produces highly competitive results.\n[BOS] Beam search can be incorporated into our parser as well.\n[BOS] We leave this investigation to future work.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Reflection", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Transition", "Narrative_cite", "Transition", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 93, "token_end": 156, "char_start": 475, "char_end": 782, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Yamada and Matsumoto's (2003)": "13163488"}, "Reference": {}}}, {"token_start": 157, "token_end": 233, "char_start": 789, "char_end": 1140, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Sagae and Lavie (2006b)": "6133066"}, "Reference": {}}}, {"token_start": 295, "token_end": 329, "char_start": 1479, "char_end": 1606, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sagae and Lavie, 2006a;": "10160110", "Zhang and Clark, 2008;": null, "Titov and Henderson, 2007)": "2790981"}}}]}
{"id": "3541996_1", "paragraph": "[BOS] Handling rare words is an important problem for NMT that has been approached in various ways.\n[BOS] Some have focused on reducing the number of UNKs by enabling NMT to learn from a larger vocabulary (Jean et al., 2015; Mi et al., 2016) ; others have focused on replacing UNKs by copying source words (Gulcehre et al., 2016; Gu et al., 2016; Luong et al., 2015b) .\n[BOS] However, these methods only help with unknown words, not rare words.\n[BOS] An approach that addresses both unknown and rare words is to use subword-level information (Sennrich et al., 2016; Chung et al., 2016; Luong and Manning, 2016) .\n[BOS] Our approach is different in that we try to identify and address the root of the rare word problem.\n[BOS] We expect that our models would benefit from more advanced UNKreplacement or subword-level techniques as well.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Narrative_cite", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 25, "token_end": 56, "char_start": 127, "char_end": 241, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Jean et al., 2015;": "2863491", "Mi et al., 2016)": "1371374"}}}, {"token_start": 57, "token_end": 95, "char_start": 244, "char_end": 367, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Gulcehre et al., 2016;": "969555", "Gu et al., 2016;": "8174613", "Luong et al., 2015b)": "1245593"}}}, {"token_start": 124, "token_end": 154, "char_start": 516, "char_end": 610, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sennrich et al., 2016;": "1114678", "Chung et al., 2016;": "13495961", "Luong and Manning, 2016)": "13972671"}}}]}
{"id": "3541996_0", "paragraph": "[BOS] The closest work to our lex model is that of Arthur et al. (2016) , which we have discussed already in Section 4.\n[BOS] Recent work by Liu et al. (2016) has a similar motivation to that of our fixnorm model.\n[BOS] They reformulate the output layer in terms of directions and magnitudes, as we do here.\n[BOS] Whereas we have focused on the magnitudes, they focus on the directions, modifying the loss function to try to learn a classifier that separates the classes' directions with something like a margin.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 20, "char_start": 6, "char_end": 71, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Arthur et al. (2016)": "10086161"}}}, {"token_start": 31, "token_end": 72, "char_start": 126, "char_end": 307, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Liu et al. (2016)": "1829423"}, "Reference": {}}}]}
{"id": "26481271_2", "paragraph": "[BOS] The first known attempt at using NMT for machine translation of image descriptions is by Elliott et al. (2015) , who conditioned an NMT system with a CNN image embedding (the penultimate layer of VGG-16 (Simonyan and Zisserman, 2014) ) at the beginning of either the encoder or the decoder.\n[BOS] The WMT16 shared task on Multimodal Machine Translation has further encouraged research in this area.\n[BOS] At the time, phrase-based SMT systems (Shah et al., 2016; Libovick et al., 2016; Hitschler et al., 2016) performed better than NMT systems (Calixto et al., 2016; Huang et al., 2016; Caglayan et al., 2016) .\n[BOS] Participants used either the penultimate fully connected layer or a convolutional layer of a CNN as image representation, with the exception of Shah et al. (2016) who used the classification output of VGG-16 as features to a phrase-based SMT system.\n[BOS] In all cases, image information were found to provide only marginal improvements.\n\n", "discourse_tags": ["Single_summ", "Transition", "Narrative_cite", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 2, "token_end": 72, "char_start": 6, "char_end": 296, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Elliott et al. (2015)": "18550051"}, "Reference": {"(Simonyan and Zisserman, 2014)": "14124313"}}}, {"token_start": 96, "token_end": 128, "char_start": 424, "char_end": 515, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Shah et al., 2016;": "16585731", "Libovick\u00fd et al., 2016;": "5201435", "Hitschler et al., 2016)": "14444828"}}}, {"token_start": 131, "token_end": 160, "char_start": 538, "char_end": 615, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Calixto et al., 2016;": "2060848", "Huang et al., 2016;": "11391667", "Caglayan et al., 2016)": "1325523"}}}, {"token_start": 188, "token_end": 216, "char_start": 768, "char_end": 873, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Shah et al. (2016)": "16585731"}, "Reference": {}}}]}
{"id": "26481271_1", "paragraph": "[BOS] Currently, the two largest image description datasets are Flickr30K (Young et al., 2014) and MS COCO (Lin et al., 2014) .\n[BOS] These datasets are constructed in English and are aimed at advancing research on the generation of image descriptions in English.\n[BOS] Recent attempts have been made to incorporate multilinguality into both these largescale datasets, with the datasets being extended to other languages such as German and Japanese Hitschler et al., 2016; Miyazaki and Shimizu, 2016; Yoshikawa et al., 2017) .\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 11, "token_end": 23, "char_start": 64, "char_end": 94, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Young et al., 2014)": "3104920"}}}, {"token_start": 24, "token_end": 35, "char_start": 99, "char_end": 125, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lin et al., 2014)": "14113767"}}}, {"token_start": 86, "token_end": 114, "char_start": 429, "char_end": 524, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Hitschler et al., 2016;": "14444828", "Miyazaki and Shimizu, 2016;": "17040292", "Yoshikawa et al., 2017)": "1805495"}}}]}
{"id": "26481271_0", "paragraph": "[BOS] There has been interest in recent years in the task of generating image descriptions (also known as image captioning).\n[BOS] Bernardi et al. (2016) provide a detailed discussion on various image description generation approaches that have been developed.\n\n", "discourse_tags": ["Transition", "Single_summ"], "span_citation_mapping": [{"token_start": 27, "token_end": 50, "char_start": 131, "char_end": 260, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Bernardi et al. (2016)": "47156761"}, "Reference": {}}}]}
{"id": "3913537_2", "paragraph": "[BOS] More recently, our approach has been successfully applied to summarization (Ayana et al., 2016) .\n[BOS] They optimize neural networks for headline generation with respect to ROUGE (Lin, 2004) and also achieve significant improvements, confirming the effectiveness and applicability of our approach.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 5, "token_end": 23, "char_start": 21, "char_end": 101, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ayana et al., 2016)": "61556494"}}}, {"token_start": 25, "token_end": 43, "char_start": 110, "char_end": 197, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lin, 2004)": "964287"}}}]}
{"id": "3913537_1", "paragraph": "[BOS] The Mixed Incremental Cross-Entropy Reinforce (MIXER) algorithm (Ranzato et al., 2015) is in spirit closest to our work.\n[BOS] Building on the REINFORCE algorithm proposed by Williams (1992) , MIXER allows incremental learning and the use of hybrid loss function that combines both REINFORCE and cross-entropy.\n[BOS] The major difference is that Ranzato et al. (2015) leverage reinforcement learning while our work resorts to minimum risk training.\n[BOS] In addition, MIXER only samples one candidate to calculate reinforcement reward while MRT generates multiple samples to calculate the expected risk.\n[BOS] Figure 2 indicates that multiple samples potentially increases MRT's capability of discriminating between diverse candidates and thus benefit translation quality.\n[BOS] Our experiments confirm Ranzato et al. (2015) 's finding that taking evaluation metrics into account when optimizing model parameters does help to improve sentence-level text generation.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Reflection", "Transition", "Other", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 23, "char_start": 6, "char_end": 92, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ranzato et al., 2015)": "7147309"}}}, {"token_start": 35, "token_end": 43, "char_start": 149, "char_end": 196, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 70, "token_end": 82, "char_start": 352, "char_end": 405, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Ranzato et al. (2015)": "7147309"}}}, {"token_start": 146, "token_end": 178, "char_start": 809, "char_end": 971, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Ranzato et al. (2015)": "7147309"}}}]}
{"id": "3913537_0", "paragraph": "[BOS] Our work originated from the minimum risk training algorithms in conventional statistical machine translation Smith and Eisner, 2006; He and Deng, 2012) .\n[BOS] describes a smoothed error count to allow calculating gradients, which directly inspires us to use a parameter  to adjust the smoothness of the objective function.\n[BOS] As neural networks are non-linear, our approach has to minimize the expected loss on the sentence level rather than the loss of 1-best translations on the corpus level.\n[BOS] Smith and Eisner (2006) introduce minimum risk annealing for training log-linear models that is capable of gradually annealing to focus on the 1-best hypothesis.\n[BOS] He et al. (2012) apply minimum risk training to learning phrase translation probabilities.\n[BOS] Gao et al. (2014) leverage MRT for learning continuous phrase representations for statistical machine translation.\n[BOS] The difference is that they use MRT to optimize a sub-model of SMT while we are interested in directly optimizing end-to-end neural translation models.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Reflection", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 12, "token_end": 29, "char_start": 71, "char_end": 158, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 95, "token_end": 127, "char_start": 512, "char_end": 673, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 128, "token_end": 145, "char_start": 680, "char_end": 770, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 146, "token_end": 166, "char_start": 777, "char_end": 891, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Gao et al. (2014)": "10473972"}, "Reference": {}}}]}
{"id": "3941259_1", "paragraph": "[BOS] Multimodal caption translation on parallel caption data (see the approaches described in Specia et al. (2016) ) incorporate visual information directly into the sequence-to-sequence caption translation model or into a reranking component, or into both (see for example the attentionbased LSTM approach of Huang et al. (2016) ), or they use back-translation to generate synthetic parallel data (see for example Calixto et al. (2017) ).\n[BOS] However, obtaining parallel captioning data or retraining NMT models on large synthetic datasets is either financially or computationally expensive.\n[BOS] We thus opt for a way that does not require large amounts of parallel captions to improve translation quality.\n[BOS] Our work can be seen as an extension of the idea of Wschle and Riezler (2015) to multimodal data.\n[BOS] Their approach is based on cross-lingual retrieval techniques to find sentences in a large target-language document collection, which are then used to rerank candidate translations.\n[BOS] Our approach uses textual relevance and visual similarity (see Section 3.2) to obtain lists of multimodal pivot documents from a monolingual image-caption corpus similar to the idea described in Hitschler et al. (2016) .\n[BOS] In contrast to their approach, we do not rely on grid search for hyperparameters but instead use a machine learning approach to determine optimal weights of different rerankers to get the best scoring ensemble.\n[BOS] In order to discern the contribution of our learning method, we compare it to the monolingual reranking approach of Hitschler et al. (2016) on the MS COCO v2014 dataset that was used in their work.\n\n", "discourse_tags": ["Single_summ", "Transition", "Reflection", "Reflection", "Single_summ", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 57, "token_end": 70, "char_start": 279, "char_end": 330, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Huang et al. (2016)": "11391667"}}}, {"token_start": 75, "token_end": 96, "char_start": 346, "char_end": 437, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Calixto et al. (2017)": "373945"}}}, {"token_start": 147, "token_end": 173, "char_start": 719, "char_end": 816, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"W\u00e4schle and Riezler (2015)": "10069775"}}}, {"token_start": 233, "token_end": 255, "char_start": 1140, "char_end": 1229, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Hitschler et al. (2016)": "14444828"}}}, {"token_start": 313, "token_end": 329, "char_start": 1537, "char_end": 1594, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Hitschler et al. (2016)": "14444828"}}}]}
{"id": "3941259_0", "paragraph": "[BOS] The dataset presented in this paper is to our knowledge the first publicly available resource of user-generated image captions at the size of 4M image-caption pairs.\n[BOS] The dataset that is closest to ours is the SBU captioned photo dataset (Ordonez et al., 2011 ) that contains 1M images and captions.\n[BOS] However, this dataset was filtered to include specific terms and to limit description lengths, resulting in an average sentence length of around 13 tokens.\n[BOS] See Ferraro et al. (2015) for an overview over image-caption datasets.\n\n", "discourse_tags": ["Reflection", "Narrative_cite", "Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 46, "token_end": 61, "char_start": 221, "char_end": 270, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ordonez et al., 2011": "14579301"}}}, {"token_start": 101, "token_end": 119, "char_start": 483, "char_end": 549, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Ferraro et al. (2015)": "2044324"}}}]}
{"id": "2574224_1", "paragraph": "[BOS] Storytelling itself is one of the oldest known human activities (Wiessner, 2014) , providing a way to educate, preserve culture, instill morals, and share advice; focusing AI research towards this task therefore has the potential to bring about more humanlike intelligence and understanding.\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 2, "token_end": 21, "char_start": 6, "char_end": 86, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wiessner, 2014)": "6367066"}}}]}
{"id": "2574224_0", "paragraph": "[BOS] Work in vision to language has exploded, with researchers examining image captioning (Lin et al., 2014; Karpathy and Fei-Fei, 2015; Vinyals et al., 2015; Chen et al., 2015; Young et al., 2014; Elliott and Keller, 2013) , question answering (Antol et al., 2015; Ren et al., 2015; Malinowski and Fritz, 2014), visual phrases (Sadeghi and Farhadi, 2011) , video understanding (Ramanathan et al., 2013) , and visual concepts (Krishna et al., 2016; .\n[BOS] Such work focuses on direct, literal description of image content.\n[BOS] While this is an encouraging first step in connecting vision and language, it is far from the capabilities needed by intelligent agents for naturalistic interactions.\n[BOS] There is a significant difference, yet unexplored, between remarking that a visual scene shows \"sitting in a room\" -typical of most image captioning work -and that the same visual scene shows \"bonding\".\n[BOS] The latter description is grounded in the visual signal, yet it brings to bear information about social relations and emotions that can be additionally inferred in context (Figure 1 ).\n[BOS] Visually-grounded stories facilitate more evaluative and figurative language than has previously been seen in vision-to-language research: If a system can recognize that colleagues look bored, it can remark and act on this information directly.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Transition", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 14, "token_end": 68, "char_start": 74, "char_end": 224, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lin et al., 2014;": "3158329", "Chen et al., 2015;": "2007128", "Elliott and Keller, 2013)": "10282227"}}}, {"token_start": 69, "token_end": 97, "char_start": 227, "char_end": 312, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Antol et al., 2015;": null, "Ren et al., 2015;": "680978"}}}, {"token_start": 98, "token_end": 111, "char_start": 314, "char_end": 356, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 112, "token_end": 123, "char_start": 359, "char_end": 404, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ramanathan et al., 2013)": "680978"}}}, {"token_start": 125, "token_end": 136, "char_start": 411, "char_end": 448, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "2228719_3", "paragraph": "[BOS] Our work is also inspired by the recent work in introducing datasets to evaluate question answering and reading comprehension tasks that require reasoning and entailment.\n[BOS] In contrast to Richardson et al. (2013) , our work is focused on making a dataset for math word problems to evaluate robustness, scalability, and scope of algorithms in quantitative reasoning.\n[BOS] In contrast to Weston et al. (2015) , our work has more natural text and a larger vocabulary, does not use synthetic data, and is only focused on math word problems which is an extension of the counting sub-category introduced in that work.\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 30, "token_end": 40, "char_start": 183, "char_end": 222, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Richardson et al. (2013)": "2100831"}}}, {"token_start": 68, "token_end": 79, "char_start": 382, "char_end": 417, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Weston et al. (2015)": "3178759"}}}]}
{"id": "2228719_2", "paragraph": "[BOS] Other work has focused on more limited domains, but aims to reduce the reliance on data overlap.\n[BOS] Hosseini et al. (2014) solve addition and subtraction problems by learning to categorize verbs for the purpose of updating a world representation derived from the problem text.\n[BOS] treat arithmetic word problem templates as equation trees and introduce a method for learning the least governing node for two text quantities.\n[BOS] KoncelKedziorski et al. (2015) focus on single equation problems and use typed semantically-rich equation trees where nodes correspond to numbers and an associated type derived from the problem text, and efficiently enumerate the space of these trees using integer linear programming.\n\n", "discourse_tags": ["Transition", "Single_summ", "Transition", "Single_summ"], "span_citation_mapping": [{"token_start": 22, "token_end": 56, "char_start": 109, "char_end": 285, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hosseini et al. (2014)": "428579"}, "Reference": {}}}, {"token_start": 81, "token_end": 135, "char_start": 442, "char_end": 726, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "2228719_1", "paragraph": "[BOS] A few recent works, and Zhou et al. (2015) focus on solving math word problems by matching quantities and variables (nouns) extracted from the problem text to templates appearing in the training data.\n[BOS] These methods have a broad scope, but they rely heavily on overlap between templates in the training and test data.\n[BOS] As shown in our experiments, when that overlap is reduced, the performance of the systems drops significantly.\n\n", "discourse_tags": ["Single_summ", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 42, "char_start": 6, "char_end": 206, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhou et al. (2015)": "14407737"}, "Reference": {}}}]}
{"id": "2228719_0", "paragraph": "[BOS] Recently, automatically solving math word problems has attracted several researchers.\n[BOS] Specific topics include number word problems (Shi et al., 2015) , logic puzzle problems (Mitra and Baral, 2015) , arithmetic word problems (Hosseini et al., 2014; , algebra word problems Zhou et al., 2015; Koncel-Kedziorski et al., 2015) , and geometry word problems (Seo et al., 2015) .\n\n", "discourse_tags": ["Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 18, "token_end": 29, "char_start": 122, "char_end": 161, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Shi et al., 2015)": "15717845"}}}, {"token_start": 30, "token_end": 42, "char_start": 164, "char_end": 209, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mitra and Baral, 2015)": "2684696"}}}, {"token_start": 43, "token_end": 55, "char_start": 212, "char_end": 259, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 57, "token_end": 80, "char_start": 263, "char_end": 335, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Zhou et al., 2015;": "14407737", "Koncel-Kedziorski et al., 2015)": "4894130"}}}, {"token_start": 82, "token_end": 94, "char_start": 342, "char_end": 383, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Seo et al., 2015)": "230735"}}}]}
{"id": "18634877_6", "paragraph": "[BOS] are complementary, and can be integrated to push the performance further.\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "18634877_5", "paragraph": "[BOS] 15 These two kinds of approaches 15 For example, in Spanish and French, adjectives often appears after nouns, thus forming a right-directed arc labeled by amod, whereas in English, the amod arcs are mostly leftdirected.\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "18634877_4", "paragraph": "[BOS] It is worth mentioning that remarkable results on the universal dependency treebanks have been achieved by using annotation projection method (Tiedemann, 2014) , treebank translation method (Tiedemann and Nivre, 2014) , and distribution transferring method (Ma and Xia, 2014) .\n[BOS] Unlike our approach, all of these methods involve training a parser at the target language side.\n[BOS] Parallel bitexts are required in these methods, which limits their scalability to lower-resource languages.\n[BOS] That said, these methods have the advantage that they are capable of capturing some language-specific syntactic patterns which our approach cannot.\n\n", "discourse_tags": ["Narrative_cite", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 21, "token_end": 30, "char_start": 119, "char_end": 165, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Tiedemann, 2014)": "216772"}}}, {"token_start": 31, "token_end": 44, "char_start": 168, "char_end": 223, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Tiedemann and Nivre, 2014)": "14049482"}}}, {"token_start": 46, "token_end": 56, "char_start": 230, "char_end": 281, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ma and Xia, 2014)": "15371205"}}}]}
{"id": "18634877_3", "paragraph": "[BOS] The cross-lingual representation learning method aims at building connections across different languages by inducing languageindependent feature representations.\n[BOS] After that, a parser can be trained at the source-language side within the induced feature space, and directly be applied to the target language.\n[BOS] Typical approaches include cross-lingual word clustering (Tckstrm et al., 2012) which is employed in this paper as a baseline, projection features (Durrett et al., 2012) .\n[BOS] Xiao and Guo (2014) learns cross-lingual word embeddings and apply them with MSTParser for linguistic transfer, which inspires this work.\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite", "Single_summ"], "span_citation_mapping": [{"token_start": 58, "token_end": 75, "char_start": 353, "char_end": 405, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 85, "token_end": 96, "char_start": 453, "char_end": 495, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 98, "token_end": 128, "char_start": 504, "char_end": 641, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Xiao and Guo (2014)": "15749718"}, "Reference": {}}}]}
{"id": "18634877_2", "paragraph": "[BOS] The jointly modeling methods integrates the monolingual grammar induction with bilinguallyprojected dependency information regularization (Ganchev et al., 2009 ), manually constructed universal dependency parsing rules (Naseem et al., 2010) and manually specified typological features (Naseem et al., 2012) .\n[BOS] Besides dependency parsing, the joint modeling method has also been applied for other multilingual NLP tasks, including NER (Che et al., 2013; Wang and Manning, 2014) , SRL (Zhuang and Zong, 2010; and WSD (Guo and Diab, 2010) .\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 13, "token_end": 29, "char_start": 85, "char_end": 165, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ganchev et al., 2009": "11681086"}}}, {"token_start": 31, "token_end": 47, "char_start": 169, "char_end": 246, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Naseem et al., 2010)": "3087412"}}}, {"token_start": 48, "token_end": 63, "char_start": 251, "char_end": 312, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Naseem et al., 2012)": "3143538"}}}, {"token_start": 86, "token_end": 102, "char_start": 441, "char_end": 487, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Che et al., 2013;": "8420751", "Wang and Manning, 2014)": "6723413"}}}, {"token_start": 103, "token_end": 113, "char_start": 490, "char_end": 516, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 115, "token_end": 124, "char_start": 522, "char_end": 546, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Guo and Diab, 2010)": "5705883"}}}]}
{"id": "18634877_1", "paragraph": "[BOS] The cross-lingual annotation projection method is first proposed in for shallower NLP tasks (POS tagging, NER, etc.)\n[BOS] .\n[BOS] The central idea is to project the syntactic annotations from a resource-rich language to the target language through word alignments, and then train a supervised parser on the projected noisy annotations (Hwa et al., 2005; Smith and Eisner, 2009; Zhao et al., 2009; Jiang et al., 2011; Tiedemann, 2014; Tiedemann, 2015) .\n[BOS] Noises and errors introduced by the word alignment and annotation projection processes can be reduced with robust projection methods by using graph-based label propagation Kim and Lee, 2012) , or by incorporating auxiliary resources Khapra et al., 2010) .\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 64, "token_end": 106, "char_start": 324, "char_end": 457, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hwa et al., 2005;": "157167", "Smith and Eisner, 2009;": "10943559", "Zhao et al., 2009;": "15978883", "Jiang et al., 2011;": "2686191", "Tiedemann, 2014;": "216772", "Tiedemann, 2015)": "6335141"}}}, {"token_start": 129, "token_end": 140, "char_start": 608, "char_end": 656, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Kim and Lee, 2012)": null}}}, {"token_start": 143, "token_end": 155, "char_start": 665, "char_end": 719, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Khapra et al., 2010)": "14219801"}}}]}
{"id": "18634877_0", "paragraph": "[BOS] Existing approaches for cross-lingual dependency parsing can be divided into three categories: crosslingual annotation projection methods, jointly modeling methods and cross-lingual representation learning methods.\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "2204967_3", "paragraph": "[BOS] Global optimization and normalization has been successfully applied on many NLP tasks that involve structural prediction (Lafferty et al., 2001; Collins, 2002; McDonald et al., 2010; Zhang and Clark, 2011) , using traditional discrete features.\n[BOS] For neural models, it has recently received increasing interests (Zhou et al., 2015; Andor et al., 2016; Xu, 2016; Wiseman and Rush, 2016) , and improved performances can be achieved with global optimization accompanied by beam search.\n[BOS] Our work is in line with these efforts.\n[BOS] To our knowledge, we are the first to apply globally optimized neural models for end-to-end relation extraction, achieving the best results on standard benchmarks.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 17, "token_end": 46, "char_start": 105, "char_end": 211, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lafferty et al., 2001;": "277918", "Collins, 2002;": null, "McDonald et al., 2010;": "7747592", "Zhang and Clark, 2011)": "7245369"}}}, {"token_start": 53, "token_end": 91, "char_start": 257, "char_end": 395, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhou et al., 2015;": "17887856", "Andor et al., 2016;": "2952144", "Xu, 2016;": "7723328", "Wiseman and Rush, 2016)": "2783746"}}}]}
{"id": "2204967_2", "paragraph": "[BOS] LSTM features have been extensively exploited for NLP tasks, including tagging Lample et al., 2016) , parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016) , relation classification Vu et al., 2016; Miwa and Bansal, 2016) and sentiment analysis .\n[BOS] Based on the output of LSTM structures, Wang and Chang (2016) introduce segment features, and apply it to dependency parsing.\n[BOS] The same method is applied to constituent parsing by Cross and Huang (2016) .\n[BOS] We exploit this segmental representation for relation extraction.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 15, "token_end": 24, "char_start": 77, "char_end": 105, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Lample et al., 2016)": "6042994"}}}, {"token_start": 25, "token_end": 45, "char_start": 108, "char_end": 173, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kiperwasser and Goldberg, 2016;": "1642392", "Dozat and Manning, 2016)": "7942973"}}}, {"token_start": 46, "token_end": 63, "char_start": 176, "char_end": 239, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Vu et al., 2016;": "17297069", "Miwa and Bansal, 2016)": "2476229"}}}, {"token_start": 68, "token_end": 94, "char_start": 271, "char_end": 396, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wang and Chang (2016)": "9289495"}, "Reference": {}}}, {"token_start": 101, "token_end": 110, "char_start": 433, "char_end": 478, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Cross and Huang (2016)": "15407650"}}}]}
{"id": "2204967_1", "paragraph": "[BOS] Several studies find that extracting entities and relations jointly can benefit both tasks.\n[BOS] Early work conducts joint inference for separate models (Ji and Grishman, 2005; Yih, 2004, 2007) .\n[BOS] Recent work shows that joint learning and decoding with a single model brings more benefits for the two tasks (Li and Ji, 2014; Miwa and Sasaki, 2014; Miwa and Bansal, 2016; , and we follow this line of work in the study.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 17, "token_end": 42, "char_start": 104, "char_end": 200, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ji and Grishman, 2005;": "5654268"}}}, {"token_start": 48, "token_end": 85, "char_start": 232, "char_end": 381, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Li and Ji, 2014;": "20744", "Miwa and Sasaki, 2014;": "955518"}}}]}
{"id": "2204967_0", "paragraph": "[BOS] Entity recognition (Florian et al., 2004 (Florian et al., , 2006 Ratinov and Roth, 2009; Florian et al., 2010; Kuru et al., 2016) and relation extraction (Zhao and Grishman, 2005; Jiang and Zhai, 2007; Zhou et al., 2007; Qian and Zhou, 2010; Chan and Roth, 2010; Sun et al., 2011; Plank and Moschitti, 2013; Verga et al., 2016) have received much attention in the NLP community.\n[BOS] The dominant methods treat the two tasks separately, where relation extraction is performed assuming that entity boundaries have been given (Zelenko et al., 2003; Miwa et al., 2009; Chan and Roth, 2011; Lin et al., 2016) .\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 2, "token_end": 44, "char_start": 6, "char_end": 135, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Florian et al., 2004": null, "(Florian et al., , 2006": "3210281", "Ratinov and Roth, 2009;": "1859014", "Florian et al., 2010;": "14191468", "Kuru et al., 2016)": "18057757"}}}, {"token_start": 45, "token_end": 107, "char_start": 140, "char_end": 333, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhao and Grishman, 2005;": "5273348", "Jiang and Zhai, 2007;": "17069935", "Zhou et al., 2007;": "8835255", "Qian and Zhou, 2010;": "15227710", "Chan and Roth, 2010;": "5702359", "Sun et al., 2011;": "15013932", "Plank and Moschitti, 2013;": "3011134", "Verga et al., 2016)": "9206785"}}}, {"token_start": 134, "token_end": 170, "char_start": 497, "char_end": 611, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zelenko et al., 2003;": "195717148", "Miwa et al., 2009;": "1923592", "Chan and Roth, 2011;": "1491402", "Lin et al., 2016)": "397533"}}}]}
{"id": "17533111_2", "paragraph": "[BOS] Finally, domain adaptation and transfer learning have been studied extensively for various NLP tasks, including part of speech tagging (Blitzer et al., 2006) , name tagging (Daume III, 2007 ), parsing (McClosky et al., 2010 , relation extraction (Plank and Moschitti, 2013; Nguyen and Grishman, 2014; Nguyen et al., 2015a) , to name a few.\n[BOS] For event extraction, Miwa et al. (2013) study instance weighting and stacking models while Riedel and McCallum (2011b) examine joint models with domain adaptation.\n[BOS] However, none of them studies the new type extension setting for ED using neural networks like we do.\n\n", "discourse_tags": ["Narrative_cite", "Multi_summ", "Transition"], "span_citation_mapping": [{"token_start": 20, "token_end": 33, "char_start": 118, "char_end": 163, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Blitzer et al., 2006)": "15978939"}}}, {"token_start": 34, "token_end": 42, "char_start": 166, "char_end": 195, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Daume III, 2007": "5360764"}}}, {"token_start": 44, "token_end": 54, "char_start": 199, "char_end": 229, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 55, "token_end": 83, "char_start": 232, "char_end": 328, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Plank and Moschitti, 2013;": "3011134", "Nguyen and Grishman, 2014;": "18895336", "Nguyen et al., 2015a)": "7333692"}}}, {"token_start": 90, "token_end": 108, "char_start": 352, "char_end": 437, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Miwa et al. (2013)": "16250626"}, "Reference": {}}}, {"token_start": 109, "token_end": 126, "char_start": 444, "char_end": 516, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Riedel and McCallum (2011b)": "10743051"}, "Reference": {}}}]}
{"id": "17533111_1", "paragraph": "[BOS] The application of neural networks to EE is very recent.\n[BOS] In particular, Zhou et al. (2014) and Boros et al. (2014) use neural networks to learn word embeddings from a corpus of specific domains and then directly utilize these embeddings as features in statistical classifiers.\n[BOS] Chen et al. (2015) apply dynamic multi-pooling CNNs for EE in a pipelined framework, while Nguyen et al. (2016) propose joint event extraction using recurrent neural networks.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Multi_summ"], "span_citation_mapping": [{"token_start": 17, "token_end": 58, "char_start": 84, "char_end": 288, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhou et al. (2014)": "2076692", "Boros et al. (2014)": "14493443"}, "Reference": {}}}, {"token_start": 59, "token_end": 80, "char_start": 295, "char_end": 378, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Chen et al. (2015)": "14339673"}, "Reference": {}}}, {"token_start": 82, "token_end": 98, "char_start": 386, "char_end": 470, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Nguyen et al. (2016)": "6452487"}, "Reference": {}}}]}
{"id": "17533111_0", "paragraph": "[BOS] Early research on event extraction has primarily focused on local sentence-level representations in a pipelined architecture (Grishman et al., 2005; Ahn, 2006) .\n[BOS] Afterward, higher level features have been found to improve the performance (Ji and Grishman, 2008; Gupta and Ji, 2009; Patwardhan and Riloff, 2009; Liao and Grishman, 2010; Liao and Grishman, 2011; Hong et al., 2011; McClosky et al., 2011; Huang and Riloff, 2012; Li et al., 2013) .\n[BOS] Some recent research has proposed joint models for EE, including the methods based on Markov Logic Networks (Riedel et al., 2009; Poon and Vanderwende, 2010; Venugopal et al., 2014) , structured perceptron (Li et al., 2013; Li et al., 2014b) , and dual decomposition (Riedel et al. (2009; 2011b) ).\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 11, "token_end": 36, "char_start": 66, "char_end": 165, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Grishman et al., 2005;": null, "Ahn, 2006)": null}}}, {"token_start": 41, "token_end": 125, "char_start": 185, "char_end": 455, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ji and Grishman, 2008;": "1320606", "Gupta and Ji, 2009;": "8336242", "Patwardhan and Riloff, 2009;": "2524712", "Liao and Grishman, 2010;": "11187670", "Liao and Grishman, 2011;": "15865939", "Hong et al., 2011;": "2867611", "McClosky et al., 2011;": "2941631", "Huang and Riloff, 2012;": "6644751", "Li et al., 2013)": "2114517"}}}, {"token_start": 142, "token_end": 172, "char_start": 550, "char_end": 645, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Riedel et al., 2009;": "475213", "Poon and Vanderwende, 2010;": "1160159", "Venugopal et al., 2014)": "8247565"}}}, {"token_start": 173, "token_end": 192, "char_start": 648, "char_end": 705, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Li et al., 2013;": "2114517", "Li et al., 2014b)": "15552794"}}}, {"token_start": 194, "token_end": 208, "char_start": 712, "char_end": 759, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Riedel et al. (2009;": "475213", "2011b)": "10743051"}}}]}
{"id": "18824729_0", "paragraph": "[BOS] The Bible has been used as a resource for machine translation and multi-lingual information retrieval before, e.g., (Chew et al., 2006) .\n[BOS] It has also been used in cross-lingual POS tagging Fossum and Abney, 2005) , NP-chunking ) and cross-lingual dependency parsing (Sukhareva and Chiarcos, 2014) before.\n[BOS] and Fossum and Abney (2005) use word-aligned parallel translations of the Bible to project the predictions of POS taggers for several language pairs, including English, German, and Spanish to Czech and French.\n[BOS] The resulting annotated target language corpora enable them to train POS taggers for these languages.\n[BOS] showed similar results using just the Hansards corpus on English to French and Chinese.\n[BOS] Our work is inspired by these approaches, yet broader in scope on both the source and target side.\n[BOS] use word-aligned text to automatically create type-level tag dictionaries.\n[BOS] Earlier work on building tag dictionaries from word-aligned text includes Probst (2003) .\n[BOS] Their tag dictionaries contain target language trigrams to be able to disambiguate ambiguous target language words.\n[BOS] To handle the noise in the automatically obtained dictionaries, they use label propagation on a similarity graph to smooth and expand the label distributions.\n[BOS] Our approach is similar to theirs in using projections to obtain type-level tag dictionaries, but we keep the token supervision and type supervision apart and end up with a model more similar to that of Tckstrm et al. (2013) , who combine word-aligned text with crowdsourced type-level tag dictionaries.\n[BOS] Tckstrm et al. (2013) constrain Viterbi search via type-level tag dictionaries, pruning all tags not licensed by the dictionary.\n[BOS] For the remaining tags, they use high-confidence word alignments to further prune the Viterbi search.\n[BOS] We follow Tckstrm et al. (2013) in using our automatically created, not crowdsourced, tag dictionaries to prune tags during search, but we use word alignments to obtain token-level annotations that we use as annotated training data, similar to Fossum and Abney (2005), , and .\n[BOS] Duong et al. (2013) use word-alignment probabilities to select training data for their cross-lingual POS models.\n[BOS] They consider a simple single-source training set-up.\n[BOS] We also tried ranking projected training data by confidence, using an ensemble of projections from 17-99 source languages and majority voting to obtain probabilities for the token-level target-language projections, but this did not lead to improvements on the English development data.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Single_summ", "Single_summ", "Transition", "Reflection", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 12, "token_end": 37, "char_start": 48, "char_end": 141, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chew et al., 2006)": "2445242"}}}, {"token_start": 45, "token_end": 59, "char_start": 175, "char_end": 224, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Fossum and Abney, 2005)": "17868260"}}}, {"token_start": 60, "token_end": 84, "char_start": 227, "char_end": 308, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sukhareva and Chiarcos, 2014)": null}}}, {"token_start": 88, "token_end": 149, "char_start": 327, "char_end": 640, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Fossum and Abney (2005)": "17868260"}, "Reference": {}}}, {"token_start": 204, "token_end": 273, "char_start": 927, "char_end": 1303, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Probst (2003)": "14869214"}, "Reference": {}}}, {"token_start": 274, "token_end": 398, "char_start": 1310, "char_end": 1856, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"T\u00e4ckstr\u00f6m et al. (2013)": "14760908"}, "Reference": {}}}, {"token_start": 399, "token_end": 416, "char_start": 1863, "char_end": 1929, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"T\u00e4ckstr\u00f6m et al. (2013)": "14760908"}}}, {"token_start": 451, "token_end": 461, "char_start": 2096, "char_end": 2130, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 466, "token_end": 505, "char_start": 2146, "char_end": 2318, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Duong et al. (2013)": "992326"}, "Reference": {}}}]}
{"id": "17201701_2", "paragraph": "[BOS] Recently, NN-based approaches have achieved improvement for several NLP tasks.\n[BOS] For example, in transition-based parsing, Chen and Manning (2014) proposed an NN-based approach, where the words, POS tags, and dependency labels are first represented by embeddings individually.\n[BOS] Then, an NN-based classifier is built to make parsing decisions, where an input layer is a concatenation of embeddings of words, POS tags, and dependency labels.\n[BOS] This model has been extended by several studies (Weiss et al., 2015; .\n[BOS] In semantic role labeling, Zhou and Xu (2015) propose an end-to-end approach using recurrent NN, where an original text is the input, and semantic role labeling is performed without any intermediate syntactic knowledge.\n[BOS] Following these approaches, this paper proposes an NN-based PAS method.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 18, "token_end": 108, "char_start": 91, "char_end": 528, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Chen and Manning (2014)": "11616343"}, "Reference": {}}}, {"token_start": 111, "token_end": 154, "char_start": 538, "char_end": 757, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhou and Xu (2015)": "12688069"}, "Reference": {}}}]}
{"id": "17201701_1", "paragraph": "[BOS] Most methods for PAS analysis handle both intra-sentential and inter-sentential zero anaphora.\n[BOS] For identifying inter-sentential zero anaphora, an antecedent has to be searched in a broad search space, and the salience of discourse entities has to be captured.\n[BOS] Therefore, the task of identifying inter-sentential zero anaphora is more difficult than that of intra-sentential zero anaphora.\n[BOS] Thus, Ouchi et al. (2015) and Iida et al. (2015) focused on only intra-sentential zero anaphora.\n[BOS] Following this trend, this paper focuses on intra-sentential zero anaphora.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Multi_summ", "Reflection"], "span_citation_mapping": [{"token_start": 85, "token_end": 115, "char_start": 413, "char_end": 509, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ouchi et al. (2015)": "5698849", "Iida et al. (2015)": "9928714"}, "Reference": {}}}]}
{"id": "17201701_0", "paragraph": "[BOS] Several methods for Japanese PAS analysis have been proposed.\n[BOS] The methods can be divided into three types: (i) identifying one case argument independently per predicate (Taira et al., 2008; Imamura et al., 2009; Hayashibe et al., 2011) , (ii) identifying all the three case arguments (NOM, ACC, and DAT) simultaneously per predicate (Sasano and Kurohashi, 2011; Hangyo et al., 2013) , and (iii) identifying all case arguments of all predicates in a sentence (Ouchi et al., 2015) .\n[BOS] The third method can capture interactions between predicates and their arguments, and thus performs the best among the three types.\n[BOS] This method is adopted as our base model (see Section 3 for details).\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 25, "token_end": 58, "char_start": 123, "char_end": 247, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Taira et al., 2008;": "15825278", "Imamura et al., 2009;": "14712610", "Hayashibe et al., 2011)": "16081495"}}}, {"token_start": 62, "token_end": 97, "char_start": 255, "char_end": 394, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sasano and Kurohashi, 2011;": "13931451", "Hangyo et al., 2013)": "5797690"}}}, {"token_start": 102, "token_end": 121, "char_start": 407, "char_end": 490, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ouchi et al., 2015)": "5698849"}}}]}
{"id": "1788948_0", "paragraph": "[BOS] As can be seen in Figure 3 , the average score for each compound can be reasonably approximated by the individual scores of head and modifier.\n[BOS] Considering the goodness of fit measures R 2 geom and R 2 arith (for arithmetic and geometric means), we can see that the geometric model better represents the data.\n[BOS] Whenever annotators judged an element of the compound as too idiomatic, they have also rated the whole compound as highly idiomatic.\n[BOS] Figure 4 presents the standard deviation for each compound as a function of its average scores.\n[BOS] One can visually attest that the least consensual compound judgments fall in the middle section of the graph.\n[BOS] Even if we account for the fact that the extremities cannot follow a two-tailed distribution, those compounds still end up being easier than the ones in the middle.\n\n", "discourse_tags": ["Other", "Other", "Other", "Other", "Other", "Other"], "span_citation_mapping": []}
{"id": "19247366_4", "paragraph": "[BOS] Pioneering work by Mima et al. (1997) Table 4 : Test BLEU on the Europarl corpus.\n[BOS] Scores significantly (p < 0.05) better than the baseline are written in bold rule based MT systems.\n[BOS] In the context of datadriven systems, previous work has treated specific traits such as politeness or gender as a \"domain\" in domain adaptation models and applied adaptation techniques such as adding a \"politeness tag\" to moderate politeness (Sennrich et al., 2016a) , or doing data selection to create genderspecific corpora for training (Rabinovich et al., 2017) .\n[BOS] The aforementioned methods differ from ours in that they require explicit signal (gender, politeness.\n[BOS] .\n[BOS] . )\n[BOS] for which labeling (manual or automatic) is needed, and also handle a limited number of \"domains\" ( 2), where our method only requires annotation of the speaker, and must scale to a much larger number of \"domains\" ( 1, 800).\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Narrative_cite", "Transition", "Transition", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 2, "token_end": 50, "char_start": 6, "char_end": 193, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Mima et al. (1997)": "15464096"}, "Reference": {}}}, {"token_start": 86, "token_end": 108, "char_start": 393, "char_end": 466, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sennrich et al., 2016a)": "845121"}}}, {"token_start": 110, "token_end": 131, "char_start": 472, "char_end": 564, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rabinovich et al., 2017)": "11349626"}}}]}
{"id": "19247366_3", "paragraph": "[BOS] Domain adaptation techniques for MT often rely on data selection (Moore and Lewis, 2010; Li et al., 2010; Wang et al., 2017) , tuning (Luong and Manning, 2015; Miceli Barone et al., 2017) , or adding domain tags to NMT input (Chu et al., 2017) .\n[BOS] There are also methods that fine-tune parameters of the model on each sentence in the test set (Li et al., 2016) , and methods that adapt based on human post-edits (Turchi et al., 2017) , although these follow our baseline adaptation strategy of tuning all parameters.\n[BOS] There are also partial update methods for transfer learning, albeit for the very different task of transfer between language pairs (Zoph et al., 2016) .\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 6, "token_end": 33, "char_start": 39, "char_end": 130, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Moore and Lewis, 2010;": "8170227", "Li et al., 2010;": "15594009", "Wang et al., 2017)": "1054586"}}}, {"token_start": 34, "token_end": 54, "char_start": 133, "char_end": 193, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Luong and Manning, 2015;": null, "Miceli Barone et al., 2017)": "6552599"}}}, {"token_start": 56, "token_end": 71, "char_start": 199, "char_end": 249, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chu et al., 2017)": "35273027"}}}, {"token_start": 86, "token_end": 100, "char_start": 323, "char_end": 370, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Li et al., 2016)": "11919498"}}}, {"token_start": 102, "token_end": 121, "char_start": 377, "char_end": 443, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Turchi et al., 2017)": "49235047"}}}, {"token_start": 154, "token_end": 165, "char_start": 649, "char_end": 683, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zoph et al., 2016)": "16631020"}}}]}
{"id": "19247366_2", "paragraph": "[BOS] The results are reported in Figure 2 .\n[BOS] As can be seen from the figure, it is easier to predict the author of a sentence from the output of speakerspecific models than from the baseline.\n[BOS] This demonstrates that explicitly incorporating information about the author of a sentence allows for better transfer of personal traits during translations, although the difference from the ground truth demonstrates that this problem is still far from solved.\n[BOS] Appendix D shows qualitative examples of our model improving over the baseline.\n\n", "discourse_tags": ["Other", "Other", "Other", "Other"], "span_citation_mapping": []}
{"id": "19247366_1", "paragraph": "[BOS] However, BLEU is not a perfect evaluation metric.\n[BOS] In particular, we are interested in evaluating how much of the personal traits of each speaker our models capture.\n[BOS] To gain more insight into this aspect of the MT results, we devise a simple experiment.\n[BOS] For every language pair, we train a classifier (continuous bag-of-n-grams; details in Appendix C) to predict the author of each sentence on the target language part of the training set.\n[BOS] We then evaluate the classifier on the ground truth and the outputs from our 3 models (base, full bias and fact bias).\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "19247366_0", "paragraph": "[BOS] variation improve translation quality?\n[BOS] Table 3 shows final test scores for each model with statistical significance measured with paired boot-5 Recent NMT systems also commonly use sub-word units (Sennrich et al., 2016b) .\n[BOS] This may influence on the result, either negatively (less direct control over highfrequency words) or positively (more capacity to adapt to high-frequency words).\n[BOS] We leave a careful examination of these effects for future work.\n[BOS] Table 3 : Test BLEU.\n[BOS] Scores significantly (p < 0.05) better than the baseline are written in bold strap resampling (Koehn, 2004) .\n[BOS] As shown in the table, both proposed methods give significant improvements in BLEU score, with the biggest gains in English to French (+0.99) and smaller gains in German and Spanish (+0.74 and +0.40 respectively).\n[BOS] Reducing the number of parameters with fact bias gives slightly better (en-fr) or worse (en-de) BLEU score, but in those cases the results are still significantly better than the baseline.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Reflection", "Other", "Narrative_cite", "Other", "Transition"], "span_citation_mapping": []}
{"id": "20090034_2", "paragraph": "[BOS] All previous studies on word alignment have assumed that word alignments are untyped.\n[BOS] To our knowledge, the alignment types for word alignment provided by the LDC as annotations on word alignment links, have never been used to improve word alignment.\n[BOS] Our work differs from the previous works as it proposes a new task of jointly predicting word alignment and alignment types.\n[BOS] A semisupervised learning algorithm is presented to solve this task.\n[BOS] Our method is semi-supervised as it combines LDC data, which is annotated with alignment and alignment types, with sentence aligned (but not word aligned) data from the HK Hansards corpus.\n[BOS] Our generative algorithm makes use of the gold alignment and alignment types data to initialize the alignment type parameters.\n[BOS] The EM training is then used to re-estimate the parameters of the model in an unsupervised manner.\n[BOS] We also use POS tags to smooth the alignment type parameters, unlike the approach in (Toutanova et al., 2002) .\n\n", "discourse_tags": ["Transition", "Reflection", "Reflection", "Reflection", "Reflection", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 181, "token_end": 196, "char_start": 970, "char_end": 1017, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Toutanova et al., 2002)": null}}}]}
{"id": "20090034_1", "paragraph": "[BOS] Among the unsupervised methods, (Toutanova et al., 2002) utilizes additional source of information apart from the parallel sentences.\n[BOS] Part-of-speech tags of the words in the sentence pair are incorporated as a linguistic constraint on the HMM-based word alignment.\n[BOS] The part-of-speech tag translation probabilities in this model are then learned along with other probabilities using the EM algorithm.\n[BOS] POS tags as used in Toutanova et al. (2002) were also utilized to act similarly to word classes in (Och and Ney, 2000a; Och and Ney, 2000b) ; however, the improvements provided by the HMM with POS tag model over HMM alignment model of Och and Ney (2000b) was for small training data sizes (<50K parallel corpus).\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 82, "char_start": 6, "char_end": 417, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Toutanova et al., 2002)": null}, "Reference": {}}}, {"token_start": 83, "token_end": 165, "char_start": 424, "char_end": 736, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Toutanova et al. (2002)": null}, "Reference": {"(Och and Ney, 2000a;": null, "Och and Ney, 2000b)": "5284722", "Och and Ney (2000b)": "5284722"}}}]}
{"id": "20090034_0", "paragraph": "[BOS] There has been several studies on semi-supervised word alignment models.\n[BOS] Callison-Burch et al. (2004) improve alignment and translation quality by interpolating hand-annotated, word-aligned data and automatic sentence-aligned data.\n[BOS] They showed 4 We should note that these incorrectly predicted alignments are only kept out of the confusion matrix.\n[BOS] All alignments, correct or incorrect, are included in all the results we show in the other tables.\n[BOS] that a much higher weight should be assigned to the model trained on word-aligned data.\n[BOS] Fraser and Marcu (2006) propose a semi-supervised training approach to word alignment, based on IBM Model 4, that alternates the EM step which is applied on a large training corpus with a discriminative error training step on a small hand-annotated sub-511 corpus.\n[BOS] The alignment problem is viewed as a search problem over a log-linear space with features (submodels) coming from the IBM Model 4.\n[BOS] In the proposed algorithm, discriminative training controls the contribution of sub-models while an EM-like procedure is used to estimate the sub-model parameters.\n[BOS] Unlike previous approaches (Och and Ney, 2003; Fraser and Marcu, 2006; Fraser and Marcu, 2007) that use discriminative methods to tune the weights of generative models, Gao et al. (2010b) proposes a semi-supervised word alignment technique that integrates discriminative and generative methods.\n[BOS] They propose to use a discriminative word aligner to produce high precision partial alignments that can serve as constraints for the EM algorithm.\n[BOS] The discriminative word aligner uses the generative aligner's output as features.\n[BOS] This feedback loop iteratively improves the quality of both aligners.\n[BOS] Niehues and Vogel (2008) propose a discriminative model that directly models the alignment matrix.\n[BOS] Although the discriminative model provides the flexibility to use manually word-aligned data to tune its weights, it still relies on the model parameters of IBM models and alignment links from GIZA++ as features.\n[BOS] Gao et al. (2010a) present a semi-supervised algorithm that extends IBM Model 4 by using partial manual alignments.\n[BOS] Partial alignments are fixed and treated as constraints into the EM training.\n[BOS] DeNero and Klein (2010) present a supervised model for extracting phrase pairs under a discriminative model by using word alignments.\n[BOS] They consider two types of alignment links, sure and possible, that are extracted from the manually word-aligned data.\n[BOS] Possible alignment links dictate which phrase pairs can be extracted from a sentence pair.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Transition", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 16, "token_end": 71, "char_start": 85, "char_end": 365, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Callison-Burch et al. (2004)": "13169626"}, "Reference": {}}}, {"token_start": 112, "token_end": 230, "char_start": 571, "char_end": 1142, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Fraser and Marcu (2006)": null}, "Reference": {}}}, {"token_start": 231, "token_end": 349, "char_start": 1149, "char_end": 1760, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Gao et al. (2010b)": "5161367"}, "Reference": {"(Och and Ney, 2003;": "5219389", "Fraser and Marcu, 2006;": null, "Fraser and Marcu, 2007)": "7433743"}}}, {"token_start": 350, "token_end": 410, "char_start": 1767, "char_end": 2084, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Niehues and Vogel (2008)": "2727312"}, "Reference": {}}}, {"token_start": 411, "token_end": 450, "char_start": 2091, "char_end": 2290, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Gao et al. (2010a)": "10109731"}, "Reference": {}}}, {"token_start": 451, "token_end": 499, "char_start": 2297, "char_end": 2555, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "17886046_1", "paragraph": "[BOS] Bayesian SCFG models can induce a compact model by incorporating sophisticated nonparametric Bayesian models for an SCFG, such as a dirichlet process (DeNero et al., 2008; Blunsom et al., 2009; Chung et al., 2014) or Pitman-Yor process (Levenberg et al., 2012; Peng and Gildea, 2014) .\n[BOS] A model is learned by sampling derivation trees in a parallel corpus and by accumulating the rules in the sampled trees into the model.\n[BOS] Due to the O(|f | 3 |e| 3 ) time complexity for bi-parsing a bilingual sentence, previous studies relied on biparsing at the initialization step, and conducted Gibbs sampling by local operators (Blunsom et al., 2009; Levenberg et al., 2012) or sampling on fixed word alignments (Chung et al., 2014; Peng and Gildea, 2014) .\n[BOS] As a result, the inference can easily result in local optimum, wherein induced derivation trees may strongly depend on the initial trees.\n[BOS] Xiao et al. (2012) proposed a two-step approach for bi-parsing a bilingual sentence in O(|f | 3 ) in the context of inducing SCFG rules discriminatively; however, their approach violates the detailed balance due to its heuristic k-best pruning.\n[BOS] Blunsom and Cohn (2010) proposed a slice sampling for an SCFG, in the same manner as that for Infinite Hiden Markov Model (iHMM) (Van Gael et al., 2008) , which can efficiently prune a space of possible derivations on the basis of dynamic programming.\n[BOS] Although slice sampling can prune spans without violating the detailed balance, its time complexity of O(|f | 3 |e| 3 ) is still impractical for a large-scale experiment.\n[BOS] We efficiently carried out large-scale experiments on the basis of the two-step bi-parsing of Xiao et al. combined with slice sampling of Blunsom and Cohn.\n[BOS] After learning a Bayesian model, it is not directly used in a decoder since it is composed of only minimum rules without considering phrases of various granularities.\n[BOS] As a consequence, it is a standard practice to obtain word alignment from derivation trees and to extract SCFG rules heuristically from the word-aligned data (Cohn and Haffari, 2013) .\n[BOS] The work by Neubig et al. (2011) was the first attempt to directly use the learned model on the basis of a Bayesian ITG in which phrases of many granularities were encoded in the model by employing a hierarchical back-off procedure.\n[BOS] Our work is strongly motivated by their work, but greatly differs in that our model can incorporate many arbitrary Hiero rules, not limited to ITGstyle binary branching rules.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Narrative_cite", "Transition", "Single_summ", "Single_summ", "Transition", "Reflection", "Transition", "Narrative_cite", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 25, "token_end": 52, "char_start": 138, "char_end": 219, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(DeNero et al., 2008;": "303981", "Blunsom et al., 2009;": "1734281", "Chung et al., 2014)": "7992141"}}}, {"token_start": 53, "token_end": 76, "char_start": 223, "char_end": 289, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Levenberg et al., 2012;": "18893263", "Peng and Gildea, 2014)": "459711"}}}, {"token_start": 141, "token_end": 165, "char_start": 590, "char_end": 680, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Blunsom et al., 2009;": "1734281", "Levenberg et al., 2012)": "18893263"}}}, {"token_start": 166, "token_end": 187, "char_start": 684, "char_end": 761, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chung et al., 2014;": "7992141", "Peng and Gildea, 2014)": "459711"}}}, {"token_start": 215, "token_end": 272, "char_start": 914, "char_end": 1158, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Xiao et al. (2012)": "7926547"}, "Reference": {}}}, {"token_start": 273, "token_end": 335, "char_start": 1165, "char_end": 1416, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Van Gael et al., 2008)": "5903376"}}}, {"token_start": 470, "token_end": 484, "char_start": 2075, "char_end": 2117, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cohn and Haffari, 2013)": "443309"}}}, {"token_start": 486, "token_end": 536, "char_start": 2126, "char_end": 2358, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Neubig et al. (2011)": "2906863"}, "Reference": {}}}]}
{"id": "17886046_0", "paragraph": "[BOS] Various criteria have been proposed to prune a phrase table without decreasing translation quality, e.g., Fisher's exact test (Johnson et al., 2007) or relative entropy (Ling et al., 2012; Zens et al., 2012) .\n[BOS] Although those methods are easily applied for pruning a rule table, they heavily rely on the heuristically determined threshold parameter to trade off the translation quality and decoding speed of an MT system.\n[BOS] Previously, EM-algorithm based generative models were exploited for generating compact phrase and rule tables.\n[BOS] Joint phrase alignment model (Marcu and Wong, 2002) can directly express many-to-many word aligments without heuristic phrase extraction.\n[BOS] DeNero et al. (2006) proposed IBM Model 3 based many-to-many alignment model.\n[BOS] Rule arithmetic method (Cmejrek and Zhou, 2010) can generate SCFG rules by combining other rule pairs through an insideoutside algorithm.\n[BOS] However, those previous attempts were restricted in that the rules and phrases were induced by heuristic combination.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Single_summ", "Single_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 23, "token_end": 36, "char_start": 112, "char_end": 154, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Johnson et al., 2007)": "12131372"}}}, {"token_start": 37, "token_end": 55, "char_start": 158, "char_end": 213, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ling et al., 2012;": "2604501", "Zens et al., 2012)": "12951248"}}}, {"token_start": 113, "token_end": 142, "char_start": 556, "char_end": 693, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Marcu and Wong, 2002)": "1567400"}, "Reference": {}}}, {"token_start": 143, "token_end": 164, "char_start": 700, "char_end": 777, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"DeNero et al. (2006)": "503611"}, "Reference": {}}}, {"token_start": 165, "token_end": 194, "char_start": 784, "char_end": 921, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "211893_3", "paragraph": "[BOS] Current anaphora resolution methods rely mainly on constraint and preference heuristics, which employ morpho-syntactic information or shallow semantic analysis.\n[BOS] These methods are a deterministic algorithm which always produces the same output in a given particular condition.\n[BOS] However, even if the condition is applied, the output can be wrong.\n[BOS] ML methods which are a non-deterministic algorithm have been studied on anaphora resolution (Connolly et al., 1994; Paul et al., 1999) .\n[BOS] Since ML learns from data and makes predictions of the most likely candidate on the data, it can overcome the limitation of the deterministic method.\n[BOS] Park and Hong (2014) proposed a hybrid approach to resolve Spanish zero subjects that integrates heuristic rules and ML in the context of Spanish to Korean MT.\n[BOS] Since Spanish zero subjects can be restored from the verb ending, they use morphological flections for verbs.\n[BOS] After that, ML is utilized for some ambiguous cases.\n[BOS] Unlike this work, our work deals with Korean zero object.\n[BOS] Morphological information cannot be utilized for Korean because of the difference of the two languages.\n[BOS] For this reason, we use ML method alone to determine the antecedent of the zero objects in spoken Korean.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Narrative_cite", "Transition", "Single_summ", "Single_summ", "Single_summ", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 76, "token_end": 95, "char_start": 440, "char_end": 502, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Paul et al., 1999)": "18404957"}}}, {"token_start": 125, "token_end": 189, "char_start": 667, "char_end": 1001, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Park and Hong (2014)": null}, "Reference": {}}}]}
{"id": "211893_2", "paragraph": "[BOS] Centering theory (Grosz et al., 1995) is one of the approaches using heuristic rules.\n[BOS] It is claimed that certain entities mentioned in an utterance are more central than others, and this property has been applied to determine the antecedent of the anaphor.\n[BOS] Walker et al. (1994) applied the centering model on zero pronoun resolution in Japanese.\n[BOS] Roh and Lee (2003) proposed a generation algorithm of zero pronouns using a Cost-based Centering Model which considers the inference cost.\n[BOS] It is known that the most salient element of the given discourse is likely to be realized as a zero pronoun.\n[BOS] We take this into account in selecting the features for ML.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 55, "char_start": 6, "char_end": 268, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Grosz et al., 1995)": "11660053"}, "Reference": {}}}, {"token_start": 56, "token_end": 75, "char_start": 275, "char_end": 363, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Walker et al. (1994)": "1141127"}, "Reference": {}}}, {"token_start": 76, "token_end": 128, "char_start": 370, "char_end": 623, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Roh and Lee (2003)": "13133422"}, "Reference": {}}}]}
{"id": "211893_1", "paragraph": "[BOS] Constraints discard possible antecedents and are considered as absolute criteria.\n[BOS] Preferences being proposed as heuristic rules tend to be relative.\n[BOS] After applying constraints, if there are still unresolved candidate antecedents, preferences set priorities among candidate antecedents.\n[BOS] Nakaiwa and Shirai (1996) focus on semantic and pragmatic constraints such as cases, modal expressions, verbal semantic attributes and conjunctions in order to determine the reference of Japanese zero pronouns.\n[BOS] However, they proposed constraints focusing on zero subjects mainly.\n[BOS] Therefore, it is hard to apply their approach on zero object resolution.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 49, "token_end": 116, "char_start": 310, "char_end": 674, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Nakaiwa and Shirai (1996)": "399277"}, "Reference": {}}}]}
{"id": "211893_0", "paragraph": "[BOS] Zero pronouns have already been studied in other languages, such as Japanese (e.g. Nakaiwa and Shirai, 1996; Okumura and Tamura, 1996) and Spanish (Park and Hong, 2014; Palomar et al., 2001; Ferrndez and Peral, 2000) .\n[BOS] These studies are based on the researches about anaphora resolution.\n[BOS] It has been a wide-open research field since 1970 focusing on English.\n[BOS] Regardless of languages, similar strategies for anaphora resolution have been applied.\n[BOS] Using linguistic information is the most representative technique; constraints and preferences methods are distinguished in the related works (Baldwin, 1997; Lappin and Leass, 1994; Carbonell and Brown, 1988) .\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Transition", "Narrative_cite"], "span_citation_mapping": []}
{"id": "18860232_15", "paragraph": "[BOS] Phrase 309 \"AFK\" represents \"Away From Keyboard\".\n\n", "discourse_tags": ["Other"], "span_citation_mapping": []}
{"id": "18860232_14", "paragraph": "[BOS] \" (nie0)\" represents \" (ne0)\" and equals a descriptive exclamation.\n\n", "discourse_tags": ["Other"], "span_citation_mapping": []}
{"id": "18860232_13", "paragraph": "[BOS] Exclamation 9\n\n", "discourse_tags": ["Other"], "span_citation_mapping": []}
{"id": "18860232_12", "paragraph": "[BOS] Adverb 10 \" (fen3)\" represents \" (hen3)\" and means \"very\".\n\n", "discourse_tags": ["Other"], "span_citation_mapping": []}
{"id": "18860232_11", "paragraph": "[BOS] Verb 34 \" (cong1 bai2)\" represents \" (chong3 bai4)\" and means \"adore\".\n\n", "discourse_tags": ["Other"], "span_citation_mapping": []}
{"id": "18860232_10", "paragraph": "[BOS] Adjective 250 \"FB\" represents \" (fu3 bai4)\" and means \"corrupt\".\n\n", "discourse_tags": ["Other"], "span_citation_mapping": []}
{"id": "18860232_9", "paragraph": "[BOS] LG\" represents \" (lao3 gong1)\" and means \"husband\".\n\n", "discourse_tags": ["Other"], "span_citation_mapping": []}
{"id": "18860232_8", "paragraph": "[BOS] \"\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "18860232_7", "paragraph": "[BOS] Noun 29\n\n", "discourse_tags": ["Other"], "span_citation_mapping": []}
{"id": "18860232_6", "paragraph": "[BOS] Pronoun 9 \" \" represents \" \" and means \"I\".\n\n", "discourse_tags": ["Other"], "span_citation_mapping": []}
{"id": "18860232_5", "paragraph": "[BOS] According to the above observations, we propose to employ the existing IE techniques to handle NIL expressions.\n[BOS] Our goal is to develop a NIL expression recognition system to facilitate network-mediated communication.\n[BOS] For this purpose, we first construct the required NIL knowledge resources, namely, a NIL dictionary and n-gram statistical features.\n[BOS] Number 1 \"W\" represents \" (wan4)\" and means \"ten thousand\".\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection", "Other"], "span_citation_mapping": []}
{"id": "18860232_4", "paragraph": "[BOS] Another notable work is the project of \"Normalization of Non-standard Words\" (Sproat et al., 2001 ) which aims to detect and normalize the \"Non-Standard Words (NSW)\" such as digit sequence; capital word or letter sequence; mixed case word; abbreviation; Roman numeral; URL and e-mail address.\n[BOS] In our work, we consider most types of the NSW in English except URL and email address.\n[BOS] Moreover, we consider Chinese NIL expressions that contain same characters as the normal words.\n[BOS] For example, \" \" and \" \" both appear in common dictionaries, but they carry anomalous meanings in NIL text.\n[BOS] Ambiguity arises and basically brings NIL expressions recognition beyond the scope of NSW detection.\n\n", "discourse_tags": ["Single_summ", "Reflection", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 72, "char_start": 6, "char_end": 298, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Sproat et al., 2001": null}, "Reference": {}}}]}
{"id": "18860232_3", "paragraph": "[BOS] \" appears in conventional dictionary with the meaning of Chinese porridge, but in NIL text it represents \" \" which surprisingly represents \"like\".\n[BOS] The issue that concerns us is that these expressions like \" \" may also appear in NIL text with their formal meaning.\n[BOS] This leads to ambiguity and makes it more difficult in NIL processing.\n\n", "discourse_tags": ["Transition", "Transition", "Transition"], "span_citation_mapping": []}
{"id": "18860232_2", "paragraph": "[BOS] From the linguistic perspective, NIL expressions are rather different from named entities in nature.\n[BOS] Firstly, named entity is typically noun or noun phrase (NP), but NIL expression can be any kind, e.g. number \"94\" in NIL represents \" \" which is a verb meaning \"exactly be\".\n[BOS] Secondly, named entities often have well-defined meanings in text and are tractable from a standard dictionary; but NIL expressions are either unknown to the dictionary or ambiguous.\n[BOS] For example, \"\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Transition"], "span_citation_mapping": []}
{"id": "18860232_1", "paragraph": "[BOS] NER is a key technology for NLP applications such as IE and question & answering.\n[BOS] It typically aims to recognize names for person, organization, location, and expressions of number, time and currency.\n[BOS] The objective is achieved by employing either handcrafted knowledge or supervised learning techniques.\n[BOS] The latter is currently dominating in NER amongst which the most popular methods are decision tree (Sekine et al., 1998; Pailouras et al., 2000) , Hidden Markov Model (Zhang et al., 2003; Zhao, 2004) , maximum entropy (Chieu and Ng, 2002; Bender et al., 2003) , and support vector machines (Isozaki and Kazawa, 2002; Takeuchi and Collier, 2002; Mayfield, 2003) .\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 74, "token_end": 95, "char_start": 413, "char_end": 472, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sekine et al., 1998;": "333513", "Pailouras et al., 2000)": null}}}, {"token_start": 96, "token_end": 111, "char_start": 475, "char_end": 527, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhang et al., 2003;": null, "Zhao, 2004)": "507894"}}}, {"token_start": 112, "token_end": 130, "char_start": 530, "char_end": 587, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chieu and Ng, 2002;": "793886", "Bender et al., 2003)": "129969"}}}, {"token_start": 132, "token_end": 158, "char_start": 594, "char_end": 688, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Isozaki and Kazawa, 2002;": "2753152", "Takeuchi and Collier, 2002;": null}}}]}
{"id": "18860232_0", "paragraph": "[BOS] NIL expression recognition, in particular, can be considered as a subtask of information extraction (IE).\n[BOS] Named entity recognition (NER) happens to hold similar objective with NIL expression recognition, i.e. to extract meaningful text segments from unstructured text according to certain predefined criteria.\n\n", "discourse_tags": ["Transition", "Transition"], "span_citation_mapping": []}
{"id": "17154151_0", "paragraph": "[BOS] The existing data selection methods are mostly based on language model.\n[BOS] Yasuda et al. (2008) and Foster et al. (2010) ranked the sentence pairs in the general-domain corpus according to the perplexity scores of sentences, which are computed with respect to in-domain language models.\n[BOS] Axelrod et al. (2011) improved the perplexitybased approach and proposed bilingual crossentropy difference as a ranking function with inand general-domain language models.\n[BOS] Duh et al. (2013) employed the method of (Axelrod et al., 2011) and further explored neural language model for data selection rather than the conventional n-gram language model.\n[BOS] Although previous works in data selection (Duh et al., 2013; Axelrod et al., 2011; Foster et al., 2010; Yasuda et al., 2008) have gained good performance, the methods which only adopt language models to score the sentence pairs are sub-optimal.\n[BOS] The reason is that a sentence pair contains a source language sentence and a target language sentence, while the existing methods are incapable of evaluating the mutual translation probability of sentence pair in the target domain.\n[BOS] Thus, we propose novel methods which are based on translation model and language model for data selection.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Single_summ", "Single_summ", "Narrative_cite", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 15, "token_end": 63, "char_start": 84, "char_end": 295, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Yasuda et al. (2008)": "12973103", "Foster et al. (2010)": "6996688"}, "Reference": {}}}, {"token_start": 64, "token_end": 100, "char_start": 302, "char_end": 473, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Axelrod et al. (2011)": "10766958"}, "Reference": {}}}, {"token_start": 101, "token_end": 142, "char_start": 480, "char_end": 657, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Duh et al. (2013)": "2030497"}, "Reference": {"(Axelrod et al., 2011)": "10766958"}}}, {"token_start": 147, "token_end": 182, "char_start": 691, "char_end": 788, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Duh et al., 2013;": "2030497", "Axelrod et al., 2011;": "10766958", "Foster et al., 2010;": "6996688", "Yasuda et al., 2008)": "12973103"}}}]}
{"id": "16276288_1", "paragraph": "[BOS] Greedy Inference Goldberg and Elhadad (2010) introduced an easy-first, greedy, approach to dependency parsing.\n[BOS] Their algorithm adds at each iteration the best candidate arc, in contrast to the left to right ordering of standard transition based parsers.\n[BOS] This work is extended at (Tratz and Hovy, 2011; Goldberg and Nivre, 2013) .\n[BOS] The easy-first parser consists of a feature set and a specialized variant of the structured perceptron training algorithm, both dedicated to greedy inference.\n[BOS] In contrast, we show that a variant of the TurboParser that employs Algorithm 2 for inference and is trained with its standard global training algorithm, performs very similarly to the same parser that employs dual decomposition inference.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Narrative_cite", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 52, "char_start": 6, "char_end": 265, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Goldberg and Elhadad (2010)": "3146611"}, "Reference": {}}}, {"token_start": 53, "token_end": 74, "char_start": 272, "char_end": 345, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Tratz and Hovy, 2011;": "232249", "Goldberg and Nivre, 2013)": "815755"}}}]}
{"id": "16276288_0", "paragraph": "[BOS] Our work brings together ideas that have been considered in past, although in different forms.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "1661650_1", "paragraph": "[BOS] In (Perera and Ranta, 2007) GF was used for spoken dialogue system grammar localization, porting the English SAMMIE speech recognition grammar 2 to GF and then introducing multilinguality, thus creating GF grammars for English, Finnish, French, German, Spanish and Swedish.\n[BOS] This produced a second German SAMMIE grammar that was compared to the original one, however, the German GF SAMMIE grammar did not match the coverage of the original German SAMMIE grammar.\n[BOS] MedSLT (Buillon et al., 2007 ) is a grammar-based medical speech translation system.\n[BOS] The system supports simple medical examination dialogues about throat pain between an English-speaking physician and a Spanish-speaking patient.\n[BOS] General feature grammar resources from the REGU-LUS toolkit (Rayner et al., 2006) are compiled into flatter, domain specific grammars, translation is realized via an interlingua.\n[BOS] Alignment Based Learning (ABL) by (van Zaanen, 2000) is an unsupervised grammar induction system based on the idea of substitutability.\n[BOS] It can be applied to an untagged corpus of natural language sentences and produces a bracketed version of that corpus.\n[BOS] By clustering and selecting the bracketing hypotheses, a grammar is induced which covers the original corpus of sentences plus more similar sentences.\n[BOS] Our approach is different from both GF and MedSLT in the respect that we do not use resource grammars.\n[BOS] Even though resource grammars and the idea of re-using grammars is attractive, we wanted to implement a simple solution to the localization problem that does not rely on the introduction of a framework that requires ample resources in turn.\n[BOS] Compared to ABL, our approach requires more resources -the generated sentences, their translations, and the source grammar production rules compared to a monolingual text corpus of the target language only.\n[BOS] On the other hand, the grammar which we induce is used for natural language interpretation, whereas ABL can so far only create a grammar that determines if a given sentence is covered by the grammar or not.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 100, "char_start": 6, "char_end": 473, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 101, "token_end": 148, "char_start": 480, "char_end": 715, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 149, "token_end": 188, "char_start": 722, "char_end": 900, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Rayner et al., 2006)": null}, "Reference": {}}}, {"token_start": 189, "token_end": 268, "char_start": 907, "char_end": 1324, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(van Zaanen, 2000)": "1645458"}, "Reference": {}}}]}
{"id": "1661650_0", "paragraph": "[BOS] Making grammars re-usable for new languages is a goal also followed by (Ranta, 2004 ) via a \"Grammatical Frame-work\" (GF), a type-theoretic grammar formalism that addresses four aspects of grammars: multilinguality, semantics, modularity and grammar engineering, and re-use of grammars in different formats and as software components.\n[BOS] In (Johannson, 2006) GF was used to globalize and localize a Swedish grammar for dialogue system utterances to obtain a set of grammars for Swedish, Spanish and English.\n\n", "discourse_tags": ["Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 72, "char_start": 6, "char_end": 340, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Ranta, 2004": null}, "Reference": {}}}, {"token_start": 73, "token_end": 108, "char_start": 347, "char_end": 516, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Johannson, 2006)": null}, "Reference": {}}}]}
{"id": "2147407_3", "paragraph": "[BOS] Our approach is related to multitask systems.\n[BOS] Luong et al. (2016) proposed conjoined translation and autoencoder networks; we use a single shared encoder.\n[BOS] Further work used the same encoder and decoder for multi-way translation (Johnson et al., 2016) .\n[BOS] We have repurposed the idea to inject monolingual text for low-resource NMT.\n[BOS] Their work combined multiple translation directions (e.g. FrenchEnglish, GermanEnglish, and EnglishGerman) into one system.\n[BOS] Our work combines e.g. EnglishEnglish and TurkishEnglish into one system for the purpose of improving TurkishEnglish quality.\n[BOS] They used only parallel data; our goal is to inject monolingual data.\n\n", "discourse_tags": ["Reflection", "Single_summ", "Narrative_cite", "Reflection", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 12, "token_end": 38, "char_start": 58, "char_end": 166, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Luong et al. (2016)": "6954272"}, "Reference": {}}}, {"token_start": 44, "token_end": 60, "char_start": 200, "char_end": 268, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Johnson et al., 2016)": "6053988"}}}]}
{"id": "2147407_2", "paragraph": "[BOS] More recent approaches have used both source and target monolingual data while simultaneously training sourcetarget and targetsource NMT systems.\n[BOS] Cheng et al. (2016) accomplished this by concatenating sourcetarget and targetsource NMT systems to create an autoencoder.\n[BOS] Monolingual data was then introduced by adding an autoencoder objective.\n[BOS] This can be interpreted as back-translation with joint training.\n[BOS] similarly used a small amount of parallel data to pre-train sourcetarget and targetsource NMT systems; they then added monolingual data to the systems by translating a sentence from the monolingual corpus into the other language and then translating it back into the original language, using reinforcement learning with rewards based on the language model score of the translated sentence and the similarity of the reconstructed sentence to the original.\n[BOS] Our approach also employs an autoencoder, but rather than concatenate two NMT systems, we have flattened them into one standard NMT system.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Transition", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 27, "token_end": 69, "char_start": 158, "char_end": 359, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Cheng et al. (2016)": null}, "Reference": {}}}]}
{"id": "2147407_1", "paragraph": "[BOS] In the mirror image of back-translation, Zhang and Zong (2016) added source-side monolingual data to NMT by first translating the source data into the target language using an initial machine translation system and then using this translated data and the original parallel data to train their NMT system.\n[BOS] Our method is orthogonal: it could improve the initial system or be used alongside the translated data in the final system.\n[BOS] They also considered a multitask shared encoder setup where the monolingual source data is used in a sentence reordering task.\n\n", "discourse_tags": ["Single_summ", "Reflection", "Transition"], "span_citation_mapping": [{"token_start": 2, "token_end": 62, "char_start": 6, "char_end": 310, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "2147407_0", "paragraph": "[BOS] Early work on incorporating monolingual data into NMT concentrated on target-side monolingual data.\n[BOS] Jean et al. (2015) and Gulcehre et al. (2015) used a 5-gram language model and a recurrent neural network language model (RNNLM), respectively, to re-rank NMT outputs.\n[BOS] Gulcehre et al. (2015) also integrated a pre-trained RNNLM into NMT by concatenating hidden states.\n[BOS] Sennrich et al. (2016b) added monolingual target data directly to NMT using null source sentences and freezing encoder parameters while training with the monolingual data.\n[BOS] Our method is similar, although instead of using a null source sentence, we use a copy of the target sentence and train the encoder parameters on the copied sentence.\n[BOS] Sennrich et al. (2016b) also created synthetic parallel data by translating target-language monolingual text into the source language.\n[BOS] To perform this process, dubbed back-translation, they first trained an initial targetsource machine translation system on the available parallel data.\n[BOS] They then used this model to translate the monolingual corpus from the target language to the source language.\n[BOS] The resulting back-translated data was combined with the original parallel data and used to train the final sourcetarget NMT system.\n[BOS] Since this back-translation method outperforms previous methods that only train the decoder (Gulcehre et al., 2015; Sennrich et al., 2016b) , we use it as our baseline.\n[BOS] In addition, our method stacks with back-translation in both the targetsource and sourcetarget systems; we can use source text to improve the back-translations and target text to improve the final outputs.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Single_summ", "Single_summ", "Reflection", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 22, "token_end": 69, "char_start": 112, "char_end": 279, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Jean et al. (2015)": null, "Gulcehre et al. (2015)": "15352384"}, "Reference": {}}}, {"token_start": 70, "token_end": 97, "char_start": 286, "char_end": 385, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Gulcehre et al. (2015)": "15352384"}, "Reference": {}}}, {"token_start": 98, "token_end": 133, "char_start": 392, "char_end": 563, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Sennrich et al. (2016b)": "15600925"}, "Reference": {}}}, {"token_start": 167, "token_end": 270, "char_start": 743, "char_end": 1291, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Sennrich et al. (2016b)": "15600925"}, "Reference": {}}}, {"token_start": 282, "token_end": 306, "char_start": 1372, "char_end": 1437, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Gulcehre et al., 2015;": "15352384"}}}]}
{"id": "2389139_7", "paragraph": "[BOS] Here, f is the function that the stacked LSTM applies to the input, y t1 is the last generated target word, andh t1 is the output of previous time step of the input-feeding network itself, meaning the output of Equation 1 in the case that context vector has been computed using e t,i from Equation 6.\n\n", "discourse_tags": ["Other"], "span_citation_mapping": []}
{"id": "2389139_6", "paragraph": "[BOS] The input-feeding model changes the context vector computation in a way that at each step t the context vector is aware of the previously computed context c t1 .\n[BOS] To this end, the input-feeding model feeds back its ownh t1 to the network and uses the resulting hidden state instead of the contextindependent h t , to compare to the hidden states of RWTH data # of sentences 508 # of alignments 10534 % of sure alignments 91% % of possible alignments 9% the encoder.\n[BOS] This is defined in the following equations:\n\n", "discourse_tags": ["Other", "Other", "Other"], "span_citation_mapping": []}
{"id": "2389139_5", "paragraph": "[BOS] Here h t is the hidden state of the decoder at time t, h i is ith hidden state of the encoder and |x| is the length of the source sentence.\n[BOS] Then the computed alignment weights are used to compute a weighted sum over the encoder hidden states which results in the context vector mentioned above:\n\n", "discourse_tags": ["Other", "Other"], "span_citation_mapping": []}
{"id": "2389139_4", "paragraph": "[BOS] The difference of the two models lays in the way they compute the context vector.\n[BOS] In the nonrecurrent model, the hidden state of the decoder is compared to each hidden state of the encoder.\n[BOS] Often, this comparison is realized as the dot product of vectors.\n[BOS] Then the comparison result is fed to a softmax layer to compute the attention weight.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Transition"], "span_citation_mapping": []}
{"id": "2389139_3", "paragraph": "[BOS] Both non-recurrent and input-feeding models compute a context vector c i at each time step.\n[BOS] Subsequently, they concatenate the context vector to the hidden state of decoder and pass it through a non-linearity before it is fed into the softmax output layer of the translation network.\n\n", "discourse_tags": ["Transition", "Transition"], "span_citation_mapping": []}
{"id": "2389139_2", "paragraph": "[BOS] This section provides a short background on attention and discusses two most popular attention models which are also used in this paper.\n[BOS] The first model is a non-recurrent attention model which is equivalent to the \"global attention\" method proposed by Luong et al. (2015a) .\n[BOS] The second attention model that we use in our investigation is an input-feeding model similar to the attention model first proposed by Bahdanau et al. (2015) and turned to a more general one and called inputfeeding by Luong et al. (2015a) .\n[BOS] Below we describe the details of both models.\n\n", "discourse_tags": ["Reflection", "Narrative_cite", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 31, "token_end": 57, "char_start": 170, "char_end": 285, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Luong et al. (2015a)": "1998416"}}}, {"token_start": 71, "token_end": 92, "char_start": 360, "char_end": 451, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Bahdanau et al. (2015)": "11212020"}}}, {"token_start": 101, "token_end": 113, "char_start": 496, "char_end": 532, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Luong et al. (2015a)": "1998416"}}}]}
{"id": "2389139_1", "paragraph": "[BOS] The mixed results reported by ; Alkhouli et al. (2016) ; Liu et al. (2016) on optimizing attention with respect to alignments motivates a more thorough analysis of attention models in NMT.\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 8, "token_end": 45, "char_start": 38, "char_end": 194, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Alkhouli et al. (2016)": "13419043", "Liu et al. (2016)": "13292366"}}}]}
{"id": "2389139_0", "paragraph": "[BOS] Recently, Koehn and Knowles (2017) carried out a brief analysis of how much attention and alignment match in different languages by measuring the probability mass that attention gives to alignments obtained from an automatic alignment tool.\n[BOS] They also report differences based on the most attended words.\n\n", "discourse_tags": ["Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 57, "char_start": 6, "char_end": 315, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Koehn and Knowles (2017)": "8822680"}, "Reference": {}}}]}
{"id": "16859281_0", "paragraph": "[BOS] This work directly extends (Goldberg and Elhadad, 2010 ) with beam search and global learning.\n[BOS] We show that both the easy-first POS tagger and dependency parser can be significantly impr- PTB CTB (Collins, 2002 ) 97.11 (Hatori et al., 2012 93.82 (Shen et al., 2007) 97.33 (Li et al., 2012) 93.88 (Huang et al., 2012) oved using beam search and global learning.\n[BOS] This work can also be considered as applying (Huang et al., 2012) to the systems that exhibit spurious ambiguity.\n[BOS] One future direction might be to apply the training method to transitionbased parsers with dynamic oracle (Goldberg and Nivre, 2012) and potentially further advance performances of state-of-the-art transition-based parsers.\n[BOS] Shen et al., (2007) and (Shen and Joshi, 2008) also proposed bi-directional sequential classification with beam search for POS tagging and LTAG dependency parsing, respectively.\n[BOS] The main difference is that their training method aims to learn a classifier which distinguishes between each local action while our training method aims to distinguish between action sequences.\n[BOS] Our method can also be applied to their framework.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Narrative_cite", "Narrative_cite", "Multi_summ", "Multi_summ", "Reflection"], "span_citation_mapping": [{"token_start": 6, "token_end": 22, "char_start": 33, "char_end": 100, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Goldberg and Elhadad, 2010": null}}}, {"token_start": 43, "token_end": 51, "char_start": 200, "char_end": 222, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Collins, 2002": null}}}, {"token_start": 52, "token_end": 63, "char_start": 225, "char_end": 251, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 63, "token_end": 74, "char_start": 252, "char_end": 277, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Shen et al., 2007)": "15876808"}}}, {"token_start": 74, "token_end": 85, "char_start": 278, "char_end": 301, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Li et al., 2012)": "2063580"}}}, {"token_start": 85, "token_end": 96, "char_start": 302, "char_end": 328, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Huang et al., 2012)": "367732"}}}, {"token_start": 113, "token_end": 122, "char_start": 415, "char_end": 444, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Huang et al., 2012)": "367732"}}}, {"token_start": 147, "token_end": 157, "char_start": 590, "char_end": 631, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 177, "token_end": 246, "char_start": 729, "char_end": 1107, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Shen et al., (2007)": "15876808"}, "Reference": {}}}]}
{"id": "2163772_3", "paragraph": "[BOS] The algorithms described in Section 2 can be applied to any parallel corpora.\n[BOS] The scoring function is simple and accommodates arbitrary features.\n[BOS] While our approach specifically assumes the documents (verses) within the corpora are already aligned, knowing which documents are similar (e.g. through clustering) is sufficient -perhaps at the cost of quality -to align and generate the subsequent resources.\n\n", "discourse_tags": ["Transition", "Transition", "Reflection"], "span_citation_mapping": []}
{"id": "2163772_2", "paragraph": "[BOS] By generating resources specifically for the Bible, we hope to foster future computational methods for studying religious texts.\n[BOS] Current Biblical visualization (Zhang et al., 2016) and authorship (Moritz et al., 2016) works use a small subset of the translations to perform their analysis.\n[BOS] Our resources would encourage analysis across all versions of the Bible, which would be less biased than picking a small set.\n[BOS] By weighing the votes cast by each token in a relation, it is even possible to emphasize a specific corpus.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 24, "token_end": 36, "char_start": 141, "char_end": 192, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 37, "token_end": 47, "char_start": 197, "char_end": 229, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Moritz et al., 2016)": "18043318"}}}]}
{"id": "2163772_1", "paragraph": "[BOS] The Bible has been productively used as a key resource for cross-lingual knowledge transfer (Yarowsky et al., 2001; Agi et al., 2015) .\n[BOS] Specifically, Johannsen et al. (2016) suggests a method for projecting POS tags and dependency parses onto a target language.\n[BOS] Our approach can be modified in a similar way.\n[BOS] By restricting the scoring function to use entirely language-independent features (e.g. pairwise alignments), our algorithm still maximizes the score of the matching by relearning an improved dictionary between iterations.\n[BOS] The corpus alignment may also be desirable over separate alignments for multi-source projection tasks in noisier data because a word or phrase may only align with only a subset of the sources.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Reflection", "Reflection", "Transition"], "span_citation_mapping": [{"token_start": 12, "token_end": 39, "char_start": 48, "char_end": 139, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yarowsky et al., 2001;": "15279538", "Agi\u0107 et al., 2015)": "18824729"}}}, {"token_start": 41, "token_end": 67, "char_start": 148, "char_end": 273, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Johannsen et al. (2016)": "2393768"}, "Reference": {}}}]}
{"id": "2163772_0", "paragraph": "[BOS] While monolingual insights like paraphrases have potential applications in semantic textual similarity (Agirre et al., 2012) , there exist bigger corpora for those tasks, such as PPDB (Ganitkevitch et al., 2013) .\n[BOS] However, as the Bible is often the only significant parallel text for many of the world's languages, improved 27-way consensus English resources created here have value for annotation projection to low-resource languages.\n\n", "discourse_tags": ["Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 14, "token_end": 27, "char_start": 81, "char_end": 130, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Agirre et al., 2012)": "12549805"}}}, {"token_start": 28, "token_end": 52, "char_start": 133, "char_end": 217, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ganitkevitch et al., 2013)": "6067240"}}}]}
{"id": "17219996_1", "paragraph": "[BOS] The two problems of parsing error and flatness also exist in constituency tree .\n[BOS] In order to make full use of the sub-structures, there have been a lot of work, including tree sequence to string translation (Liu et al., 2007) , tree binarization (Zhang et al., 2006) , forest-based translation (Mi et al., 2008) and fuzzy rule matching (Zhang et al., 2011) .\n\n", "discourse_tags": ["Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 40, "token_end": 53, "char_start": 183, "char_end": 237, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Liu et al., 2007)": "18616120"}}}, {"token_start": 54, "token_end": 66, "char_start": 240, "char_end": 278, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhang et al., 2006)": "2506060"}}}, {"token_start": 67, "token_end": 79, "char_start": 281, "char_end": 323, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mi et al., 2008)": null}}}, {"token_start": 80, "token_end": 91, "char_start": 328, "char_end": 368, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhang et al., 2011)": "1945213"}}}]}
{"id": "17219996_0", "paragraph": "[BOS] The work that is most similar to ours is (Xie et al., 2014) .\n[BOS] However, there are several significant differences between these two work.\n[BOS] First They incorporate well-formed dependency rules during decoding by modify the matched dependency rules \"on the fly\".\n[BOS] For example, assume there is a matched rule \"X1:NR X2:AD X3:VV X1:     X2: ||| X1 X2 X3 provide X5 X4\" for the headdependents structure in Figure 1 (b) .\n[BOS] in order to use the phrase \"||| us won't\" during decoding, they will compress the three nodes into one pseudo node \"NR AD VV\".\n[BOS] Then the above rule will become \"X1:NR AD VV X2: *      X3:||| X1 provide X3 X2\".\n[BOS] This new rule will inherit the translation probabilities from the original rule.\n[BOS] In the case that there is no matched rule or the probability estimation is unreliable due to sparsity, this method won't work well.\n[BOS] Another difference is that they only use phrasal rules corresponding to well formed dependency structures, while we allow variables to be contained in the well-formed dependency rules.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Other", "Transition", "Other", "Transition", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 2, "token_end": 19, "char_start": 6, "char_end": 65, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Xie et al., 2014)": "86680267"}}}]}
{"id": "1691239_2", "paragraph": "[BOS] Chiang (2010) extended SAMT-style labels to both source-and target-side parses, also introducing a mechanism by which SCFG rules may apply at run time even if their labels do not match.\n[BOS] Under Chiang's soft matching constraint, a rule headed by a label A::Z may still plug into a substitution site labeled B::Y by paying additional model costs subst BA and subst Y Z .\n[BOS] This is an on-the-fly method of coarsening the effective label set on a case-by-case basis.\n[BOS] Unfortunately, it also requires tuning a separate decoder feature for each pair of source-side and each pair of target-side labels.\n[BOS] This tuning can become prohibitively complex when working with standard parser label sets, which typically contain between 30 and 70 labels on each side.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 2, "token_end": 117, "char_start": 6, "char_end": 477, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "1691239_1", "paragraph": "[BOS] These compound nonterminals in practice lead to a very large label set.\n[BOS] Probability estimates for rules with the same structure up to labeling can be combined with the use of a preference grammar (Venugopal et al., 2009) , which replaces the variant labelings with a single SCFG rule using generic \"X\" labels.\n[BOS] The generic rule's \"preference\" over possible labelings is stored as a probability distribution inside the rule for use at decoding time.\n[BOS] Preference grammars thus reduce the label set size to one for the purposes of some feature calculations -which avoids the fragmentation of rule scores due to labeling ambiguity -but the original labels persist for specifying which rules may combine with which others.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 37, "token_end": 49, "char_start": 189, "char_end": 232, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Venugopal et al., 2009)": "917915"}}}]}
{"id": "1691239_0", "paragraph": "[BOS] One example of modifying the SCFG nonterminal set is seen in the Syntax-Augmented MT (SAMT) system of Zollmann and Venugopal (2006) .\n[BOS] In SAMT rule extraction, rules whose left-hand sides correspond exactly to a target-side parse node t retain that label in the grammar.\n[BOS] Additional nonterminal labels of the form t 1 + t 2 are created for rules spanning two adjacent parse nodes, while categorial grammar-style nonterminals t 1 /t 2 and t 1 \\t 2 are used for rules spanning a partial t 1 node that is missing a t 2 node to its right or left.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 16, "token_end": 36, "char_start": 71, "char_end": 137, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Zollmann and Venugopal (2006)": "819325"}}}]}
{"id": "16445327_2", "paragraph": "[BOS] As our task is to align words between versioned sentences to assist in identification of significant changes between versioned texts, it is important to consider the semantics of sentences.\n[BOS] Lee et. al. (2014) reviewed the limitations of information retrieval methods (i.e., the Boolean model, the vector space model and the statistical probability model) that calculate the similarity of natural language sentences, but did not consider the meaning of the sentences.\n[BOS] Their proposal was to use link grammar to measure similarity based on grammatical structures, combined with the use of an ontology to measure the similarity of the meaning.\n[BOS] Their method was shown to be effective for the problem of paraphrase.\n[BOS] Paraphrase addresses detecting alternative ways of conveying the same information (Ibrahim et al., 2003) and we observe paraphrase problem as a subset to our task because sentence re-phrasing is part of revision.\n[BOS] However, the paraphrase problem effectively try to normalize away differences, while versioned sentences analysis focuses more directly on evaluating the meaning impact of differences.\n\n", "discourse_tags": ["Reflection", "Single_summ", "Single_summ", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 36, "token_end": 137, "char_start": 202, "char_end": 733, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Lee et. al. (2014)": "1444765"}, "Reference": {}}}, {"token_start": 138, "token_end": 161, "char_start": 740, "char_end": 844, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ibrahim et al., 2003)": "6576948"}}}]}
{"id": "16445327_1", "paragraph": "[BOS] Research has shown that predefined edit categories such as fluency edits (i.e. edits to improve on style and readability) and factual edits (i.e. edits that alter the meaning) in Wikipedia, where revision history data is abundant, can be classified using a supervised approach (Bronner and Monz, 2012; Daxenberger and Gurevych, 2013) .\n[BOS] The distinction of the edits can be linked to Faigley and Witte's (1981) taxonomy: fluency edits to surface changes and factual edits to text-base changes.\n[BOS] Supervised classification would be difficult to apply to other types of revised documents, due to more limited training data in most domain-specific contexts.\n[BOS] They too did not consider the significance of edits.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 61, "token_end": 84, "char_start": 263, "char_end": 339, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bronner and Monz, 2012;": "16832577", "Daxenberger and Gurevych, 2013)": "7823280"}}}, {"token_start": 86, "token_end": 166, "char_start": 348, "char_end": 727, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Faigley and Witte's (1981)": null}, "Reference": {}}}]}
{"id": "16445327_0", "paragraph": "[BOS] Research on revision concentrates on detecting edits and aligning sentences between versioned text documents.\n[BOS] Considering sentences from the first and last draft of essays, Zhang and Litman (2014; 2015) proposed an automated approach to detect whether a sentence has been edited between these versions.\n[BOS] Their proposed method starts with sen- Figure 1 : Taxonomy for revision analysis (Faigley and Witte, 1981) tence alignment, and then identifies the sequence of edits (i.e., the edit operations of Add, Modify, Delete and Keep) between the two sentences.\n[BOS] They further consider automated classification of the reason for a revision (i.e., claim, evidence, rebuttal, etc.\n[BOS] ), which they hypothesised can help writers to improve their writing.\n[BOS] Classifying revisions based on the reasons of revision does not indicate the significance of revision changes.\n[BOS] What we are attempting is to represent these revision changes in a meaningful way to assist in assessment of the significance.\n[BOS] We concentrate on identification of significant revision changes, or revision changes that have higher impact of meaning change for the purpose of prioritising revision changes, especially in multi-author revision.\n[BOS] Nevertheless, the work by Zhang and Litman (2014; 2015) provides insights to revisions from a different perspective.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection", "Reflection", "Reflection", "Multi_summ"], "span_citation_mapping": [{"token_start": 21, "token_end": 58, "char_start": 122, "char_end": 314, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhang and Litman (2014;": "16916653", "2015)": "237763"}, "Reference": {}}}, {"token_start": 69, "token_end": 161, "char_start": 371, "char_end": 770, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Faigley and Witte, 1981)": null}, "Reference": {}}}, {"token_start": 239, "token_end": 262, "char_start": 1248, "char_end": 1364, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhang and Litman (2014;": "16916653", "2015)": "237763"}, "Reference": {}}}]}
{"id": "2326390_1", "paragraph": "[BOS] A joint syntactic analysis and VMWE identification approach using off-the-shelf parsers is another interesting alternative that has shown to help VMWE identification such as light verb constructions (Eryigit et al., 2011; Vincze et al., 2013) .\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 28, "token_end": 55, "char_start": 152, "char_end": 248, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Eryigit et al., 2011;": "2809518", "Vincze et al., 2013)": "10831519"}}}]}
{"id": "2326390_0", "paragraph": "[BOS] Previous approaches for VMWE identification include the two-pass method of candidate extraction followed by binary classification (Fazly et al., 2009; Nagy T. and Vincze, 2014) .\n[BOS] VMWE identification has also been performed using sequence labeling approaches, with IOBscheme.\n[BOS] For instance, Diab and Bhutada (2009) apply a sequential SVM to identify verb-noun idiomatic combinations in English.\n[BOS] Such approaches were used for MWE identification in general (including verbal expressions) ranging from contiguous expressions (Blunsom and Baldwin, 2006 ) to gappy ones (Schneider et al., 2014) .\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 19, "token_end": 41, "char_start": 114, "char_end": 182, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Fazly et al., 2009;": "2390655", "Nagy T. and Vincze, 2014)": "1922244"}}}, {"token_start": 62, "token_end": 88, "char_start": 293, "char_end": 410, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Diab and Bhutada (2009)": "10738628"}, "Reference": {}}}, {"token_start": 106, "token_end": 117, "char_start": 521, "char_end": 570, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Blunsom and Baldwin, 2006": "10995480"}}}, {"token_start": 119, "token_end": 130, "char_start": 576, "char_end": 611, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Schneider et al., 2014)": "2839348"}}}]}
{"id": "19227196_3", "paragraph": "[BOS] Directly learning from PA based on a forestbased objective in LLGPar is first proposed by Li et al. (2014) , inspired by the idea of ambiguous labeling.\n[BOS] Similar ideas have been extensively explored recently in sequence labeling tasks (Liu et al., 2014; Yang and Vozila, 2014; Marcheggiani and Artires, 2014) .\n[BOS] Hwa (1999) pioneers the idea of exploring PA for constituent grammar induction based on a variant Inside-Outside re-estimation algorithm (Pereira and Schabes, 1992) .\n[BOS] Clark and Curran (2006) propose to train a Combinatorial Categorial Grammar parser using partially labeled data only containing predicate-argument dependencies.\n[BOS] Mielens et al. (2015) propose to impute missing dependencies based on Gibbs sampling in order to enable traditional parsers to learn from partial trees.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 13, "token_end": 27, "char_start": 68, "char_end": 112, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Li et al. (2014)": "7527306"}}}, {"token_start": 45, "token_end": 74, "char_start": 222, "char_end": 319, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Liu et al., 2014;": "8431320", "Yang and Vozila, 2014;": "14283390", "Marcheggiani and Arti\u00e8res, 2014)": "13642704"}}}, {"token_start": 76, "token_end": 113, "char_start": 328, "char_end": 492, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hwa (1999)": "7117045"}, "Reference": {"(Pereira and Schabes, 1992)": "696805"}}}, {"token_start": 115, "token_end": 142, "char_start": 501, "char_end": 661, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Clark and Curran (2006)": "2547341"}, "Reference": {}}}, {"token_start": 143, "token_end": 175, "char_start": 668, "char_end": 820, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Mielens et al. (2015)": "17978489"}, "Reference": {}}}]}
{"id": "19227196_2", "paragraph": "[BOS] LGPar and LTPar for directly learning from PA.\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "19227196_1", "paragraph": "[BOS] Directly learning from PA based on constrained decoding is previously proposed by Jiang et al. (2013) for Chinese word segmentation, which is treated as a character-level sequence labeling problem.\n[BOS] In this work, we first apply the idea to\n\n", "discourse_tags": ["Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 8, "token_end": 21, "char_start": 41, "char_end": 107, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Jiang et al. (2013)": "14014145"}}}]}
{"id": "19227196_0", "paragraph": "[BOS] In parsing community, most previous works adopt ad-hoc methods to learn from PA. Sassano and Kurohashi (2010) , Jiang et al. (2010) , and Flannery and Mori (2015) convert partially annotated instances into local dependency/non-dependency classification instances, which may suffer from the lack of non-local correlation between dependencies in a tree.\n[BOS] Mirroshandel and Nasr (2011) and Majidi and Crane (2013) adopt the complete-then-learn method.\n[BOS] They use parsers coarsely trained on existing data with FA for completion via constrained decoding.\n[BOS] However, our experiments show that this leads to dramatic decrease in parsing accuracy.\n[BOS] Nivre et al. (2014) present a constrained decoding procedure for arc-eager transition-based parsers.\n[BOS] However, their work focuses on allowing their parser to effectively exploit external constraints during the evaluation phase.\n[BOS] In this work, we directly employ their method and show that constrained decoding is effective for LTPar and thus irresponsible for its ineffectiveness in learning PA.\n\n", "discourse_tags": ["Multi_summ", "Multi_summ", "Single_summ", "Reflection", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 78, "char_start": 6, "char_end": 357, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Sassano and Kurohashi (2010)": "14925259", "Jiang et al. (2010)": "6866394"}, "Reference": {}}}, {"token_start": 79, "token_end": 126, "char_start": 364, "char_end": 564, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Majidi and Crane (2013)": "150387488"}, "Reference": {}}}, {"token_start": 143, "token_end": 187, "char_start": 665, "char_end": 897, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Nivre et al. (2014)": "13174124"}, "Reference": {}}}]}
{"id": "17830435_1", "paragraph": "[BOS] Previous research has showed that word segmentation has a great impact on parsing accuracy in the pipeline method (Harper and Huang, 2009 ).\n[BOS] In (Jiang et al., 2009) , additional data was used to improve Chinese word segmentation, which resulted in significant improvement on the parsing task using the pipeline framework.\n[BOS] Joint segmentation and parsing was also investigated for Arabic (Green and Manning, 2010) .\n[BOS] A study that is closely related to ours is (Goldberg and Tsarfaty, 2008 ), where a single generative model was proposed for joint morphological segmentation and syntactic parsing for Hebrew.\n[BOS] Different from that work, we use a discriminative model, which benefits from large amounts of features and is easier to deal with unknown words.\n[BOS] Another main difference is that, besides segmentation and parsing, we also incorporate the POS tagging model into the CYK parsing framework.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Narrative_cite", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 18, "token_end": 27, "char_start": 104, "char_end": 143, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Harper and Huang, 2009": null}}}, {"token_start": 30, "token_end": 64, "char_start": 153, "char_end": 333, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Jiang et al., 2009)": "15016194"}, "Reference": {}}}, {"token_start": 73, "token_end": 82, "char_start": 397, "char_end": 429, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Green and Manning, 2010)": null}}}, {"token_start": 93, "token_end": 123, "char_start": 481, "char_end": 628, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Goldberg and Tsarfaty, 2008": "14857072"}, "Reference": {}}}]}
{"id": "17830435_0", "paragraph": "[BOS] There is very limited previous work on joint Chinese word segmentation, POS tagging, and parsing.\n[BOS] Previous joint models mainly focus on word segmentation and POS tagging task, such as the virtual nodes method (Qian et al., 2010) , cascaded linear model (Jiang et al., 2008a) , perceptron (Zhang and Clark, 2008) , sub-word based stacked learning (Sun, 2011) , reranking (Jiang et al., 2008b) .\n[BOS] These joint models showed about 0.2  1% F-score improvement over the pipeline method.\n[BOS] Recently, joint tagging and dependency parsing has been studied as well (Li et al., 2011; Lee et al., 2011) .\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 37, "token_end": 49, "char_start": 200, "char_end": 240, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Qian et al., 2010)": "15357409"}}}, {"token_start": 50, "token_end": 63, "char_start": 243, "char_end": 286, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Jiang et al., 2008a)": "9285364"}}}, {"token_start": 64, "token_end": 73, "char_start": 289, "char_end": 323, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhang and Clark, 2008)": "105219"}}}, {"token_start": 74, "token_end": 85, "char_start": 326, "char_end": 369, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sun, 2011)": "429415"}}}, {"token_start": 86, "token_end": 98, "char_start": 372, "char_end": 403, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Jiang et al., 2008b)": "12138087"}}}, {"token_start": 122, "token_end": 147, "char_start": 514, "char_end": 611, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Li et al., 2011;": "14410909", "Lee et al., 2011)": "10492123"}}}]}
{"id": "18089897_0", "paragraph": "[BOS] Previous work on using WSD for SMT has yielded mixed results.\n[BOS] (Carpuat and Wu, 2005) report a negative impact on BLEU scores.\n[BOS] They used a supervised WSD system to select translation candidates for the SMT system, but, contrary to common sense expectations, this only made the translation model perform worse.\n[BOS] Several reasons for this are suggested, chiefly that the SMT model works well enough on its own and state-of-the-art WSD systems cannot really boost it in a significant number of cases, and also that SMT architectures might not be welladapted to make use of the output of WSD systems.\n[BOS] (Cabezas and Resnik, 2005) present an approach to using WSD for SMT, whereby target language lexical items are treated as \"sense tags\", given as soft translation alternatives to the translation model, which chooses the final version in accordance with its language model.\n[BOS] The study reported a small gain against a base-line that is, according to the authors, stronger than the one used in (Carpuat and Wu, 2005) .\n[BOS] (Vickrey et al., 2005) recasts WSD as a translation task, defining the different sense options for the separate words as the words or phrases aligned to them in a parallel corpus.\n[BOS] The authors demonstrate that this approach is successful, as tested on word translation and blankfilling, thus showing that WSD and SMT have a lot in common and improving one should be helpful for improving the other.\n[BOS] (Chan et al., 2007) present another study in which WSD is beneficial to SMT.\n[BOS] Disambiguation is performed between the possible translations of each source phrase.\n[BOS] Translations are selected so as to maximize the length of the chunk proposed by the WSD model; the score provided by the WSD model is also taken into consideration.\n[BOS] This approach yields a statistically significant improvement in terms of BLEU score.\n[BOS] In a study that builds on their previously discouraging results, (Carpuat and Wu, 2007) show how a deeper integration of WSD into SMT systems can help systematically and significantly.\n[BOS] Instead of performing disambiguation on single words, their system performs multiword phrasal disambiguation, thus achieving improvements over the baseline, as measured by eight different translation metrics.\n[BOS] The rich context provided by the supervised WSD system helps rank correct translations higher than erroneous ones suggested by the baseline SMT system; also, it helps the decoder pick longer translation sequences, which often results in better translations.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 17, "token_end": 70, "char_start": 74, "char_end": 326, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Carpuat and Wu, 2005)": "819020"}, "Reference": {}}}, {"token_start": 136, "token_end": 227, "char_start": 624, "char_end": 1041, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Cabezas and Resnik, 2005)": null, "(Carpuat and Wu, 2005)": "819020"}, "Reference": {}}}, {"token_start": 229, "token_end": 313, "char_start": 1050, "char_end": 1453, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Vickrey et al., 2005)": "7241107"}, "Reference": {}}}, {"token_start": 314, "token_end": 398, "char_start": 1460, "char_end": 1889, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Chan et al., 2007)": "14598745"}, "Reference": {}}}, {"token_start": 399, "token_end": 473, "char_start": 1896, "char_end": 2295, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Carpuat and Wu, 2007)": "135295"}, "Reference": {}}}]}
{"id": "1835119_1", "paragraph": "[BOS] fine decoding strategy.\n[BOS] The output is then discriminatively reranked (Charniak and Johnson, 2005) to select the best analysis.\n[BOS] In contrast, the parser used in this paper constructs the parse tree in a greedy manner and relies only on word, POS tags and morphological embeddings.\n[BOS] Several other papers have reported results for the SPMRL Shared Task 2014.\n[BOS] (Hall et al., 2014) introduced an approach where, instead of propagating contextual information from the leaves of the tree to internal nodes in order to refine the grammar, the structural complexity of the grammar is minimized.\n[BOS] This is done by moving as much context as possible onto local surface features.\n[BOS] This work was refined in (Durrett and Klein, 2015) , taking advantage of continuous word representations.\n[BOS] The system used in this paper also leverages words embeddings but has two major differences.\n[BOS] First, it proceeds step-by-step in a greedy manner (Durrett and Klein, 2015) by using structured inference (CKY).\n[BOS] Second, it leverages a compositional node feature which propagates information from the leaves to internal nodes, which is exactly what is claimed not to be done.\n[BOS] (Fernndez-Gonzlez and Martins, 2015) proposed a procedure to turn a dependency tree into a constituency tree.\n[BOS] They showed that encoding order information in the dependency tree make it isomorphic to the constituent tree, allowing any dependency parser to produce constituents.\n[BOS] Like the parser we used, their parser do not need to binarize the treebank as most of the others constituency parsers.\n[BOS] Unlike this system, we do not use the dependency structure as an intermediate representation and directly perform constituency parsing over raw words.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 7, "token_end": 25, "char_start": 36, "char_end": 109, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Charniak and Johnson, 2005)": "11599080"}}}, {"token_start": 77, "token_end": 136, "char_start": 384, "char_end": 698, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Hall et al., 2014)": "3107882"}, "Reference": {}}}, {"token_start": 137, "token_end": 238, "char_start": 705, "char_end": 1198, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Durrett and Klein, 2015)": "543551"}, "Reference": {}}}, {"token_start": 239, "token_end": 322, "char_start": 1205, "char_end": 1612, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Fern\u00e1ndez-Gonz\u00e1lez and Martins, 2015)": "5754528"}, "Reference": {}}}]}
{"id": "1835119_0", "paragraph": "[BOS] Both the baseline (Berkeley parser) and the current state-of-the-art model on the SPMRL Shared Task 2014 (Bjrkelund et al., 2014) rely on probabilistic context free grammar (PCFG)-based features.\n[BOS] The latter uses a product of PCFG with latent annotation based models (Petrov, 2010) , with a coarse-to- O : B-SQ I-SQ I-SQ E-SQ Figure 1 : Greedy parsing algorithm (3 iterations), on the sentence \"Did you hear the falling bombs ?\".\n[BOS] I W , I T and O stand for input words (or composed word representations R i ), input syntactic tags (parsing or part-of-speech) and output tags (parsing), respectively.\n[BOS] The tree produced after 3 greedy iterations can be reconstructed as the following: (SQ (VBD Did) (NP (PRP you)) (VP (VB hear) (NP (DT the) (VBG falling) (NNS bombs))) (.\n[BOS] ?\n[BOS] )).\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Other", "Other", "Other", "Other"], "span_citation_mapping": [{"token_start": 22, "token_end": 38, "char_start": 88, "char_end": 135, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bj\u00f6rkelund et al., 2014)": "61766611"}}}, {"token_start": 62, "token_end": 72, "char_start": 247, "char_end": 292, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Petrov, 2010)": "1163327"}}}]}
{"id": "220553_2", "paragraph": "[BOS] proach to collecting attention maps.\n[BOS] They collected large-scale attention annotations for MS COCO (Lin et al., 2014) on Amazon Mechanical Turk (AMT).\n[BOS] While (Jiang et al., 2015) studies natural exploration and collects task-independent human annotations by asking subjects to freely move the mouse cursor to anywhere they wanted to look on a blurred image, our approach is task-driven.\n[BOS] Specifically, as described in 3, we collect ground truth attention annotations by instructing subjects to sharpen parts of a blurred image that are important for answering the questions accurately.\n[BOS] Section 4 covers evaluation of unsupervised attention maps generated by VQA models against our human attention maps.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 18, "token_end": 29, "char_start": 102, "char_end": 128, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lin et al., 2014)": "14113767"}}}, {"token_start": 39, "token_end": 87, "char_start": 168, "char_end": 402, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Jiang et al., 2015)": "16445820"}, "Reference": {}}}]}
{"id": "220553_1", "paragraph": "[BOS] We work with the free-form and open-ended VQA dataset released by (Antol et al., 2015) .\n[BOS] VQA Models.\n[BOS] Attention-based models for VQA typically use convolutional neural networks to highlight relevant regions of image given a question.\n[BOS] Stacked Attention Networks (SAN) proposed in (Yang et al., 2015) use LSTM encodings of question words to produce a spatial attention distribution over the convolutional layer features of the image.\n[BOS] Hierarchical Co-Attention Network (Lu et al., 2016) generates multiple levels of image attention based on words, phrases and complete questions, and is the top entry on the VQA Challenge 1 as of the time of this submission.\n[BOS] Another interesting approach uses question parsing to compose the neural network from modules, attention being one of the sub-tasks addressed by these modules (Andreas et al., 2016) .\n[BOS] Note that all these works are unsupervised attention models, where \"attention\" is simply an intermediate variable (a spatial distribution) that is produced by the model to optimize downstream loss (VQA cross-entropy).\n[BOS] The fact that some (it's unclear how many) of these spatial distributions end up being interpretable is simply fortuitous.\n[BOS] In contrast, we study where humans choose to look to answer visual questions.\n[BOS] These human attention maps can be used to evaluate unsupervised maps.\n[BOS] Human Studies.\n[BOS] There's a rich history of work in collecting eye tracking data from human subjects to gain an understanding of image saliency and visual perception (Jiang et al., 2014; Judd et al., 2009; Fei-Fei et al., 2007; Yarbus, 1967) .\n[BOS] Eye tracking data to study natural visual exploration (Jiang et al., 2014; Judd et al., 2009 ) is useful but difficult and expensive to collect on a large scale.\n[BOS] (Jiang et al., 2015) established mouse tracking as an accurate ap- Figure 3 : Deblurring procedure to collect attention maps.\n[BOS] We present subjects with a blurred image and ask them to sharpen regions of the image that will help them answer the question correctly, in a smooth, click-and-drag, 'coloring' motion with the mouse.\n\n", "discourse_tags": ["Reflection", "Other", "Transition", "Single_summ", "Single_summ", "Narrative_cite", "Transition", "Transition", "Reflection", "Reflection", "Reflection", "Narrative_cite", "Narrative_cite", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 6, "token_end": 27, "char_start": 23, "char_end": 92, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Antol et al., 2015)": "3180429"}}}, {"token_start": 57, "token_end": 96, "char_start": 257, "char_end": 454, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Yang et al., 2015)": "8849206"}, "Reference": {}}}, {"token_start": 97, "token_end": 144, "char_start": 461, "char_end": 684, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Lu et al., 2016)": "15690314"}, "Reference": {}}}, {"token_start": 159, "token_end": 180, "char_start": 786, "char_end": 872, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Andreas et al., 2016)": "3130692"}}}, {"token_start": 307, "token_end": 345, "char_start": 1526, "char_end": 1638, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Jiang et al., 2014;": "16445820", "Judd et al., 2009;": "16445820", "Fei-Fei et al., 2007;": "6359641", "Yarbus, 1967)": null}}}, {"token_start": 352, "token_end": 370, "char_start": 1674, "char_end": 1739, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Jiang et al., 2014;": "16445820", "Judd et al., 2009": "16445820"}}}, {"token_start": 385, "token_end": 400, "char_start": 1815, "char_end": 1880, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Jiang et al., 2015)": "16445820"}, "Reference": {}}}]}
{"id": "220553_0", "paragraph": "[BOS] Our work draws on recent work in attention-based VQA and human studies in saliency prediction.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "1945213_1", "paragraph": "[BOS] Chiang (2010) proposes a method for learning to translate with both source and target syntax in the framework of a hierarchical phrase-based system.\n[BOS] He not only executes 0-1 matching on both sides of rules, but also designs numerous features such as . '\n[BOS] X X root which counts the number of rules whose source-side root label is X and target-side root label is ' X .\n[BOS] This fuzzy use of source and target syntax enables the translation system to learn which tree labels are similar enough to be compatible, which ones are harmful to combine, and which ones can be ignored.\n[BOS] The differences between us are twofold: 1) his work applies fuzzy syntax in both sides, while ours bases on the string- Reference the europen union said in a joint statement issued after its summit meeting with china 's premier wen jiabao  in a joint statement released after the summit with chinese premier wen jiabao , the europen union said  Joshua the europen union with chinese premier wen jiabao in a joint statement issued after the summit meeting said  s2t the europen union in a joint statement issued after the summit meeting with chinese premier wen jiabao said  3 FT2ET-DeepSim the europen union said in a joint statement issued after the summit meeting with chinese premier wen jiabao  Table 3 : Some translation examples produced by Joshua, string-to-tree system s2t and source-syntax-augmented string-to-tree system FT2ET with deep similarity matching algorithm to-tree model and applies fuzzy syntax on source side; and 2) we not only adopt the 0-1 fuzzy rule matching algorithm, but also investigate likelihood matching and deep similarity matching algorithms.\n\n", "discourse_tags": ["Single_summ", "Other", "Other", "Other", "Other"], "span_citation_mapping": [{"token_start": 2, "token_end": 31, "char_start": 6, "char_end": 154, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "1945213_0", "paragraph": "[BOS] Several studies have tried to incorporate source or target syntax into translation models in a fuzzy manner.\n[BOS] Zollmann and Venugopal (2006) augment the hierarchical string-to-string rules (Chiang, 2005) with target-side syntax.\n[BOS] They annotate the target side of each string-to-string rule using SAMT-style syntactic categories and aim to generate the output more syntactically.\n[BOS] Zhang et al. (2010) base their approach on tree-to-string models, and generate grammatical output more reliably with the help of tree-to-tree sequence rules.\n[BOS] Neither of them builds target syntactic trees using target syntax, however.\n[BOS] Thus they can be viewed as integrating target syntax in a fuzzy manner.\n[BOS] By contrast, we base our approach on a string-to-tree model which does construct target syntactic trees during decoding.\n[BOS] (Marton and Resnik, 2008; Chiang et al., 2009 and Huang et al., 2010) apply fuzzy techniques for integrating source syntax into hierarchical phrasebased systems (Chiang, 2005 (Chiang, , 2007 .\n[BOS] The first two studies employ 0-1 matching and the last tries deep similarity matching between two tag sequences.\n[BOS] By contrast, we incorporate source syntax into a string-to-tree model.\n[BOS] Furthermore, we apply fuzzy syntactic annotation on each rule's source string and design three fuzzy rule matching algorithms.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection", "Multi_summ", "Transition", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 21, "token_end": 84, "char_start": 121, "char_end": 393, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zollmann and Venugopal (2006)": "819325"}, "Reference": {"(Chiang, 2005)": "384994"}}}, {"token_start": 85, "token_end": 150, "char_start": 400, "char_end": 717, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhang et al. (2010)": "997446"}, "Reference": {}}}, {"token_start": 176, "token_end": 223, "char_start": 851, "char_end": 1041, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Marton and Resnik, 2008;": "2442439", "Chiang et al., 2009 and": "3544821", "Huang et al., 2010)": "8988829"}, "Reference": {"(Chiang, 2005": "384994", "(Chiang, , 2007": "3505719"}}}]}
{"id": "17443483_1", "paragraph": "[BOS] The success of Durrett and Klein is possible due to the large training dataset provided by OntoNotes (Pradhan et al., 2007) .\n[BOS] In this work, we successfully extend data-driven specialisation still further: Section 4 shows how we can discover fine-grained patterns in reference expression usage, and Section 5 how these patterns can be used to significantly improve the performance of a strong coreference system.\n\n", "discourse_tags": ["Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 19, "token_end": 32, "char_start": 97, "char_end": 129, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Pradhan et al., 2007)": "207152554"}}}]}
{"id": "17443483_0", "paragraph": "[BOS] A particularly successful way to leverage mention classification has been to specialise modelling by mention type.\n[BOS] Denis and Baldridge (2008) learn five different models, one each for proper name, definite nominal, indefinite nominal, third person pronoun, and non-third person pronoun.\n[BOS] Bengtson and Roth (2008) and Durrett and Klein (2013) implement specialisation at the level of features within a model, rather than explicitly learning separate models.\n[BOS] Bengtson and Roth (2008) prefix each base feature generated with the type of the current mention, one of proper name, nominal, or pronoun, for instance nominal-head match:true.\n[BOS] Durrett and Klein (2013) extend from this by learning a model over three versions of each base feature: unprefixed, conjoined with the type of the current mention, and conjoined with concatenation of the types of the current mention and candidate antecedent mention: nominal+nominal-head match=true.\n\n", "discourse_tags": ["Transition", "Single_summ", "Multi_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 21, "token_end": 58, "char_start": 127, "char_end": 298, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Denis and Baldridge (2008)": "535939"}, "Reference": {}}}, {"token_start": 59, "token_end": 94, "char_start": 305, "char_end": 473, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Bengtson and Roth (2008)": "8179642"}, "Reference": {}}}, {"token_start": 95, "token_end": 135, "char_start": 480, "char_end": 656, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 136, "token_end": 200, "char_start": 663, "char_end": 962, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "19305645_3", "paragraph": "[BOS] In previous approaches, the attention (Bahdanau et al., 2014; Xu et al., 2015) mechanism is mostly word-based and flat-structured (Kadlec et al., 2016; Sordoni et al., 2016; Wang and Jiang, 2016; Yu et al., 2016) : the attention scores are computed between individual words, are normalized globally and are used to summarize word-level encodings in a flat manner.\n[BOS] Cui et al. (2016) ; Xiong et al. (2016) explored a coattention mechanism to learn question-topassage and passage-to-question summaries.\n[BOS] Seo et al. (2016) proposed to directly use the attention weights as augmented features instead of applying them for early summarization.\n\n", "discourse_tags": ["Narrative_cite", "Multi_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 6, "token_end": 25, "char_start": 30, "char_end": 84, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bahdanau et al., 2014;": "11212020", "Xu et al., 2015)": "1055111"}}}, {"token_start": 28, "token_end": 66, "char_start": 105, "char_end": 218, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kadlec et al., 2016;": "11022639", "Sordoni et al., 2016;": "14500125", "Wang and Jiang, 2016;": "5592690", "Yu et al., 2016)": "17654651"}}}, {"token_start": 95, "token_end": 131, "char_start": 376, "char_end": 511, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Cui et al. (2016)": null, "Xiong et al. (2016)": "3714278"}, "Reference": {}}}, {"token_start": 132, "token_end": 159, "char_start": 518, "char_end": 654, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Seo et al. (2016)": "8535316"}, "Reference": {}}}]}
{"id": "19305645_2", "paragraph": "[BOS] The representation learning in previous approaches is conducted over individual words using the following encoders: LSTM in Xiong et al., 2016) ; bi-directional gated recurrent unit (Chung et al., 2014) in (Yu et al., 2016) ; match-LSTM in (Wang and Jiang, 2016) ; bi-directional LSTM in (Seo et al., 2016) .\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 19, "token_end": 30, "char_start": 122, "char_end": 149, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Xiong et al., 2016)": "3714278"}}}, {"token_start": 31, "token_end": 45, "char_start": 152, "char_end": 208, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chung et al., 2014)": "5201925"}}}, {"token_start": 45, "token_end": 54, "char_start": 209, "char_end": 229, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yu et al., 2016)": "17654651"}}}, {"token_start": 55, "token_end": 67, "char_start": 232, "char_end": 268, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wang and Jiang, 2016)": "5592690"}}}, {"token_start": 68, "token_end": 83, "char_start": 271, "char_end": 312, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Seo et al., 2016)": "8535316"}}}]}
{"id": "19305645_1", "paragraph": "[BOS] Two ways were investigated for candidate answer generation: (1) chunking: candidates are preselected based on lexical and syntactic analysis, such as constituent parsing (Rajpurkar et al., 2016) and part-of-speech pattern (Yu et al., 2016) ; (2) directly predicting the start and end position of the answer span, using feed-forward neural network , LSTM (Seo et al., 2016) , pointer network (Vinyals et al., 2015; Wang and Jiang, 2016) , dynamic pointer decoder (Xiong et al., 2016) .\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 31, "token_end": 43, "char_start": 156, "char_end": 200, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rajpurkar et al., 2016)": "11816014"}}}, {"token_start": 44, "token_end": 58, "char_start": 205, "char_end": 245, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yu et al., 2016)": "17654651"}}}, {"token_start": 81, "token_end": 92, "char_start": 355, "char_end": 378, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Seo et al., 2016)": "8535316"}}}, {"token_start": 93, "token_end": 111, "char_start": 381, "char_end": 441, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vinyals et al., 2015;": "5692837", "Wang and Jiang, 2016)": "5592690"}}}, {"token_start": 112, "token_end": 124, "char_start": 444, "char_end": 488, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Xiong et al., 2016)": "3714278"}}}]}
{"id": "19305645_0", "paragraph": "[BOS] Several neural network based approaches have been proposed to solve the SQuAD QA problem, which we briefly review from three aspects: candidate answer generation, representation learning and attention mechanism.\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "1899153_0", "paragraph": "[BOS] In terms of methodology, our work is closely related to previous works that incorporate copying mechanism with neural models (Glehre et al., 2016; Gu et al., 2016; Ling et al., 2016; .\n[BOS] Our models are similar to models proposed in Merity et al., 2016) where the generation of each word can be conditioned on a particular entry in knowledge lists and previous words.\n[BOS] In our work, we describe a model with broader applications, allowing us to condition, on databases, lists and dynamic lists.\n[BOS] In terms of applications, our work is related to chit-chat dialogue Sordoni et al., 2015; Serban et al., 2016; Shang et al., 2015) and task oriented dialogue (Wen et al., 2015; Bordes and Weston, 2016; Williams and Zweig, 2016; Wen et al., 2016) .\n[BOS] Most of previous works on task oriented dialogues embed the seq2seq model in traditional dialogue systems, in which the table query part is not differentiable, while our model queries the database directly.\n[BOS] Recipe generation was proposed in (Kiddon et al., 2016) .\n[BOS] They use attention mechanism over the checklists, whereas our work models explicit references to them.\n[BOS] Context dependent language models (Mikolov et al., 2010; Jozefowicz et al., 2016; Mikolov et al., 2010; Wang and Cho, 2015) are proposed to capture long term dependency of text.\n[BOS] There are also lots of works on coreference resolution (Haghighi and Klein, 2010; Wiseman et al., 2016) .\n[BOS] We are the first to combine coreference with language modeling, to the best of our knowledge.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Reflection", "Narrative_cite", "Reflection", "Narrative_cite", "Reflection", "Narrative_cite", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 16, "token_end": 45, "char_start": 82, "char_end": 187, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(G\u00fcl\u00e7ehre et al., 2016;": "969555", "Gu et al., 2016;": "8174613"}}}, {"token_start": 48, "token_end": 64, "char_start": 197, "char_end": 262, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 121, "token_end": 151, "char_start": 563, "char_end": 644, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Sordoni et al., 2015;": "94285", "Serban et al., 2016;": "6126582"}}}, {"token_start": 152, "token_end": 185, "char_start": 649, "char_end": 759, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wen et al., 2015;": "739696", "Bordes and Weston, 2016;": "2129889", "Williams and Zweig, 2016;": null}}}, {"token_start": 226, "token_end": 241, "char_start": 981, "char_end": 1036, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 262, "token_end": 302, "char_start": 1154, "char_end": 1277, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mikolov et al., 2010;": "17048224", "Jozefowicz et al., 2016;": "260422", "Mikolov et al., 2010;": "17048224"}}}, {"token_start": 313, "token_end": 340, "char_start": 1338, "char_end": 1441, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Haghighi and Klein, 2010;": "9203411"}}}]}
{"id": "21703007_2", "paragraph": "[BOS] The training strategy employed in our paper is reinforcement learning, which is inspired by recent work exploiting it into question answering problem.\n[BOS] The above mentioned coarse-to-fine framework (Choi et al., 2017; Wang et al., 2018a) treated sentence selection as a latent variable and jointly trained the sentence selection module with the answer generation module via RL.\n[BOS] Shen et al. (2017) modeled the multi-hop reasoning procedure with a termination state to decide when it is adequate to produce an answer.\n[BOS] RL is suitable to capture this stochastic behavior.\n[BOS] Hu et al. (2018) merely modeled the extraction process, using F1 as rewards in addition to maximum likelihood estimation.\n[BOS] RL was utilized in their training process, as the F1 measure is not differentiable.\n\n", "discourse_tags": ["Reflection", "Multi_summ", "Single_summ", "Transition", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 27, "token_end": 74, "char_start": 163, "char_end": 387, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Choi et al., 2017;": null, "Wang et al., 2018a)": "21928029"}, "Reference": {}}}, {"token_start": 75, "token_end": 104, "char_start": 394, "char_end": 531, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Shen et al. (2017)": "6300274"}, "Reference": {}}}, {"token_start": 115, "token_end": 158, "char_start": 596, "char_end": 807, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hu et al. (2018)": "64515663"}, "Reference": {}}}]}
{"id": "21703007_1", "paragraph": "[BOS] More recently, open-domain RC has attracted increasing attention (Nguyen et al., 2016; Dunn et al., 2017; Dhingra et al., 2017b; He et al., 2017) and raised new challenges for question answering techniques.\n[BOS] In these scenarios, a question is paired with multiple passages, which are often collected by exploiting unstructured documents or web data.\n[BOS] Aforementioned approaches often rely on recurrent neural networks and sophisticated attentions, which are prohibitively time-consuming if passages are concatenated altogether.\n[BOS] Therefore, some work tried to alleviate this problem in a coarse-to-fine schema.\n[BOS] Wang et al. (2018a) combined a ranker for selecting the relevant passage and a reader for producing the answer from it.\n[BOS] However, this approach only depended on one passage when producing the answer, hence put great demands on the precisions of both components.\n[BOS] Worse still, this framework cannot handle the situation where multiple passages are needed to answer correctly.\n[BOS] In consideration of evidence aggregation, Wang et al. (2018b) proposed a re-ranking method to resolve the above issue.\n[BOS] However, their re-ranking stage was totally isolated from the candidate extraction procedure.\n[BOS] Being different from the re-ranking perspective, we propose a novel selection model to combine the information from all the extracted candidates.\n[BOS] Moreover, with reinforcement learning, our candidate extraction and answer selection models can be learned in a joint manner.\n[BOS] Trischler et al. (2016) also proposed a two-step extractor-reasoner model, which first extracted K most probable single-token answer candidates and then compared the hypotheses with all the sentences in the passage.\n[BOS] However, in their work, each candidate was considered isolatedly, and their objective only took into account the ground truths compared with our RL treatment.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection", "Reflection", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 5, "token_end": 45, "char_start": 21, "char_end": 151, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Nguyen et al., 2016;": "1289517", "Dunn et al., 2017;": "11606382", "Dhingra et al., 2017b;": "2417413", "He et al., 2017)": "3662564"}}}, {"token_start": 126, "token_end": 199, "char_start": 635, "char_end": 1019, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wang et al. (2018a)": "21928029"}, "Reference": {}}}, {"token_start": 200, "token_end": 243, "char_start": 1026, "char_end": 1244, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wang et al. (2018b)": "13764176"}, "Reference": {}}}, {"token_start": 292, "token_end": 369, "char_start": 1535, "char_end": 1915, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Trischler et al. (2016)": "711424"}, "Reference": {}}}]}
{"id": "21703007_0", "paragraph": "[BOS] In recent years, reading comprehension has made remarkable progress in methodology and dataset construction.\n[BOS] Most existing approaches mainly focus on modeling sophisticated interactions between questions and passages, then use the pointer networks (Vinyals et al., 2015) to directly model the answers (Dhingra et al., 2017a; Wang and Jiang, 2017; Seo et al., 2017; .\n[BOS] These methods prove to be effective in existing close-domain datasets (Hermann et al., 2015; Hill et al., 2015; Rajpurkar et al., 2016) .\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 36, "token_end": 48, "char_start": 243, "char_end": 282, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vinyals et al., 2015)": "5692837"}}}, {"token_start": 49, "token_end": 77, "char_start": 286, "char_end": 375, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dhingra et al., 2017a;": null, "Wang and Jiang, 2017;": "5592690"}}}, {"token_start": 88, "token_end": 117, "char_start": 433, "char_end": 520, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hermann et al., 2015;": "6203757", "Hill et al., 2015;": "14915449", "Rajpurkar et al., 2016)": "11816014"}}}]}
{"id": "2269872_0", "paragraph": "[BOS] Reddit is relatively understudied compared to other social networks such as Facebook, but an increasing body of work has used its data to look at topics ranging from online user behavior (Hamilton et al., 2017) to user migration across social media platforms (Newell et al., 2016) .\n[BOS] A map of Reddit using commenter co-occurrences has also been previously created using a much smaller sample of comment data (Olsen and Neal, 2015) by treating the co-occurrence matrix as a weighted graph and extracting the network backbone.\n[BOS] Relatedly, there has been interest in developing vector representations of graph structures as shown by techniques like DeepWalk (Perozzi et al., 2014) and node2vec (Grover and Leskovec, 2016) , which we could potentially use to create additional vector representations to test below.\n[BOS] Reddit communities do not have a built-in explicit graph structure though, as there are not defined links between communities in the same manner as users can be linked by friendship requests on sites like Facebook.\n[BOS] In this paper we show that semantically meaningful maps of communities can be created using the NLP toolbox originally created for mapping the semantic similarity of words, without a need for defining an explicit graph.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Narrative_cite", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 32, "token_end": 46, "char_start": 152, "char_end": 216, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hamilton et al., 2017)": "2768002"}}}, {"token_start": 47, "token_end": 62, "char_start": 220, "char_end": 286, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Newell et al., 2016)": "6932997"}}}, {"token_start": 85, "token_end": 98, "char_start": 396, "char_end": 441, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Olsen and Neal, 2015)": "8882900"}}}, {"token_start": 135, "token_end": 148, "char_start": 662, "char_end": 693, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Perozzi et al., 2014)": "3051291"}}}, {"token_start": 149, "token_end": 162, "char_start": 698, "char_end": 734, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Grover and Leskovec, 2016)": "207238980"}}}]}
{"id": "18990362_2", "paragraph": "[BOS] When we look into other research areas in natural language processing, Gimnez and Mrquez (2008) proposed an automatic error analysis approach in machine translation (MT) technologies.\n[BOS] They developed a metric set which could capture features in MT outputs at different linguistic levels with different levels of granularity.\n[BOS] Like we considered parsing systems, they explored ways to resolve costly and rewardless error analysis in the MT field.\n[BOS] One of their objectives was to enable researchers to easily obtain detailed linguistic reports on the behavior of their systems, and to concentrate on analyses for the system improvements.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 115, "char_start": 6, "char_end": 656, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Gim\u00e9nez and M\u00e0rquez (2008)": "12009874"}, "Reference": {}}}]}
{"id": "18990362_1", "paragraph": "[BOS] In the field of parsing, McDonald and Nivre (2007) compared parsing errors between graphbased and transition-based parsers.\n[BOS] They considered accuracy transitions from various points of view, and the obtained statistical data suggested that error propagation seemed to occur in the graph structures of parsing outputs.\n[BOS] Our research proceeded one step further and attempted to reveal the nature of the propagations.\n[BOS] In examining the combination of the two types of parsing, they utilized approaches similar to our method for capturing inter-dependencies of errors.\n[BOS] They allowed a parser to give only structures produced by the parsers and utilized the ideas for evaluating the parser's potentials, whereas we utilized it for observing error propagations.\n[BOS] Dredze et al. (2007) showed that many of the parsing errors in domain adaptation tasks may come from inconsistencies between the annotations of training resources.\n[BOS] This would suggest that just error comparisons without considering the inconsistencies could lead to a misunder-standing of what happens in domain transitions.\n[BOS] The summarized error cause categories and interdependencies given by our methods would be useful clues for extracting such domain-dependent error phenomena.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Reflection", "Reflection", "Reflection", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 59, "char_start": 6, "char_end": 328, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 141, "token_end": 199, "char_start": 788, "char_end": 1117, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Dredze et al. (2007)": "5811151"}, "Reference": {}}}]}
{"id": "18990362_0", "paragraph": "[BOS] Although there have been many researchers who analyzed errors in their own systems in the experiments, there has been little research which focused on error analysis itself.\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "17042632_2", "paragraph": "[BOS] Most of the works dealing with semantic similar-ity use n-grams, metadata features and stop words as we do.\n[BOS] Our scores are not among the highest in subtask A of Task 3, but they come close to and substantially differ from the average score in this field of works.\n\n", "discourse_tags": ["Transition", "Reflection"], "span_citation_mapping": []}
{"id": "17042632_1", "paragraph": "[BOS] Another work of (Vilarino et al., 2014 ) also uses n-grams, cosine similarity and that is a common feature with our system.\n[BOS] Some differing features are Jaccard coefficient, Latent Semantic Analysis, Pointwise Mutual Information.\n[BOS] Their results are very close to ours.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 60, "char_start": 6, "char_end": 284, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Vilarino et al., 2014": "10738010"}, "Reference": {}}}]}
{"id": "17042632_0", "paragraph": "[BOS] Several recent systems were created and used for similar analysis.\n[BOS] Although their applications have some differences from the system described in this paper, we consider them relevant because they deal with semantic similarity.\n[BOS] (Bakaya, 2014) uses Vector Space Models which have some similarity to our usage of word2vec centroid metrics with the difference that we do not organize the whole text according to the structure of the result matrix, as the VSMs do.\n[BOS] The cosine similarity is common for both systems.\n[BOS] The big difference is that we use only the input words while in his system the words' likely synonyms according to a language model are also used.\n[BOS] We believe this contributes to the consistently higher scores of his system.\n\n", "discourse_tags": ["Transition", "Reflection", "Single_summ", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 40, "token_end": 99, "char_start": 246, "char_end": 534, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Ba\u015fkaya, 2014)": null}, "Reference": {}}}]}
{"id": "1830932_7", "paragraph": "[BOS] A convenient aspect of this approach is that 2 log  is asymptotically  2 distributed.\n[BOS] So for a resulting 2 log  value, we can use the  2 table to find the significance level with which the null hypothesis H 1 can be rejected.\n[BOS] For example, a value of 10 corresponds to a significance level of 0.001 and is standardly used as the cutoff.\n[BOS] Words with 2 log  > 10 are considered topic words.\n[BOS] Conroy et al. (2006)'s system gives a weight of 1 to the topic words and scores sentences using the number of topic words normalized by sentence length.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Transition", "Single_summ"], "span_citation_mapping": [{"token_start": 88, "token_end": 122, "char_start": 417, "char_end": 569, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "1830932_6", "paragraph": "[BOS] p, p 1 and p 2 are estimated by maximum likelihood.\n[BOS] p = c t /N where c t is the number of times word t appears in the total set of tokens comprising {B, I}.\n[BOS] p 1 = c I t /N I and p 2 = c B t /N B are the probabilities of t estimated only from the input and only from the background respectively.\n\n", "discourse_tags": ["Other", "Other", "Other"], "span_citation_mapping": []}
{"id": "1830932_5", "paragraph": "[BOS] The likelihood ratio compares the likelihood of the data D = {B, I} under the two hypotheses.\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "1830932_4", "paragraph": "[BOS] H 1 : A word t is not a topic word and occurs with equal probability in I and B, i.e. p(t|I) = p(t|B) = p H 2 : t is a topic word, hence p(t|I) = p 1 and p(t|B) = p 2 and p 1 > p 2 A set of documents D containing N tokens is viewed as a sequence of words w 1 w 2 ...w N .\n[BOS] The word in each position i is assumed to be generated by a Bernoulli trial which succeeds when the generated word w i = t and fails when w i is not t. Suppose that the probability of success is p. Then the probability of a word t appearing k times in a dataset of N tokens is the binomial probability:\n\n", "discourse_tags": ["Other", "Other"], "span_citation_mapping": []}
{"id": "1830932_3", "paragraph": "[BOS] Computing topic words: Let us call the input set I and the background B.\n[BOS] The log-likelihood ratio test compares two hypotheses:\n\n", "discourse_tags": ["Transition", "Other"], "span_citation_mapping": []}
{"id": "1830932_2", "paragraph": "[BOS] Even for generic summarization, some of the best results were obtained by Conroy et al. (2006) by using a large random corpus of news articles as the background while summarizing a new article, an idea first proposed by Lin and Hovy (2000) .\n[BOS] Central to this approach is the use of a likelihood ratio test to compute topic words, words that have significantly higher probability in the input compared to the background corpus, and are hence descriptive of the input's topic.\n[BOS] In this work, we compare our system to topic word based ones since the latter is also a general method to find surprising new words in a set of input documents but is not a bayesian approach.\n[BOS] We briefly explain the topic words based approach below.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 8, "token_end": 24, "char_start": 38, "char_end": 100, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Conroy et al. (2006)": "7096616"}}}, {"token_start": 42, "token_end": 54, "char_start": 200, "char_end": 245, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Lin and Hovy (2000)": "8598694"}}}]}
{"id": "1830932_1", "paragraph": "[BOS] For update summarization of news, methods range from textual entailment techniques (Bentivogli et al., 2010) to find facts in the input which are not entailed by the background, to Bayesian topic models (Delort and Alfonseca, 2012) which aim to learn and use topics discussed only in background, those only in the update input and those that overlap across the two sets.\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 12, "token_end": 27, "char_start": 59, "char_end": 114, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bentivogli et al., 2010)": null}}}, {"token_start": 43, "token_end": 56, "char_start": 187, "char_end": 237, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Delort and Alfonseca, 2012)": "639440"}}}]}
{"id": "1830932_0", "paragraph": "[BOS] Computing new information is useful in many applications.\n[BOS] The TREC novelty tasks (Allan et al., 2003; Soboroff and Harman, 2005; Schiffman, 2005) tested the ability of systems to find novel information in an IR setting.\n[BOS] Systems were given a list of documents ranked according to relevance to a query.\n[BOS] The goal is to find sentences in each document which are relevant to the query, and at the same time is new information given the content of documents higher in the relevance list.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 12, "token_end": 41, "char_start": 70, "char_end": 157, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Allan et al., 2003;": "1411108", "Soboroff and Harman, 2005;": "355852", "Schiffman, 2005)": null}}}]}
{"id": "17899072_2", "paragraph": "[BOS] Our work also has interesting connections with research on automatic textual entailment (Dagan et al., 2005) , where the goal is to determine whether a given sentence can be inferred from text.\n[BOS] While we are not assessing an inference relation between a reference and a system output, the two tasks face similar challenges.\n[BOS] Methods for entailment recognition extensively rely on lexico-semantic resources (Haghighi et al., 2005; Harabagiu et al., 2001 ), and we believe that our method for contextual substitution can be beneficial in that context.\n\n", "discourse_tags": ["Narrative_cite", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 11, "token_end": 24, "char_start": 65, "char_end": 114, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 74, "token_end": 98, "char_start": 396, "char_end": 468, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Haghighi et al., 2005;": "779551", "Harabagiu et al., 2001": null}}}]}
{"id": "17899072_1", "paragraph": "[BOS] Our approach differs from traditional work on automatic paraphrasing in goal and methodology.\n[BOS] Unlike previous approaches, we are not aiming to produce any paraphrase of a given sentence since paraphrases induced from a parallel corpus do not necessarily produce a rewriting that makes a reference closer to the system output.\n[BOS] Thus, we focus on words that appear in the system output and aim to determine whether they can be used to rewrite a reference sentence.\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "17899072_0", "paragraph": "[BOS] Automatic Paraphrasing and Entailment Our work is closely related to research in automatic paraphrasing, in particular, to sentence level paraphrasing (Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004) .\n[BOS] Most of these approaches learn paraphrases from a parallel or comparable monolingual corpora.\n[BOS] Instances of such corpora include multiple English translations of the same source text written in a foreign language, and different news articles about the same event.\n[BOS] For example, Pang et al. (2003) expand a set of reference translations using syntactic alignment, and generate new reference sentences that could be used in automatic evaluation.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Single_summ"], "span_citation_mapping": [{"token_start": 26, "token_end": 56, "char_start": 129, "char_end": 220, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Barzilay and Lee, 2003;": "6387310", "Pang et al., 2003;": "11728052", "Quirk et al., 2004)": "13043395"}}}, {"token_start": 105, "token_end": 139, "char_start": 504, "char_end": 682, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Pang et al. (2003)": "11728052"}, "Reference": {}}}]}
{"id": "2174668_2", "paragraph": "[BOS] There has also been a strand of work applying global optimization to neural network parsing.\n[BOS] Zhou et al. (2015) and Andor et al. (2016) extend the parser of Zhang and Clark (2011) , using beam search and early update training.\n[BOS] They set a max-likelihood training objective, using probability mass in the beam to approximate partition function of CRF training.\n[BOS] Watanabe and Sumita (2015) study constituent parsing by using a large-margin objective, where the negative example is the expected score of all states in the beam for transition-based parsing.\n[BOS] Xu et al. (2016) build CCG parsing models with a training objective of maximizing the expected F1 score of all items in the beam when parsing finishes, under the transition-based system.\n[BOS] More relatedly, Wiseman and Rush (2016) use beam search and global maxmargin training for the method of .\n[BOS] In contrast, we use greedy local model; our method is orthogonal to these techniques.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 19, "token_end": 78, "char_start": 105, "char_end": 376, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhou et al. (2015)": "17887856", "Andor et al. (2016)": "2952144"}, "Reference": {"Zhang and Clark (2011)": "7245369"}}}, {"token_start": 79, "token_end": 117, "char_start": 383, "char_end": 575, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Watanabe and Sumita (2015)": null}, "Reference": {}}}, {"token_start": 118, "token_end": 159, "char_start": 582, "char_end": 768, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Xu et al. (2016)": "2778076"}, "Reference": {}}}, {"token_start": 160, "token_end": 186, "char_start": 775, "char_end": 880, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wiseman and Rush (2016)": "2783746"}, "Reference": {}}}]}
{"id": "2174668_1", "paragraph": "[BOS] The only existing method that directly applies the encoder-decoder structure of NMT to parsing is , who applied two-lay LSTM for the encoder, and two-layer LSTM decoder to generate bracket syntactic trees.\n[BOS] To our knowledge, we are the first to try a straight forward attention over the encoder-decoder structure for shift-reduce parsing.\n[BOS] can also be understood as building a language model over bracket constitute trees.\n[BOS] A similar idea is proposed by Choe and Charniak (2016) , who directly use LSTMs to model such output forms.\n[BOS] The language model is used to rerank candidate trees from a baseline parser, and trained over large automatically parsing data using tri-training, achieving a current best results for constituent parsing.\n[BOS] Our work is similar in that it can be regarded as a form of language model, over shift-reduce actions rather than bracketed syntactic trees.\n[BOS] Hence, our model can potentially be used for under tri-training settings also.\n\n", "discourse_tags": ["Transition", "Reflection", "Reflection", "Single_summ", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 88, "token_end": 153, "char_start": 445, "char_end": 763, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Choe and Charniak (2016)": "81026"}, "Reference": {}}}]}
{"id": "2174668_0", "paragraph": "[BOS] LSTM encoder structures have been used in both transition-based and graph-based parsing.\n[BOS] Among transition-based parsers, Kiperwasser and Goldberg (2016) use two-layer encoder to encode input sentence, extracting 11 different features from a given state in order to predict the next transition action, showing that the encoder structure lead to significant accuracy improvements over the baseline parser of Chen and Manning (2014) .\n[BOS] Among graph-based parsers, Dozat and Manning (2017) exploit 4-layer LSTM encoder over the input, using conceptually simple biaffine attention mechanism to model dependency arcs over the encoder, resulting in the stat-of-the-art accuracy in dependency parsing.\n[BOS] Their success forms a strong motivation of our work.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 21, "token_end": 86, "char_start": 101, "char_end": 441, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kiperwasser and Goldberg (2016)": "1642392"}, "Reference": {"Chen and Manning (2014)": null}}}, {"token_start": 88, "token_end": 146, "char_start": 450, "char_end": 709, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Dozat and Manning (2017)": "7942973"}, "Reference": {}}}]}
{"id": "18044955_1", "paragraph": "[BOS] We follow an open IE approach (Banko and Etzioni, 2008) and use dependencies to identify the elements.\n[BOS] In contrast to most previous work (Pardo et al., 2006; Yao et al., 2011; Yao et al., 2012) , we have no pre-defined set of types, but try to learn it along with the relations.\n[BOS] Some approaches use types from general data bases such as Wikipedia, Freebase, etc.\n[BOS] (Yan et al., 2009; Eichler et al., 2008; Syed and Viegas, 2010) , sidestepping the question how to construct those DBs in the first place.\n[BOS] We are less concerned with extraction performance, but focus on the accuracy of the learned type system by measuring how well it performs in a prediction task.\n[BOS] Talukdar et al. (2008) and Talukdar and Pereira (2010) present graph-based approaches to the similar problem of class-instance learning.\n[BOS] While this provides a way to discover types, it requires a large graph that does not easily generalize to new instances (transductive), since it produces no predictive model.\n[BOS] The models we use are transductive and can be applied to unseen data.\n[BOS] Our approach follows Hovy et al. (2011) .\n[BOS] However, they only evaluate one model on football by collecting sensibility ratings from Mechanical Turk.\n[BOS] Our method provides extrinsic measures of performance on several domains.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Transition", "Narrative_cite", "Reflection", "Multi_summ", "Multi_summ", "Reflection", "Single_summ", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 18, "char_start": 6, "char_end": 61, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Banko and Etzioni, 2008)": "6983197"}}}, {"token_start": 27, "token_end": 56, "char_start": 115, "char_end": 205, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Pardo et al., 2006;": "11478093", "Yao et al., 2011;": "226832", "Yao et al., 2012)": "1290272"}}}, {"token_start": 96, "token_end": 138, "char_start": 387, "char_end": 525, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yan et al., 2009;": "5715019", "Eichler et al., 2008;": "15371289", "Syed and Viegas, 2010)": "5607473"}}}, {"token_start": 169, "token_end": 239, "char_start": 698, "char_end": 1015, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Talukdar et al. (2008)": "11311232", "Talukdar and Pereira (2010)": "12363276"}, "Reference": {}}}, {"token_start": 257, "token_end": 268, "char_start": 1098, "char_end": 1137, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Hovy et al. (2011)": "1801556"}}}]}
{"id": "18044955_0", "paragraph": "[BOS] In relation extraction, we have to identify the relation elements, and then map the arguments to types.\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "17476563_3", "paragraph": "[BOS] Finally, our work is related to the reinforcement learning literature.\n[BOS] Hard and soft attention were examined in the context of caption generation (Xu et al., 2015) .\n[BOS] Curriculum learning was investigated in Sachan and Xing (2016) , but they focused on the ordering of training examples while we combine supervision signals.\n[BOS] Reinforcement learning recently gained popularity in tasks such as coreference resolution (Clark and Manning, 2016) , information extraction (Narasimhan et al., 2016) , semantic parsing (Andreas et al., 2016) and textual games (Narasimhan et al., 2015; He et al., 2016) .\n\n", "discourse_tags": ["Reflection", "Narrative_cite", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 25, "token_end": 36, "char_start": 139, "char_end": 175, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Xu et al., 2015)": "1055111"}}}, {"token_start": 38, "token_end": 51, "char_start": 184, "char_end": 246, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Sachan and Xing (2016)": "16503693"}}}, {"token_start": 77, "token_end": 88, "char_start": 414, "char_end": 462, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Clark and Manning, 2016)": "2012188"}}}, {"token_start": 89, "token_end": 102, "char_start": 465, "char_end": 513, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Narasimhan et al., 2016)": "12203802"}}}, {"token_start": 103, "token_end": 114, "char_start": 516, "char_end": 555, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Andreas et al., 2016)": "3130692"}}}, {"token_start": 115, "token_end": 135, "char_start": 560, "char_end": 616, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Narasimhan et al., 2015;": "8395799", "He et al., 2016)": "2047201"}}}]}
{"id": "17476563_2", "paragraph": "[BOS] Hierarchical models which treats sentence selection as a latent variable have been applied text categorization (Yang et al., 2016b) , extractive summarization (Cheng and Lapata, 2016) , machine translation (Ba et al., 2014) and sentiment analysis (Yessenalina et al., 2010; Lei et al., 2016) .\n[BOS] To the best of our knowledge, we are the first to use the hierarchical nature of a document for QA.\n\n", "discourse_tags": ["Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 15, "token_end": 26, "char_start": 97, "char_end": 137, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yang et al., 2016b)": "6857205"}}}, {"token_start": 27, "token_end": 39, "char_start": 140, "char_end": 189, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cheng and Lapata, 2016)": "1499080"}}}, {"token_start": 40, "token_end": 50, "char_start": 192, "char_end": 229, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ba et al., 2014)": "14814581"}}}, {"token_start": 51, "token_end": 71, "char_start": 234, "char_end": 297, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yessenalina et al., 2010;": "9021280", "Lei et al., 2016)": "7205805"}}}]}
{"id": "17476563_1", "paragraph": "[BOS] Answer sentence selection is studied with the TREC QA (Voorhees and Tice, 2000), WikiQA (Yang et al., 2016b) and SelQA (Jurczyk et al., 2016) datasets.\n[BOS] Recently, neural networks models (Wang and Nyberg, 2015; Severyn and Moschitti, 2015; dos Santos et al., 2016) achieved improvements.\n[BOS] Sultan et al. (2016) optimized the answer sentence extraction and the answer extraction jointly, but with gold labels for both parts.\n[BOS] Trischler et al. (2016b) proposed a model that shares the intuition of observing inputs at multiple granularities (sentence, word), but deals with multiple choice questions.\n[BOS] Our answer sentence selection as latent and generates answer strings instead of selecting text spans.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 9, "token_end": 22, "char_start": 52, "char_end": 85, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 23, "token_end": 35, "char_start": 87, "char_end": 114, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yang et al., 2016b)": "6857205"}}}, {"token_start": 36, "token_end": 48, "char_start": 119, "char_end": 147, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Jurczyk et al., 2016)": "7560935"}}}, {"token_start": 53, "token_end": 85, "char_start": 174, "char_end": 297, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wang and Nyberg, 2015;": "2654931", "Severyn and Moschitti, 2015;": "3356807", "dos Santos et al., 2016)": "14163772"}}}, {"token_start": 86, "token_end": 113, "char_start": 304, "char_end": 437, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Sultan et al. (2016)": "8566145"}, "Reference": {}}}, {"token_start": 114, "token_end": 151, "char_start": 444, "char_end": 617, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Trischler et al. (2016b)": "12834729"}, "Reference": {}}}]}
{"id": "17476563_0", "paragraph": "[BOS] There has been substantial interest in datasets for reading comprehension.\n[BOS] MCTest (Richardson et al., 2013 ) is a smaller-scale datasets focusing on common sense reasoning; bAbi is a synthetic dataset that captures various aspects of reasoning; and SQuAD (Rajpurkar et al., 2016; Xiong et al., 2016) and NewsQA (Trischler et al., 2016a) are QA datasets where the answer is a span in the document.\n[BOS] Compared to Wikireading, some datasets covers shorter passages (average 122 words for SQuAD).\n[BOS] Cloze-style question answering datasets (Hermann et al., 2015; Onishi et al., 2016; Hill et al., 2015) assess machine comprehension but do not form questions.\n[BOS] The recently released MS MARCO dataset (Nguyen et al., 2016) consists of query logs, web documents and crowd-sourced answers.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Transition", "Narrative_cite", "Single_summ"], "span_citation_mapping": [{"token_start": 14, "token_end": 48, "char_start": 87, "char_end": 255, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Richardson et al., 2013": "2100831"}, "Reference": {}}}, {"token_start": 50, "token_end": 97, "char_start": 261, "char_end": 408, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Rajpurkar et al., 2016;": "11816014", "Xiong et al., 2016)": "3714278", "(Trischler et al., 2016a)": "1167588"}, "Reference": {}}}, {"token_start": 119, "token_end": 159, "char_start": 515, "char_end": 673, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hermann et al., 2015;": "6203757", "Onishi et al., 2016;": null, "Hill et al., 2015)": "14915449"}}}, {"token_start": 160, "token_end": 189, "char_start": 680, "char_end": 805, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Nguyen et al., 2016)": "1289517"}, "Reference": {}}}]}
{"id": "16048023_0", "paragraph": "[BOS] There are several studies that tried to overcome the limited feature scope of graph-based dependency parsing models .\n[BOS] Nakagawa (2007) proposed a method to deal with the intractable inference problem in a graphbased model by introducing the Gibbs sampling algorithm.\n[BOS] Compared with their approach, our approach is much simpler yet effective.\n[BOS] Hall (2007) used a re-ranking scheme to provide global features while we simply augment the features of an existing parser.\n[BOS] Nivre and McDonald (2008) and Zhang and Clark (2008) proposed stacking methods to combine graph-based parsers with transition-based parsers.\n[BOS] One parser uses dependency predictions made by another parser.\n[BOS] Our results show that our approach can be used in the stacking frameworks to achieve higher accuracy.\n\n", "discourse_tags": ["Transition", "Single_summ", "Reflection", "Single_summ", "Multi_summ", "Multi_summ", "Reflection"], "span_citation_mapping": [{"token_start": 23, "token_end": 50, "char_start": 130, "char_end": 277, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Nakagawa (2007)": "9310605"}, "Reference": {}}}, {"token_start": 65, "token_end": 90, "char_start": 364, "char_end": 487, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hall (2007)": "5625678"}, "Reference": {}}}, {"token_start": 91, "token_end": 133, "char_start": 494, "char_end": 703, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Nivre and McDonald (2008)": "9431510", "Zhang and Clark (2008)": "15533677"}, "Reference": {}}}]}
{"id": "19488885_3", "paragraph": "[BOS] Much effort has been devoted to joint learning of syntactic and semantic parsing, including two CoNLL shared tasks (Surdeanu et al., 2008; Haji et al., 2009) .\n[BOS] Despite their conceptual and practical appeal, such joint models rarely outperform the pipeline approach Henderson et al., 2013; Lewis et al., 2015; Swayamdipta et al., 2016 Swayamdipta et al., , 2017 .\n[BOS] Peng et al. (2017a) performed MTL for SDP in a closely related setting to ours.\n[BOS] They tackled three tasks, annotated over the same text and sharing the same formal structures (bilexical DAGs), with considerable edge overlap, but differing in target representations (see 3).\n[BOS] For all tasks, they reported an increase of 0.5-1 labeled F 1 points.\n[BOS] Recently, Peng et al. (2018) applied a similar approach to joint frame-semantic parsing and semantic dependency parsing, using disjoint datasets, and reported further improvements.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 17, "token_end": 41, "char_start": 98, "char_end": 163, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Surdeanu et al., 2008;": "6534839", "Haji\u010d et al., 2009)": "9210201"}}}, {"token_start": 56, "token_end": 94, "char_start": 259, "char_end": 372, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Henderson et al., 2013;": "1392116", "Lewis et al., 2015;": "9991841", "Swayamdipta et al., 2016": "926149", "Swayamdipta et al., , 2017": "3170713"}}}, {"token_start": 96, "token_end": 179, "char_start": 381, "char_end": 735, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Peng et al. (2017a)": "15939234"}, "Reference": {}}}, {"token_start": 180, "token_end": 213, "char_start": 742, "char_end": 922, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Peng et al. (2018)": "4956705"}, "Reference": {}}}]}
{"id": "19488885_2", "paragraph": "[BOS] Sharing parameters with a low-level task has shown great benefit for transition-based syntactic parsing, when jointly training with POS tagging (Bohnet and Nivre, 2012; Zhang and Weiss, 2016) , and with lexical analysis (Constant and Nivre, 2016; More, 2016) .\n[BOS] Recent work has achieved state-of-the-art results in multiple NLP tasks by jointly learning the tasks forming the NLP standard pipeline using a single neural model (Collobert et al., 2011; Hashimoto et al., 2017) , thereby avoiding cascading errors, common in pipelines.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 25, "token_end": 42, "char_start": 138, "char_end": 197, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bohnet and Nivre, 2012;": "1500270", "Zhang and Weiss, 2016)": null}}}, {"token_start": 44, "token_end": 59, "char_start": 204, "char_end": 264, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Constant and Nivre, 2016;": "17999656", "More, 2016)": null}}}, {"token_start": 91, "token_end": 111, "char_start": 417, "char_end": 485, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Collobert et al., 2011;": "351666", "Hashimoto et al., 2017)": "2213896"}}}]}
{"id": "19488885_1", "paragraph": "[BOS] Neural MTL has mostly been effective in tackling formally similar tasks , including multilingual syntactic dependency parsing (Ammar et al., 2016; Guo et al., 2016) , as well as multilingual (Duong et al., 2017) , and cross-domain semantic parsing (Herzig and Berant, 2017; Fan et al., 2017) .\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 17, "token_end": 38, "char_start": 90, "char_end": 170, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Guo et al., 2016)": "16284963"}}}, {"token_start": 42, "token_end": 53, "char_start": 184, "char_end": 217, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Duong et al., 2017)": "11320015"}}}, {"token_start": 55, "token_end": 76, "char_start": 224, "char_end": 297, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Herzig and Berant, 2017;": "14215409", "Fan et al., 2017)": "5955929"}}}]}
{"id": "19488885_0", "paragraph": "[BOS] MTL has been used over the years for NLP tasks with varying degrees of similarity, examples including joint classification of different arguments in semantic role labeling (Toutanova et al., 2005) , and joint parsing and named entity recognition (Finkel and Manning, 2009) .\n[BOS] Similar ideas, of parameter sharing across models trained with different datasets, can be found in studies of domain adaptation (Blitzer et al., 2006; Daume III, 2007; Ziser and Reichart, 2017) .\n[BOS] For parsing, domain adaptation has been applied successfully in parser combination and co-training (McClosky et al., 2010; Baucom et al., 2013) .\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 28, "token_end": 42, "char_start": 155, "char_end": 202, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Toutanova et al., 2005)": "10432514"}}}, {"token_start": 44, "token_end": 59, "char_start": 209, "char_end": 278, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Finkel and Manning, 2009)": "10473638"}}}, {"token_start": 80, "token_end": 105, "char_start": 397, "char_end": 480, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Blitzer et al., 2006;": "15978939", "Daume III, 2007;": "5360764", "Ziser and Reichart, 2017)": "8462113"}}}, {"token_start": 117, "token_end": 142, "char_start": 553, "char_end": 632, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(McClosky et al., 2010;": "10585087", "Baucom et al., 2013)": "9510501"}}}]}
{"id": "2202801_2", "paragraph": "[BOS] In work largely concurrent to our own, Kim et al. (2017) explore the use of conditional random fields (CRFs) to impose a variety of constraints on attention distributions achieving strong results on several sentence level tasks.\n\n", "discourse_tags": ["Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 46, "char_start": 6, "char_end": 234, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kim et al. (2017)": "6961760"}, "Reference": {}}}]}
{"id": "2202801_1", "paragraph": "[BOS] Outside the task of reading comprehension there has been other work on soft attention over text, largely focusing on the problem of attending over single sentences.\n[BOS] Luong et al. (2015) study several issues in the design of soft attention models in the context of translation, and introduce the bilinear scoring function.\n[BOS] They also propose the idea of attention input-feeding where the original attention vectors are concatenated with the hidden representations of the words and fed into the next RNN step.\n[BOS] The goal is to make the model fully aware of the previous alignment choices.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 31, "token_end": 112, "char_start": 177, "char_end": 606, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Luong et al. (2015)": "1998416"}, "Reference": {}}}]}
{"id": "2202801_0", "paragraph": "[BOS] In addition to Chen et al. (2016) 's Stanford Reader model, there have been several other modeling approaches developed to address these reading comprehension tasks.\n[BOS] Seo et al. (2016) introduced the Bi-Directional Attention Flow which consists of a multi-stage hierarchical process to represent context at different levels of granularity; it use the concatenation of passage word representation, question word representation, and the element-wise product of these vectors in their attention flow layer.\n[BOS] This is a more complex variant of the classic bi-linear term that multiplies this concatenated vector with a vector of weights, producing attention scalars.\n[BOS] Dhingra et al. (2016) 's Gated-Attention Reader integrates a multi-hop structure with a novel attention mechanism, essentially building query specific representations of the tokens in the document to improve prediction.\n[BOS] This model conducts a classic dotproduct soft attention to weight the query representations which are then multiplied element-wise with the context representations, and fed into the next layer of RNN.\n[BOS] After several hidden layers that repeat the same process, the dot product between the context representation and the query is used to compute a classic soft-attention.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 17, "char_start": 6, "char_end": 64, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Chen et al. (2016)": "6360322"}}}, {"token_start": 34, "token_end": 126, "char_start": 178, "char_end": 677, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Seo et al. (2016)": "8535316"}, "Reference": {}}}, {"token_start": 127, "token_end": 238, "char_start": 684, "char_end": 1284, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Dhingra et al. (2016)": "6529193"}, "Reference": {}}}]}
{"id": "16041630_3", "paragraph": "[BOS] Differently from Kotlerman et al. (2010) , here we focus on applying directional, asymmetric similarity measures to identify hypernyms.\n[BOS] We assume the classical definition of a hypernymy, such that Y is an hypernym of X if and only if X is a kind of Y , or equivalently every X is a Y .\n\n", "discourse_tags": ["Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 13, "char_start": 6, "char_end": 46, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Kotlerman et al. (2010)": "7187022"}}}]}
{"id": "16041630_2", "paragraph": "[BOS] Automatic identification of hypernyms in corpora is a long-standing research line, but most methods have adopted semi-supervised, pattern-based approaches (Hearst, 1992; Pantel and Pennacchiotti, 2006) .\n[BOS] Fully unsupervised hypernym identification with DSMs is still a largely open field.\n[BOS] Various models to represent hypernyms in vector spaces have recently been proposed (Weeds and Weir, 2003; Weeds et al., 2004; Clarke, 2009) , usually grounded on the Distributional Inclusion Hypothesis (for a different approach based on representing word meaning as \"regions\" in vector space, see Erk (2009a; 2009b) ).\n[BOS] The same hypothesis has been adopted by Kotlerman et al. (2010) to identify (substitutable) lexical entailments\" .\n[BOS] Within the context of the Textual Entailment (TE) paradigm, Zhitomirsky-Geffet and Dagan (2005; 2009) define (substitutable) lexical entailment as a relation holding between two words, if there are some contexts in which one of the words can be substituted by the other and the meaning of the original word can be inferred from the new one.\n[BOS] Its relevance for TE notwithstanding, this notion of lexical entailment is more general and looser than hypernymy.\n[BOS] In fact, it encompasses several standard semantic relations such as synonymy, hypernymy, metonymy, some cases of meronymy, etc.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Narrative_cite", "Single_summ", "Multi_summ", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 23, "token_end": 47, "char_start": 119, "char_end": 207, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hearst, 1992;": "15763200", "Pantel and Pennacchiotti, 2006)": "7463996"}}}, {"token_start": 70, "token_end": 101, "char_start": 334, "char_end": 445, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Weeds and Weir, 2003;": null, "Weeds et al., 2004;": "3016990", "Clarke, 2009)": "7634844"}}}, {"token_start": 106, "token_end": 136, "char_start": 472, "char_end": 621, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Erk (2009a;": "2926844", "2009b)": "6585702"}}}, {"token_start": 139, "token_end": 164, "char_start": 631, "char_end": 742, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kotlerman et al. (2010)": "7187022"}, "Reference": {}}}, {"token_start": 167, "token_end": 243, "char_start": 752, "char_end": 1092, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhitomirsky-Geffet and Dagan (2005;": "2784831", "2009)": "7634844"}, "Reference": {}}}]}
{"id": "16041630_1", "paragraph": "[BOS] The purpose of this paper is to explore the possibility of identifying hypernyms in DSMs with directional (or asymmetric) similarity measures (Kotlerman et al., 2010) .\n[BOS] These measures all rely on some variation of the Distributional Inclusion Hypothesis, according to which if u is a semantically narrower term than v, then a significant number of salient distributional features of u is included in the feature vector of v as well.\n[BOS] Since hypernymy is an asymmetric relation and hypernyms are semantically broader terms than their hyponyms, then we can predict that directional similarity measures are better suited to identify terms related by the hypernymy relation.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 21, "token_end": 38, "char_start": 100, "char_end": 172, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kotlerman et al., 2010)": "7187022"}}}]}
{"id": "16041630_0", "paragraph": "[BOS] Distributional Semantic Models (DSMs) measure the semantic similarity between words with proximity in distributional space.\n[BOS] However, semantically similar words in turn differ for the type of relation holding between them: e.g., dog is strongly similar to both animal and cat, but with different types of relations.\n[BOS] Current DSMs accounts for these facts only partially.\n[BOS] While they may correctly place both animal and cat among the nearest distributional neighbors of dog, they are not able to characterize the different semantic properties of these relations, for instance the fact that hypernymy is an asymmetric semantic relation, since being a dog entails being an animal, but not the other way round.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Transition"], "span_citation_mapping": []}
{"id": "1762731_5", "paragraph": "[BOS] Lastly, we should point out that we are among the recent efforts that deploy reinforcement learning to the field of natural language processing, some of which have achieved encouraging results in the realm of such as neural symbolic machine (Liang et al., 2017) , machine reasoning (Shen et al., 2016) and sequence generation (Ranzato et al., 2015) .\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 40, "token_end": 51, "char_start": 223, "char_end": 267, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Liang et al., 2017)": "2742513"}}}, {"token_start": 52, "token_end": 62, "char_start": 270, "char_end": 307, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Shen et al., 2016)": "6300274"}}}, {"token_start": 63, "token_end": 75, "char_start": 312, "char_end": 354, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ranzato et al., 2015)": "7147309"}}}]}
{"id": "1762731_4", "paragraph": "[BOS] At the high-level, our model can be viewed as a simplified trainable Turing machine, where the controller can move on the input tape.\n[BOS] It is therefore related to the prior work on Neural Turing Machines and especially its RL version (Zaremba and Sutskever, 2015) .\n[BOS] Compared to (Zaremba and Sutskever, 2015) , the output tape in our method is more simple and reward signals in our problems are less sparse, which explains why our model is easy to train.\n[BOS] It is worth noting that Zaremba and Sutskever report difficulty in using policy gradients to train their model.\n[BOS] Our method, by skipping irrelevant content, shortens the length of recurrent networks, thereby addressing the vanishing or exploding gradients in them (Hochreiter et al., 2001) .\n[BOS] The baseline method itself, Long Short Term Memory (Hochreiter and Schmidhuber, 1997), belongs to the same category of methods.\n[BOS] In this category, there are several recent methods that try to achieve the same goal, such as having recurrent networks that operate in different frequency (Koutnik et al., 2014) or is organized in a hierarchical fashion (Chan et al., 2015; Chung et al., 2016) .\n\n", "discourse_tags": ["Reflection", "Narrative_cite", "Narrative_cite", "Single_summ", "Narrative_cite", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 41, "token_end": 60, "char_start": 191, "char_end": 273, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zaremba and Sutskever, 2015)": "17710225"}}}, {"token_start": 62, "token_end": 75, "char_start": 282, "char_end": 323, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zaremba and Sutskever, 2015)": "17710225"}}}, {"token_start": 106, "token_end": 129, "char_start": 476, "char_end": 587, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 150, "token_end": 167, "char_start": 704, "char_end": 770, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hochreiter et al., 2001)": "17278462"}}}, {"token_start": 174, "token_end": 190, "char_start": 807, "char_end": 864, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 220, "token_end": 237, "char_start": 1014, "char_end": 1091, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Koutnik et al., 2014)": "14936429"}}}, {"token_start": 239, "token_end": 259, "char_start": 1098, "char_end": 1173, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chan et al., 2015;": "14177763", "Chung et al., 2016)": "1463401"}}}]}
{"id": "1762731_3", "paragraph": "[BOS] Our method belongs to adaptive computation of neural networks, whose idea is recently explored by (Graves, 2016; Jernite et al., 2016) , where different amount of computations are allocated dynamically per time step.\n[BOS] The main difference between our method and Graves; Jernite et al. 's methods is that our method can set the amount of computation to be exactly zero for many steps, thereby achieving faster scanning over texts.\n[BOS] Even though our method requires policy gradient methods to train, which is a disadvantage compared to (Graves, 2016; Jernite et al., 2016) , we do not find training with policy gradient methods problematic in our experiments.\n\n", "discourse_tags": ["Narrative_cite", "Reflection", "Narrative_cite"], "span_citation_mapping": [{"token_start": 2, "token_end": 32, "char_start": 6, "char_end": 140, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Graves, 2016;": "8224916", "Jernite et al., 2016)": "2514328"}}}, {"token_start": 56, "token_end": 89, "char_start": 280, "char_end": 439, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 90, "token_end": 121, "char_start": 446, "char_end": 584, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Graves, 2016;": "8224916", "Jernite et al., 2016)": "2514328"}}}]}
{"id": "1762731_2", "paragraph": "[BOS] The concept of \"hard\" attention has also been used successfully in the context of making neural network predictions more interpretable (Lei et al., 2016) .\n[BOS] The key difference between our work and Lei et al. (2016) 's method is that our method optimizes for faster inference, and is more dynamic in its jumping.\n[BOS] Likewise is the difference between our approach and the \"soft\" attention approach by (Bahdanau et al., 2014) .\n[BOS] Recently, (Hahn and Keller, 2016) investigate how machine can fixate and skip words, focusing on the comparison between the behavior of machine and human, while our goal is to make reading faster.\n[BOS] They model the probability that each single word should be read in an unsupervised way while ours directly model the probability of how many words should be skipped with supervised learning.\n\n", "discourse_tags": ["Narrative_cite", "Reflection", "Narrative_cite", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 19, "token_end": 33, "char_start": 95, "char_end": 159, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lei et al., 2016)": "7205805"}}}, {"token_start": 35, "token_end": 71, "char_start": 168, "char_end": 322, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Lei et al. (2016)": "7205805"}}}, {"token_start": 82, "token_end": 97, "char_start": 386, "char_end": 437, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bahdanau et al., 2014)": "11212020"}}}, {"token_start": 99, "token_end": 174, "char_start": 446, "char_end": 839, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Hahn and Keller, 2016)": "5239642"}, "Reference": {}}}]}
{"id": "1762731_1", "paragraph": "[BOS] This idea has recently been explored in the context of natural language processing applications, where the main goal is to filter irrelevant content using a small network (Choi et al., 2016) .\n[BOS] Perhaps the most closely related to our work is the concurrent work on learning to reason with reinforcement learning (Shen et al., 2016) .\n[BOS] The key difference between our work and Shen et al. (2016) is that they focus on early stopping after multiple pass of data to ensure accuracy whereas our method focuses on selective reading with single pass to enable fast processing.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Single_summ"], "span_citation_mapping": [{"token_start": 25, "token_end": 38, "char_start": 147, "char_end": 196, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Choi et al., 2016)": "17476563"}}}, {"token_start": 57, "token_end": 67, "char_start": 300, "char_end": 342, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Shen et al., 2016)": "6300274"}}}, {"token_start": 76, "token_end": 113, "char_start": 391, "char_end": 585, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Shen et al. (2016)": "6300274"}, "Reference": {}}}]}
{"id": "1762731_0", "paragraph": "[BOS] Closely related to our work is the idea of learning visual attention with neural networks Sermanet et al., 2014) , where a recurrent model is used to combine visual evidence at multiple fixations processed by a convolutional neural network.\n[BOS] Similar to our approach, the model is trained end-to-end using the REINFORCE algorithm (Williams, 1992) .\n[BOS] However, a major difference between those work and ours is that we have to sample from discrete jumping distribution, while they can sample from continuous distribution such as Gaussian.\n[BOS] The difference is mainly due to the inborn characteristics of text and image.\n[BOS] In fact, as pointed out by , it was difficult to learn policies over more than 25 possible discrete locations.\n\n", "discourse_tags": ["Single_summ", "Narrative_cite", "Reflection", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 12, "token_end": 48, "char_start": 58, "char_end": 246, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Sermanet et al., 2014)": null}, "Reference": {}}}, {"token_start": 65, "token_end": 72, "char_start": 320, "char_end": 356, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Williams, 1992)": "19115634"}}}]}
{"id": "18391662_3", "paragraph": "[BOS] ORE via semantic parsing.\n[BOS] Recently, a method based on SRL, called SRL-IE, has shown that the effectiveness of ORE methods can be improved with semantic features (Christensen et al., 2011) .\n[BOS] We implemented our version of SRL-IE by relying on the output of two SRL systems: Lund (Johansson and Nugues, 2008) and SwiRL (Surdeanu et al., 2003) .\n[BOS] SwiRL is trained on PropBank and expands upon the syntactic features used in previous work.\n[BOS] One of its major limitations is that it is only able to label arguments with verb predicates.\n[BOS] Lund, on the other hand, is based on dependency parsing and is trained on both PropBank and NomBank, making it able to extract relations with both verb and noun predicates.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Reflection", "Transition", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 27, "token_end": 46, "char_start": 105, "char_end": 199, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Christensen et al., 2011)": "15188996"}}}, {"token_start": 68, "token_end": 79, "char_start": 290, "char_end": 323, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Johansson and Nugues, 2008)": "109294"}}}, {"token_start": 80, "token_end": 94, "char_start": 328, "char_end": 357, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Surdeanu et al., 2003)": "5224609"}}}]}
{"id": "18391662_2", "paragraph": "[BOS] The TreeKernel (Xu et al., 2013) method uses a dependency tree kernel to classify whether candidate tree paths are indeed instances of relations.\n[BOS] The shortest path between the two entities along with the shortest path between relational words and an entity are used as input to the tree kernel.\n[BOS] An expanded set of syntactic patterns based on those from ReVerb are used to generate relation candidates.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 2, "token_end": 60, "char_start": 6, "char_end": 306, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Xu et al., 2013)": "6398201"}, "Reference": {}}}]}
{"id": "18391662_1", "paragraph": "[BOS] Shallow ORE. TextRunner (Banko and Etzioni, 2008) and its successor ReVerb (Fader et al., 2011) are based on the idea that most relations are expressed using few syntactic patterns.\n[BOS] ReVerb, for example, detects only three types of relations (\"verb\", \"verb+preposition\" and \"verb+noun+preposition\") .\n[BOS] Following a similar approach, SONEX (Merhav et al., 2012) path in the dependency graph that connects the two named entities.\n[BOS] They limit the search to only paths that start with one of these dependencies: nsubj, rcmod and partmod.\n[BOS] OLLIE (Mausam et al., 2012 ) also extracts relations between two entities.\n[BOS] It applies pattern templates over the dependency subtree containing pairs of entities.\n[BOS] Pattern templates are learned automatically from a large training set that is bootstrapped from high confidence extractions from ReVerb.\n[BOS] OL-LIE merges binary relations that differ only in the preposition and second argument to produce n-ary extractions, as in: (A, \"met with\", B) and (A, \"met in\", C) leading to (A, \"met\", [with B, in C] ).\n\n", "discourse_tags": ["Multi_summ", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Other"], "span_citation_mapping": [{"token_start": 2, "token_end": 47, "char_start": 6, "char_end": 187, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Banko and Etzioni, 2008)": "6983197", "(Fader et al., 2011)": "10318045"}, "Reference": {}}}, {"token_start": 83, "token_end": 138, "char_start": 318, "char_end": 553, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Merhav et al., 2012)": "5449658"}, "Reference": {}}}, {"token_start": 139, "token_end": 196, "char_start": 560, "char_end": 870, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Mausam et al., 2012": "74065"}, "Reference": {}}}]}
{"id": "18391662_0", "paragraph": "[BOS] Others have pointed out the importance of understanding the trade-off between \"shallow\" versus \"deep\" NLP in ORE. One side of the argument favors shallow methods, claiming deep NLP costs orders of magnitude more and provide much less dramatic gains in terms of effectiveness (Christensen et al., 2011) .\n[BOS] The counterpoint, illustrated with a recent analysis on a industrial-scale Web crawl (Dalvi et al., 2012) , is that the diversity with which information is encoded in text is too high.\n[BOS] Framing the debate as \"shallow\" versus \"deep\" is perhaps convenient, but nevertheless an oversimplification.\n[BOS] This paper sheds more light into the debate by comparing the state-of-the-art from three broad classes of approaches.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 38, "token_end": 65, "char_start": 178, "char_end": 307, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Christensen et al., 2011)": "15188996"}}}, {"token_start": 78, "token_end": 93, "char_start": 374, "char_end": 421, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dalvi et al., 2012)": "6820835"}}}]}
{"id": "19783999_1", "paragraph": "[BOS] Similar approaches, that is, usage of sentiment lexicons in a supervised setup, word embeddings, etc.\n[BOS] were also seen in the proposed systems of SemEval 2015 Task 10 (Subtask E) (Rosenthal et al., 2015) .\n\n", "discourse_tags": ["Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 32, "token_end": 52, "char_start": 156, "char_end": 213, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rosenthal et al., 2015)": "17175925"}}}]}
{"id": "19783999_0", "paragraph": "[BOS] In SemEval 2016 Task 7 the objective was to attribute an intensity score to English and Arabic phrases (Kiritchenko et al., 2016) .\n[BOS] Mostly supervised methods were used, with a variety of features, including different sentiment lexicons, word embeddings, point wise mutual information (PMI) scores between terms (single words and multiword phrases), lists of words which express negation, modifiers etc.\n[BOS] Team ECNU (Wang et al., 2016) approached it as a ranking task, using Random Forest algorithm.\n[BOS] UWB, iLab-Edinburgh and NileTMRG all treated the task as a regression problem, and had supervised approaches.\n[BOS] UWB used Gaussian Regression (Hercig et al., 2016) , while iLab-Edinburgh went in for linear regression (Refaee and Rieser, 2016) .\n[BOS] Team LSIS (Htait et al., 2016) had a completely unsupervised approach, using sentiment lexicons and PMI scores.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Single_summ", "Transition", "Narrative_cite", "Single_summ"], "span_citation_mapping": [{"token_start": 18, "token_end": 33, "char_start": 82, "char_end": 135, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kiritchenko et al., 2016)": null}}}, {"token_start": 89, "token_end": 112, "char_start": 421, "char_end": 514, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Wang et al., 2016)": "18911085"}, "Reference": {}}}, {"token_start": 139, "token_end": 153, "char_start": 637, "char_end": 687, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hercig et al., 2016)": "13997378"}}}, {"token_start": 155, "token_end": 174, "char_start": 696, "char_end": 766, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Refaee and Rieser, 2016)": "11861671"}}}, {"token_start": 176, "token_end": 204, "char_start": 775, "char_end": 886, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Htait et al., 2016)": "17666789"}, "Reference": {}}}]}
{"id": "17163745_2", "paragraph": "[BOS] For answer sentence selection.\n[BOS] Prior work in measuring the relevance between question and answer is mainly in word-level and syntactic-level (Wang and Manning, 2010; Heilman and Smith, 2010; Yih et al., 2013) .\n[BOS] Learning representation by neural network architecture (Yu et al., 2014; Wang and Nyberg, 2015; Severyn and Moschitti, 2015) has become a hot research topic to go beyond word-level or phrase-level methods.\n[BOS] Compared to previous works we find that, (i) Large scale existing resources with noise have more advantages as training data.\n[BOS] (ii) Knowledge-based semantic models can play important roles.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 21, "token_end": 51, "char_start": 122, "char_end": 220, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wang and Manning, 2010;": "16725676", "Heilman and Smith, 2010;": "279533", "Yih et al., 2013)": "10402642"}}}, {"token_start": 56, "token_end": 84, "char_start": 256, "char_end": 353, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yu et al., 2014;": null, "Wang and Nyberg, 2015;": "2654931", "Severyn and Moschitti, 2015)": "3356807"}}}]}
{"id": "17163745_1", "paragraph": "[BOS] For short text conversation.\n[BOS] With the fast development of social media, such as microblog and CQA services, large scale conversation data and data-driven approaches become possible.\n[BOS] Ritter et al. (2011) proposed an SMT based method, which treats response generation as a machine translation task.\n[BOS] Shang et al. (2015) presented an RNN based method, which is trained based on a large number of single round conversation data.\n[BOS] Grammatical and fluency problems are the biggest issue for such generation-based approaches.\n[BOS] Retrievalbased methods selects the most suitable response to the current utterance from the large number of Q-R pairs.\n[BOS] Ji et al. (2014) built a conversation system using learning to rank and semantic matching techniques.\n[BOS] However, collecting enough Q-R pairs to build chatbots is often intractable for many domains.\n[BOS] Compared to previous methods, DocChat learns internal relationships between utterances and responses based on statistical models at different levels of granularity, and relax the dependency on Q-R pairs as response sources.\n[BOS] These make DocChat as a general response generation solution to chatbots, with high adaptation capability.\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ", "Single_summ", "Transition", "Transition", "Single_summ", "Single_summ", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 38, "token_end": 63, "char_start": 200, "char_end": 314, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ritter et al. (2011)": "780171"}, "Reference": {}}}, {"token_start": 64, "token_end": 93, "char_start": 321, "char_end": 447, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Shang et al. (2015)": "7356547"}, "Reference": {}}}, {"token_start": 133, "token_end": 174, "char_start": 678, "char_end": 879, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ji et al. (2014)": "18380963"}, "Reference": {}}}]}
{"id": "17163745_0", "paragraph": "[BOS] For modeling dialogue.\n[BOS] Previous works mainly focused on rule-based or learning-based approaches (Litman et al., 2000; Schatzmann et al., 2006; Williams and Young, 2007) .\n[BOS] These methods require efforts on designing rules or labeling data for training, which suffer the coverage issue.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 12, "token_end": 43, "char_start": 68, "char_end": 180, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Litman et al., 2000;": "901267", "Schatzmann et al., 2006;": null, "Williams and Young, 2007)": "13903063"}}}]}
{"id": "16692427_0", "paragraph": "[BOS] This year's CoNLL 2016 Shared Task on Shallow Discourse Parsing (Xue et al., 2016) is the second edition of the shared task after the CoNLL 2015 Shared task on Shallow Discourse Parsing .\n[BOS] The difference to last year's task is that there is a new Supplementary Task on Discourse Relation Sense classification, where participants are not required to build an end-to-end discourse relation parser but can participate with a sense classification system only.\n[BOS] Discourse relations in the task are divided in two major types: Explicit and Non-Explicit (Implicit, EntRel and AltLex).\n[BOS] Detecting the sense of Explicit relations is an easy task: given the discourse connective, the relation sense can be determined with very high accuracy (Pitler et al., 2008) .\n[BOS] A challenging task is to detect the sense of NonExplicit discourse relations, as they usually don't have a connective that can help to determine their sense.\n[BOS] In last year's task Non-Explicit relations have been tackled with features based on Brown clusters (Chiarcos and Schenk, 2015; Wang and Lan, 2015; Stepanov et al., 2015) , VerbNet classes (Kong et al., 2015; Lalitha Devi et al., 2015) and MPQA polarity lexicon (Wang and Lan, 2015; Lalitha Devi et al., 2015) .\n[BOS] Earlier work (Rutherford and Xue, 2014) employed Brown cluster and coreference patterns to identify senses of implicit discourse relations in naturally occurring text.\n[BOS] More recently improved inference of implicit discourse relations via classifying explicit discourse connectives, extending prior research (Marcu and Echihabi, 2002; Sporleder and Lascarides, 2008) .\n[BOS] Several neural network approaches have been proposed, e.g., Multi-task Neural Networks (Liu et al., 2016) and Shallow-Convolutional Neural Networks (Zhang et al., 2015) .\n[BOS] Braud and Denis (2015) compare word representations for implicit discourse relation classification and find that denser representations systematically outperform sparser ones.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Narrative_cite", "Transition", "Narrative_cite", "Single_summ", "Narrative_cite", "Narrative_cite", "Single_summ"], "span_citation_mapping": [{"token_start": 6, "token_end": 23, "char_start": 18, "char_end": 88, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Xue et al., 2016)": null}}}, {"token_start": 137, "token_end": 156, "char_start": 691, "char_end": 773, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Pitler et al., 2008)": "464400"}}}, {"token_start": 208, "token_end": 234, "char_start": 1030, "char_end": 1115, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chiarcos and Schenk, 2015;": "16784668", "Wang and Lan, 2015;": null, "Stepanov et al., 2015)": "17016748"}}}, {"token_start": 235, "token_end": 256, "char_start": 1118, "char_end": 1180, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kong et al., 2015;": "15461160", "Lalitha Devi et al., 2015)": "18819968"}}}, {"token_start": 257, "token_end": 278, "char_start": 1185, "char_end": 1254, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wang and Lan, 2015;": null, "Lalitha Devi et al., 2015)": "18819968"}}}, {"token_start": 280, "token_end": 310, "char_start": 1263, "char_end": 1430, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 326, "token_end": 349, "char_start": 1550, "char_end": 1633, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Marcu and Echihabi, 2002;": "210363", "Sporleder and Lascarides, 2008)": null}}}, {"token_start": 364, "token_end": 377, "char_start": 1702, "char_end": 1747, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Liu et al., 2016)": "15782198"}}}, {"token_start": 378, "token_end": 391, "char_start": 1752, "char_end": 1810, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhang et al., 2015)": "12968123"}}}, {"token_start": 393, "token_end": 421, "char_start": 1819, "char_end": 1994, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Braud and Denis (2015)": "8961026"}, "Reference": {}}}]}
{"id": "18037181_0", "paragraph": "[BOS] The first attempt to automatically generate AMR structures from sentences was the work of Flanigan et al. (2014) .\n[BOS] They used a graph-based structured prediction algorithm with two stages: the first stage is a semi-Markov model concerned with identification of concepts, the second stage connects these concepts by finding the maximum spanning connected subgraph from a graph where all possible relations between concepts are realized.\n[BOS] They achieve an F-score of 0.58 on the LDC2013E117 corpus.\n[BOS] Werling, Angeli, and Manning (2015) improve the AMR parsing concept of Flanigan et al. (2014) by supporting the critical task of concept identification with a predefined set of actions for concept subgraph generation that are evoked after a statistical classification procedure.\n[BOS] Besides graph-based approaches, there exist also other strategies on AMR parsing: Peng, Song, and Gildea (2015) learn synchronous hyperedge replacement grammar rules from string- graph pairs.\n[BOS] An Earley algorithm with cube-pruning then performs string-to-AMR parsing with these rules.\n[BOS] Pust et al. (2015) treat English and AMR as a language pair and use a machine translation approach to parse AMRs from sentences.\n[BOS] They convert AMRs into into a grammar of string-to-tree rules that can be handled by syntax-based machine translation formalisms and use these rules with a bottomup chart decoder to parse AMRs with given local features and a language model.\n[BOS] Wang, Xue, and S. Pradhan (2015a) use a transition-based system that transforms dependency graphs into AMR structures by evoking specific actions at each reached state while traversing the dependency tree.\n[BOS] As can be seen, there are many different point of views on AMR parsing.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 2, "token_end": 102, "char_start": 6, "char_end": 511, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Flanigan et al. (2014)": "5000956"}, "Reference": {}}}, {"token_start": 103, "token_end": 158, "char_start": 518, "char_end": 796, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Werling, Angeli, and Manning (2015)": "500600"}, "Reference": {"Flanigan et al. (2014)": "5000956"}}}, {"token_start": 159, "token_end": 221, "char_start": 803, "char_end": 1092, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Peng, Song, and Gildea (2015)": "7138313"}, "Reference": {}}}, {"token_start": 222, "token_end": 303, "char_start": 1099, "char_end": 1474, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Pust et al. (2015)": "6971327"}, "Reference": {}}}, {"token_start": 304, "token_end": 348, "char_start": 1481, "char_end": 1686, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "19186315_0", "paragraph": "[BOS] Machine reading comprehension made rapid progress in recent years, especially for singlepassage MRC task, such as SQuAD (Rajpurkar et al., 2016) .\n[BOS] Mainstream studies (Seo et al., 2016; Wang and Jiang, 2016; Xiong et al., 2016) treat reading comprehension as extracting answer span from the given passage, which is usually achieved by predicting the start and end position of the answer.\n[BOS] We implement our boundary model similarly by employing the boundary-based pointer network (Wang and Jiang, 2016) .\n[BOS] Another inspiring work is from Wang et al. (2017c) , where the authors propose to match the passage against itself so that the representation can aggregate evidence from the whole passage.\n[BOS] Our verification model adopts a similar idea.\n[BOS] However, we collect information across passages and our attention is based on the answer representation, which is much more efficient than attention over all passages.\n[BOS] For the model training, Xiong et al. (2017) argues that the boundary loss encourages exact answers at the cost of penalizing overlapping answers.\n[BOS] Therefore they propose a mixed objective that incorporates rewards derived from word overlap.\n[BOS] Our joint training approach has a similar function.\n[BOS] By taking the content and verification loss into consideration, our model will give less loss for overlapping answers than those unmatched answers, and our loss function is totally differentiable.\n[BOS] Recently, we also see emerging interests in multi-passage MRC from both the academic (Dunn et al., 2017; Joshi et al., 2017) and industrial community (Nguyen et al., 2016; He et al., 2017) .\n[BOS] Early studies (Shen et al., 2017; Wang et al., 2017c) usually concat those passages and employ the same models designed for singlepassage MRC.\n[BOS] However, more and more latest studies start to design specific methods that can read multiple passages more effectively.\n[BOS] In the aspect of passage selection, Wang et al. (2017a) introduced a pipelined approach that rank the passages first and then read the selected passages for answering questions.\n[BOS] Tan et al. (2017) treats the passage ranking as an auxiliary task that can be trained jointly with the reading comprehension model.\n[BOS] Actually, the target of our answer verification is very similar to that of the passage selection, while we pay more attention to the answer content and the answer verification process.\n[BOS] Speaking of the answer verification, Wang et al. (2017b) has a similar motivation to ours.\n[BOS] They attempt to aggregate the evidence from different passages and choose the final answer from n-best candidates.\n[BOS] However, they implement their idea as a separate reranking step after reading comprehension, while our answer verification is a component of the whole model that can be trained end-to-end.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Reflection", "Single_summ", "Reflection", "Reflection", "Single_summ", "Single_summ", "Reflection", "Reflection", "Narrative_cite", "Narrative_cite", "Transition", "Single_summ", "Single_summ", "Reflection", "Reflection", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 14, "token_end": 34, "char_start": 88, "char_end": 150, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rajpurkar et al., 2016)": "11816014"}}}, {"token_start": 36, "token_end": 61, "char_start": 159, "char_end": 238, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Seo et al., 2016;": "8535316", "Wang and Jiang, 2016;": "5592690", "Xiong et al., 2016)": "3714278"}}}, {"token_start": 98, "token_end": 110, "char_start": 464, "char_end": 517, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wang and Jiang, 2016)": "5592690"}}}, {"token_start": 112, "token_end": 149, "char_start": 526, "char_end": 714, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wang et al. (2017c)": "12501880"}, "Reference": {}}}, {"token_start": 188, "token_end": 233, "char_start": 947, "char_end": 1192, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Xiong et al. (2017)": "21196492"}, "Reference": {}}}, {"token_start": 293, "token_end": 310, "char_start": 1536, "char_end": 1584, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dunn et al., 2017;": "11606382", "Joshi et al., 2017)": "26501419"}}}, {"token_start": 311, "token_end": 328, "char_start": 1589, "char_end": 1648, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Nguyen et al., 2016;": "1289517", "He et al., 2017)": "3662564"}}}, {"token_start": 330, "token_end": 348, "char_start": 1657, "char_end": 1710, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Shen et al., 2017;": "6300274", "Wang et al., 2017c)": "12501880"}}}, {"token_start": 387, "token_end": 422, "char_start": 1933, "char_end": 2110, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 423, "token_end": 449, "char_start": 2117, "char_end": 2248, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Tan et al. (2017)": "33257417"}, "Reference": {}}}, {"token_start": 484, "token_end": 498, "char_start": 2446, "char_end": 2502, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Wang et al. (2017b)": "13764176"}}}]}
{"id": "16767253_1", "paragraph": "[BOS] Bayesian WSI systems have been developed by several authors.\n[BOS] Brody and Lapata (2009) apply Latent Dirichlet Allocation (LDA) (Blei et al., 2003) to WSI.\n[BOS] They run a topic modeling algorithm on texts with some fixed number of topics that correspond to senses and induce a cluster by finding target words assigned to the same topic.\n[BOS] Their system is evaluated on SemEval-2007 noun data (Agirre and Soroa, 2007) .\n[BOS] Lau et al. (2012) apply a nonparametric model, Hierarchical Dirichlet Processes (HDP), to SemEval-2010 data .\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Narrative_cite", "Single_summ"], "span_citation_mapping": [{"token_start": 14, "token_end": 75, "char_start": 73, "char_end": 347, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Brody and Lapata (2009)": "10171569"}, "Reference": {}}}, {"token_start": 81, "token_end": 98, "char_start": 383, "char_end": 430, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Agirre and Soroa, 2007)": "1005694"}}}, {"token_start": 100, "token_end": 127, "char_start": 439, "char_end": 546, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Lau et al. (2012)": "2368184"}, "Reference": {}}}]}
{"id": "16767253_0", "paragraph": "[BOS] Yarowsky (1995) introduces a semi-supervised bootstrapping algorithm with two assumptions that rivals supervised algorithms: one-sense-percollocation and one-sense-per-discourse.\n[BOS] But this algorithm cannot easily be scaled up because for any new ambiguous word humans need to pick a few seed words, which initialize the algorithm.\n[BOS] In order to automate the semi-supervised system, Eisner and Karakos (2005) propose an unsupervised bootstrapping algorithm.\n[BOS] Their system tries many different seeds for bootstrapping and chooses the \"best\" classifier at the end.\n[BOS] Eisner and Karakos's algorithm is limited in that their system is designed for disambiguating words that have only 2 senses.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 69, "char_start": 6, "char_end": 341, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 70, "token_end": 118, "char_start": 348, "char_end": 581, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Eisner and Karakos (2005)": "5145179"}, "Reference": {}}}, {"token_start": 119, "token_end": 146, "char_start": 588, "char_end": 712, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "16240382_4", "paragraph": "[BOS] Therefore, despite that relation classification has mostly been studied separately from SRL, they have a substantial amount of commonalities.\n[BOS] It inspires us to develop a potentially unified architecture to take advantage of the progress in each research direction.\n\n", "discourse_tags": ["Other", "Reflection"], "span_citation_mapping": []}
{"id": "16240382_3", "paragraph": "[BOS] Relation Classification Early research on RC has also been relying heavily on human-engineered features (Rink and Harabagiu, 2010) .\n[BOS] Recent years have seen a great deal of work on using neural networks to alleviate the intensive engineering on contextual and syntactic features.\n[BOS] For example, Socher et al. (2012) propose recursive neural networks for modeling the syntactic paths between the two entities whose relation is to be determined.\n[BOS] Zeng et al. (2014) use convolutional neural network for learning sentence-level features of contexts and obtain good performance even without using syntactic features.\n[BOS] Later approaches have used more sophisticated models for better handling long-term dependencies, such as sequential LSTMs and tree LSTMs (Liu et al., 2015; Xu et al., 2015b; Miwa and Bansal, 2016) .\n[BOS] In addition, Yu et al. (2014) and (2015) investigate tensor-based approaches for learning the combination of embedding features and lexicalized sparse features.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Single_summ", "Single_summ", "Narrative_cite", "Multi_summ"], "span_citation_mapping": [{"token_start": 14, "token_end": 29, "char_start": 84, "char_end": 136, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rink and Harabagiu, 2010)": "12390391"}}}, {"token_start": 56, "token_end": 87, "char_start": 297, "char_end": 458, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Socher et al. (2012)": "806709"}, "Reference": {}}}, {"token_start": 88, "token_end": 117, "char_start": 465, "char_end": 632, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zeng et al. (2014)": "12873739"}, "Reference": {}}}, {"token_start": 135, "token_end": 168, "char_start": 744, "char_end": 835, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Liu et al., 2015;": "7475429", "Xu et al., 2015b;": "5403702", "Miwa and Bansal, 2016)": "2476229"}}}, {"token_start": 170, "token_end": 202, "char_start": 844, "char_end": 1004, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Yu et al. (2014)": "203685131"}, "Reference": {}}}]}
{"id": "16240382_2", "paragraph": "[BOS] Several approaches have been studied to alleviate the intensive feature engineering in SRL and get better generalization.\n[BOS] Moschitti et al. (2008) introduce different kinds of tree kernels for capturing the structural similarity of syntactic trees.\n[BOS] While attractive in automatic feature learning, the kernel-based approaches typically suffer from high computational cost.\n[BOS] Lei et al. (2015) instead use low-rank tensors for automatic feature composition based on four kinds of basic feature sets.\n[BOS] However, tensor-based approaches cannot well generalize the high-sparsity structural features like syntactic path.\n[BOS] Besides, they still need a relatively small amount of feature engineering to make use of the local contexts.\n[BOS] Another line of research focuses on neural models (Collobert et al., 2011; Zhou and Xu, 2015; FitzGerald et al., 2015) , which have shown great effectiveness in automatic feature learning on a variety of NLP tasks.\n[BOS] Most recently, Roth and Lapata (2016) employ LSTM-based recurrent neural networks to obtain the representations of syntactic path features, which is similar to our work.\n[BOS] Aside from the distributed path features, they also use a set of binary input feature sets from Anders et al. (2010) .\n[BOS] In contrast to these prior work, our model jointly leverages both global contexts and syntactic path features using bidirectional LSTMs.\n\n", "discourse_tags": ["Transition", "Single_summ", "Transition", "Single_summ", "Transition", "Transition", "Narrative_cite", "Single_summ", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 22, "token_end": 47, "char_start": 134, "char_end": 259, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Moschitti et al. (2008)": "2340513"}, "Reference": {}}}, {"token_start": 68, "token_end": 94, "char_start": 395, "char_end": 518, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Lei et al. (2015)": "5730387"}, "Reference": {}}}, {"token_start": 142, "token_end": 169, "char_start": 797, "char_end": 879, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Collobert et al., 2011;": "351666", "Zhou and Xu, 2015;": "12688069", "FitzGerald et al., 2015)": "15048880"}}}, {"token_start": 188, "token_end": 222, "char_start": 982, "char_end": 1151, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Roth and Lapata (2016)": "5779419"}, "Reference": {}}}, {"token_start": 236, "token_end": 248, "char_start": 1223, "char_end": 1274, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Anders et al. (2010)": null}}}]}
{"id": "16240382_1", "paragraph": "[BOS] Semantic Role Labeling A great deal of previous SRL research has been dedicated to designing rich and expressive features, pioneered by Gildea and Jurafsky (2002) .\n[BOS] For instance, the top performing system on the CoNLL-2009 shared task employs over 50 language-specific feature templates (Che et al., 2009 ).\n[BOS] These features mostly involve the predicate, the candidate argument, their contexts and the syntactic path between them (Surdeanu et al., 2003; Xue and Palmer, 2004; Pradhan et al., 2005) .\n[BOS] Besides, higher-order features involving several arguments or multiple predicates have also been explored (Toutanova et al., 2008; Martins and Almeida, 2014; Yang and Zong, 2014) .\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 18, "token_end": 36, "char_start": 99, "char_end": 168, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Gildea and Jurafsky (2002)": "62182406"}}}, {"token_start": 56, "token_end": 68, "char_start": 263, "char_end": 316, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Che et al., 2009": "2251827"}}}, {"token_start": 83, "token_end": 116, "char_start": 401, "char_end": 513, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Surdeanu et al., 2003;": "5224609", "Xue and Palmer, 2004;": "18312340", "Pradhan et al., 2005)": "12982947"}}}, {"token_start": 126, "token_end": 161, "char_start": 571, "char_end": 700, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Toutanova et al., 2008;": "2243454", "Martins and Almeida, 2014;": "7345848", "Yang and Zong, 2014)": "2797875"}}}]}
{"id": "16240382_0", "paragraph": "[BOS] The present work ties together several strands of previous studies.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "213195_3", "paragraph": "[BOS] There are also works on reordering error analysis like Han et al. (2012) which examined an existing reordering method and refined it after a detailed linguistic analysis on reordering issues.\n[BOS] Although they discovered that parsing errors affect the reordering quality, they did not observe the concrete relationship.\n[BOS] On the other hand, Gimnez and Mrquez (2008) proposed an automatic error analysis method of machine translation output, by compiling a set of metric variants.\n[BOS] However, they did not provide insight on what SMT component caused low translation performance.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 60, "char_start": 6, "char_end": 327, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Han et al. (2012)": "539235"}, "Reference": {}}}, {"token_start": 61, "token_end": 113, "char_start": 334, "char_end": 593, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Gim\u00e9nez and M\u00e0rquez (2008)": "12009874"}, "Reference": {}}}]}
{"id": "213195_2", "paragraph": "[BOS] There are more works on parsing error analysis.\n[BOS] For instance, Hara et al. (2009) defined several types of parsing error patterns on predicate argument relation and tested them with a Headdriven phrase structure grammar (HPSG) (Pollard and Sag, 1994) parser (Miyao and Tsujii, 2008) .\n[BOS] McDonald and Nivre (2007) explored parsing errors for data-driven dependency parsing by comparing a graph-based parser with a transitionbased parser, which are representing two dominant parsing models.\n[BOS] At the same time, Dredze et al. (2007) provided a comparison analysis on differences in annotation guidelines among treebanks which were suspected to be responsible for dependency parsing errors in domain adaptation tasks.\n[BOS] Unlike analyzing parsing errors, authors in Yu et al. (2011) focused on the difficulties in Chinese deep parsing by comparing the linguistic properties between Chinese and English.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 12, "token_end": 67, "char_start": 60, "char_end": 293, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hara et al. (2009)": "5540873"}, "Reference": {"(Pollard and Sag, 1994)": null, "(Miyao and Tsujii, 2008)": "885002"}}}, {"token_start": 69, "token_end": 106, "char_start": 302, "char_end": 503, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"McDonald and Nivre (2007)": "1900468"}, "Reference": {}}}, {"token_start": 107, "token_end": 149, "char_start": 510, "char_end": 732, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Dredze et al. (2007)": "5811151"}, "Reference": {}}}, {"token_start": 150, "token_end": 182, "char_start": 739, "char_end": 919, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Yu et al. (2011)": "8416967"}, "Reference": {}}}]}
{"id": "213195_1", "paragraph": "[BOS] One most relevant work to ours is observing the impact of parsing accuracy on a SMT system introduced in Quirk and Corston-Oliver (2006) .\n[BOS] They showed the general idea that syntax-based SMT models are sensitive to syntactic analysis.\n[BOS] However, they did not further analyze concrete parsing error types that affect task accuracy.\n[BOS] Green (2011) explored the effects of noun phrase bracketing in dependency parsing in English, and further on English to Czech machine translation.\n[BOS] But the work focused on using noun phrase structure to improve a machine translation framework.\n[BOS] In the work of Katz-Brown et al. (2011) , they proposed a training method to improve a parser's performance by using reordering quality to examine the parse quality.\n[BOS] But they did not study the relationship between reordering quality and parse quality.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 69, "char_start": 6, "char_end": 345, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Quirk and Corston-Oliver (2006)": "2988643"}, "Reference": {}}}, {"token_start": 70, "token_end": 114, "char_start": 352, "char_end": 600, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Green (2011)": "1226415"}, "Reference": {}}}, {"token_start": 115, "token_end": 168, "char_start": 607, "char_end": 864, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Katz-Brown et al. (2011)": "489244"}, "Reference": {}}}]}
{"id": "213195_0", "paragraph": "[BOS] Although there are studies on analyzing parsing errors and reordering errors, as far as we know, there is not any work on observing the relationship between these two types of errors.\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "19766124_3", "paragraph": "[BOS] We believe that, while these work address a problem similar to ours, their strategies are not adequate for our case since they do not perform the analysis on a sufficiently grained fashion, as in [Jansen et al. 2009 ], or they rely too strongly on the structure of the domain, as in [Silva and TEAM 2011] .\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 25, "token_end": 47, "char_start": 128, "char_end": 221, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"[Jansen et al. 2009": "14654754"}}}, {"token_start": 50, "token_end": 69, "char_start": 228, "char_end": 310, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"[Silva and TEAM 2011]": null}}}]}
{"id": "19766124_2", "paragraph": "[BOS] Silva et al. [Silva and TEAM 2011] describe the construction of the Twittmetro -a tool for subsentential sentiment analysis on Twitter for the political domain.\n[BOS] They explore a dictionary-based approach combined with lexico-syntactic rules to identify and compose opinions and to attribute reference to them.\n\n", "discourse_tags": ["Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 65, "char_start": 6, "char_end": 319, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"[Silva and TEAM 2011]": null}, "Reference": {}}}]}
{"id": "19766124_1", "paragraph": "[BOS] More related to our work, however, are the work of Jansen et al. [Jansen et al. 2009 ] and Silva et al. [Silva and TEAM 2011] .\n[BOS] Jansen et al. use out-of-the-box commercial tool -no longer available -to perform Entity-centric subsentential sentiment analysis on Twitter.\n[BOS] They apply their strategy on brand names for word-of-mouth detection.\n\n", "discourse_tags": ["Reflection", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 38, "char_start": 6, "char_end": 131, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"[Jansen et al. 2009": "14654754", "[Silva and TEAM 2011]": null}}}, {"token_start": 40, "token_end": 91, "char_start": 140, "char_end": 357, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "19766124_0", "paragraph": "[BOS] While multiple solutions have been proposed for identification of opinionated expressions in text, work on entity-centric sentiment analysis, i.e. to associate opinions with its referent, fall over three major approaches: those which use the context of an entity -as a fixed window of words around the entity or its syntactic context -to identify an opinion about the entity [Grefenstette et al. 2004, Hu and Liu 2004] ; those which use pre-defined rules and linguistics resources -such as FrameNet -to identify the opinion reference as [Ding et al. 2008 , Kim and Hovy 2006 , Wu et al. 2009 ; and those which relies on machine learning techniques as [Popescu and Etzioni 2005 , Kobayashi et al. 2007 , Ding and Liu 2010 .\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 67, "token_end": 89, "char_start": 341, "char_end": 424, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Liu 2004]": "207155218"}}}, {"token_start": 106, "token_end": 130, "char_start": 506, "char_end": 597, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"[Ding et al. 2008": "12442299", ", Kim and Hovy 2006": "8346698", ", Wu et al. 2009": "13573624"}}}, {"token_start": 136, "token_end": 160, "char_start": 626, "char_end": 726, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"[Popescu and Etzioni 2005": "631855", ", Kobayashi et al. 2007": "17068090", ", Ding and Liu 2010": "14759124"}}}]}
{"id": "18198203_1", "paragraph": "[BOS] While tailored towards (biomedical) event extraction, we believe that our models can also be effective in a more general Semantic Role Labeling (SRL) context.\n[BOS] Using variants of Model 1, we can enforce many of the SRL constraints-such as \"unique agent\" constraints (Punyakanok et al., 2004) -without having to call out to ILP optimizers.\n[BOS] Meza-Ruiz and Riedel (2009) showed that inducing pressure on arguments to be attached to at least one predicate is helpful; this is a soft incoming edge constraint.\n[BOS] Finally, Model 3 can be used to efficiently capture compatibilities between semantic arguments; such compatibilities have also been shown to be helpful in SRL (Toutanova et al., 2005) .\n\n", "discourse_tags": ["Reflection", "Narrative_cite", "Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 53, "token_end": 68, "char_start": 250, "char_end": 301, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Punyakanok et al., 2004)": "2969247"}}}, {"token_start": 81, "token_end": 117, "char_start": 355, "char_end": 519, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Meza-Ruiz and Riedel (2009)": "1015652"}, "Reference": {}}}, {"token_start": 134, "token_end": 158, "char_start": 622, "char_end": 709, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Toutanova et al., 2005)": "10432514"}}}]}
{"id": "18198203_0", "paragraph": "[BOS] We follow a long line of research in NLP that addresses search problems using (Integer) Linear Programs (Germann et al., 2001; Riedel and Clarke, 2006) .\n[BOS] However, instead of using off-the-shelf solvers, we work in the framework of dual decomposition.\n[BOS] Here we extend the approach of in that in addition to equality constraints we dualize more complex coupling constraints between models.\n[BOS] This requires us to work with a projected version of subgradient descent.\n\n", "discourse_tags": ["Narrative_cite", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 17, "token_end": 38, "char_start": 84, "char_end": 157, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Germann et al., 2001;": "90111", "Riedel and Clarke, 2006)": "6270377"}}}]}
{"id": "1801716_4", "paragraph": "[BOS] Beyond information extraction, cross-lingual training has offered benefits for a variety of tasks.\n[BOS] McDonald et al. (2011) use a delexicalized English parser to seed a lexicalized parser in the target language, and then iteratively improve upon this model via constraint driven learning.\n[BOS] Duong et al. (2014) develop a POS tagger for low resource languages by first projecting predicted English POS tags across parallel data to obtain target language training data, and then further augment this with a small amount of annotated data in the target language.\n[BOS] Ammar et al. (2016) developed a language universal dependency parser by using language-independent features to create a general model, and fine-tuning the resulting model with language-specific features and embeddings.\n[BOS] Similarly to our model, this method has no requirement about the availability of alignments and parallel text.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 21, "token_end": 59, "char_start": 111, "char_end": 298, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"McDonald et al. (2011)": "6698104"}, "Reference": {}}}, {"token_start": 60, "token_end": 111, "char_start": 305, "char_end": 573, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Duong et al. (2014)": "2349255"}, "Reference": {}}}, {"token_start": 112, "token_end": 153, "char_start": 580, "char_end": 798, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ammar et al. (2016)": "2868247"}, "Reference": {}}}]}
{"id": "1801716_3", "paragraph": "[BOS] There does exist some prior work on the broader field of cross-lingual information extraction.\n[BOS] Riloff et al. (2002) start with English annotated source texts, create a parallel corpus via machine translation, and project the annotations via alignments.\n[BOS] The projected annotations are then used to conduct training in the target language.\n[BOS] Sudo et al. (2004) presents an approach for extracted patterns in a source language and translating these patterns for use on a target language.\n[BOS] However, these works are limited to entity extraction, whereas our focus is on event extraction.\n[BOS] Furthermore, both works rely on having high-quality machine translation output.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 21, "token_end": 67, "char_start": 107, "char_end": 354, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Riloff et al. (2002)": "7516719"}, "Reference": {}}}, {"token_start": 68, "token_end": 116, "char_start": 361, "char_end": 608, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Sudo et al. (2004)": "18312172"}, "Reference": {}}}]}
{"id": "1801716_2", "paragraph": "[BOS] Another related work is that of Piskorski et al. (2011) , who use cross-lingual information to refine the results of event extraction.\n[BOS] In particular, they run several monolingual event extraction systems independently, translate the extracted argument fillers into English, and merge together argument fillers across documents.\n[BOS] Using this cross-document information fusion, they find improved performance over monolingual systems.\n[BOS] However, this work relies on having documents across multiple languages that describe the exact same event, which is an unrealistic case in practice.\n[BOS] Additionally, they also rely on having high quality machine translation in order to translate the argument output of each monolingual system into English.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 138, "char_start": 6, "char_end": 765, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Piskorski et al. (2011)": "13341873"}, "Reference": {}}}]}
{"id": "1801716_1", "paragraph": "[BOS] The most related work to our approach is that of Chen and Ji (2009a) .\n[BOS] In their model, they designed a co-training approach to augment a small Chinese training corpus with additional examples from an unlabeled corpus.\n[BOS] Given a parallel corpus of English-Chinese documents and a monolingual English event extraction system (trained on annotated English documents), they used the system to predict the event labels on the English part of the parallel documents and project the predicted labels to the Chinese part of the parallel corpus based on gold standard alignments.\n[BOS] The Chinese system is then trained using a combination of the originally annotated Chinese document and the parallel texts with the projected labels.\n[BOS] This approach offered slight improvements in the event trigger extraction task and the event argument extraction task (see Section 2 for definitions), but relies on having in-domain parallel texts either aligned by humans or by high quality machine translation models between the source and target languages.\n[BOS] In contrast, our proposed approach has no such limitation, and hence is easier to apply to any target language of interest.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 187, "char_start": 6, "char_end": 1057, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Chen and Ji (2009a)": "7085001"}, "Reference": {}}}]}
{"id": "1801716_0", "paragraph": "[BOS] A variety of machine learning methods have been used for event extraction in the past, including pipelines of classifiers (Grishman et al., 2005; Ji and Grishman, 2008; Liao and Grishman, 2011) , joint inference models (Li et al., 2013; Li and Ji, 2014; Yang and Mitchell, 2016) , and neural networks (Nguyen and Grishman, 2015; Chen et al., 2015) -the vast majority of which focus solely on the English monolingual training scenario.\n[BOS] A subset of the event extraction literature has considered the study of Chinese event extraction (Chen and Ji, 2009b; Li et al., 2012; Chen and Ng, 2012; Chen and Ng, 2014) .\n[BOS] However, most of these works also focus solely on the monolingual case, and do not leverage any additional training data from other languages.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 19, "token_end": 50, "char_start": 103, "char_end": 199, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Grishman et al., 2005;": null, "Ji and Grishman, 2008;": "1320606", "Liao and Grishman, 2011)": "15865939"}}}, {"token_start": 51, "token_end": 74, "char_start": 202, "char_end": 284, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Li et al., 2013;": "2114517", "Li and Ji, 2014;": "15552794", "Yang and Mitchell, 2016)": "2367456"}}}, {"token_start": 76, "token_end": 94, "char_start": 291, "char_end": 353, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Nguyen and Grishman, 2015;": "10913456", "Chen et al., 2015)": "14339673"}}}, {"token_start": 123, "token_end": 153, "char_start": 519, "char_end": 619, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chen and Ji, 2009b;": "1588410", "Li et al., 2012;": "6830189", "Chen and Ng, 2012;": null, "Chen and Ng, 2014)": "1415632"}}}]}
{"id": "21715690_2", "paragraph": "[BOS] Data Augmentation Data augmentation has the capability to improve the robustness of NMT models.\n[BOS] In NMT, there is a number of work that augments the training data with monolingual corpora (Sennrich et al., 2016a; He et al., 2016a; Zhang and Zong, 2016) .\n[BOS] They all leverage complex models such as inverse NMT models to generate translation equivalents for monolingual corpora.\n[BOS] Then they augment the parallel corpora with these pseudo corpora to improve NMT models.\n[BOS] Some authors have recently endeavored to achieve zero-shot NMT through transferring knowledge from bilingual corpora of other language pairs (Chen et al., 2017; Zheng et al., 2017; or monolingual corpora (Lample et al., 2018; Artetxe et al., 2018) .\n[BOS] Our work significantly differs from these work.\n[BOS] We do not resort to any complicated models to generate perturbed data and do not depend on extra monolingual or bilingual corpora.\n[BOS] The way we exploit is more convenient and easy to implement.\n[BOS] We focus more on improving the robustness of NMT models.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Transition", "Narrative_cite", "Reflection", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 36, "token_end": 65, "char_start": 179, "char_end": 263, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sennrich et al., 2016a;": null, "He et al., 2016a;": "5758868", "Zhang and Zong, 2016)": "17667087"}}}, {"token_start": 123, "token_end": 143, "char_start": 592, "char_end": 672, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chen et al., 2017;": "2166461"}}}, {"token_start": 145, "token_end": 166, "char_start": 677, "char_end": 740, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lample et al., 2018;": "3518190", "Artetxe et al., 2018)": "3515219"}}}]}
{"id": "21715690_1", "paragraph": "[BOS] Adversarial Learning Generative Adversarial Network (GAN) and its related derivative have been widely applied in computer vision (Radford et al., 2015; Salimans et al., 2016) and natural language processing .\n[BOS] Previous work has constructed adversarial examples to attack trained networks and make networks resist them, which has proved to improve the robustness of networks (Goodfellow et al., 2015; Miyato et al., 2016; Zheng et al., 2016) .\n[BOS] Belinkov and Bisk (2018) introduce adversarial examples to training data for character-based NMT models.\n[BOS] In contrast to theirs, adversarial stability training aims to stabilize both the encoder and decoder in NMT models.\n[BOS] We adopt adversarial learning to learn the perturbation-invariant encoder.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 21, "token_end": 40, "char_start": 119, "char_end": 180, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Radford et al., 2015;": "11758569", "Salimans et al., 2016)": "1687220"}}}, {"token_start": 69, "token_end": 97, "char_start": 362, "char_end": 451, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Goodfellow et al., 2015;": "6706414", "Miyato et al., 2016;": "9398766", "Zheng et al., 2016)": "2102547"}}}, {"token_start": 99, "token_end": 147, "char_start": 460, "char_end": 686, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Belinkov and Bisk (2018)": "3513372"}, "Reference": {}}}]}
{"id": "21715690_0", "paragraph": "[BOS] Our work is inspired by two lines of research: (1) adversarial learning and (2) data augmentation.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "20744_1", "paragraph": "[BOS] Our previous work (Li et al., 2013) used structured perceptron with token-based decoder to jointly predict event triggers and arguments based on the assumption that entity mentions and other argument candidates are given as part of the input.\n[BOS] In this paper, we solve a more challenging problem: take raw texts as input and identify the boundaries, types of entity mentions and relations all together in a single model.\n[BOS] Sarawagi and Cohen (2004) proposed a segment-based CRFs model for name tagging.\n[BOS] Zhang and Clark (2008) used a segment-based decoder for word segmentation and pos tagging.\n[BOS] We extended the similar idea to our end-to-end task by incrementally predicting relations along with entity mention segments.\n\n", "discourse_tags": ["Single_summ", "Reflection", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 49, "char_start": 6, "char_end": 248, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Li et al., 2013)": "2114517"}, "Reference": {}}}, {"token_start": 86, "token_end": 106, "char_start": 437, "char_end": 516, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Sarawagi and Cohen (2004)": null}, "Reference": {}}}, {"token_start": 107, "token_end": 126, "char_start": 523, "char_end": 613, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhang and Clark (2008)": "105219"}, "Reference": {}}}]}
{"id": "20744_0", "paragraph": "[BOS] Entity mention extraction (e.g., (Florian et al., 2004; Florian et al., 2006; Florian et al., 2010; Zitouni and Florian, 2008; Ohta et al., 2012) ) and relation extraction (e.g., (Reichartz et al., 2009; Sun et al., 2011; Jiang and Zhai, 2007; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Culotta and Sorensen, 2004; Zhou et al., 2007; Qian and Zhou, 2010; Qian et al., 2008; Chan and Roth, 2011; Plank and Moschitti, 2013) Table 3 : 5-fold cross-validation on ACE'04 corpus.\n[BOS] Bolded scores indicate highly statistical significant improvement as measured by paired t-test (p < 0.01) usually studied separately.\n[BOS] Most relation extraction work assumed that entity mention boundaries and/or types were given.\n[BOS] Chan and Roth (2011) reported the best results using predicted entity mentions.\n[BOS] Some previous work used relations and entity mentions to enhance each other in joint inference frameworks, including re-ranking (Ji and Grishman, 2005) , Integer Linear Programming (ILP) (Roth and Yih, 2004; Roth and Yih, 2007; Yang and Cardie, 2013) , and Card-pyramid Parsing (Kate and Mooney, 2010).\n[BOS] All these work noted the advantage of exploiting crosscomponent interactions and richer knowledge.\n[BOS] However, they relied on models separately learned for each subtask.\n[BOS] As a key difference, our approach jointly extracts entity mentions and relations using a single model, in which arbitrary soft constraints can be easily incorporated.\n[BOS] Some other work applied probabilistic graphical models for joint extraction (e.g., (Singh et al., 2013; Yu and Lam, 2010) ).\n[BOS] By contrast, our work employs an efficient joint search algorithm without modeling joint distribution over numerous variables, therefore it is more flexible and computationally simpler.\n[BOS] In addition, (Singh et al., 2013) used goldstandard mention boundaries.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Single_summ", "Narrative_cite", "Transition", "Transition", "Reflection", "Narrative_cite", "Reflection", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 53, "char_start": 6, "char_end": 151, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Florian et al., 2004;": "14831480", "Florian et al., 2006;": "3210281", "Florian et al., 2010;": "14191468", "Zitouni and Florian, 2008;": "2901242", "Ohta et al., 2012)": "13066880"}}}, {"token_start": 55, "token_end": 149, "char_start": 158, "char_end": 435, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Reichartz et al., 2009;": "18979967", "Sun et al., 2011;": "15013932", "Jiang and Zhai, 2007;": "17069935", "Bunescu and Mooney, 2005;": "5165854", "Zhao and Grishman, 2005;": "5273348", "Culotta and Sorensen, 2004;": "7395989", "Zhou et al., 2007;": "8835255", "Qian and Zhou, 2010;": "15227710", "Qian et al., 2008;": "2010873", "Chan and Roth, 2011;": "1491402", "Plank and Moschitti, 2013)": "3011134"}}}, {"token_start": 209, "token_end": 225, "char_start": 734, "char_end": 813, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 245, "token_end": 257, "char_start": 937, "char_end": 971, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ji and Grishman, 2005)": "5654268"}}}, {"token_start": 258, "token_end": 287, "char_start": 974, "char_end": 1070, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Roth and Yih, 2004;": "10048734", "Roth and Yih, 2007;": "125504208", "Yang and Cardie, 2013)": "1594813"}}}, {"token_start": 289, "token_end": 303, "char_start": 1077, "char_end": 1122, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 368, "token_end": 394, "char_start": 1505, "char_end": 1602, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Singh et al., 2013;": "13965810", "Yu and Lam, 2010)": "6814450"}}}, {"token_start": 426, "token_end": 443, "char_start": 1804, "char_end": 1875, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Singh et al., 2013)": "13965810"}, "Reference": {}}}]}
{"id": "2010873_7", "paragraph": "[BOS] Rather than constructing a composite kernel, we incorporate the EST into the DSPT to produce a Unified Parse and Semantic Tree (UPST) to investigate the contribution of the EST to relation extraction.\n[BOS] The entity features can be attached under the top node, the entity nodes, or directly combined with the entity nodes as in Figure 1 .\n[BOS] However, detailed evaluation (Qian et al., 2007) indicates that the UPST achieves the best performance when the feature nodes are attached under the top node.\n[BOS] Hence, we also attach three kinds of entity-related semantic trees (i.e. BOF, FPT and EPT) under the top node of the DSPT right after its original children.\n[BOS] Thereafter, we employ the standard CTK (Collins and Duffy, 2001) to compute the similarity between two UPSTs, since this CTK and its variations are successfully applied in syntactic parsing, semantic role labeling (Moschitti, 2004) and relation extraction (Zhang et al., 2006; Zhou et al., 2007) as well.\n\n", "discourse_tags": ["Reflection", "Reflection", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "2010873_6", "paragraph": "[BOS] In fact, the BOF only captures the individual entity features, while the FPT/EPT can additionally capture the bi-gram/tri-gram features respectively.\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "2010873_5", "paragraph": "[BOS] (c) Entity-Paired Tree (EPT, e.g. Fig.\n[BOS] 2(c) ): all the features relating to an entity are grouped to nodes \"E1\" or \"E2\", thus this tree kernel can further explore the equivalence of combined entity features only relating to one of the entities between two relation instances.\n\n", "discourse_tags": ["Transition", "Transition"], "span_citation_mapping": []}
{"id": "2010873_4", "paragraph": "[BOS] (b) Feature-Paired Tree (FPT, e.g. Fig.\n[BOS] 2(b) ): the features of two entities are grouped into different types according to their feature names, e.g. \"TP1\" and \"TP2\" are grouped to \"TP\".\n[BOS] This tree setup is aimed to capture the additional similarity of the single feature combined from different entities, i.e., the first and the second entities.\n\n", "discourse_tags": ["Transition", "Transition", "Transition"], "span_citation_mapping": []}
{"id": "2010873_3", "paragraph": "[BOS] In order to effectively capture entity-related semantic features, and their combined features as well, especially bi-gram or tri-gram features, we build an Entity-related Semantic Tree (EST) in three ways as illustrated in Figure 2 .\n[BOS] In the example sentence \"they 're here\", which is excerpted from the ACE RDC 2004 corpus, there exists a relationship \"Physical.Located\" between the entities \"they\" [PER] and \"here\" [GPE.\n[BOS] Population-Center] .\n[BOS] The features are encoded as \"TP\", \"ST\", \"MT\" and \"PVB\", which denote type, subtype, mention-type of the two entities, and the base form of predicate verb if existing (nearest to the 2 nd entity along the path connecting the two entities) respectively.\n[BOS] For example, the tag \"TP1\" represents the type of the 1 st entity, and the tag \"ST2\" represents the subtype of the 2 nd entity.\n[BOS] The three entity-related semantic tree setups are depicted as follows: (a) Bag of Features (BOF, e.g. Fig.\n[BOS] 2(a) ): all feature nodes uniformly hang under the root node, so the tree kernel simply counts the number of common features between two relation instances.\n[BOS] This tree setup is similar to linear entity kernel explored by Zhang et al. (2006) .\n\n", "discourse_tags": ["Other", "Other", "Other", "Other", "Other", "Other", "Other", "Narrative_cite"], "span_citation_mapping": []}
{"id": "2010873_2", "paragraph": "[BOS] Entity semantic features, such as entity headword, entity type and subtype etc., impose a strong constraint on relation types in terms of relation definition by the ACE RDC task.\n[BOS] Experiments by Zhang et al. (2006) show that linear kernel using only entity features contributes much when combined with the convolution parse tree kernel.\n[BOS] Qian et al. (2007) further indicates that among these entity features, entity type, subtype, and mention type, as well as the base form of predicate verb, contribute most while the contribution of other features, such as entity class, headword and GPE role, can be ignored.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ"], "span_citation_mapping": []}
{"id": "2010873_1", "paragraph": "[BOS] In order to fully utilize the advantages of feature-based methods and kernel-based methods, researchers turn to composite kernel methods.\n[BOS] Zhao and Grishman (2005) In this paper, we will further study how to dynamically determine a concise and effective tree span for a relation instance by exploiting constituent dependencies inherent in the parse tree derivation.\n[BOS] We also attempt to fully capture both the structured syntactic parse information and entity-related semantic information, especially combined entity features, via a unified parse and semantic tree.\n[BOS] Finally, we validate the effectiveness of a composite kernel for relation extraction, which combines a tree kernel and a linear kernel.\n\n", "discourse_tags": ["Transition", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "2010873_0", "paragraph": "[BOS] Due to space limitation, here we only review kernel-based methods used in relation extraction.\n[BOS] For those interested in feature-based methods, please refer to Zhou et al. (2005) for more details.\n[BOS] Zelenko et al. (2003) described a kernel between shallow parse trees to extract semantic relations, where a relation instance is transformed into the least common sub-tree connecting the two entity nodes.\n[BOS] The kernel matches the nodes of two corresponding sub-trees from roots to leaf nodes recursively layer by layer in a topdown manner.\n[BOS] Their method shows successful results on two simple extraction tasks.\n[BOS] Culotta and Sorensen (2004) proposed a slightly generalized version of this kernel between dependency trees, in which a successful match of two relation instances requires the nodes to be at the same layer and in the identical path starting from the roots to the current nodes.\n[BOS] These strong constraints make their kernel yield high precision but very low recall on the ACE RDC 2003 corpus.\n[BOS] Bunescu and Mooney (2005) develop a shortest path dependency tree kernel, which simply counts the number of common word classes at each node in the shortest paths between two entities in dependency trees.\n[BOS] Similar to Culotta and Sorensen (2004) , this method also suffers from high precision but low recall.\n[BOS] Zhang et al. (2006) describe a convolution tree kernel (CTK, Collins and Duffy, 2001 ) to investigate various structured information for relation extraction and find that the Shortest Pathenclosed Tree (SPT) achieves the F-measure of 67.7 on the 7 relation types of the ACE RDC 2004 corpus.\n[BOS] One problem with SPT is that it loses the contextual information outside SPT, which is usually critical for relation extraction.\n[BOS] Zhou et al. (2007) point out that both SPT and the convolution tree kernel are context-free.\n[BOS] They expand SPT to CS-SPT by dynamically including necessary predicate-linked path information and extending the standard CTK to contextsensitive CTK, obtaining the F-measure of 73.2 on the 7 relation types of the ACE RDC 2004 corpus.\n[BOS] However, the CS-SPT only recovers part of contextual information and may contain noisy information as much as SPT.\n\n", "discourse_tags": ["Reflection", "Narrative_cite", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Narrative_cite", "Single_summ", "Transition", "Single_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 21, "token_end": 44, "char_start": 107, "char_end": 206, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Zhou et al. (2005)": "3160937"}}}, {"token_start": 45, "token_end": 124, "char_start": 213, "char_end": 632, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zelenko et al. (2003)": "195717148"}, "Reference": {}}}, {"token_start": 125, "token_end": 200, "char_start": 639, "char_end": 1034, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Culotta and Sorensen (2004)": "7395989"}, "Reference": {}}}, {"token_start": 201, "token_end": 241, "char_start": 1041, "char_end": 1245, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Bunescu and Mooney (2005)": "5165854"}, "Reference": {}}}, {"token_start": 242, "token_end": 253, "char_start": 1252, "char_end": 1290, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Culotta and Sorensen (2004)": "7395989"}}}, {"token_start": 266, "token_end": 333, "char_start": 1360, "char_end": 1650, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhang et al. (2006)": "5282346"}, "Reference": {"Collins and Duffy, 2001": "396794"}}}, {"token_start": 359, "token_end": 435, "char_start": 1792, "char_end": 2125, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhou et al. (2007)": "8835255"}, "Reference": {}}}]}
{"id": "1670126_3", "paragraph": "[BOS] Several research groups used syntactic labels as non-terminal symbols in their SCFG rules and develop new features (Zollmann and Venugopal, 2006; Zhao and Al-Onaizan, 2008; Chiang, 2010; Hoang and Koehn, 2010) .\n[BOS] However, all these methods still resort to rule extraction procedures similar to that of the standard phrase/hierarchical rule extraction method.\n[BOS] In contrast, we use the GHKM method which is a mature technique to extract rules from tree-string pairs but does not impose those Hiero-style constraints on rule extraction.\n[BOS] More importantly, we consider the hierarchical syntactic tree structure to make use of well-formed rules in decoding, while such information is not used in standard SCFG-based systems.\n[BOS] We also keep to the simpler non-terminals of Hiero, and do not 'decorate' any non-terminals with syntactic or other information.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 15, "token_end": 58, "char_start": 85, "char_end": 215, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zollmann and Venugopal, 2006;": "819325", "Zhao and Al-Onaizan, 2008;": "10651440", "Chiang, 2010;": "3514217", "Hoang and Koehn, 2010)": "2554349"}}}]}
{"id": "1670126_2", "paragraph": "[BOS] Another related line of work is to introduce syntactic constraints or annotations to hierarchical phrasebased systems.\n[BOS] Marton and Resnik (2008) and Li et al. (2013) proposed several soft or hard constraints to model syntactic compatibility of Hiero derivations and input source language parse trees.\n[BOS] We note that, despite significant development effort, we were not able to improve our baseline through the use of these soft syntactic constraints; it was this experience that led us to develop the hybrid approach described in this paper.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Reflection"], "span_citation_mapping": [{"token_start": 21, "token_end": 58, "char_start": 131, "char_end": 311, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Marton and Resnik (2008)": "2442439", "Li et al. (2013)": "523485"}, "Reference": {}}}]}
{"id": "1670126_1", "paragraph": "[BOS] There are several lines of work for augmenting hierarchical phrase-based systems with the use of source language phrase-structure trees.\n[BOS] Liu et al. (2009b) describe novel approaches to translation under multiple translation grammars.\n[BOS] Their approach is very much motivated by system combination, and they develop procedures for joint decoding and optimisation within a single system that give the benefit of combining hypotheses from multiple systems.\n[BOS] They demonstrate their approach by combining full treeto-string and Hiero systems.\n[BOS] Our approach is much simpler and emphasises changes to the grammar rather than the decoder or its parameter optimisation (MERT).\n[BOS] Our aim is to augment the search space of Hiero with linguistically-motivated hypotheses, and not to develop a new decoder that is capable of translation under multiple grammars.\n[BOS] Moreover, we consider Hiero as the backbone model and only introduce tree-to-string rules where they can contribute; we show that extracting tree-to-string rules from just 10% of the data suffices to get good gains.\n[BOS] This results in a small number of tree-to-string rules and does not slow down the decoder.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Reflection", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 28, "token_end": 98, "char_start": 149, "char_end": 557, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Liu et al. (2009b)": "2815754"}, "Reference": {}}}]}
{"id": "1670126_0", "paragraph": "[BOS] Recently linguistically-motivated models have been intensively investigated in MT.\n[BOS] In particular, source tree-based models (Liu et al., 2006; Huang et al., 2006; Eisner, 2003; Zhang et al., 2008; Liu et al., 2009a; Xie et al., 2011) have received growing interest due to their good abilities in modelling source language syntax for better lexicon selection and reordering.\n[BOS] Alternatively, the hierarchical phrase-based approach (Chiang, 2005) considers the underlying hierarchical structures of sentences but does not require linguistically syntactic trees on either language side.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Single_summ"], "span_citation_mapping": [{"token_start": 19, "token_end": 66, "char_start": 110, "char_end": 244, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Liu et al., 2006;": "10184967", "Huang et al., 2006;": "5479016", "Eisner, 2003;": "1542925", "Zhang et al., 2008;": "46602", "Liu et al., 2009a;": "2828132", "Xie et al., 2011)": "7803592"}}}, {"token_start": 89, "token_end": 123, "char_start": 391, "char_end": 598, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Chiang, 2005)": "384994"}, "Reference": {}}}]}
{"id": "235645_2", "paragraph": "[BOS] The primary focus of our work is integration of character and word-level reasoning in neural models for GEC, to capture global fluency errors and local errors in spelling and closely related morphological variants, while obtaining open vocabulary coverage.\n[BOS] This is achieved with the help of character and word-level encoders and decoders with two nested levels of attention.\n[BOS] Our model is inspired by advances in sub-word level modeling in neural machine translation.\n[BOS] We build mostly on the hybrid model of Luong and Manning (2016) to expand its capability to correct rare words by fine-grained character-level attention.\n[BOS] We directly compare our model to the one of Luong and Manning (2016) on the grammar correction task.\n[BOS] Alternative methods for MT include modeling of word pieces to achieve open vocabulary (Sennrich et al., 2016) , and more recently, fully character-level modeling (Lee et al., 2017) .\n[BOS] None of these models integrate two nested levels of attention although an empirical evaluation of these approaches for GEC would also be interesting.\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection", "Reflection", "Reflection", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 97, "token_end": 108, "char_start": 514, "char_end": 554, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Luong and Manning (2016)": "13972671"}}}, {"token_start": 127, "token_end": 150, "char_start": 651, "char_end": 751, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Luong and Manning (2016)": "13972671"}}}, {"token_start": 162, "token_end": 174, "char_start": 828, "char_end": 867, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sennrich et al., 2016)": "1114678"}}}, {"token_start": 179, "token_end": 192, "char_start": 889, "char_end": 938, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lee et al., 2017)": "10509498"}}}]}
{"id": "235645_1", "paragraph": "[BOS] Two prior works explored sequence to sequence neural models for GEC (Xie et al., 2016; Yuan and Briscoe, 2016) , while Chollampatt et al. (2016) integrated neural features in a phrase-based system for the task.\n[BOS] Neural models were also applied to the related sub-task of grammatical error identification (Schmaltz et al., 2016) .\n[BOS] Yuan and Briscoe (2016) demonstrated the promise of neural MT for GEC but did not adapt the basic sequence-to-sequence with attention to its unique challenges, falling back to traditional word-alignment models to address vocabulary coverage with a post-processing heuristic.\n[BOS] Xie et al. (2016) built a character-level sequence to sequence model, which achieves open vocabulary and character-level modeling, but has difficulty with global word-level decisions.\n\n", "discourse_tags": ["Multi_summ", "Narrative_cite", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 6, "token_end": 30, "char_start": 31, "char_end": 116, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Xie et al., 2016;": "8880428", "Yuan and Briscoe, 2016)": "16766006"}}}, {"token_start": 32, "token_end": 55, "char_start": 125, "char_end": 216, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Chollampatt et al. (2016)": "11257257"}, "Reference": {}}}, {"token_start": 68, "token_end": 81, "char_start": 282, "char_end": 338, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Schmaltz et al., 2016)": "588859"}}}, {"token_start": 83, "token_end": 137, "char_start": 347, "char_end": 621, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Yuan and Briscoe (2016)": "16766006"}, "Reference": {}}}, {"token_start": 138, "token_end": 175, "char_start": 628, "char_end": 811, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Xie et al. (2016)": "8880428"}, "Reference": {}}}]}
{"id": "235645_0", "paragraph": "[BOS] A variety of classifier-based and MT-based techniques have been applied to grammatical error correction.\n[BOS] The CoNLL-14 shared task overview paper of Ng et al. (2014) provides a comparative evaluation of approaches.\n[BOS] Two notable advances after the shared task have been in the areas of combining classifiers and phrase-based MT (Rozovskaya and Roth, 2016) and adapting phrase-based MT to the GEC task (Junczys-Dowmunt and Grundkiewicz, 2016) .\n[BOS] The latter work has reported the highest performance to date on the task of 49.5 in F 0.5 score on the CoNLL-14 test set.\n[BOS] This method integrates discriminative training toward the task-specific evaluation function, a rich set of features, and multiple large language models.\n[BOS] Neural approaches to the task are less explored.\n[BOS] We believe that the advances from Junczys-Dowmunt and Grundkiewicz (2016) are complementary to the ones we propose for neural MT, and could be integrated with neural models to achieve even higher performance.\n\n", "discourse_tags": ["Transition", "Single_summ", "Narrative_cite", "Transition", "Transition", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 22, "token_end": 46, "char_start": 117, "char_end": 225, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ng et al. (2014)": null}, "Reference": {}}}, {"token_start": 61, "token_end": 78, "char_start": 311, "char_end": 370, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rozovskaya and Roth, 2016)": "18563136"}}}, {"token_start": 79, "token_end": 105, "char_start": 375, "char_end": 456, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Junczys-Dowmunt and Grundkiewicz, 2016)": "6820419"}}}, {"token_start": 176, "token_end": 197, "char_start": 807, "char_end": 880, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Junczys-Dowmunt and Grundkiewicz (2016)": "6820419"}}}]}
{"id": "16090988_1", "paragraph": "[BOS] Regarding the basic unit of transliteration, traditional systems are mostly phoneme-based (e.g. Knight and Graehl, 1998) .\n[BOS] Li et al. (2004) suggested a grapheme-based Joint Source-Channel Model within the Direct Orthographic Mapping framework.\n[BOS] Models based on characters (e.g. Shishtla et al., 2009), syllables (e.g. Wutiwiwatchai and Thangthai, 2010) , as well as hybrid units (e.g. Oh and Choi, 2005) , are also seen.\n[BOS] In addition to phonetic features, others like temporal, semantic, and tonal features have also been found useful in transliteration (e.g. Tao et al., 2006; Li et al., 2007; Kwong, 2009a) .\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 15, "token_end": 33, "char_start": 82, "char_end": 126, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 35, "token_end": 62, "char_start": 135, "char_end": 255, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Li et al. (2004)": "1693404"}, "Reference": {}}}, {"token_start": 66, "token_end": 82, "char_start": 278, "char_end": 317, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 83, "token_end": 102, "char_start": 319, "char_end": 369, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Thangthai, 2010)": "81906"}}}, {"token_start": 106, "token_end": 119, "char_start": 383, "char_end": 420, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Oh and Choi, 2005)": "17161141"}}}, {"token_start": 147, "token_end": 175, "char_start": 560, "char_end": 630, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Tao et al., 2006;": "8424232", "Li et al., 2007;": "7527801", "Kwong, 2009a)": "14074548"}}}]}
{"id": "16090988_0", "paragraph": "[BOS] The reports of the shared task in NEWS 2009 (Li et al., 2009 ) and NEWS 2010 (Li et al., 2010) highlighted two particularly popular approaches for transliteration generation among the participating systems.\n[BOS] One is phrase-based statistical machine transliteration (e.g. Song et al., 2010; Finch and Sumita, 2010) and the other is Conditional Random Fields which treats the task as one of sequence labelling (e.g. Shishtla et al., 2009) .\n[BOS] Besides these popular methods, for instance, Huang et al. (2011) used a non-parametric Bayesian learning approach in a recent study.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Single_summ"], "span_citation_mapping": [{"token_start": 9, "token_end": 18, "char_start": 40, "char_end": 66, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Li et al., 2009": "62005149"}}}, {"token_start": 20, "token_end": 30, "char_start": 73, "char_end": 100, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Li et al., 2010)": "16427389"}}}, {"token_start": 48, "token_end": 76, "char_start": 226, "char_end": 323, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Song et al., 2010;": "8792705", "Finch and Sumita, 2010)": "14251200"}}}, {"token_start": 80, "token_end": 107, "char_start": 341, "char_end": 446, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Shishtla et al., 2009)": null}}}, {"token_start": 109, "token_end": 137, "char_start": 455, "char_end": 587, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Huang et al. (2011)": "5852298"}, "Reference": {}}}]}
{"id": "21730715_0", "paragraph": "[BOS] Noising While for images, there are natural noising primitives such as rotations, small translational shifts, and additive Gaussian noise, similar primitives are not as well developed for text data.\n[BOS] Similarly, while denoising autoencoders for images have been shown to help with representation learning (Vincent et al., 2010) , similar methods for learning representations are not well developed for text.\n[BOS] Some recent work has proposed noisingin the form of dropping or replacing individual tokens-as a regularizer when training sequence models, where it has been demonstrated to have a smoothing effect on the softmax output distribution (Bowman et al., 2015; Xie et al., 2017; Dai and Le, 2015; Kumar et al., 2015) .\n[BOS] Grammar correction Recent work by Chollampatt and Ng (2018) has achieved impressive performance on the benchmarks we consider using convolutional encoder-decoder models.\n[BOS] Previous work using data synthesis for grammatical error correction (GEC) has introduced errors by examining the distribution of error types, then applying errors according to those distributions together with lexical or part-of-speech features based on a small context window (Brockett et al., 2006; Felice, 2016) .\n[BOS] While these methods can introduce many possible edits, they are not as flexible as our approach inspired by the backtranslation procedure for machine translation (Sennrich et al., 2015) .\n[BOS] This is important as neural language models not explicitly trained to track long-range linguistic dependencies can fail to capture even simple noun-verb errors (Linzen et al., 2016) .\n[BOS] Recently, in the work perhaps most similar to ours, Rei et al. (2017) propose using statistical machine translation and backtranslation along with syntactic patterns for generating errors, albeit for the error detection task.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite", "Single_summ", "Narrative_cite", "Narrative_cite", "Narrative_cite", "Single_summ"], "span_citation_mapping": [{"token_start": 54, "token_end": 64, "char_start": 291, "char_end": 337, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vincent et al., 2010)": "17804904"}}}, {"token_start": 116, "token_end": 149, "char_start": 629, "char_end": 734, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bowman et al., 2015;": "748227", "Xie et al., 2017;": "10635893", "Dai and Le, 2015;": "7138078", "Kumar et al., 2015)": "2319779"}}}, {"token_start": 151, "token_end": 181, "char_start": 743, "char_end": 912, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Chollampatt and Ng (2018)": "19236015"}, "Reference": {}}}, {"token_start": 215, "token_end": 244, "char_start": 1129, "char_end": 1233, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Brockett et al., 2006;": "757808", "Felice, 2016)": "63986816"}}}, {"token_start": 267, "token_end": 284, "char_start": 1354, "char_end": 1427, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sennrich et al., 2015)": "15600925"}}}, {"token_start": 298, "token_end": 322, "char_start": 1512, "char_end": 1617, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Linzen et al., 2016)": "14091946"}}}, {"token_start": 324, "token_end": 367, "char_start": 1626, "char_end": 1851, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Rei et al. (2017)": "6751341"}, "Reference": {}}}]}
{"id": "21669304_1", "paragraph": "[BOS] Mathematically, this work builds upon the linear algebraic understanding of modern word embeddings developed by Arora et al. (2018b) via an extension to the latent-variable embedding model of Arora et al. (2016) .\n[BOS] Although there have been several other applications of this model for natural language representation Mu and Viswanath, 2018) , ours is the first to provide a general approach for learning semantic features using corpus context.\n[BOS] cation information using a standard algorithm (e.g. word2vec / GloVe).\n[BOS] Our goal is to construct a good embedding v f  R d of a text feature f given a set C f of contexts it occurs in.\n[BOS] Both f and its contexts are assumed to arise via the same process that generates the large corpus C V .\n[BOS] In many settings below, the number |C f | of contexts available for a feature f of interest is much smaller than the number |C w | of contexts that the typical word w  V occurs in.\n[BOS] This could be because the feature is rare (e.g. unseen words, n-grams) or due to limited human annotation (e.g. word senses, named entities).\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Transition", "Reflection", "Reflection", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 13, "token_end": 27, "char_start": 82, "char_end": 138, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Arora et al. (2018b)": null}}}, {"token_start": 32, "token_end": 46, "char_start": 163, "char_end": 217, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Arora et al. (2016)": "12744871"}}}, {"token_start": 59, "token_end": 70, "char_start": 296, "char_end": 351, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Mu and Viswanath, 2018)": null}}}]}
{"id": "21669304_0", "paragraph": "[BOS] Many methods have been proposed for extending word embeddings to semantic feature vectors, with the aim of using them as interpretable and structure-aware building blocks of NLP pipelines (Kiros et al., 2015; Yamada et al., 2016) .\n[BOS] Many exploit the structure and resources available for specific feature types, such as methods for sense, synsets, and lexemes (Rothe and Schtze, 2015; Iacobacci et al., 2015) that make heavy use of the graph structure of the Princeton WordNet (PWN) and similar resources (Fellbaum, 1998) .\n[BOS] By contrast, our work is more general, with incorporation of structure left as an open problem.\n[BOS] Embeddings of n-grams are of special interest because they do not need annotation or expert knowledge and can often be effective on downstream tasks.\n[BOS] Their computation has been studied both explicitly (Yin and Schutze, 2014; Poliak et al., 2017) and as an implicit part of models for document embeddings (Hill et al., 2016; Pagliardini et al., 2018) , which we use for comparison.\n[BOS] Supervised and multitask learning of text embeddings has also been attempted (Wang et al., 2017; Wu et al., 2017) .\n[BOS] A main motivation of our work is to learn good embeddings, of both words and features, from only one or a few examples.\n[BOS] Efforts in this area can in many cases be split into contextual approaches (Lazaridou et al., 2017; Herbelot and Baroni, 2017 ) and morphological methods (Luong et al., 2013; Bojanowski et al., 2016; Pado et al., 2016) .\n[BOS] The current paper provides a more effective formulation for context-based embeddings, which are often simpler to implement, can improve with more context information, and do not require morphological annotation.\n[BOS] Subword approaches, on the other hand, are often more compositional and flexible, and we leave the extension of our method to handle subword information to future work.\n[BOS] Our work is also related to some methods in domain adaptation and multi-lingual correlation, such as that of Bollegala et al. (2014) .\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Reflection", "Transition", "Narrative_cite", "Narrative_cite", "Reflection", "Narrative_cite", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 23, "token_end": 53, "char_start": 127, "char_end": 235, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Yamada et al., 2016)": "5267356"}}}, {"token_start": 71, "token_end": 99, "char_start": 343, "char_end": 419, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rothe and Sch\u00fctze, 2015;": "15687295", "Iacobacci et al., 2015)": "16863934"}}}, {"token_start": 105, "token_end": 125, "char_start": 447, "char_end": 532, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Fellbaum, 1998)": null}}}, {"token_start": 181, "token_end": 199, "char_start": 839, "char_end": 894, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yin and Schutze, 2014;": "14956855", "Poliak et al., 2017)": "6292995"}}}, {"token_start": 202, "token_end": 227, "char_start": 905, "char_end": 998, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hill et al., 2016;": "2937095", "Pagliardini et al., 2018)": "16251657"}}}, {"token_start": 235, "token_end": 262, "char_start": 1036, "char_end": 1149, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wang et al., 2017;": "33229545", "Wu et al., 2017)": null}}}, {"token_start": 302, "token_end": 323, "char_start": 1337, "char_end": 1409, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lazaridou et al., 2017;": "205032138", "Herbelot and Baroni, 2017": "26628362"}}}, {"token_start": 325, "token_end": 353, "char_start": 1416, "char_end": 1502, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Luong et al., 2013;": "14276764", "Bojanowski et al., 2016;": "207556454", "Pado et al., 2016)": "5577853"}}}, {"token_start": 434, "token_end": 456, "char_start": 1948, "char_end": 2036, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Bollegala et al. (2014)": "14754679"}}}]}
{"id": "17515652_6", "paragraph": "[BOS] 1 Type 1 models may be considered a special case of Type 2 models: Setting  = 0 reduces Type 2 models to Type 1 models trained solely on parallel data, e.g., (Hermann and Blunsom, 2014b; Chandar et al., 2014) .\n[BOS]  = 1 results in the models from (Klementiev et al., 2012; Gouws et al., 2015; Soyer et al., 2015; Coulmance et al., 2015) .\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 31, "token_end": 58, "char_start": 143, "char_end": 214, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Chandar et al., 2014)": "217774"}}}, {"token_start": 65, "token_end": 104, "char_start": 243, "char_end": 344, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Klementiev et al., 2012;": "6758088", "Gouws et al., 2015;": "7021865", "Soyer et al., 2015;": "10316648"}}}]}
{"id": "17515652_5", "paragraph": "[BOS] (Type 4) Post-Hoc Mapping with Seed Lexicons: These models learn post-hoc mapping functions between monolingual WE spaces induced separately for two different languages (e.g., by SGNS).\n[BOS] All Type 4 models (Mikolov et al., 2013a; Faruqui and Dyer, 2014; rely on readily available seed lexicons of highly frequent words obtained by e.g. Google Translate (GT) to learn the mapping (again colliding with P2), but they are able to satisfy P1.\n\n", "discourse_tags": ["Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 46, "token_end": 70, "char_start": 198, "char_end": 262, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mikolov et al., 2013a;": "1966640"}}}]}
{"id": "17515652_4", "paragraph": "[BOS] With pseudo-bilingual documents, the \"context\" of a word is redefined as a mixture of neighbouring words (in the original language) and words that appeared in the same region of the document (in the \"foreign\" language).\n[BOS] The bilingual contexts for each word in each document steer the final model towards constructing a SBWES.\n[BOS] The advantage over other BWE model types lies in exploiting weaker document-level bilingual signals (satisfying P2), but these models are unable to exploit monolingual corpora during training (unlike Type 2 or Type 4; thus colliding with P1).\n\n", "discourse_tags": ["Transition", "Transition", "Transition"], "span_citation_mapping": []}
{"id": "17515652_3", "paragraph": "[BOS] (Type 3) Pseudo-Bilingual Training: This set of models requires document alignments as bilingual signal to induce a SBWES.\n[BOS] create a collection of pseudo-bilingual documents by merging every pair of aligned documents in training data, in a way that preserves important local information: words that appeared next to other words within the same language and those that appeared in the same region of the document across different languages.\n[BOS] This collection is then used to train word embeddings with monolingual SGNS from word2vec.\n\n", "discourse_tags": ["Transition", "Transition", "Transition"], "span_citation_mapping": []}
{"id": "17515652_2", "paragraph": "[BOS] (Type 2) Joint Bilingual Training: These models jointly optimize two monolingual objectives, with the cross-lingual objective acting as a cross-lingual regularizer during training (Klementiev et al., 2012; Gouws et al., 2015; Soyer et al., 2015; Shi et al., 2015; Coulmance et al., 2015) .\n[BOS] The idea may be summarized by the simplified formulation (Luong et al., 2015) : (Mono S +Mono T )+Bi.\n[BOS] The monolingual objectives M ono S and M ono T ensure that similar words in each language are assigned similar embeddings and aim to capture the semantic structure of each language, whereas the cross-lingual objective Bi ensures that similar words across languages are assigned similar embeddings.\n[BOS] It ties the two monolingual spaces together into a SBWES (thus satisfying P1).\n[BOS] Parameters  and  govern the influence of the monolingual and bilingual components.\n[BOS] 1 The main disadvantage of Type 2 models is the costly parallel data needed for the bilingual signal (thus colliding with P2).\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Transition", "Transition", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 29, "token_end": 81, "char_start": 144, "char_end": 293, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Klementiev et al., 2012;": "6758088", "Gouws et al., 2015;": "7021865", "Soyer et al., 2015;": "10316648", "Shi et al., 2015;": "17196524", "Coulmance et al., 2015)": "14656950"}}}, {"token_start": 90, "token_end": 101, "char_start": 336, "char_end": 379, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Luong et al., 2015)": "13603998"}}}]}
{"id": "17515652_1", "paragraph": "[BOS] (Type 1) Parallel-Only: This group of BWE models relies on sentence-aligned and/or word-aligned parallel data as the only data source (Zou et al., 2013; Hermann and Blunsom, 2014a; Koisk et al., 2014; Hermann and Blunsom, 2014b; Chandar et al., 2014) .\n[BOS] In addition to an expensive bilingual signal (colliding with P2), these models do not leverage larger monolingual datasets for training (not satisfying P1).\n\n", "discourse_tags": ["Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 32, "token_end": 79, "char_start": 128, "char_end": 256, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zou et al., 2013;": "931054", "Ko\u010disk\u00fd et al., 2014;": "5809776"}}}]}
{"id": "17515652_0", "paragraph": "[BOS] Bilingual Signals BWE models may be clustered into four different types according to bilingual signals used in training, and properties P1 and P2 (see Sect.\n[BOS] 1).\n[BOS] Upadhyay et al. (2016) provide a similar overview of recent bilingual embedding learning architectures regarding different bilingual signals required for the embedding induction.\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ"], "span_citation_mapping": [{"token_start": 38, "token_end": 68, "char_start": 179, "char_end": 357, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Upadhyay et al. (2016)": "5357629"}, "Reference": {}}}]}
{"id": "18493113_0", "paragraph": "[BOS] Deep residual learning proposed in learns a residual representation with a deep neural network.\n[BOS] As stacking new layers does not lengthen the backprop path of early layers, residual learning enables the training of very deep networks, such as those with 1000 layers.\n[BOS] Deep residual nets won the 1st place in ILSVRC 2015 classification task.\n[BOS] The success of deep residual learning gives the insight of a better deep architecture of neural nets.\n[BOS] Beyond the success of residual learning, applying this technique to recurrent nets is a promising direction, which is researched in several previous works.\n[BOS] Recurrent Highway Networks (Srivastava et al., 2015) enhance the LSTM by adding an extra residual computation in each step.\n[BOS] The experiments show the Recurrent Highway Networks can achieve better perplexity in language modeling task with a limited parameter budget.\n[BOS] (Liao and Poggio, 2016) achieved similar classification performance when using shared weights in a ResNet, which is exactly a RNN.\n[BOS] Pixel Recurrent Neural Networks (van den Oord et al., 2016) demonstrates a novel architecture of neural nets with two-dimensional recurrent neural nets using residual connections.\n[BOS] Their models achieved better log-likelihood on image generation tasks.\n[BOS] Remarkably, the neural network architecture described in a lecture report 3 is similar to our models in spirit, where they applied stochastic residual learning to both depth and horizontal timesteps, which leads to better classification accuracy in Stanford Sentiment Treebank dataset.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Transition", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 114, "token_end": 166, "char_start": 633, "char_end": 903, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 167, "token_end": 197, "char_start": 910, "char_end": 1040, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Liao and Poggio, 2016)": "16045384"}, "Reference": {}}}, {"token_start": 198, "token_end": 244, "char_start": 1047, "char_end": 1303, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "17246494_5", "paragraph": "[BOS] We employ NMT as the backbone of our paraphrasing model.\n[BOS] In its simplest form our model exploits a one-to-one NMT architecture: the source English sentence is translated into k candidate foreign sentences and then back-translated into English.\n[BOS] Inspired by multi-way machine translation which has shown performance gains over single-pair models (Zoph and Knight, 2016; Dong et al., 2015; Firat et al., 2016a) , we also explore an alternative pivoting technique which uses multiple languages rather than a single one.\n[BOS] Our model inherits advantages from NMT such as a small memory footprint and conceptually easy decoding (implemented as beam search).\n[BOS] Beyond paraphrase generation, we experimentally show that the representations learned by our model are useful in semantic relatedness tasks.\n\n", "discourse_tags": ["Reflection", "Reflection", "Narrative_cite", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 67, "token_end": 96, "char_start": 343, "char_end": 425, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zoph and Knight, 2016;": "8677917", "Dong et al., 2015;": "3666937", "Firat et al., 2016a)": "6359641"}}}]}
{"id": "17246494_4", "paragraph": "[BOS] Neural Machine Translation There has been a surge of interest recently in repurposing sequence transduction neural network models for machine translation (Sutskever et al., 2014) .\n[BOS] Central to this approach is an encoder-decoder architecture implemented by recurrent neural networks.\n[BOS] The encoder reads the source sequence into a list of continuous-space representations from which the decoder generates the target sequence.\n[BOS] An attention mechanism (Bahdanau et al., 2014 ) is used to generate the region of focus during decoding.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Single_summ"], "span_citation_mapping": [{"token_start": 17, "token_end": 35, "char_start": 92, "char_end": 184, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sutskever et al., 2014)": "7961699"}}}, {"token_start": 78, "token_end": 102, "char_start": 447, "char_end": 551, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Bahdanau et al., 2014": "11212020"}, "Reference": {}}}]}
{"id": "17246494_3", "paragraph": "[BOS] In contrast, our model is syntax-agnostic, paraphrases are represented on the surface level without knowledge of any underlying grammar.\n[BOS] We capture paraphrases at varying levels of granularity, words, phrases or sentences without having to explicitly create a phrase table.\n\n", "discourse_tags": ["Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "17246494_2", "paragraph": "[BOS] Motivated by the wish to model sentential paraphrases, follow-up work focused on syntaxdriven techniques again within the bilingual pivoting framework.\n[BOS] Extensions include representing paraphrases via rules obtained from a synchronous context free grammar (Ganitkevitch et al., 2011; Madnani et al., 2007) as well as labeling paraphrases with linguistic annotations such as CCG categories (Callison-Burch, 2008 ) and partof-speech tags (Zhao et al., 2008) .\n\n", "discourse_tags": ["Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 43, "token_end": 68, "char_start": 234, "char_end": 316, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ganitkevitch et al., 2011;": "10616734", "Madnani et al., 2007)": "5180342"}}}, {"token_start": 71, "token_end": 91, "char_start": 328, "char_end": 421, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Callison-Burch, 2008": "2755801"}}}, {"token_start": 93, "token_end": 106, "char_start": 428, "char_end": 466, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhao et al., 2008)": "18556126"}}}]}
{"id": "17246494_1", "paragraph": "[BOS] Bilingual Pivoting Paraphrase extraction using bilingual parallel corpora was proposed by Bannard and Callison-Burch (2005) .\n[BOS] Their method first extracts a bilingual phrase table and then obtains English paraphrases by pivoting through foreign language phrases.\n[BOS] Paraphrases for a given phrase are ranked using a paraphrase probability defined in terms of the translation model probabilities P( f |e) and P(e| f ) where f and e are the foreign and English strings, respectively.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 102, "char_start": 6, "char_end": 495, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Callison-Burch (2005)": "15728911"}, "Reference": {}}}]}
{"id": "17246494_0", "paragraph": "[BOS] The literature on paraphrasing is vast with methods varying according to the type of paraphrase being induced (lexical or structural), the type of data used (e.g., monolingual or parallel corpus), the underlying representation (surface form or syntax trees), and the acquisition method itself.\n[BOS] For an overview of these issues we refer the interested reader to Madnani and Dorr (2010) .\n[BOS] We focus on bilingual pivoting methods and aspects of neural machine translation pertaining to our model.\n[BOS] We also discuss related work on paraphrastic embeddings.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 65, "token_end": 86, "char_start": 306, "char_end": 395, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Madnani and Dorr (2010)": "17652653"}}}]}
{"id": "22602753_3", "paragraph": "[BOS] Yet another approach which uses an existing translation system to extract parallel sentences from comparable documents was proposed by Yasuda and Sumita (2008) .\n[BOS] They describe a framework for machine translation using multilingual Wikipedia articles.\n[BOS] The parallel corpus is assembled iteratively, by using a statistical machine translation system trained on a preliminary sentence-aligned corpus, to score sentence-level en-jp BLEU scores.\n[BOS] After filtering out the unaligned pairs based on the MT evaluation metric, the SMT is retrained on the filtered pairs.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 107, "char_start": 6, "char_end": 582, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Yasuda and Sumita (2008)": "16742293"}, "Reference": {}}}]}
{"id": "22602753_2", "paragraph": "[BOS] Abdul-Rauf and Schwenk (2009) extract parallel sentences without the use of a classifier.\n[BOS] Target language candidate sentences are found using the translation of source side comparable corpora.\n[BOS] Sentence tail removal is used to strip the tail parts of sentence pairs which differ only at the end.\n[BOS] This, along with the use of parallel sentences enhanced the BLEU score and helped to determine if the translated source sentence and candidate target sentence are parallel by measuring the word and translation error rate.\n[BOS] This method succeeds in eliminating the need for domain specific text by using the target side as a source of candidate sentences.\n[BOS] However, this approach is not feasible if there isn't a good source side translation system to begin with, like in our case.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 124, "char_start": 6, "char_end": 677, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "22602753_1", "paragraph": "[BOS] Similar to our proposed approach, Barrn-Cedeo et al. (2015) showed how using parallel documents from Wikipedia for domain specific alignment would improve translation quality of SMT systems on in-domain data.\n[BOS] In this method, similarity between all pairs of cross-language sentences with different text similarity measures are estimated.\n[BOS] The issue of domain definition is overcome by the use of IR techniques which use the characteristic vocabulary of the domain to query a Lucene search engine over the entire corpus.\n[BOS] The candidate sentences are defined based on word overlap and the decision whether a sentence pair is parallel or not using the maximum entropy classifier.\n[BOS] The difference in the BLEU scores between out of domain and domain-specific translation is proved clearly using the word embeddings from characteristic vocabulary extracted using the extracted additional bitexts.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 2, "token_end": 101, "char_start": 6, "char_end": 535, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Barr\u00f3n-Cede\u00f1o et al. (2015)": "15575006"}, "Reference": {}}}]}
{"id": "22602753_0", "paragraph": "[BOS] A lot of work has been done on the problem of automatic sentence alignment from comparable corpora, but a majority of them (Abdul-Rauf and Schwenk, 2009; Irvine and Callison-Burch, 2013; Yasuda and Sumita, 2008 ) use a pre-existing translation system as a precursor to ranking the candidate sentence pairs, which the low resource language pairs are not at the luxury of having; or use statistical machine learning approaches, where a Maximum Entropy classifier is used that relies on surface level features such as word overlap in order to obtain parallel sentence pairs (Munteanu and Marcu, 2005) .\n[BOS] However, the deep neural network model used in our paper is probably the first of its kind, which does not need any feature engineering and also does not need a pre-existing translation system.\n[BOS] Munteanu and Marcu (2005) proposed a parallel sentence extraction system which used comparable corpora from newspaper articles to extract the parallel sentence pairs.\n[BOS] In this procedure, a maximum entropy classifier is designed for all sentence pairs possible from the Cartesian product of a pair of documents and passed through a sentence-length ratio filter in order to obtain candidate sentence pairs.\n[BOS] SMT systems were trained on the extracted sentence pairs using the additional features from the comparable corpora like distortion and position of current and previously aligned sentences.\n[BOS] This resulted in a state of the art approach with respect to the translation performance of low resource languages.\n\n", "discourse_tags": ["Multi_summ", "Reflection", "Single_summ", "Single_summ", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 2, "token_end": 130, "char_start": 6, "char_end": 603, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Abdul-Rauf and Schwenk, 2009;": "8731445", "Irvine and Callison-Burch, 2013;": "1395225", "Yasuda and Sumita, 2008": "16742293"}, "Reference": {"(Munteanu and Marcu, 2005)": "15289038"}}}, {"token_start": 171, "token_end": 244, "char_start": 812, "char_end": 1221, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Munteanu and Marcu (2005)": "15289038"}, "Reference": {}}}]}
{"id": "1933463_0", "paragraph": "[BOS] Apart from the model of Canny et al. (2013) , there have been a few attempts at using GPUs in NLP contexts before.\n[BOS] Johnson (2011) and Yi et al. (2011) both had early attempts at porting parsing algorithms to the GPU.\n[BOS] However, they did not demonstrate significantly increased speed over a CPU implementation.\n[BOS] In machine translation, He et al. (2013) adapted algorithms designed for GPUs in the computational biology literature to speed up on-demand phrase table extraction.\n\n", "discourse_tags": ["Narrative_cite", "Multi_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 15, "char_start": 6, "char_end": 49, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Canny et al. (2013)": "18110049"}}}, {"token_start": 33, "token_end": 73, "char_start": 127, "char_end": 325, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Johnson (2011)": "7027815", "Yi et al. (2011)": "1102745"}, "Reference": {}}}, {"token_start": 74, "token_end": 106, "char_start": 332, "char_end": 496, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"He et al. (2013)": "8823877"}, "Reference": {}}}]}
{"id": "18032263_0", "paragraph": "[BOS] Most work in sentiment analysis focuses on classifying explicit sentiments and extracting explicit opinion expressions, holders and targets Johansson and Moschitti, 2013; Yang and Cardie, 2013) .\n[BOS] There is some work investigating features that directly indicate implicit sentiments (Zhang and Liu, 2011; Feng et al., 2013) .\n[BOS] In contrast, we focus on how we can bridge between explicit and implicit sentiments via inference.\n[BOS] To infer the implicit sentiments related to gfbf events, some work mines various syntactic patterns (Choi and Cardie, 2008) , proposes linguistic templates (Zhang and Liu, 2011; Anand and Reschke, 2010; Reschke and Anand, 2011) , or generates a lexicon of patient polarity verbs (Goyal et al., 2013) .\n[BOS] Different from their work, which do not cover all cases relevant to gfbf events, ) defines a generalized set of implicature rules and proposes a graph-based model to achieve sentiment propagation between the agents and themes of gfbf events.\n[BOS] However, that system requires all of the gfbf information (Q1)-(Q4) to be input from the manual annotations; the only ambiguity it resolves is sentiments toward entities.\n[BOS] In contrast, the method in this paper tackles four ambiguities simultaneously.\n[BOS] Further, as we will see below in Section 6, the improvement over the local detectors by the current method is greater than that by the previous method, even though it operates over the noisy output of local components automatically.\n[BOS] Different from pipeline architectures, where each step is computed independently, joint inference has often achieved better results.\n[BOS] formulate the task of information extraction using Integer Linear Programming (ILP).\n[BOS] Since then, ILP has been widely used in various tasks in NLP, including semantic role labeling (Punyakanok et al., 2004; Punyakanok et al., 2008; Das et al., 2012) , joint extraction of opinion entities and relations (Choi et al., 2006; Yang and Cardie, 2013) , co-reference resolution (Denis and Baldridge, 2007) , and summarization (Martins and Smith, 2009 ).\n[BOS] The most similar ILP model to ours is (Somasundaran and Wiebe, 2009) , which improves opinion polarity classification using discourse constraints in an ILP model.\n[BOS] However, their work addresses discourse relations among explicit opinions in different sentences.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Reflection", "Narrative_cite", "Transition", "Transition", "Reflection", "Reflection", "Transition", "Transition", "Narrative_cite", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 9, "token_end": 40, "char_start": 49, "char_end": 199, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Johansson and Moschitti, 2013;": "8525297", "Yang and Cardie, 2013)": "1594813"}}}, {"token_start": 51, "token_end": 68, "char_start": 273, "char_end": 333, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhang and Liu, 2011;": "7125621", "Feng et al., 2013)": "3011971"}}}, {"token_start": 102, "token_end": 117, "char_start": 504, "char_end": 570, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Choi and Cardie, 2008)": "2527473"}}}, {"token_start": 118, "token_end": 146, "char_start": 573, "char_end": 674, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhang and Liu, 2011;": "7125621", "Anand and Reschke, 2010;": null, "Reschke and Anand, 2011)": "739268"}}}, {"token_start": 148, "token_end": 164, "char_start": 680, "char_end": 746, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Goyal et al., 2013)": null}}}, {"token_start": 370, "token_end": 401, "char_start": 1806, "char_end": 1897, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Punyakanok et al., 2004;": "2969247", "Punyakanok et al., 2008;": "11162815", "Das et al., 2012)": "1071069"}}}, {"token_start": 402, "token_end": 424, "char_start": 1900, "char_end": 1993, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Choi et al., 2006;": "250761", "Yang and Cardie, 2013)": "1594813"}}}, {"token_start": 425, "token_end": 438, "char_start": 1996, "char_end": 2047, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Denis and Baldridge, 2007)": "18699296"}}}, {"token_start": 440, "token_end": 449, "char_start": 2054, "char_end": 2092, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Martins and Smith, 2009": "16148301"}}}, {"token_start": 452, "token_end": 502, "char_start": 2102, "char_end": 2368, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Somasundaran and Wiebe, 2009)": "2845337"}, "Reference": {}}}]}
{"id": "1770806_0", "paragraph": "[BOS] Related work can be divided into two categories: NER and NEN.\n\n", "discourse_tags": ["Other"], "span_citation_mapping": []}
{"id": "17667087_1", "paragraph": "[BOS] Our focus in this work is aiming to make full use of the source-side large-scale monolingual data in NMT, which is not fully explored before.\n[BOS] The most related works lie in three aspects: 1) applying target-side monolingual data in NMT, 2) targeting knowledge sharing with multi-task NMT, and 3) using source-side monolingual data in conventional SMT and NMT.\n[BOS] Gulcehre et al. (2015) first investigate the targetside monolingual data in NMT.\n[BOS] They propose shallow and deep fusion methods to enhance the decoder network by training a big language model on targetside large-scale monolingual data.\n[BOS] Sennrich et al. (2015) further propose a new approach to use targetside monolingual data.\n[BOS] They generate the synthetic bilingual data by translating the target monolingual sentences to source language sentences and retrain NMT with the mixture of original bilingual data and the synthetic parallel data.\n[BOS] It is similar to our selflearning algorithm in which we concern the sourceside monolingual data.\n[BOS] Furthermore, their method requires to train an additional NMT from target language to source language, which may negatively influence the attention model in the decoder network.\n[BOS] Dong et al. (2015) propose a multi-task learning method for translating one source language into multiple target languages in NMT so that the encoder network can be shared when dealing with several sets of bilingual data.\n[BOS] , and Firat et al. (2016) further deal with more complicated cases (e.g. multi-source languages).\n[BOS] Note that all these methods require bilingual training corpus.\n[BOS] Instead, we adapt the multitask learning framework to better accommodate the source-side monolingual data.\n[BOS] Ueffing et al. (2007) and Wu et al. (2008) explore the usage of source-side monolingual data in conventional SMT with a self-learning algorithm.\n[BOS] Although we apply self-learning in this work, we use it to enhance the encoder network in NMT rather than generating more translation rules in SMT and we also adapt a multi-task learning framework to take full advantage of the source-side monolingual data.\n[BOS] Luong et al. (2015a) also investigate the source-side monolingual data in the multi-task learning framework, in which a simple autoencoder or skip-thought vectors are employed to model the monolingual data.\n[BOS] Our sentence reordering model is more powerful than simple autoencoder in encoder enhancement.\n[BOS] Furthermore, they do not carefully prepare the monolingual data for which we show that only related monolingual data leads to big improvements.\n[BOS] In parallel to our work, Cheng et al. (2016b) propose a similar semi-supervised framework to handle both source and target language monolingual data.\n[BOS] If source-side monolingual data is considered, a reconstruction framework including two NMTs is employed.\n[BOS] One NMT translates the source-side monolingual data into target language translations, from which the other NMT attempts to reconstruct the original source-side monolingual data.\n[BOS] In contrast to their approach, we propose a sentence reordering model rather than the sentence reconstruction model.\n[BOS] Furthermore, we carefully investigate the relationship between the monolingual data quality and the translation performance improvement.\n\n", "discourse_tags": ["Reflection", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection", "Single_summ", "Single_summ", "Single_summ", "Transition", "Reflection", "Multi_summ", "Reflection", "Single_summ", "Reflection", "Reflection", "Single_summ", "Single_summ", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 89, "token_end": 140, "char_start": 377, "char_end": 616, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 141, "token_end": 250, "char_start": 623, "char_end": 1218, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Sennrich et al. (2015)": "15600925"}, "Reference": {}}}, {"token_start": 251, "token_end": 294, "char_start": 1225, "char_end": 1446, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Dong et al. (2015)": "3666937"}, "Reference": {}}}, {"token_start": 297, "token_end": 322, "char_start": 1459, "char_end": 1550, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Firat et al. (2016)": "6359641"}, "Reference": {}}}, {"token_start": 355, "token_end": 393, "char_start": 1739, "char_end": 1883, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ueffing et al. (2007)": "12615023", "Wu et al. (2008)": "3702321"}, "Reference": {}}}, {"token_start": 448, "token_end": 495, "char_start": 2153, "char_end": 2359, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Luong et al. (2015a)": "6954272"}, "Reference": {}}}, {"token_start": 547, "token_end": 629, "char_start": 2642, "char_end": 3063, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Cheng et al. (2016b)": "256189"}, "Reference": {}}}]}
{"id": "17667087_0", "paragraph": "[BOS] As a new paradigm for machine translation, the encoder-decoder based NMT has drawn more and more attention.\n[BOS] Most of the existing methods mainly focus on designing better alignment mechanisms (attention model) for the decoder network (Cheng et al., 2016a; Luong et al., 2015b; Cohn et al., 2016; Feng et al., 2016; Tu et al., 2016; , better objective functions for BLEU evaluation and better strategies for handling unknown words (Luong et al., 2015c; Sennrich et al., 2015; or large vocabularies (Jean et al., 2015; Mi et al., 2016c) .\n\n", "discourse_tags": ["Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 43, "token_end": 84, "char_start": 229, "char_end": 341, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cheng et al., 2016a;": "3935945", "Luong et al., 2015b;": "1998416", "Cohn et al., 2016;": "1964946", "Feng et al., 2016;": "8063399"}}}, {"token_start": 86, "token_end": 118, "char_start": 345, "char_end": 484, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Luong et al., 2015c;": "1245593"}}}, {"token_start": 120, "token_end": 140, "char_start": 489, "char_end": 545, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Jean et al., 2015;": "2863491", "Mi et al., 2016c)": null}}}]}
{"id": "2280062_4", "paragraph": "[BOS] Our work follows a long line of joint modeling research that has demonstrated great success for various NLP tasks Punyakanok et al., 2004; Finkel and Manning, 2010; Rush et al., 2010; Choi et al., 2006; Yang and Cardie, 2013) .\n[BOS] Methods tend to fall into one of two joint modeling frameworks: the first learns a joint model that captures global dependencies; the other uses independently-learned models and considers global dependencies only during inference.\n[BOS] In this work, we study both types of joint approaches for opinion expression extraction and opinion attribute classification.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 19, "token_end": 62, "char_start": 110, "char_end": 231, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Punyakanok et al., 2004;": "2969247", "Finkel and Manning, 2010;": "8016551", "Rush et al., 2010;": "1994530", "Choi et al., 2006;": "250761", "Yang and Cardie, 2013)": "1594813"}}}]}
{"id": "2280062_3", "paragraph": "[BOS] Compared to the existing approaches, our joint models have the advantage of modeling opinion expression extraction and attribute classification at the segment-level, and more importantly, they provide a principled way of combining the segmentation and classification components.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "2280062_2", "paragraph": "[BOS] There has been limited work on the joint modeling of opinion expression extraction and attribute classification.\n[BOS] Choi and Cardie (2010) first developed a joint sequence labeler that jointly tags opinions, polarity and intensity by training CRFs with hierarchical features (Zhao et al., 2008) .\n[BOS] One major drawback of their approach is that it models both opinion extraction and attribute labeling as tasks in token-level sequence labeling, and thus cannot model their inter-actions at the expression-level.\n[BOS] Johansson and Moschitti (2011) and Johansson and Moschitti (2013) propose a joint approach to opinion expression extraction and polarity classification by re-ranking its k-best output using global features.\n[BOS] One major issue with their approach is that the k-best candidates were obtained without global reasoning about the relative uncertainty in the individual stages.\n[BOS] As the number of considered attributes grows, it also becomes harder to decide how many predictions to select from each attribute classifier.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Multi_summ", "Multi_summ", "Transition"], "span_citation_mapping": [{"token_start": 20, "token_end": 98, "char_start": 125, "char_end": 523, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Choi and Cardie (2010)": "15266179"}, "Reference": {"(Zhao et al., 2008)": "8701766"}}}, {"token_start": 99, "token_end": 172, "char_start": 530, "char_end": 904, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Johansson and Moschitti (2011)": "1935038", "Johansson and Moschitti (2013)": "8525297"}, "Reference": {}}}]}
{"id": "2280062_1", "paragraph": "[BOS] Opinion expression extraction has been successfully tackled via sequence tagging methods.\n[BOS] Breck et al. (2007) applied conditional random fields to assign each token a label indicating whether it belongs to an opinion expression or not.\n[BOS] Yang and Cardie (2012) employed a segment-level sequence labeler based on semi-CRFs with rich phrase-level syntactic features.\n[BOS] In this work, we also utilize semi-CRFs to model opinion expression extraction.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 16, "token_end": 45, "char_start": 102, "char_end": 247, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Breck et al. (2007)": "1746806"}, "Reference": {}}}, {"token_start": 46, "token_end": 75, "char_start": 254, "char_end": 380, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Yang and Cardie (2012)": "11176977"}, "Reference": {}}}]}
{"id": "2280062_0", "paragraph": "[BOS] Significant research effort has been invested in the task of fine-grained opinion analysis in recent years Wilson et al., 2009) .\n[BOS] first motivated and studied phraselevel polarity classification on an open-domain corpus.\n[BOS] Choi and Cardie (2008) developed inference rules to capture compositional effects at the lexical level on phrase-level polarity classification.\n[BOS] Yessenalina and Cardie (2011) and Socher et al. (2013) learn continuous-valued phrase representations by combining the representations of words within an opinion expression and using them as features for classifying polarity and intensity.\n[BOS] All of these approaches assume the opinion expressions are available before training the classifiers.\n[BOS] However, in real-world settings, the spans of opinion expressions within the sentence are not available.\n[BOS] In fact, Choi and Cardie (2008) demonstrated that the performance of expression-level polarity classification degrades as more surrounding (but irrelevant) context is considered.\n[BOS] This motivates the additional task of identifying the spans of opinion expressions.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Single_summ", "Multi_summ", "Transition", "Transition", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 12, "token_end": 28, "char_start": 67, "char_end": 133, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Wilson et al., 2009)": "9423000"}}}, {"token_start": 46, "token_end": 71, "char_start": 238, "char_end": 381, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Choi and Cardie (2008)": "2527473"}, "Reference": {}}}, {"token_start": 72, "token_end": 118, "char_start": 388, "char_end": 627, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Socher et al. (2013)": "990233"}, "Reference": {}}}, {"token_start": 156, "token_end": 203, "char_start": 853, "char_end": 1121, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Choi and Cardie (2008)": "2527473"}, "Reference": {}}}]}
{"id": "16709891_3", "paragraph": "[BOS] The third method uses multiple prediction models to predict different categories of erroneous character.\n[BOS] For example, Xin, Zhao, Wang, & Jia (2014) converted the problem of erroneous characters into the problem of seeking the shortest pathway in a graph.\n[BOS] Because the graph model can only identify erroneous characters in long words, for erroneous single-character words it additionally uses rule-based methods and a CRF model to make corrections.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 18, "token_end": 85, "char_start": 117, "char_end": 464, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Xin, Zhao, Wang, & Jia (2014)": "1288087"}, "Reference": {}}}]}
{"id": "16709891_2", "paragraph": "[BOS] The second category is the direct utilization of a probability model to detect an erroneous character.\n[BOS] Han and Chang (2013) proposed using maximum entropy in relation to 5311 characters and the seven-grams trained model to correct erroneous characters.\n[BOS] The fundamental hypothesis of this study was: if there was a possible erroneous character in the sentence, then the matched pairs that the character and the characters preceding and following it produced may not exist in the text corpus.\n[BOS] Conversely, if the matched pair made by the character and the character preceding it or following it is commonly seen in the text corpus, then that character's degree of erroneousness is very low here.\n[BOS] Xiong et al. (2014) proposed using the Hidden Markov Model (HMM) as the basis for a model to detect and correct erroneous characters.\n[BOS] This method presupposes that unknown erroneous characters exist in the sentence, and seeks out each character's substitute character by means of phonetic writing (pinyin) and the Cangjie input code using Bayes' rule as its basis.\n[BOS] Because there are many substitute characters, this method then uses methods such as n-gram and statistics from internet search results to determine substitute words.\n[BOS] Gu, Wang, & Liang (2014) use SSCS as their target in the same way but use character blocks within SSCS.\n[BOS] Exploiting the statistical method of serial computer characters forming character blocks, it is possible to detect and correct erroneous characters while not utilizing a word segmentation system.\n\n", "discourse_tags": ["Transition", "Single_summ", "Transition", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 20, "token_end": 48, "char_start": 115, "char_end": 264, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Han and Chang (2013)": "14226984"}, "Reference": {}}}, {"token_start": 133, "token_end": 240, "char_start": 723, "char_end": 1264, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Xiong et al. (2014)": "13253132"}, "Reference": {}}}, {"token_start": 241, "token_end": 298, "char_start": 1271, "char_end": 1576, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Gu, Wang, & Liang (2014)": "14637976"}, "Reference": {}}}]}
{"id": "16709891_1", "paragraph": "[BOS] These methods can be divided essentially into three categories.\n[BOS] The first consists of initially processing the sentence using a Chinese word segmentation tool, then detecting whether erroneous characters occur among serial single Chinese character sequences (abbreviated to SSCS below).\n[BOS] Chang, Chen, Tseng, & Zheng (2013) searched for possible correct words among each character in an SSCS, and using the three parameters of \"similarity of phonetic value,\" \"similarity of form,\" and \"probability of co-occurrence of adjacent characters\" established a linear regression prediction model.\n[BOS] Wang and Liao (2014) used the Chinese word segmentation system to analyze a sentence's word segments, and then, if there was a suspected occurrence of an erroneous character in a two-character word or single-character word, used a character with a high degree of similarity of phonetic value and form to replace the possible erroneous character.\n[BOS] Finally, they used a tri-gram model to assess whether to conduct a replacement.\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 50, "token_end": 112, "char_start": 305, "char_end": 604, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Chang, Chen, Tseng, & Zheng (2013)": "7948161"}, "Reference": {}}}, {"token_start": 113, "token_end": 199, "char_start": 611, "char_end": 1042, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wang and Liao (2014)": "7206597"}, "Reference": {}}}]}
{"id": "16709891_0", "paragraph": "[BOS] Proposed automated detection and correction methods for Chinese erroneous characters can be traced back to the detection and correction method put forward by Chang (1995) .\n[BOS] This method used the four commonly occurring forms of erroneous characters-\"characters with similar pronunciation,\" \"characters with similar form,\" \"characters with similar connotation,\" and \"characters with similar input code value\"-to establish relationships of confusion between the characters.\n[BOS] Using such databases of computer characters that may produce erroneous character relationships, it is possible to provide a list of corrections for use in attempting to detect erroneous characters and correct sentences.\n[BOS] The input sentences use confused character sets one by one as substitutes for the Chinese computer characters in the sentence, producing a variety of possible combination sentences as candidate sentences.\n[BOS] By calculating sentence probability based on a bi-gram model, the system seeks to obtain the optimum solution in relation to the candidate sentences that have been produced.\n[BOS] If the optimum solution differs from the original sentence, it then compares the differing computer character and serves as the corrected result.\n[BOS] In recent years, since some competitions have been held to correct Chinese erroneous characters, many studies have proposed a wide variety of methods to resolve this problem.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Transition", "Transition", "Transition", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 2, "token_end": 86, "char_start": 6, "char_end": 482, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Chang (1995)": null}, "Reference": {}}}]}
{"id": "17786494_3", "paragraph": "[BOS] Besides combining pairwise classification and mention clustering, there has also been some work that jointly performs mention detection and coreference resolution.\n[BOS] Daum and Marcu (2005) developed such a model based on the Learning as Search Optimization (LaSO) framework.\n[BOS] Rahman and Ng (2009) proposed to learn a cluster-ranker for discourse-new mention detection jointly with coreference resolution.\n[BOS] Denis and Baldridge (2007) adopted an Integer Linear Programming (ILP) formulation for coreference resolution which models anaphoricity and coreference as a joint task.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 27, "token_end": 52, "char_start": 176, "char_end": 283, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 53, "token_end": 80, "char_start": 290, "char_end": 418, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Rahman and Ng (2009)": "7177672"}, "Reference": {}}}, {"token_start": 81, "token_end": 116, "char_start": 425, "char_end": 593, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "17786494_2", "paragraph": "[BOS] In recent years, Markov logic has been widely used in natural language processing problems (Poon and Domingos, 2009; Yoshikawa et al., 2009; Che and Liu, 2010) .\n[BOS] For coreference resolution, the most notable one is unsupervised coreference resolution by Poon and Domingos (2008) .\n[BOS] Poon and Domingos (2008) followed the entity-mention model while we follow the mention-pair model, which are quite different approaches.\n[BOS] To seek good performance in an unsupervised way, Poon and Domingos (2008) highly rely on two important strong indicators: appositives and predicate nominatives.\n[BOS] However, OntoNotes corpus (state-of-art NLP data collection) on coreference layer for CoNLL-2011 has excluded these two conditions of annotations (appositives and predicate nominatives) from their judging guidelines.\n[BOS] Compared with it, our methods are more applicable for real dataset.\n[BOS] Huang et al. (2009) used Markov logic to predict coreference probabilities for mention pairs followed by correlation clustering to generate the final results.\n[BOS] Although they also perform joint learning, at the inference stage, they still make pairwise coreference decisions and cluster mentions sequentially.\n[BOS] Unlike their method, We formulate the two steps into a single framework.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Single_summ", "Single_summ", "Transition", "Reflection", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 13, "token_end": 41, "char_start": 60, "char_end": 165, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Poon and Domingos, 2009;": "5337047", "Yoshikawa et al., 2009;": "6945139"}}}, {"token_start": 53, "token_end": 67, "char_start": 226, "char_end": 289, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Poon and Domingos (2008)": "7124715"}}}, {"token_start": 69, "token_end": 135, "char_start": 298, "char_end": 601, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Poon and Domingos (2008)": "7124715"}, "Reference": {}}}, {"token_start": 200, "token_end": 254, "char_start": 905, "char_end": 1218, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "17786494_1", "paragraph": "[BOS] To perform joint learning of pairwise classification and mention clustering, in (McCallum and Wellner, 2005) , each mention pair corresponds to a binary variable indicating whether the two mentions are coreferential, and the dependence between these variables is modeled by conditional undirected graphical models.\n[BOS] Finley and Joachims (2005) proposed a general SVM-based framework for supervised clustering that learns item-pair similarity measures, and applied the framework to noun phrase coreference resolution.\n[BOS] In our work, we take a different approach and apply Markov logic.\n[BOS] As we have shown in Section 3, given the flexibility of Markov logic, it is straightforward to perform joint learning of pairwise classification and mention clustering.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 57, "char_start": 6, "char_end": 320, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(McCallum and Wellner, 2005)": "11747348"}, "Reference": {}}}, {"token_start": 58, "token_end": 96, "char_start": 327, "char_end": 526, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "17786494_0", "paragraph": "[BOS] Supervised noun phrase coreference resolution has been extensively studied.\n[BOS] Besides the mention-pair model, two other commonly used models are the entity-mention model (Luo et al., 2004; Yang et al., 2008) and ranking models (Denis and Baldridge, 2008; Rahman and Ng, 2009) .\n[BOS] Interested readers can refer to the literature review by Ng (2010) .\n[BOS] Under the mention-pair model, Klenner (2007) and Finkel and Manning (2008) applied Integer Linear Programming (ILP) to enforce transitivity on the pairwise classification results.\n[BOS] Chang et al. (2011) used the same ILP technique to incorporate best-first clustering and generate the mention clusters.\n[BOS] In all these studies, however, mention clustering is combined with pairwise classification only at the inference stage but not at the learning stage.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite", "Multi_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 28, "token_end": 47, "char_start": 159, "char_end": 217, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Yang et al., 2008)": "126675"}}}, {"token_start": 48, "token_end": 66, "char_start": 222, "char_end": 285, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Rahman and Ng, 2009)": "7177672"}}}, {"token_start": 68, "token_end": 81, "char_start": 294, "char_end": 360, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Ng (2010)": "825928"}}}, {"token_start": 83, "token_end": 123, "char_start": 369, "char_end": 548, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 124, "token_end": 149, "char_start": 555, "char_end": 674, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "22421874_3", "paragraph": "[BOS] Knowledge Distillation.\n[BOS] Knowledge distillation was introduced for model compression to learn small models from larger models (Bucilu et al., 2006; Hinton et al., 2015) .\n[BOS] For example, from a large neural network model a smaller model can be distilled such that it generalizes in the same way as the large model (Romero et al., 2014) .\n[BOS] Knowledge distillation was also used by (Hu et al., 2016) to distill knowledge from logical rules in the tasks of named entity recognition and sentiment analysis, thereby enforcing constraints on the trained model.\n[BOS] Our approach is different from this prior work on knowledge distillation in that we distill knowledge from mapping functions of related languages into mapping functions of languages with only small seed dictionaries.\n[BOS] Domain adaptation, for which there is a long history, is also related to our work (Ben-David et al., 2007; Daum III, 2007; Pan et al., 2010; Long and Wang, 2015) .\n[BOS] (Daum III, 2007) proposed feature augmentation, suggesting that a model should have features that are general across domains, as well as features that are domainspecific.\n[BOS] Thus the model learns from all domains while preserving domain-specific information.\n[BOS] These kinds of models have to be retrained when a new domain is added.\n[BOS] Our work however only has to train mapping functions that involve a new language, all others can be distilled without retraining them.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite", "Single_summ", "Reflection", "Narrative_cite", "Single_summ", "Single_summ", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 8, "token_end": 42, "char_start": 36, "char_end": 179, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bucilu et al., 2006;": null, "Hinton et al., 2015)": "7200347"}}}, {"token_start": 47, "token_end": 81, "char_start": 201, "char_end": 349, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Romero et al., 2014)": "2723173"}}}, {"token_start": 83, "token_end": 126, "char_start": 358, "char_end": 572, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 165, "token_end": 211, "char_start": 802, "char_end": 963, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ben-David et al., 2007;": "10908021", "Daum\u00e9 III, 2007;": "5360764", "Pan et al., 2010;": "5984940", "Long and Wang, 2015)": "17901965"}}}, {"token_start": 213, "token_end": 261, "char_start": 972, "char_end": 1233, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Daum\u00e9 III, 2007)": "5360764"}, "Reference": {}}}]}
{"id": "22421874_2", "paragraph": "[BOS] On the aspect of enriching word embeddings with linguistic knowledge for the purpose of machine translation, Sennrich and Barry (Sennrich and Haddow, 2016) introduce linguistic features in sequence to sequence neural machine translation.\n[BOS] Like our work, they also represent such features in the embedding layer.\n[BOS] In addition to part-ofspeech tags and morphological features, they also use syntactic dependency labels which are not applicable to our model since we work at the word level while their model is at the sentence level.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 104, "char_start": 6, "char_end": 546, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Sennrich and Haddow, 2016)": "16126936"}, "Reference": {}}}]}
{"id": "22421874_1", "paragraph": "[BOS] Bilingual word embeddings.\n[BOS] There is a rich body of work on bilingual embeddings.\n[BOS] Bilingual word embedding learning methods produce a shared bilingual word embedding space where words from two languages are represented in the new space so that similar words, which may be in different languages, have similar representations.\n[BOS] Such bilingual word embeddings have been used in a number of tasks including semantic word similarity (Faruqui and Dyer, 2014; Ammar et al., 2016) learning bilingual word lexicons (Mikolov et al., 2013; Gouws et al., 2015; Vulic and Korhonen, 2016) , parsing (Guo et al., 2015; Tckstrm et al., 2012) , information retrieval (Vulic and Moens, 2015) , and cross-lingual document classification (Klementiev et al., 2012; Koisk et al., 2014) .\n[BOS] Some bilingual word embedding methods such as Gouws et al., 2015) require sentence or word aligned data, which our approach does not require.\n[BOS] We compare our approach to the bilingual embeddings produced by the recent method of (Ammar et al., 2016) .\n[BOS] Like our approach, this work does not require availability of parallel corpora but only a seed dictionary.\n\n", "discourse_tags": ["Other", "Transition", "Transition", "Narrative_cite", "Multi_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 72, "token_end": 94, "char_start": 426, "char_end": 495, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Faruqui and Dyer, 2014;": "3792324", "Ammar et al., 2016)": "1227830"}}}, {"token_start": 94, "token_end": 127, "char_start": 496, "char_end": 597, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mikolov et al., 2013;": "1966640", "Gouws et al., 2015;": "7021865", "Vulic and Korhonen, 2016)": "17515652"}}}, {"token_start": 128, "token_end": 147, "char_start": 600, "char_end": 648, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Guo et al., 2015;": "18634877"}}}, {"token_start": 148, "token_end": 159, "char_start": 651, "char_end": 696, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 161, "token_end": 186, "char_start": 703, "char_end": 786, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Klementiev et al., 2012;": "6758088"}}}, {"token_start": 225, "token_end": 242, "char_start": 974, "char_end": 1048, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "22421874_0", "paragraph": "[BOS] Cross Vector Space Mapping with Seed Dictionaries.\n[BOS] Our work is most related to models that do zero-shot learning for bilingual dictionary induction, using maps between vector spaces with seed dictionaries as training data.\n[BOS] Examples include the models of (Mikolov et al., 2013; Dinu et al., 2014; Lazaridou et al., 2015; Vulic and Korhonen, 2016) .\n[BOS] Like these approaches, we first learn word embeddings for each language, then use a seed dictionary to train a mapping function between the two vector spaces.\n[BOS] In a departure from these prior methods, we propose to distill knowledge from trilingual paths of nearby languages for languages with small seed dictionaries using a distillation training objective.\n[BOS] Additionally, we model linguistic information in the vector space of the source and target languages.\n[BOS] Another line of research in this vein is the work of (Vulic and Korhonen, 2016) , who analyze how properties of the seed dictionary affect bilingual dictionary induction across different dimensions (i.e., lexicon source, lexicon size, translation method, translation pair reliability).\n[BOS] However, methodologically, their approach is based on prior work (Mikolov et al., 2013; Dinu et al., 2014) .\n\n", "discourse_tags": ["Reflection", "Reflection", "Narrative_cite", "Reflection", "Reflection", "Reflection", "Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 44, "token_end": 86, "char_start": 241, "char_end": 363, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mikolov et al., 2013;": "1966640", "Dinu et al., 2014;": "17910711", "Lazaridou et al., 2015;": "12187767", "Vulic and Korhonen, 2016)": "17515652"}}}, {"token_start": 174, "token_end": 231, "char_start": 850, "char_end": 1135, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Vulic and Korhonen, 2016)": "17515652"}, "Reference": {}}}, {"token_start": 232, "token_end": 262, "char_start": 1142, "char_end": 1248, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mikolov et al., 2013;": "1966640", "Dinu et al., 2014)": "17910711"}}}]}
{"id": "2368709_1", "paragraph": "[BOS] On the second group, Koo et al. (2008) presented a semisupervised method for training dependency parsers, introducing features that incorporate word clusters automatically acquired from a large unannotated corpus.\n[BOS] The clusters include strongly semantic associations like {apple, pear} or {Apple, IBM} and also syntactic clusters like {of, in}.\n[BOS] They demonstrated its effectiveness in dependency parsing experiments on the PTB and the Prague Dependency Treebank.\n[BOS] Suzuki et al. (2009) , Sagae and Gordon (2009) and Candito and Seddah (2010) also experiment with the same cluster method.\n[BOS] Recently, Tckstrm et al. (2012) tested the incorporation of cluster features from unlabeled corpora in a multilingual setting, giving an algorithm for inducing cross-lingual clusters.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Multi_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 94, "char_start": 6, "char_end": 478, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Koo et al. (2008)": "1916754"}, "Reference": {}}}, {"token_start": 95, "token_end": 128, "char_start": 485, "char_end": 607, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Suzuki et al. (2009)": "160543", "Sagae and Gordon (2009)": "797950", "Candito and Seddah (2010)": "1283622"}, "Reference": {}}}, {"token_start": 129, "token_end": 167, "char_start": 614, "char_end": 797, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"T\u00e4ckstr\u00f6m et al. (2012)": "891605"}, "Reference": {}}}]}
{"id": "2368709_0", "paragraph": "[BOS] Broadly speaking, we can classify the attempts to add external knowledge to a parser in two sets: using large semantic repositories such as WordNet and approaches that use information automatically acquired from corpora.\n[BOS] In the first group, Agirre et al. (2008) trained two state-of-the-art constituency-based statistical parsers (Charniak, 2000; Bikel, 2004) on semantically-enriched input, substituting content words with their semantic classes, trying to overcome the limitations of lexicalized approaches to parsing (Collins, 2003) where related words, like scissors and knife, cannot be generalized.\n[BOS] The results showed a signicant improvement, giving the first results over both WordNet and the Penn Treebank (PTB) to show that semantics helps parsing.\n[BOS] Later, Agirre et al. (2011) successfully introduced WordNet classes in a dependency parser, obtaining improvements on the full PTB using gold POS tags, trying different combinations of semantic classes.\n[BOS] MacKinlay et al. (2012) investigate the addition of semantic annotations in the form of word sense hypernyms, in HPSG parse ranking, reducing error rate in dependency F-score by 1%, while some methods produce substantial decreases in performance.\n[BOS] Fujita et al. (2010) showed that fully disambiguated sense-based features smoothed using ontological information are effective for parse selection.\n\n", "discourse_tags": ["Transition", "Single_summ", "Transition", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 40, "token_end": 128, "char_start": 233, "char_end": 616, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Agirre et al. (2008)": "9904828"}, "Reference": {"(Charniak, 2000;": null, "Bikel, 2004)": "862713", "(Collins, 2003)": "7901127"}}}, {"token_start": 162, "token_end": 202, "char_start": 782, "char_end": 984, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Agirre et al. (2011)": "4328886"}, "Reference": {}}}, {"token_start": 203, "token_end": 255, "char_start": 991, "char_end": 1237, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"MacKinlay et al. (2012)": "7202228"}, "Reference": {}}}, {"token_start": 256, "token_end": 283, "char_start": 1244, "char_end": 1391, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Fujita et al. (2010)": "14711007"}, "Reference": {}}}]}
{"id": "1704410_4", "paragraph": "[BOS] Another thread of relevant research has explored the use of features in unsupervised POS induction (Smith and Eisner, 2005; Berg-Kirkpatrick et al., 2010; Hasan and Ng, 2009) .\n[BOS] These methods demonstrated the benefits of incorporating linguistic features using a log-linear parameterization, but requires elaborate machinery for training.\n[BOS] In our work, we demonstrate that using a simple nave-Bayes approach also yields substantial performance gains, without the associated training complexity.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 14, "token_end": 44, "char_start": 78, "char_end": 180, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Smith and Eisner, 2005;": "259144", "Berg-Kirkpatrick et al., 2010;": "1322232", "Hasan and Ng, 2009)": "1323015"}}}]}
{"id": "1704410_3", "paragraph": "[BOS] In contrast to these approaches, our method directly incorporates these constraints into the structure of the model.\n[BOS] This design leads to a significant reduction in the computational complexity of training and inference.\n\n", "discourse_tags": ["Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "1704410_2", "paragraph": "[BOS] Other approaches encode sparsity as a soft constraint.\n[BOS] For instance, by altering the emission distribution parameters, Johnson (2007) encourages the model to put most of the probability mass on few tags.\n[BOS] This design does not guarantee \"structural zeros,\" but biases towards sparsity.\n[BOS] A more forceful approach for encoding sparsity is posterior regularization, which constrains the posterior to have a small number of expected tag assignments (Graa et al., 2009 ).\n[BOS] This approach makes the training objective more complex by adding linear constraints proportional to the number of word types, which is rather prohibitive.\n[BOS] A more rigid mechanism for modeling sparsity is proposed by Ravi and Knight (2009) , who minimize the size of tagging grammar as measured by the number of transition types.\n[BOS] The use of ILP in learning the desired grammar significantly increases the computational complexity of this method.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Narrative_cite", "Transition", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 12, "token_end": 56, "char_start": 67, "char_end": 301, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Johnson (2007)": "1512774"}, "Reference": {}}}, {"token_start": 66, "token_end": 91, "char_start": 358, "char_end": 484, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Gra\u00e7a et al., 2009": "1186682"}}}, {"token_start": 121, "token_end": 176, "char_start": 656, "char_end": 950, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ravi and Knight (2009)": "2049079"}, "Reference": {}}}]}
{"id": "1704410_1", "paragraph": "[BOS] The extent to which this constraint is enforced varies greatly across existing methods.\n[BOS] On one end of the spectrum are clustering approaches that assign a single POS tag to each word type (Schutze, 1995; Lamar et al., 2010) .\n[BOS] These clusters are computed using an SVD variant without relying on transitional structure.\n[BOS] While our method also enforces a singe tag per word constraint, it leverages the transition distribution encoded in an HMM, thereby benefiting from a richer representation of context.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 29, "token_end": 51, "char_start": 167, "char_end": 235, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Schutze, 1995;": "140101838", "Lamar et al., 2010)": "8465818"}}}]}
{"id": "1704410_0", "paragraph": "[BOS] Recent work has made significant progress on unsupervised POS tagging (Mrialdo, 1994; Smith and Eisner, 2005; Haghighi and Klein, 2006; Johnson, 2007; Goldwater and Griffiths, 2007; Gao and Johnson, 2008; Ravi and Knight, 2009 ).\n[BOS] Our work is closely related to recent approaches that incorporate the sparsity constraint into the POS induction process.\n[BOS] This line of work has been motivated by empirical findings that the standard EM-learned unsupervised HMM does not exhibit sufficient word tag sparsity.\n\n", "discourse_tags": ["Narrative_cite", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 9, "token_end": 59, "char_start": 51, "char_end": 232, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(M\u00e9rialdo, 1994;": "2727455", "Smith and Eisner, 2005;": "259144", "Haghighi and Klein, 2006;": "8709299", "Johnson, 2007;": "1512774", "Goldwater and Griffiths, 2007;": "11020320", "Gao and Johnson, 2008;": "17664832", "Ravi and Knight, 2009": "2049079"}}}]}
{"id": "16636082_1", "paragraph": "[BOS] Non-local features have been successfully used for constituent parsing (Charniak and Johnson, 2005; Collins and Koo, 2005; Huang, 2008) .\n[BOS] However, almost all of the previous work use nonlocal features at the parse reranking stage.\n[BOS] The reason is that the single-stage chart-based parser cannot use non-local structural features.\n[BOS] In contrast, the transition-based parser can use arbitrarily complex structural features.\n[BOS] Therefore, we can concisely utilize non-local features in a single-stage parsing system.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 11, "token_end": 33, "char_start": 57, "char_end": 141, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Charniak and Johnson, 2005;": "11599080", "Collins and Koo, 2005;": "405878", "Huang, 2008)": "1131864"}}}]}
{"id": "16636082_0", "paragraph": "[BOS] Joint POS tagging with parsing is not a new idea.\n[BOS] In PCFG-based parsing (Collins, 1999; Charniak, 2000; Petrov et al., 2006) , POS tagging is considered as a natural step of parsing by employing lexical rules.\n[BOS] For transition-based parsing, Hatori et al. (2011) proposed to integrate POS tagging with dependency parsing.\n[BOS] Our joint approach can be seen as an adaption of Hatori et al. (2011) 's approach for constituent parsing.\n[BOS] proposed a transition-based constituent parser to process an input sentence from the character level.\n[BOS] However, manual annotation of the word-internal structures need to be added to the original Treebank in order to train such a parser.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Single_summ", "Reflection", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 15, "token_end": 39, "char_start": 65, "char_end": 136, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Collins, 1999;": null, "Charniak, 2000;": null, "Petrov et al., 2006)": "6684426"}}}, {"token_start": 56, "token_end": 79, "char_start": 228, "char_end": 337, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hatori et al. (2011)": "5404235"}, "Reference": {}}}, {"token_start": 88, "token_end": 106, "char_start": 381, "char_end": 450, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hatori et al. (2011)": "5404235"}, "Reference": {}}}]}
{"id": "198957268_1", "paragraph": "[BOS] To our knowledge, the only work that specifically uses the MTNT dataset attempts to improve the system robustness by emulating the noise in the clean data (Vaibhav et al., 2019) .\n[BOS] They introduce two techniques for noise induction, one employing hand-crafted rules, and one based on back-translation.\n[BOS] The techniques offer a similar translation quality gains as fine-tuning on MTNT data.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 13, "token_end": 88, "char_start": 65, "char_end": 403, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Vaibhav et al., 2019)": "67856759"}, "Reference": {}}}]}
{"id": "198957268_0", "paragraph": "[BOS] There have been several attempts to increase the robustness of MT systems in recent years.\n[BOS] Cheng et al. (2018) employ an adversarial training scheme in a multi-task learning setup in order to increase the system robustness.\n[BOS] For each training example, its noisy counterpart is randomly generated.\n[BOS] The network is trained to yield such input representations such that it is not possible to train a discriminator that decides (based on the input representation) which input is the noisy one.\n[BOS] This method improves both the robustness and the translation quality on the clean data.\n[BOS] attempt to make the translation more robust towards noise from homophones.\n[BOS] This type of noise is common in languages with non-phonetic writing systems and concerns words or phrases which are pronounced in the same way, but spelled differently.\n[BOS] The authors of the paper train the word embeddings to capture the phonetic information which eventually leads not only to bigger robustness but also to improved translation quality in general.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Transition", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 19, "token_end": 113, "char_start": 103, "char_end": 605, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Cheng et al. (2018)": "21715690"}, "Reference": {}}}]}
{"id": "53623352_3", "paragraph": "[BOS] Rather than rely only on the textual content of social media posts in identifying hate speech, (Qian et al., 2018; Founta et al., 2018) investigated the use of metadata about the users or the posts.\n[BOS] However, the metadata may not be readily available from the social media platforms.\n[BOS] In addition, this approach may breakdown where a social media platform allows anonymous posting.\n\n", "discourse_tags": ["Multi_summ", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 2, "token_end": 50, "char_start": 6, "char_end": 204, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Qian et al., 2018;": "4718302", "Founta et al., 2018)": "3397484"}, "Reference": {}}}]}
{"id": "53623352_2", "paragraph": "[BOS] The identification of aggression in social media is closely related to existing studies in hate speech, abuse, and cyberbullying detection.\n[BOS] Methods used to tackle these tasks as supervised classification broadly falls into two.\n[BOS] One approach is based on manual feature engineering.\n[BOS] With the feature engineering approach, extracted features serve as input to classic machine learning algorithms such as naive bayes, logistic regression, support vector machines, and random forest (Schmidt and Wiegand, 2017; Malmasi and Zampieri, 2017) .\n[BOS] The other approach is based on deep neural networks that automatically learn features from input data.\n[BOS] (Gambck and Sikdar, 2017) employed convolutional neural networks to classify hate speech and (Zhang et al., 2018 ) used a combination of convolutional neural network and gated recurrent unit (GRU) for the same task.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Narrative_cite", "Transition", "Multi_summ"], "span_citation_mapping": [{"token_start": 56, "token_end": 104, "char_start": 314, "char_end": 557, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Schmidt and Wiegand, 2017;": "9626793", "Malmasi and Zampieri, 2017)": "19182892"}}}, {"token_start": 124, "token_end": 144, "char_start": 675, "char_end": 763, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Gamb\u00e4ck and Sikdar, 2017)": "20335790"}, "Reference": {}}}, {"token_start": 145, "token_end": 173, "char_start": 768, "char_end": 890, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Zhang et al., 2018": "46939253"}, "Reference": {}}}]}
{"id": "53623352_1", "paragraph": "[BOS] Acknowledging the cost of annotating data for hateful comments in social media, (Gao et al., 2017) proposed a system that leverages the availability of unlabeled data.\n[BOS] Their result shows improvement over systems that rely only on manually annotated data.\n[BOS] This approach is most related to our work.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 54, "char_start": 6, "char_end": 266, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Gao et al., 2017)": "25407580"}, "Reference": {}}}]}
{"id": "53623352_0", "paragraph": "[BOS] The task of aggression detection in social media can be considered as a document classification task.\n[BOS] This task can also be sub-divided into binary classification or multi-class classification.\n[BOS] In the context of detection of aggressiveness, the binary classification would imply the presence or absence of some anti-social phenomena such as abuse (Nobata et al., 2016) in a given example (abusive or not abusive).\n[BOS] Whereas in the multi-class scenario, specific types of anti-social behaviour are of interest (Waseem et al., 2017) such as racism, sexism, hate speech, and bullying.\n[BOS] It has been observed that contents which contain anti-social phenomena are rare in a collection of social media posts.\n[BOS] This usually leads to imbalanced datasets where posts which lack the phenomena of interest are overwhelming.\n[BOS] This problem is even more pronounced in the multi-class scenario which leads to difficulty in learning discriminative features by classifiers.\n[BOS] In (Malmasi and Zampieri, 2018) , it was concluded that subtle distinction between types of anti-social behaviour: profanity and hate speech is a difficult task for machine learning classifiers.\n[BOS] By extension, it will be difficult to differentiate between overt and covert aggression.\n[BOS] Also, submits that posts that does not contain explicit aggressive words are likely to be difficult to identify.\n[BOS] This would be likely applicable to the covertly aggressive class in this study.\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite", "Narrative_cite", "Transition", "Transition", "Transition", "Single_summ", "Transition", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 58, "token_end": 75, "char_start": 329, "char_end": 386, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Nobata et al., 2016)": "11546523"}}}, {"token_start": 92, "token_end": 133, "char_start": 453, "char_end": 603, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Waseem et al., 2017)": "8821211"}}}, {"token_start": 200, "token_end": 243, "char_start": 999, "char_end": 1193, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Malmasi and Zampieri, 2018)": "3936688"}, "Reference": {}}}]}
{"id": "67865542_3", "paragraph": "[BOS] The benefit of the proposed MTL approach is not surprising, resembling from existing works, e.g., jointly training translation models from/to multiple languages (Dong et al., 2015) ; jointly training the encoders (Zoph and Knight, 2016) or both encoders and decoders (Johnson et al., 2017) .\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 24, "token_end": 41, "char_start": 104, "char_end": 186, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dong et al., 2015)": "3666937"}}}, {"token_start": 42, "token_end": 56, "char_start": 189, "char_end": 242, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zoph and Knight, 2016)": "8677917"}}}, {"token_start": 57, "token_end": 71, "char_start": 246, "char_end": 295, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Johnson et al., 2017)": "6053988"}}}]}
{"id": "67865542_2", "paragraph": "[BOS] Also, Michel and Neubig (2018) proposed to personalise neural MT systems by taking the variance that each speaker speaks/writes on his own into consideration.\n[BOS] They proposed the adaptation process which takes place in the \"output\" layer, similar to our output layer incorporation method.\n\n", "discourse_tags": ["Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 4, "token_end": 59, "char_start": 12, "char_end": 298, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Michel and Neubig (2018)": "19247366"}, "Reference": {}}}]}
{"id": "67865542_1", "paragraph": "[BOS] Our methods for incorporating side information as suffix, prefix for either source or target sequences have been adapted from (Sennrich et al., 2016a; Johnson et al., 2017) .\n[BOS] Also working on the same patent dataset, Jehl and Riezler (2018) proposed to incorporate document meta information as special tokens, similar to our source prefix/suffix method, or by concatenating the tag with each source word.\n[BOS] They report an improvements, consistent with our findings, although the changes they observe are larger, of about 1 BLEU point, albeit from a lower baseline.\n\n", "discourse_tags": ["Reflection", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 5, "token_end": 40, "char_start": 22, "char_end": 178, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sennrich et al., 2016a;": "845121", "Johnson et al., 2017)": "6053988"}}}, {"token_start": 42, "token_end": 121, "char_start": 187, "char_end": 579, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Jehl and Riezler (2018)": "51878127"}, "Reference": {}}}]}
{"id": "67865542_0", "paragraph": "[BOS] Our work is mainly inspired from Hoang et al. (2016a) who proposed the use of side information for boosting the performance of recurrent neural network language models.\n[BOS] We further apply this idea for a downstream task in neural machine translation.\n[BOS] We've adapted different methods in the literature for this specific problem and evaluated using different datasets with different kinds of side information.\n\n", "discourse_tags": ["Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 8, "token_end": 35, "char_start": 39, "char_end": 174, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hoang et al. (2016a)": "12672327"}, "Reference": {}}}]}
{"id": "53295957_4", "paragraph": "[BOS] Neural Memory Networks.\n[BOS] Many memory network models have been proposed to improve memorization capability of neural networks Na et al., 2017; Yoo et al., 2019) .\n[BOS] Weston et al. (2014) propose one of early memory networks for language question answering (QA); since then, many memory networks have been proposed for QA tasks (Sukhbaatar et al., 2015; Kumar et al., 2016; Miller et al., 2016) .\n[BOS] Park et al. (2017) propose a convolutional read memory network for personalized image cap- tioning.\n[BOS] One of the closest works to ours may be Singh et al. (2017) , which use a memory network for text summarization.\n[BOS] However, they only deal with extractive summarization by storing embeddings of individual sentences into memory.\n[BOS] Compared to previous memory networks, our MMN has four novel features: (i) building a multi-level memory network that better abstracts multi-level representation of a long document, (ii) employing a dilated convolutional memory write mechanism to correlate adjacent memory cells, (iii) proposing normalized gated tanh units to avoid covariate shift within the network, and (iv) generating an output sequence without RNNs.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Multi_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 9, "token_end": 38, "char_start": 48, "char_end": 170, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Na et al., 2017;": "3957104", "Yoo et al., 2019)": "195750677"}}}, {"token_start": 40, "token_end": 61, "char_start": 179, "char_end": 273, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 66, "token_end": 99, "char_start": 292, "char_end": 406, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sukhbaatar et al., 2015;": null, "Kumar et al., 2016;": "2319779", "Miller et al., 2016)": "2711679"}}}, {"token_start": 101, "token_end": 122, "char_start": 415, "char_end": 514, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Park et al. (2017)": "17618504"}, "Reference": {}}}, {"token_start": 132, "token_end": 170, "char_start": 561, "char_end": 752, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Singh et al. (2017)": "24104873"}, "Reference": {}}}]}
{"id": "53295957_3", "paragraph": "[BOS] Rotten Tomatoes and Idebate dataset (Wang and Ling, 2016) use online text as source, but they are relatively small in scale: 3.7K posts of RottenTomatoes compared to 80K posts of TIFU-short as shown in Table 1 .\n[BOS] Moreover, Rotten Tomatoes use multiple movie reviews written by different users as single source text, and one-sentence consensus made by another professional editor as summary.\n[BOS] Thus, each pair of this dataset could be less coherent than that of our TIFU, which is written by the same user.\n[BOS] The Idebate dataset is collected from short arguments of debates on controversial topics, and thus the text is rather formal.\n[BOS] On the other hand, our dataset contains the posts of interesting stories happened in daily life, and thus the text is more unstructured and informal.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 144, "char_start": 6, "char_end": 652, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Wang and Ling, 2016)": "5968971"}, "Reference": {}}}]}
{"id": "53295957_2", "paragraph": "[BOS] Summarization Datasets.\n[BOS] Most existing summarization datasets use formal documents as source text.\n[BOS] News articles are exploited the most, including in DUC (Over et al., 2007) , Gigaword (Napoles et al., 2012) , CNN/DailyMail (Nallapati et al., 2016; Hermann et al., 2015) , Newsroom (Grusky et al., 2018) and XSum (Narayan et al., 2018a ) datasets.\n[BOS] Cohan et al. (2018 and PubMed.\n[BOS] Hu et al. (2015) propose the LC-STS dataset as a collection of Chinese microblog's short text each paired with a summary.\n[BOS] However, it selects only formal text posted by verified organizations such as news agencies or government institutions.\n[BOS] Compared to previous summarization datasets, our dataset is novel in that it consists of posts from the online forum Reddit.\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite", "Narrative_cite", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 20, "token_end": 99, "char_start": 116, "char_end": 352, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Over et al., 2007)": null, "(Napoles et al., 2012)": null, "(Narayan et al., 2018a": "52096531"}}}, {"token_start": 103, "token_end": 110, "char_start": 371, "char_end": 389, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 114, "token_end": 164, "char_start": 408, "char_end": 655, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hu et al. (2015)": "11597846"}, "Reference": {}}}]}
{"id": "53295957_1", "paragraph": "[BOS] Compared to existing neural methods of abstractive summarization, our approach is novel to replace an RNN-based encoder with explicit multi-level convolutional memory.\n[BOS] While RNNbased encoders always consider the whole sequence to represent each hidden state, our multilevel memory network exploits convolutions to control the extent of representation in multiple levels of sentences, paragraphs, and the whole text.\n\n", "discourse_tags": ["Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "53295957_0", "paragraph": "[BOS] Our work can be uniquely positioned in the context of the following three topics.\n[BOS] Neural Abstractive Summarization.\n[BOS] Many deep neural network models have been proposed for abstractive summarization.\n[BOS] One of the most dominant architectures is to employ RNN-based seq2seq models with attention mechanism such as (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; Cohan et al., 2018; Hsu et al., 2018; Gehrmann et al., 2018) .\n[BOS] In addition, recent advances in deep network research have been promptly adopted for improving abstractive summarization.\n[BOS] Some notable examples include the use of variational autoencoders (VAEs) (Miao and Blunsom, 2016; Li et al., 2017) , graph-based attention (Tan et al., 2017) , pointer-generator models (See et al., 2017) , self-attention networks (Liu et al., 2018) , reinforcement learning (Paulus et al., 2018; Pasunuru and Bansal, 2018) , contextual agent attention (Celikyilmaz et al., 2018) and integration with extractive models (Hsu et al., 2018; Gehrmann et al., 2018) .\n\n", "discourse_tags": ["Reflection", "Transition", "Transition", "Narrative_cite", "Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 58, "token_end": 112, "char_start": 304, "char_end": 457, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rush et al., 2015;": "1918428", "Nallapati et al., 2016;": "8928715", "Hsu et al., 2018;": "21723747"}}}, {"token_start": 143, "token_end": 168, "char_start": 635, "char_end": 708, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Miao and Blunsom, 2016;": "10480989", "Li et al., 2017)": "1508909"}}}, {"token_start": 169, "token_end": 181, "char_start": 711, "char_end": 751, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Tan et al., 2017)": "26698484"}}}, {"token_start": 182, "token_end": 194, "char_start": 754, "char_end": 797, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(See et al., 2017)": null}}}, {"token_start": 195, "token_end": 207, "char_start": 800, "char_end": 842, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Liu et al., 2018)": "3608234"}}}, {"token_start": 208, "token_end": 229, "char_start": 845, "char_end": 916, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Paulus et al., 2018;": "21850704", "Pasunuru and Bansal, 2018)": "4940548"}}}, {"token_start": 230, "token_end": 245, "char_start": 919, "char_end": 972, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 246, "token_end": 268, "char_start": 977, "char_end": 1053, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hsu et al., 2018;": "21723747"}}}]}
{"id": "198962066_1", "paragraph": "[BOS] In addition to that, a large body of work has focused on addressing the hubness problem that arises when directly inducing bilingual dictionaries from cross-lingual embeddings, either through the retrieval method Smith et al., 2017; or the mapping itself Shigeto et al., 2015; .\n[BOS] While all these previous methods directly induce bilingual dictionaries from cross-lingually mapped embeddings, our proposed method combines them with unsupervised machine translation techniques, outperforming them all by a substantial margin.\n\n", "discourse_tags": ["Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 38, "token_end": 46, "char_start": 202, "char_end": 237, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 48, "token_end": 59, "char_start": 242, "char_end": 281, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "198962066_0", "paragraph": "[BOS] While BLI has been previously tackled using count-based vector space models (Vuli and Moens, 2013) and statistical decipherment (Ravi and Knight, 2011; Dou and Knight, 2012) , these methods have recently been superseded by crosslingual embedding mappings, which work by aligning independently trained word embeddings in different languages.\n[BOS] For that purpose, early methods required a training dictionary, which was used to learn a linear transformation that mapped these embeddings into a shared crosslingual space (Mikolov et al., 2013; Artetxe et al., 2018a) .\n[BOS] The resulting cross-lingual embeddings are then used to induce the translations of words that were missing in the training dictionary by taking their nearest neighbor in the target language.\n[BOS] The amount of required supervision was later reduced through self-learning methods (Artetxe et al., 2017) , and then completely eliminated through adversarial training (Zhang et al., 2017a; or more robust iterative approaches combined with initialization heuristics (Artetxe et al., 2018b; Hoshen and Wolf, 2018) .\n[BOS] At the same time, several recent methods have formulated embedding mappings as an optimal transport problem (Zhang et al., 2017b; .\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Transition", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 11, "token_end": 26, "char_start": 50, "char_end": 104, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vuli\u0107 and Moens, 2013)": "10068440"}}}, {"token_start": 27, "token_end": 48, "char_start": 109, "char_end": 179, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ravi and Knight, 2011;": "6060648", "Dou and Knight, 2012)": "5727182"}}}, {"token_start": 86, "token_end": 128, "char_start": 396, "char_end": 572, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mikolov et al., 2013;": "1966640", "Artetxe et al., 2018a)": "4334731"}}}, {"token_start": 168, "token_end": 187, "char_start": 801, "char_end": 883, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Artetxe et al., 2017)": "13335042"}}}, {"token_start": 190, "token_end": 204, "char_start": 895, "char_end": 966, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 207, "token_end": 232, "char_start": 976, "char_end": 1090, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Artetxe et al., 2018b;": "21728524", "Hoshen and Wolf, 2018)": "51978342"}}}, {"token_start": 243, "token_end": 259, "char_start": 1145, "char_end": 1227, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "53234911_1", "paragraph": "[BOS] Mohammad (2018) have proposed a VAD lexicon for emotion detection systems.\n[BOS] We use VAD features together with ELMo (Peters et al., 2018) .\n[BOS] Recently, the ELMo model has been shown to boost performance on a number of Natural Language Processing (NLP) tasks.\n[BOS] To the best of our knowledge, we are the first to make use of VAD features in a deep learning setting for emotion prediction.\n\n", "discourse_tags": ["Single_summ", "Reflection", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 18, "char_start": 6, "char_end": 80, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 26, "token_end": 36, "char_start": 121, "char_end": 147, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Peters et al., 2018)": "3626819"}}}]}
{"id": "53234911_0", "paragraph": "[BOS] Emotion prediction is related to the task of sentiment analysis.\n[BOS] The best performance in sentiment analysis has been attained using supervised techniques as outlined in a survey by Medhat et al. (2014) .\n[BOS] Recent breakthroughs in deep learning have shown strong results in sentence classification (Joulin et al., 2016) , language modeling (Dauphin et al., 2016) and sentence embedding (Peters et al., 2018) .\n[BOS] Our emotion prediction model is also based on deep learning techniques.\n[BOS] Recently, fastText (Joulin et al., 2016) has been proposed for generating word representations which have shown state-of-the-art performance on a number of text related tasks.\n[BOS] Our model makes use of a fastText model for emotion classification.\n[BOS] Chen et al. (2018) introduce an emotion corpus based on conversations taken from Friends TV scripts and propose a similar emotion classification model using a CNN-BiLSTM.\n[BOS] Our model is similar to the model proposed by (Chen et al., 2018 ), but we use a pre-trained ELMo instead of a BiLSTM.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite", "Reflection", "Single_summ", "Reflection", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 24, "token_end": 40, "char_start": 144, "char_end": 213, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Medhat et al. (2014)": "16768404"}}}, {"token_start": 53, "token_end": 64, "char_start": 289, "char_end": 334, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Joulin et al., 2016)": "1210515"}}}, {"token_start": 65, "token_end": 77, "char_start": 337, "char_end": 377, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dauphin et al., 2016)": "16119010"}}}, {"token_start": 78, "token_end": 88, "char_start": 382, "char_end": 422, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Peters et al., 2018)": "3626819"}}}, {"token_start": 105, "token_end": 142, "char_start": 519, "char_end": 684, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Joulin et al., 2016)": "1210515"}, "Reference": {}}}, {"token_start": 157, "token_end": 190, "char_start": 765, "char_end": 935, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Chen et al. (2018)": "3526062"}, "Reference": {}}}, {"token_start": 200, "token_end": 224, "char_start": 988, "char_end": 1060, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chen et al., 2018": "3526062"}}}]}
{"id": "53588846_1", "paragraph": "[BOS] The performance of the rule-based models (ari et al., 2012; B r et al., 2012; Banea et al., 2012 ) mostly rely on word pairings and knowledge derived from large corpora, e.g., Wikipedia.\n[BOS] Regardless of details, each word in sent1 is paired with the word in sent2 that is most similar according to some similarity measure.\n[BOS] Then, all similarities are added and normalized by the length of sent1 to obtain the similarity score from sent1 to sent2.\n[BOS] The process is repeated to obtain the similarity score from sent2 to sent1, and both scores are then averaged to determine the overall textual similarity.\n[BOS] Several word to word similarity measures are often combined with other shallow features (e.g. n-gram overlap, syntactic dependencies) to obtain the final similarity score.\n[BOS] Shao (2017) proposed a simple convolutional neural network (CNN) models for STS.\n[BOS] He used a CNN as the sentence embedding component to encode the original sentences into sentence-level vectors and generated a semantic difference vector by concatenating the element-wise absolute difference and the element-wise multiplication of the corresponding sentence vectors.\n[BOS] He then passed the semantic difference vector into a fully connected neural network to perform regression to generate the similarity score on a continuous inclusive scale from 0 to 5.\n[BOS] His model ranked 3rd on the primary track of SemEval 2017.\n[BOS] Prijatelj et al. (2017) wrote a survey on neural networks for semantic textual similarity.\n[BOS] The framework of their model is similar to Shao's, but they explored various neural network architectures, from simple to complex, and reported the results of applying the combination of these neural network models within this framework.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Transition", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 6, "token_end": 54, "char_start": 29, "char_end": 192, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(\u0160ari\u0107 et al., 2012;": "12233462", "B\u073d\u1237 r et al., 2012;": "6964767", "Banea et al., 2012": "18741766"}}}, {"token_start": 174, "token_end": 287, "char_start": 807, "char_end": 1431, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Shao (2017)": "30958369"}, "Reference": {}}}, {"token_start": 288, "token_end": 353, "char_start": 1438, "char_end": 1772, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Prijatelj et al. (2017)": null}, "Reference": {}}}]}
{"id": "53588846_0", "paragraph": "[BOS] Determining textual similarity is relatively new as a stand-alone task since SemEval-2012, but it is often a component of NLP applications such as information retrieval, paraphrase recognition, grading answers to questions and many other tasks.\n[BOS] In this section, we only list the works that are involved in our evaluation systems: the top performers in SemEval 2012 and recent neural network-based approaches in SemEval 2017.\n\n", "discourse_tags": ["Transition", "Reflection"], "span_citation_mapping": []}
{"id": "84846285_2", "paragraph": "[BOS] Our sample re-weighting scheme bears some resemblance to previous MTL techniques that assign weights to tasks (Kendall et al., 2018) .\n[BOS] However, our method gives a more granular score for each sample and provides better performance for multitask learning MRC.\n\n", "discourse_tags": ["Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 13, "token_end": 31, "char_start": 63, "char_end": 138, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kendall et al., 2018)": "4800342"}}}]}
{"id": "84846285_1", "paragraph": "[BOS] Multi-task learning (Caruana, 1997) has been widely used in machine learning to improve generalization using data from multiple tasks.\n[BOS] For natural language processing, MTL has been successfully applied to low-level parsing tasks (Collobert et al., 2011) , sequence-to-sequence learning (Luong et al., 2015) , and web search (Liu et al., 2015) .\n[BOS] More recently, (McCann et al., 2018) proposes to cast all tasks from parsing to translation as a QA problem and use a single network to solve all of them.\n[BOS] However, their results show that multi-task learning hurts the performance of most tasks when tackling them together.\n[BOS] Differently, we focus on applying MTL to the MRC task and show significant improvement over single-task baselines.\n\n", "discourse_tags": ["Single_summ", "Narrative_cite", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 29, "char_start": 6, "char_end": 140, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Caruana, 1997)": null}, "Reference": {}}}, {"token_start": 42, "token_end": 56, "char_start": 217, "char_end": 265, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Collobert et al., 2011)": "351666"}}}, {"token_start": 57, "token_end": 72, "char_start": 268, "char_end": 318, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Luong et al., 2015)": "6954272"}}}, {"token_start": 74, "token_end": 84, "char_start": 325, "char_end": 354, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Liu et al., 2015)": "11754890"}}}, {"token_start": 89, "token_end": 146, "char_start": 378, "char_end": 641, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(McCann et al., 2018)": "49393754"}, "Reference": {}}}]}
{"id": "84846285_0", "paragraph": "[BOS] Studies in machine reading comprehension mostly focus on architecture design of neural networks, such as bidirectional attention (Seo et al., 2016) , dynamic reasoning (Xu et al., 2017) , and parallelization (Yu et al., 2018) .\n[BOS] Some recent work has explored transfer learning that leverages outdomain data to learn MRC models when no training data is available for the target domain (Golub et al., 2017) .\n[BOS] In this work, we explore multi-task learning to make use of the data from other domains, while we still have access to target domain training data.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 10, "token_end": 29, "char_start": 63, "char_end": 153, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Seo et al., 2016)": "8535316"}}}, {"token_start": 30, "token_end": 40, "char_start": 156, "char_end": 191, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Xu et al., 2017)": "3596708"}}}, {"token_start": 42, "token_end": 52, "char_start": 198, "char_end": 231, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yu et al., 2018)": "4842909"}}}]}
{"id": "199569853_2", "paragraph": "[BOS] Mainland China, Singapore and Taiwan.\n[BOS] (Xu et al., 2016) deals with 6 varieties of Mandarin: Maninland, Hong Kong, Taiwan, Macao, Malaysia and Singapore.\n[BOS] They discover that character bigram and word segmentation based feature work better than traditional character unigram, and some features such as character form, PMI-based and word alignment-based features can help improve performance.\n[BOS] However, a thorough comparison of different algorithms and architectures has yet to be conducted.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 11, "token_end": 86, "char_start": 50, "char_end": 406, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Xu et al., 2016)": "17707827"}, "Reference": {}}}]}
{"id": "199569853_1", "paragraph": "[BOS] Methods to discriminate between varieties of Mandarin Chinese haven't been well studied.\n[BOS] (Huang and Lee, 2008 ) uses a top-bag-of-word similarity based contrastive approach to reflect distance among three varieties of Mandarin:\n\n", "discourse_tags": ["Transition", "Single_summ"], "span_citation_mapping": [{"token_start": 19, "token_end": 49, "char_start": 101, "char_end": 238, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Huang and Lee, 2008": "7181443"}, "Reference": {}}}]}
{"id": "199569853_0", "paragraph": "[BOS] A number of works have devoted to differentiate between language varieties or related languages, especially since the series of VarDial evaluation campaigns (Zampieri et al., 2017 (Zampieri et al., , 2018 (Zampieri et al., , 2019 .\n[BOS] (Lui and Cook, 2013 ) studies on English dialect identification and presents serveral classification approaches to classify Australia, British and Caniadian English.\n[BOS] (Zampieri and Gebre, 2012 ) utilizes a character n-gram and a word ngram language model for automatic classificaton of two written varieties of Portuguese: European and Brazilian.\n[BOS] (Ciobanu and Dinu, 2016) conducts an intial study on the dialects of Romanian and proposes using the orthographic and phonetic features of the words to build a dialect classifier.\n[BOS] (Clematide and Makarov, 2017 ) uses a majority-vote ensemble of the Navie Bayes, CRF and SVM systems for Swiss German dialects identification.\n[BOS] (Kreutz and Daelemans, 2018) uses two SVM classifiers: one trained on word n-grams fewtures and one trained on Pos n-grams to determine whether a document is in Flemish Dutch or Netherlandic Dutch.\n[BOS] (ltekin et al., 2018) uses a unified SVM model based on character and word n-grams features with careful hyperparameter tuning for 4 language/dialect identification tasks.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 22, "token_end": 56, "char_start": 134, "char_end": 235, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zampieri et al., 2017": "4654482", "(Zampieri et al., , 2018": "53368499", "(Zampieri et al., , 2019": "148564946"}}}, {"token_start": 58, "token_end": 90, "char_start": 244, "char_end": 409, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Lui and Cook, 2013": "16753286"}, "Reference": {}}}, {"token_start": 91, "token_end": 130, "char_start": 416, "char_end": 595, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Zampieri and Gebre, 2012": null}, "Reference": {}}}, {"token_start": 131, "token_end": 174, "char_start": 602, "char_end": 781, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Ciobanu and Dinu, 2016)": "33353746"}, "Reference": {}}}, {"token_start": 175, "token_end": 209, "char_start": 788, "char_end": 930, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Clematide and Makarov, 2017": "3911279"}, "Reference": {}}}, {"token_start": 210, "token_end": 260, "char_start": 937, "char_end": 1134, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Kreutz and Daelemans, 2018)": "53574229"}, "Reference": {}}}, {"token_start": 261, "token_end": 298, "char_start": 1141, "char_end": 1312, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "198163709_2", "paragraph": "[BOS] A few number of works on conversational language explore SRL task in the context of Spoken Language Understanding.\n[BOS] Coppola et al. (2009) exploits machine learning of frame semantics on spoken dialogs.\n[BOS] They design and evaluate automatic FrameNet-based parsers both for English written texts and for Italian dialog utterances.\n[BOS] Dinarelli et al. (2009) describe and analyze the annotation process in order to train semantic statistical parsers.\n[BOS] Spoken conversations from both a human-machine and a human-human spoken dialog corpus are semantically annotated with predicate argument structure.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 24, "token_end": 67, "char_start": 127, "char_end": 342, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Coppola et al. (2009)": "6027270"}, "Reference": {}}}, {"token_start": 68, "token_end": 92, "char_start": 349, "char_end": 464, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Dinarelli et al. (2009)": "10859435"}, "Reference": {}}}]}
{"id": "198163709_1", "paragraph": "[BOS] In order to minimize the number of hand-crafted features, Collobert et al. (2011) utilized deep learning for solving NLP tasks including Part-of-Speech (POS) Tagging, Chunking, Named Entity Recognition (NER), and Semantic Role Labeling (SRL).\n[BOS] The research aims to prevent using any task-specific feature in order to achieve state-of-the-art performance.\n[BOS] The word embedding is used as the main feature across tasks, combined with Convolutional Neural Networks (CNN) architecture to train the model.\n[BOS] To achieve competitive result for the SRL, the features engineered from the parser are still needed.\n[BOS] Zhou and Xu (2015) and He et al. (2017) view SRL as a sequence labeling problem in which the arguments are labeled sequentially instead of independently.\n[BOS] They proposed an end-to-end learning of SRL using Deep Bi-Directional Long Short-Term Memories (DB-LSTM), with word embedding as the main feature.\n[BOS] Their analysis suggests that the DB-LSTM model implicitly extracts the syntactic information over the sentences and thus, syntactic parser is not needed.\n[BOS] The research result outperforms the previous state-of-the-art traditional SLR systems.\n[BOS] The research also shows that the performance of the sequence labeling approach using DB-LSTM is better than the classification approach using CNN, since the DB-LSTM architecture can extract syntactic information implicitly.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Transition", "Multi_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 5, "token_end": 114, "char_start": 18, "char_end": 515, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Collobert et al. (2011)": "351666"}, "Reference": {}}}, {"token_start": 135, "token_end": 293, "char_start": 629, "char_end": 1418, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhou and Xu (2015)": "12688069", "He et al. (2017)": "33626727"}, "Reference": {}}}]}
{"id": "198163709_0", "paragraph": "[BOS] SRL can be seen as either a classification or sequence labeling problem.\n[BOS] The earlier research on SRL was conducted with the classification approach, meaning that each argument is being predicted independently from the others.\n[BOS] Those research focused on how to extract meaningful features out of syntactic parsers (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Pradhan et al., 2005) , such as the path to predicate and constituent type.\n[BOS] This syntactic information plays a pivotal role in solving SRL problem (Punyakanok et al., 2008) as it addresses SLR's long distance dependency.\n[BOS] Thus, traditional SRL system heavily depends on the quality of the parsers.\n[BOS] Pradhan et al. (2005) analyzes that most errors of the SRL system were caused by the parser's error.\n[BOS] In addition, those parsers are costly to build, since it needs linguistic experts to annotate the data.\n[BOS] If we want to create an SRL system on another language, one should build a new parser all over again for it.\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite", "Narrative_cite", "Transition", "Single_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 49, "token_end": 85, "char_start": 277, "char_end": 404, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Gildea and Jurafsky, 2002;": "62182406", "Gildea and Palmer, 2002;": null, "Pradhan et al., 2005)": "2440012"}}}, {"token_start": 98, "token_end": 131, "char_start": 470, "char_end": 609, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Punyakanok et al., 2008)": "11162815"}}}, {"token_start": 149, "token_end": 199, "char_start": 698, "char_end": 908, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Pradhan et al. (2005)": "2440012"}, "Reference": {}}}]}
{"id": "88517356_2", "paragraph": "[BOS] Finally, Byte-Pair Encoding (BPE) (Sennrich et al., 2016) has become a standard preprocessing step in NMT pipelines and provides an easy way to generate sequences with a mixture of full words and word fragments.\n[BOS] Note that BPE splits are agnostic to any morphological pattern present in the language, for example the token politely in our dataset is split into pol+itely, instead of the linguistically plausible split polite+ly.\n[BOS] 3 Our approach can be applied to word-level sequences and sequences at any BPE merge hyperparameter greater than 0.\n[BOS] Increasing the hyperparameter results in more words and longer subwords that can exhibit morphological patterns.\n[BOS] Our goal is to exploit these morphological patterns and enrich the word (or subword) representations with character-awareness.\n\n", "discourse_tags": ["Single_summ", "Transition", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 4, "token_end": 51, "char_start": 15, "char_end": 217, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Sennrich et al., 2016)": "1114678"}, "Reference": {}}}]}
{"id": "88517356_1", "paragraph": "[BOS] There is additionally a line of work on purely character-level NMT, which generates words one character at a time (Ling et al., 2015; Chung et al., 2016; Passban et al., 2018) .\n[BOS] While initial results here were not strong, Cherry et al. (2018) revisit this with deeper architectures and sweeping dropout parameters and find that they outperform BPE across settings of the merge hyperparameter.\n[BOS] They examine different data sizes and observe improvements in the smaller data size settingshowever, the smallest size is about 2 million sentence pairs.\n[BOS] In contrast, we look at a smaller order of magnitude data size and present an alternate approach which doesn't require substantial tuning of parameters across different languages.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 11, "token_end": 48, "char_start": 53, "char_end": 181, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ling et al., 2015;": "5799549", "Chung et al., 2016;": "13495961", "Passban et al., 2018)": "4948856"}}}, {"token_start": 58, "token_end": 120, "char_start": 234, "char_end": 564, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Cherry et al. (2018)": "52119281"}, "Reference": {}}}]}
{"id": "88517356_0", "paragraph": "[BOS] NMT has benefited from character-aware word representations on the source side (Costa-juss and Fonollosa, 2016), which follows language modeling work by Kim et al. (2016) and generate source-side input embeddings using a CNN over the character sequence of each word.\n[BOS] Further analysis revealed that hidden states of such characteraware models have increased knowledge of morphology (Belinkov et al., 2017) .\n[BOS] They additionally try using character-aware representations in the target side embedding layer, leaving the softmax matrix with standard word representations, and found no improvements.\n[BOS] Our work is also aligned with the characteraware models proposed in (Kim et al., 2016) , but we additionally employ a gating mechanism between character-aware representations and standard word representations similar to language modeling work by (Miyamoto and Cho, 2016) .\n[BOS] However, our gating is a learned type-specific vector rather than a fixed hyperparameter.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 8, "token_end": 29, "char_start": 29, "char_end": 117, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 32, "token_end": 43, "char_start": 133, "char_end": 176, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Kim et al. (2016)": "686481"}}}, {"token_start": 62, "token_end": 120, "char_start": 279, "char_end": 610, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Belinkov et al., 2017)": "7100502"}, "Reference": {}}}, {"token_start": 128, "token_end": 141, "char_start": 651, "char_end": 703, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kim et al., 2016)": "686481"}}}, {"token_start": 160, "token_end": 172, "char_start": 837, "char_end": 887, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Miyamoto and Cho, 2016)": "9662991"}}}]}
{"id": "53248591_0", "paragraph": "[BOS] Recent studies on slot filling in conversational systems are mostly based on neural models.\n[BOS] Wang et al. (2018) introduce a bi-model (RNN) structure to consider cross-impact between intent detection and slot filling.\n[BOS] Liu and Lane (2016) propose an attention mechanism on the encoder-decoder model for joint intent classification and slot filling.\n[BOS] (Goo et al., 2018) extend the attention mechanism using a slot gated model to learn relationship between slot and intent attention vectors.\n[BOS] HakkaniTr et al. (2016) use bidirectional RNN as a single model that handle multiple domains by adding a final state that contains domain identifier.\n[BOS] The work by Jha et al. (2018) ; Kim et al. (2017) uses expert based domain adaptation while Jaech et al. (2016) propose a multi-task learning approach to guide the training of a model for new domain.\n[BOS] All of these studies train their model solely on slot filling datasets, while our focus is to exploit a more \"general\" resource, such as NER, by training the model jointly with slot filling through MTL with different supervision level.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Multi_summ", "Reflection"], "span_citation_mapping": [{"token_start": 19, "token_end": 48, "char_start": 104, "char_end": 227, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wang et al. (2018)": "44069187"}, "Reference": {}}}, {"token_start": 49, "token_end": 73, "char_start": 234, "char_end": 363, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Liu and Lane (2016)": "7476732"}, "Reference": {}}}, {"token_start": 74, "token_end": 102, "char_start": 370, "char_end": 509, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Goo et al., 2018)": "44169697"}, "Reference": {}}}, {"token_start": 103, "token_end": 135, "char_start": 516, "char_end": 665, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 139, "token_end": 160, "char_start": 684, "char_end": 757, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Jha et al. (2018)": "44112039", "Kim et al. (2017)": "39130572"}, "Reference": {}}}, {"token_start": 161, "token_end": 187, "char_start": 764, "char_end": 871, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Jaech et al. (2016)": "16595919"}, "Reference": {}}}]}
{"id": "67864863_0", "paragraph": "[BOS] The current state-of-the-art in compositionality prediction involves the use of word embeddings (Salehi et al., 2015a) .\n[BOS] The vector representations of each component word (e.g. couch and potato) and the overall MWE (e.g. couch potato) are taken as a proxy for their respective meanings, and compositionality of the MWE is then assumed to be proportional to the relative similarity between each of the components and overall MWE embedding.\n[BOS] However, word-level embeddings require token-level identification of each MWE in the training corpus, meaning that if the set of MWEs changes, the model needs to be retrained.\n[BOS] This limitation led to research on character-level models, since character-level models can implic-itly handle an unbounded vocabulary of component words and MWEs (Hakimi Parizi and Cook, 2018) .\n[BOS] There has also been work in the extension of word embeddings to document embeddings that map entire sentences or documents to vectors (Le and Mikolov, 2014; Conneau et al., 2017) .\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 12, "token_end": 31, "char_start": 38, "char_end": 124, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Salehi et al., 2015a)": "203279"}}}, {"token_start": 154, "token_end": 185, "char_start": 704, "char_end": 832, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hakimi Parizi and Cook, 2018)": null}}}, {"token_start": 202, "token_end": 227, "char_start": 930, "char_end": 1019, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Le and Mikolov, 2014;": "2407601", "Conneau et al., 2017)": "28971531"}}}]}
{"id": "81978314_2", "paragraph": "[BOS] VAE-based semi-supervised methods, on the other hand, are able to cooperate with various kinds of classifiers.\n[BOS] VAE has been applied in many semi-supervised NLP tasks, ranging from text classification [Xu et al., 2017] , relation extraction [Marcheggiani and Titov, 2016 ] to sequence tagging [?]\n[BOS] .\n[BOS] Different from text classification where sentiment polarity is related to an entire sentence, the ATSA needs to extract the informative description and perform sophisticated reasoning about a given aspect-term.\n[BOS] To circumvent this problem, a novel structure is proposed.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 43, "token_end": 53, "char_start": 192, "char_end": 229, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"[Xu et al., 2017]": "2060721"}}}, {"token_start": 54, "token_end": 66, "char_start": 232, "char_end": 281, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"[Marcheggiani and Titov, 2016": null}}}, {"token_start": 68, "token_end": 73, "char_start": 287, "char_end": 307, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "81978314_1", "paragraph": "[BOS] Another related topic is semi-supervised learning for the text classification.\n[BOS] A simple but efficient method is to use pre-trained modules, e.g., initializing the word embedding or bottom layers with pre-trained parameters.\n[BOS] Although word embedding technique has been wildly used in NLP models, e.g., Glove [Pennington et al., 2014] and ELMo [Peters et al., 2018] , other pretraining-based method is modeldependent.\n[BOS] The ELMo and BERT [Devlin et al., 2018] replace the context-free embedding layer to context-dependent layer with the pre-trained Bidirectional Language Model and Transformer to capture the contextual representation.\n[BOS] This method is complementary to the proposed method.\n[BOS] The combination with our method may yield better performance than either of them alone, but that investigation is beyond the scope of this paper.\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 68, "token_end": 101, "char_start": 318, "char_end": 432, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"[Peters et al., 2018]": null}}}, {"token_start": 106, "token_end": 144, "char_start": 452, "char_end": 654, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"[Devlin et al., 2018]": null}, "Reference": {}}}]}
{"id": "81978314_0", "paragraph": "[BOS] Sentiment analysis is a traditional research hotspot in the NLP field [Wang and Manning, 2012] .\n[BOS] Rather than obtaining the sentimental inclination of the entire text, ATSA instead aims to extract the sentimental expression w.r.t.\n[BOS] a target entity.\n[BOS] With the release of online completions, abundant methods were proposed to explore the limits of current models.\n[BOS] Tang et al. [Tang et al., 2016a] proposed to make use of bidirectional Long Short-Term Memory (LSTM) [Hochreiter and Schmidhuber, 1997 ] to encode the sentence from the left and right to the aspect-term.\n[BOS] This model primarily verifies the effectiveness of deep models for ATSA Tang et al. [Tang et al., 2016b] then put forward a neural reasoning model in analogy to the memory network to perform the reasoning in many steps.\n[BOS] There are also many other works dedicating to solve this task [Pan and Wang, 2018; Liu et al., 2018; Zhang and Liu, 2017] .\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Transition", "Single_summ", "Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 2, "token_end": 23, "char_start": 6, "char_end": 100, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"[Wang and Manning, 2012]": "217537"}}}, {"token_start": 80, "token_end": 182, "char_start": 389, "char_end": 818, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Tang et al. [Tang et al., 2016a]": "10870417"}, "Reference": {"[Tang et al., 2016b]": "359042"}}}, {"token_start": 194, "token_end": 216, "char_start": 877, "char_end": 946, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Wang, 2018;": null, "Zhang and Liu, 2017]": "2060721"}}}]}
{"id": "13752894_0", "paragraph": "[BOS] Until the appearance of the CWI shared task of 2016, there was no manually annotated and verified CWI dataset.\n[BOS] The 2016 shared task brought us one of the largest CWI datasets to that date, consisting of a total of 9,200 sentences manually annotated by 400 different non-native English speakers.\n[BOS] In total, 200 sentences are used as a training set where each target is annotated by 20 annotators.\n[BOS] The rest of the dataset (9,000 sentences) are used for test set where each target is annotated by a single annotator from the entire pool of 400 annotators.\n[BOS] The approaches used in the first SemEval 2016 Task 11: Complex Word Identification are described in Table 1 .\n\n", "discourse_tags": ["Other", "Other", "Other", "Other", "Other"], "span_citation_mapping": []}
{"id": "1212315_4", "paragraph": "[BOS] Our system adopts the architecture of Chiu and Nichols (2016) , which combined BLSTMs to maximize context over the tagged word sequence and word-level CNNs to automatically generate characterlevel features with a partial-matching lexicon to achieve the state-of-the-art for NER on both CoNLL 2003 and OntoNotes datasets.\n[BOS] Our system can be viewed as an investigation into how well state-of-theart neural approaches adapt to the challenges of NER on noisy Web data.\n\n", "discourse_tags": ["Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 6, "token_end": 16, "char_start": 28, "char_end": 67, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Chiu and Nichols (2016)": "6300165"}}}]}
{"id": "1212315_3", "paragraph": "[BOS] The earliest work on NER for Twitter, used a CRF model with global features from tweet clusters to conduct NER with the MUC-7 4 class task definition (Liu et al., 2011) .\n[BOS] Ritter et al. (2011) developed a suite of NLP tools explicitly for Twitter and expanded the task to the 10 class definition used in the WNUT shared tasks.\n[BOS] A key difference between NER for Twitter and conventional NER is that the former also considers peripheral tasks such as named entity tokenization (Li et al., 2012) , normalization (Liu et al., 2012) , and linking (Guo et al., 2013; Yamada et al., 2015) .\n[BOS] The WNUT 2015 Shared Task included text normalization and named entity tokenization and detection tasks (Baldwin et al., 2015) , with most systems using machine learning methods like CRF together with a variety of features including lexicons, orthographic features, and distributional information.\n[BOS] In contrast with conventional NER, there was only one neural network entry (Godin et al., 2015) , and most systems tended to prefer Brown clusters to word embeddings.\n[BOS] The state of the art at WNUT 2015 used a cascaded model of entity tokenization, followed by linking to knowledge bases, and, finally, classification with random forests (Yamada et al., 2015) .\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Narrative_cite", "Narrative_cite", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 23, "token_end": 41, "char_start": 113, "char_end": 174, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Liu et al., 2011)": "417913"}}}, {"token_start": 43, "token_end": 78, "char_start": 183, "char_end": 337, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ritter et al. (2011)": "12861120"}, "Reference": {}}}, {"token_start": 99, "token_end": 111, "char_start": 465, "char_end": 508, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Li et al., 2012)": "15809395"}}}, {"token_start": 112, "token_end": 121, "char_start": 511, "char_end": 543, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Liu et al., 2012)": "1770806"}}}, {"token_start": 123, "token_end": 140, "char_start": 550, "char_end": 597, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Guo et al., 2013;": "5883983", "Yamada et al., 2015)": "288028"}}}, {"token_start": 149, "token_end": 168, "char_start": 641, "char_end": 732, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Baldwin et al., 2015)": "14500933"}}}, {"token_start": 208, "token_end": 220, "char_start": 964, "char_end": 1005, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Godin et al., 2015)": "15943168"}}}, {"token_start": 264, "token_end": 277, "char_start": 1217, "char_end": 1273, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yamada et al., 2015)": "288028"}}}]}
{"id": "1212315_2", "paragraph": "[BOS] Recently, the state-of-the-art for NER neural networks have overtaken other approaches to NER.\n[BOS] Most approaches build on the pioneering work of Collobert et al. (2011) , which showed that word embeddings could be employed in a deep FFNN to achieve near state-of-the-art results on POS tagging, chunking, NER, and SRL.\n[BOS] Santos et al. (2015) augmented the architecture of Collobert et al. (2011) with character-level CNNs, reporting improved performance on Spanish and Portuguese NER.\n[BOS] Huang et al. (2015) employed BLSTMs in place of FFNNs for the POS-tagging, chunking, and NER tasks, but they employed heavy feature engineering instead of using a CNN to automatically extract character-level features.\n[BOS] Lample et al. (2016) proposed LSTM-CRF and Stack-LSTM architectures for NER.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 25, "token_end": 80, "char_start": 107, "char_end": 328, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Collobert et al. (2011)": "351666"}, "Reference": {}}}, {"token_start": 81, "token_end": 116, "char_start": 335, "char_end": 498, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Santos et al. (2015)": "9150889"}, "Reference": {"Collobert et al. (2011)": "351666"}}}, {"token_start": 117, "token_end": 166, "char_start": 505, "char_end": 722, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Huang et al. (2015)": "12740621"}, "Reference": {}}}, {"token_start": 167, "token_end": 189, "char_start": 729, "char_end": 805, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Lample et al. (2016)": "6042994"}, "Reference": {}}}]}
{"id": "1212315_1", "paragraph": "[BOS] Most recent approaches to NER have been characterized by the use of CRF, SVM, and perceptron models, where performance is heavily dependent on feature engineering.\n[BOS] Ratinov and Roth (2009) used non-local features, a gazetteer extracted from Wikipedia, and Brown-cluster-like word representations.\n[BOS] Lin and Wu (2009) used phrase features obtained by performing k-means clustering over a private database of search engine query logs in place of a lexicon.\n[BOS] Passos et al. (2014) proposed a model that infused word embeddings with lexical knowledge.\n[BOS] In order to combat the problem of sparse features, Suzuki et al. (2011) performed feature reduction with large-scale unlabelled data.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 33, "token_end": 63, "char_start": 176, "char_end": 307, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ratinov and Roth (2009)": "1859014"}, "Reference": {}}}, {"token_start": 64, "token_end": 95, "char_start": 314, "char_end": 469, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Lin and Wu (2009)": "8148140"}, "Reference": {}}}, {"token_start": 96, "token_end": 115, "char_start": 476, "char_end": 566, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Passos et al. (2014)": "9345583"}, "Reference": {}}}, {"token_start": 116, "token_end": 144, "char_start": 573, "char_end": 706, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Suzuki et al. (2011)": "16437269"}, "Reference": {}}}]}
{"id": "1212315_0", "paragraph": "[BOS] Named entity recognition is a task with a long history, dating back to MUC-7 (Chinchor and Robinson, 1997) .\n[BOS] In this section, we describe the NER research that influenced our system and give an overview of the work on NER for Twitter.\n[BOS] For a more detailed survey, see (Chiu and Nichols, 2016) .\n\n", "discourse_tags": ["Transition", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 16, "token_end": 28, "char_start": 77, "char_end": 112, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chinchor and Robinson, 1997)": "59916135"}}}, {"token_start": 56, "token_end": 72, "char_start": 253, "char_end": 309, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chiu and Nichols, 2016)": "6300165"}}}]}
{"id": "15571177_0", "paragraph": "[BOS] Many syntax-based approaches to machine translation rely on bilingual treebanks to extract transfer rules or train statistical translation models.\n[BOS] In order to build bilingual treebanks a number of methods for automatic tree alignment have been developed, e.g., (Gildea, 2003; Groves et al., 2004; Tinsley et al., 2007; Lavie et al., 2008) .\n[BOS] Most related to our approach is the work on discriminative tree alignment by Tiedemann & Kotz (2009) .\n[BOS] However, these algorithms assume that source and target sentences express the same information (i.e. parallel text) and cannot cope with comparable text where parts may remain unaligned.\n[BOS] See for further arguments and empirical evidence that MT alignment algorithms are not suitable for aligning parallel monolingual text.\n[BOS] MacCartney, Galley, and Manning (2008) describe a system for monolingual phrase alignment based on supervised learning which also exploits external resources for knowledge of semantic relatedness.\n[BOS] In contrast to our work, they do not use syntactic trees or similarity relation labels.\n[BOS] Partly similar semantic relations are used in for modeling semantic containment and exclusion in natural language inference.\n[BOS] Marsi & Krahmer (2005a) is closely related to our work, but follows a more complicated method: first a dynamic programmingbased tree alignment algorithm is applied, followed by a classification of similarity relations using a supervised-classifier.\n[BOS] Other differences are that their data set is much smaller and consists of parallel rather than comparable text.\n[BOS] A major drawback of this algorithmic approach it that it cannot cope with crossing alignments.\n[BOS] We are not aware of other work that combines alignment with semantic relation labeling, or algorithms which perform both tasks simultaneously.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Multi_summ", "Transition", "Transition", "Single_summ", "Single_summ", "Reflection", "Single_summ", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 41, "token_end": 84, "char_start": 231, "char_end": 350, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Gildea, 2003;": "464827", "Groves et al., 2004;": null, "Tinsley et al., 2007;": null, "Lavie et al., 2008)": "1113327"}}}, {"token_start": 95, "token_end": 107, "char_start": 403, "char_end": 459, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Tiedemann & Kotz\u00e9 (2009)": null}}}, {"token_start": 169, "token_end": 224, "char_start": 802, "char_end": 1092, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 244, "token_end": 312, "char_start": 1230, "char_end": 1596, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Marsi & Krahmer (2005a)": "4944917"}, "Reference": {}}}]}
{"id": "1477515_5", "paragraph": "[BOS] where w C , w L are the weights assigned to cosine similarity and LexNET's scores respectively, such that w C + w L = 1.\n[BOS] We tuned the weights and a threshold t using the validation set, and classified (x, y) as related if Rel(x, y)  t. The word vectors used to compute the cosine similarity scores were chosen among several available pre-trained embeddings.\n[BOS] 4 For completeness we also report the performance of two baselines: cosine similarity (w C = 1) and LexNET (w L = 1, fixed t = 0.5).\n[BOS] Table 2 : Performance scores on the test set in each subtask, of the selected methods and the baselines.\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection", "Other"], "span_citation_mapping": []}
{"id": "1477515_4", "paragraph": "[BOS] We computed cosine similarity for each (x, y) pair in the dataset: cos( v wx , v wy ) = vw x  vw y vw x  vw y , and normalized it to the range [0, 1].\n[BOS] We scored each (x, y) pair by a combination of LexNET's score for the RELATED class and the cosine similarity score:\n\n", "discourse_tags": ["Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "1477515_3", "paragraph": "[BOS] We tuned LexNET's hyper-parameters on the validation set, disregarding the similarity measure at this point, and then chose the model that performed best on the validation set and combined it with the similarity measure.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "1477515_2", "paragraph": "[BOS] While path-based approaches have been commonly used for semantic relation classification (A. Hearst, 1992; Snow et al., 2004; Nakashole et al., 2012; Necsulescu et al., 2015) , they have never been used for word relatedness, which is considered a \"classical\" task for distributional methods.\n[BOS] We argue that path-based information can improve performance of word relatedness tasks as well (see Section 4.1).\n[BOS] We train LexNET to distinguish between two classes: RELATED and UNRELATED, and combine it with the common cosine similarity measure to tackle subtask 1.\n\n", "discourse_tags": ["Narrative_cite", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 12, "token_end": 49, "char_start": 62, "char_end": 180, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(A. Hearst, 1992;": "15763200", "Snow et al., 2004;": "1854720", "Nakashole et al., 2012;": "2257688", "Necsulescu et al., 2015)": "1605984"}}}]}
{"id": "1477515_1", "paragraph": "[BOS] Most commonly, vector cosine is adopted as a similarity measure (Turney et al., 2010) .\n[BOS] Many other measures exist, including but not limited to Euclidean distance, KL divergence (Cover and Thomas, 2012 ), Jaccard's coefficient (Salton and McGill, 1986) , and more recently neighbor rank (Hare et al., 2009; Lapesa and Evert, 2013) and APSyn (Santus et al., 2016a) .\n[BOS] 2 To turn this task into a binary classification task, where x and y are classified as either related or not, one can set a threshold to separate similarity scores of related and unrelated word pairs.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 11, "token_end": 22, "char_start": 51, "char_end": 91, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Turney et al., 2010)": "1500900"}}}, {"token_start": 37, "token_end": 45, "char_start": 176, "char_end": 213, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Thomas, 2012": "190432"}}}, {"token_start": 47, "token_end": 61, "char_start": 217, "char_end": 264, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Salton and McGill, 1986)": null}}}, {"token_start": 65, "token_end": 85, "char_start": 285, "char_end": 342, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hare et al., 2009;": "5738729", "Lapesa and Evert, 2013)": "3906552"}}}, {"token_start": 86, "token_end": 98, "char_start": 347, "char_end": 375, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Santus et al., 2016a)": "7321763"}}}]}
{"id": "1477515_0", "paragraph": "[BOS] Recognizing word relatedness is typically addressed by distributional methods.\n[BOS] To determine to what extent two terms x and y are related, a vector similarity or distance measure is applied to their distributional representations: sim( v wx , v wy ).\n[BOS] This is a straightforward application of the distributional hypothesis (Harris, 1954) , according to which related words occur in similar contexts, hence have similar vector representations.\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 57, "token_end": 65, "char_start": 313, "char_end": 353, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Harris, 1954)": null}}}]}
{"id": "12541398_3", "paragraph": "[BOS] Also new to this edition of the shared task is the evaluation on social media data.\n[BOS] In 2014, the Tweet-LID shared task specifically addressed the problem of language identification in very short texts (Zubiaga et al., 2014) .\n[BOS] This brought to light some of the challenges inherent to the genre: a need for a better external resources to train systems, low accuracy on underrepresented languages and the inability to identify multilingual tweets.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 35, "token_end": 51, "char_start": 169, "char_end": 235, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zubiaga et al., 2014)": null}}}]}
{"id": "12541398_2", "paragraph": "[BOS] Previous approaches for Arabic dialect detection, a new task introduced in this shared task edition, use similar approaches.\n[BOS] Sadat et al. (2014) argue that character n-gram models are well suited for dialect identification tasks because most of the variation is based on affixation, which can be easily modeled at the character level.\n\n", "discourse_tags": ["Transition", "Single_summ"], "span_citation_mapping": [{"token_start": 25, "token_end": 70, "char_start": 137, "char_end": 346, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Sadat et al. (2014)": "1897457"}, "Reference": {}}}]}
{"id": "12541398_1", "paragraph": "[BOS] State-of-the-art approaches to related language identification rely heavily on word and character ngram representations.\n[BOS] Other features include the use of blacklists and whitelists, language models, POS tag distributions and language-specific orthographical conventions (Bali, 2006; Zampieri and Gebre, 2012) .\n[BOS] For systems, a wide range of machine learning algorithms have been applied (Naive Bayes and SVM classifiers in particular), with work on optimization and dimensionality reduction (Goutte et al., 2014) , and on ensembling and cascading, which yielded the best-performing systems in the 2015 edition (Goutte and Lger, 2015; Malmasi and Dras, 2015) .\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 31, "token_end": 68, "char_start": 167, "char_end": 320, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bali, 2006;": null, "Zampieri and Gebre, 2012)": null}}}, {"token_start": 96, "token_end": 110, "char_start": 466, "char_end": 529, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Goutte et al., 2014)": "15527754"}}}, {"token_start": 113, "token_end": 149, "char_start": 539, "char_end": 674, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Goutte and L\u00e9ger, 2015;": "16728697", "Malmasi and Dras, 2015)": "15597888"}}}]}
{"id": "12541398_0", "paragraph": "[BOS] Language identification is an active field of research, where in recent years increased attention has been given to the identification of closely related languages, language variants and dialects, which are harder to distinguish.\n[BOS] The three editions of the DSL shared task on detecting similar languages have provided a forum for benchmarking various approaches.\n[BOS] For a detailed overview of the previous editions and their related work, we refer to the overview papers of Zampieri et al. (2014) and Zampieri et al. (2015) .\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 83, "token_end": 105, "char_start": 469, "char_end": 537, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Zampieri et al. (2014)": null, "Zampieri et al. (2015)": null}}}]}
{"id": "15749718_4", "paragraph": "[BOS] From the perspective of applying deep networks in natural language processing systems, there are a number of works in the literature (Collobert and Weston, 2008; Collobert et al., 2011; Henderson, 2004; Socher et al., 2011; Titov and Henderson, 2010; Turian et al., 2010) .\n[BOS] Socher et al. (2011) applied recursive autoencoders to address sentencelevel sentiment classification problems.\n[BOS] Collobert and Weston (2008) and Collobert et al. (2011) employed a deep learning framework for jointly multi-task learning and empirically evaluated it with four NLP tasks, including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling.\n[BOS] Henderson (2004) proposed discriminative training methods for learning a neural network statistical parser.\n[BOS] Titov and Henderson (2010) extended the incremental sigmoid Belief networks (Titov and Henderson, 2007) to a generative latent variable model for dependency parsing.\n[BOS] Turian et al. (2010) employed neural networks to induce word representations for sequence labeling tasks such as named entity recognition.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Multi_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 15, "token_end": 70, "char_start": 93, "char_end": 277, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Collobert and Weston, 2008;": "2617020", "Collobert et al., 2011;": "351666", "Henderson, 2004;": "1588411", "Socher et al., 2011;": "3116311", "Titov and Henderson, 2010;": "9830566", "Turian et al., 2010)": "629094"}}}, {"token_start": 72, "token_end": 93, "char_start": 286, "char_end": 397, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Socher et al. (2011)": "3116311"}, "Reference": {}}}, {"token_start": 94, "token_end": 152, "char_start": 404, "char_end": 674, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Collobert and Weston (2008)": "2617020", "Collobert et al. (2011)": "351666"}, "Reference": {}}}, {"token_start": 153, "token_end": 170, "char_start": 681, "char_end": 788, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Henderson (2004)": "1588411"}, "Reference": {}}}, {"token_start": 171, "token_end": 204, "char_start": 795, "char_end": 960, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Titov and Henderson (2010)": "9830566"}, "Reference": {"(Titov and Henderson, 2007)": "2847717"}}}, {"token_start": 205, "token_end": 230, "char_start": 967, "char_end": 1105, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Turian et al. (2010)": "629094"}, "Reference": {}}}]}
{"id": "15749718_3", "paragraph": "[BOS] Cross-lingual representation learning methods induce language-independent features to bridge the cross-lingual difference in the original wordlevel representation space and build connections across different languages.\n[BOS] They train a dependency parser in the induced representation space by exploiting labeled data from the source language and apply it in the target language (Dur-rett et al., 2012; Tckstrm et al., 2012; Zhang et al., 2012) .\n[BOS] A variety of auxiliary resources have been used to induce interlingual features, including bilingual lexicon (Durrett et al., 2012) , and unlabeled parallel sentences (Tckstrm et al., 2013) .\n[BOS] Based on different learning mechanisms (whether or not using labeled data) for inducing language-independent features, cross-lingual representation learning methods can be categorized into unsupervised representation learning (Tckstrm et al., 2013) and supervised representation learning (Durrett et al., 2012) .\n[BOS] The language-independent features include bilingual word clusters (Tckstrm et al., 2012) , language-independent projection features (Durrett et al., 2012) , and automatically induced languageindependent POS tags (Zhang et al., 2012) .\n[BOS] Besides cross-lingual dependency parsing, in the literature cross-lingual representation learning methods have also demonstrated efficacy in different NLP applications such as cross language named entity recognition (Tckstrm et al., 2012) and cross language semantic role labeling .\n[BOS] Our work shares similarity with these cross-lingual representation learning methods on inducing new language-independent features, but differs from them in that we learn cross-lingual word embeddings.\n[BOS] Though multilingual word embeddings have been employed in the literature, they are developed for other NLP tasks such as cross-lingual sentiment analysis , and machine translation (Zou et al., 2013) .\n[BOS] Moreover, the method in requires parallel sentences with observed word-level alignments, and the method in (Zou et al., 2013) first learns language-specific word embeddings in each language separately and then transforms representations from one language to another language with machine translation alignments, while we jointly learn crosslingual word embeddings in the two languages by only exploiting a small set of bilingual word pairs.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite", "Narrative_cite", "Narrative_cite", "Narrative_cite", "Reflection", "Narrative_cite", "Single_summ"], "span_citation_mapping": [{"token_start": 60, "token_end": 90, "char_start": 370, "char_end": 451, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"T\u00e4ckstr\u00f6m et al., 2012;": "891605"}}}, {"token_start": 108, "token_end": 119, "char_start": 551, "char_end": 591, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 121, "token_end": 135, "char_start": 598, "char_end": 649, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 168, "token_end": 182, "char_start": 847, "char_end": 906, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 183, "token_end": 195, "char_start": 911, "char_end": 968, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 203, "token_end": 217, "char_start": 1019, "char_end": 1065, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 218, "token_end": 232, "char_start": 1068, "char_end": 1131, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 234, "token_end": 248, "char_start": 1138, "char_end": 1209, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 279, "token_end": 295, "char_start": 1394, "char_end": 1456, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 362, "token_end": 381, "char_start": 1835, "char_end": 1912, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "15749718_2", "paragraph": "[BOS] Multilingual model learning methods train cross-lingual dependency parsers with parameter constraints obtained from parallel data Ganchev et al., 2009) or linguistic knowledges (Naseem et al., 2010; Naseem et al., 2012) .\n[BOS] Among these methods, some proposed to train a joint dependency parsing system with parameters shared across the dependency parsing models in individual languages .\n[BOS] Other works used posterior regularization techniques to encode the linguistic constraints in learning dependency parsing models (Ganchev et al., 2009; Naseem et al., 2010; Naseem et al., 2012) .\n[BOS] The linguistic constraints may either come from manually constructed universal dependency parsing rules (Naseem et al., 2010) or manually specified typological features (Naseem et al., 2012) , or be learned from parallel sentences (Ganchev et al., 2009) .\n[BOS] Besides cross-lingual dependency parsing, multilingual model learning methods have also achieved good empirical results for other multilingual NLP tasks, including named entity recognition (Burkett et al., 2010; Che et al., 2013; Wang and Manning, 2014) , syntactic parsing (Burkett et al., 2010) , semantic role labeling (Zhuang and Zong, 2010; Kozhevnikov and Titov, 2012) , and word sense disambiguation (Guo and Diab, 2010) .\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Narrative_cite", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 20, "token_end": 31, "char_start": 122, "char_end": 157, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Ganchev et al., 2009)": "11681086"}}}, {"token_start": 32, "token_end": 54, "char_start": 161, "char_end": 225, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Naseem et al., 2010;": "3087412", "Naseem et al., 2012)": "3143538"}}}, {"token_start": 95, "token_end": 126, "char_start": 506, "char_end": 596, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ganchev et al., 2009;": "11681086", "Naseem et al., 2010;": "3087412", "Naseem et al., 2012)": "3143538"}}}, {"token_start": 137, "token_end": 151, "char_start": 674, "char_end": 730, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Naseem et al., 2010)": "3087412"}}}, {"token_start": 152, "token_end": 167, "char_start": 734, "char_end": 795, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Naseem et al., 2012)": "3143538"}}}, {"token_start": 170, "token_end": 184, "char_start": 804, "char_end": 858, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ganchev et al., 2009)": "11681086"}}}, {"token_start": 214, "token_end": 240, "char_start": 1031, "char_end": 1120, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Burkett et al., 2010;": "9451332", "Che et al., 2013;": "8420751", "Wang and Manning, 2014)": "17952977"}}}, {"token_start": 241, "token_end": 252, "char_start": 1123, "char_end": 1163, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Burkett et al., 2010)": "9451332"}}}, {"token_start": 253, "token_end": 276, "char_start": 1166, "char_end": 1241, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhuang and Zong, 2010;": "8368480", "Kozhevnikov and Titov, 2012)": "7432243"}}}, {"token_start": 278, "token_end": 289, "char_start": 1248, "char_end": 1294, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Guo and Diab, 2010)": "5705883"}}}]}
{"id": "15749718_1", "paragraph": "[BOS] Cross-lingual annotation projection methods use parallel sentences to project the annotations from the source language side to the target language side and then train dependency parsers on the target data with projected annotations (Hwa et al., 2005; Smith and Eisner, 2009; Zhao et al., 2009) .\n[BOS] For cross-lingual annotation projection methods, both the word alignment training step and the annotation projection step can introduce errors or noise.\n[BOS] Thus much work developed in the literature has focused on designing robust projection algorithms such as graph-based projection with label propagations (Das and Petrov, 2011) , improving projection performance by using auxiliary resources such as Wikipedia metadata (Kim and Lee, 2012) or WordNet (Khapra et al., 2010) , or boosting projection performance by heuristically modifying or correcting the projected annotations (Hwa et al., 2005; Kim et al., 2010) .\n[BOS] Some work has also proposed to project the discrete dependency arc instances instead of treebank as the training set .\n[BOS] Moreover, besides cross-lingual dependency parsing, cross-lingual annotation projection methods have also demonstrated success in various other sequence labeling tasks including POS tagging (Das and Petrov, 2011; Yarowsky and Ngai, 2001) , relation extraction , named entity recognition (Kim et al., 2010; Kim and Lee, 2012) , constituent syntax parsing (Jiang et al., 2011) , and word sense disambiguation (Khapra et al., 2010) .\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Narrative_cite", "Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 34, "token_end": 62, "char_start": 199, "char_end": 299, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hwa et al., 2005;": "157167", "Smith and Eisner, 2009;": "10943559", "Zhao et al., 2009)": "15978883"}}}, {"token_start": 107, "token_end": 123, "char_start": 572, "char_end": 641, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Das and Petrov, 2011)": "8396953"}}}, {"token_start": 133, "token_end": 142, "char_start": 714, "char_end": 752, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kim and Lee, 2012)": null}}}, {"token_start": 143, "token_end": 155, "char_start": 756, "char_end": 785, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Khapra et al., 2010)": "14219801"}}}, {"token_start": 161, "token_end": 185, "char_start": 826, "char_end": 926, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hwa et al., 2005;": "157167", "Kim et al., 2010)": null}}}, {"token_start": 237, "token_end": 256, "char_start": 1238, "char_end": 1297, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Das and Petrov, 2011;": "8396953", "Yarowsky and Ngai, 2001)": "1227006"}}}, {"token_start": 260, "token_end": 277, "char_start": 1322, "char_end": 1384, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kim et al., 2010;": null, "Kim and Lee, 2012)": null}}}, {"token_start": 278, "token_end": 289, "char_start": 1387, "char_end": 1434, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Jiang et al., 2011)": null}}}, {"token_start": 291, "token_end": 305, "char_start": 1441, "char_end": 1488, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Khapra et al., 2010)": "14219801"}}}]}
{"id": "15749718_0", "paragraph": "[BOS] Previous works developed in the literature have tackled cross-lingual dependency parsing by using cross-lingual annotation projection methods, multilingual model learning methods, and crosslingual representation learning methods.\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "13849868_2", "paragraph": "[BOS] In our work, we opt for an approach which addresses lexical substitution in a direct way and does not include an explicit disambiguation step.\n[BOS] Lexical substitution systems perform substitute ranking in context using vector-space models (Thater et al., 2011; Kremer et al., 2014; Melamud et al., 2015) .\n[BOS] Recently, Apidianaki (2016) showed that a syntax-based substitution model can successfully filter the paraphrases available in the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013) to select the ones that are adequate in specific contexts.\n[BOS] In the same line, Cocos et al. (2017) used a word embedding-based substitution model (Melamud et al., 2015) for ranking PPDB paraphrases in context.\n[BOS] We extend this work and adapt the Melamud et al. (2015) model to the simplification setting by using candidate paraphrases extracted from the Simple PPDB resource (Pavlick and Callison-Burch, 2016) , a subset of the PPDB that contains complex words and phrases, and their simpler counterparts that can be used for incontext simplification.\n\n", "discourse_tags": ["Reflection", "Narrative_cite", "Single_summ", "Multi_summ", "Reflection"], "span_citation_mapping": [{"token_start": 39, "token_end": 69, "char_start": 228, "char_end": 312, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Thater et al., 2011;": "7893614", "Kremer et al., 2014;": "14141143", "Melamud et al., 2015)": "2897037"}}}, {"token_start": 73, "token_end": 129, "char_start": 331, "char_end": 565, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Apidianaki (2016)": "18022073"}, "Reference": {"(Ganitkevitch et al., 2013)": "6067240"}}}, {"token_start": 130, "token_end": 171, "char_start": 572, "char_end": 720, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Cocos et al. (2017)": "14974539"}, "Reference": {"(Melamud et al., 2015)": "2897037"}}}, {"token_start": 179, "token_end": 188, "char_start": 761, "char_end": 782, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Melamud et al. (2015)": "2897037"}}}, {"token_start": 203, "token_end": 218, "char_start": 876, "char_end": 924, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Pavlick and Callison-Burch, 2016)": "998303"}}}]}
{"id": "13849868_1", "paragraph": "[BOS] The above-mentioned systems support the full range of transformations involved in text simplification.\n[BOS] Other works address specific subtasks, such as syntactic or lexical simplification, which involve identifying grammatical or lexical complexities in a text and rewriting these using simpler words and structures.\n[BOS] Syntactic simplification might involve operations such as sentence splitting, rewriting of sentences including passive voice and anaphora resolution (Chandrasekar and Srinivas, 1997; Klerke and Sgaard, 2013) .\n[BOS] 1 Lexical simplification involves complex word identification, substitute generation, context-based substitute selection and simplicity ranking.\n[BOS] To identify the words to be simplified, Shardlow (2013a) proposes to use a Support Vector Machine (SVM) that exploits several lexical features, such as fre-quency, character and syllable length.\n[BOS] Our approach also uses a SVM classifier for identifying complex words, but complements this set of features with context-related features that have not been exploited in previous work.\n[BOS] 2 In the lexical simplification subtask, existing methods differ in their decision to include a word sense disambiguation (WSD) step for substitute selection and in the ranking method used.\n[BOS] Ranking is often addressed in terms of word frequency in a large corpus since it has been shown that frequent words increase a text's readability (Devlin and Tait, 1998; Kauchak, 2013) .\n[BOS] Models that include a semantic processing step for substitute selection aim to ensure that the selected substitutes express the correct meaning of words in specific contexts.\n[BOS] WSD is often carried out by selecting the correct synset (i.e. set of synonyms describing a sense) for a target word in WordNet (Miller, 1995) and retrieving the synonyms describing that sense.\n[BOS] Thomas and Anderson (2012) use WordNet's tree structure (hypernymy relations) to reduce the size of the vocabulary in a document.\n[BOS] Biran et al. (2011) perform disambiguation in an unsupervised manner.\n[BOS] They learn simplification rules from comparable corpora and apply them to new sentences using vector-based context similarity measures to select words that are the most likely candidates for substitution in a given context.\n[BOS] This process does not involve an explicit WSD step, and simplification is addressed as a context-aware lexical substitution task.\n[BOS] The SemEval 2012 English Lexical Simplification task (Specia et al., 2012 ) also addresses simplification as lexical substitution (McCarthy and Navigli, 2007) , allowing systems to use external sense inventories or to directly perform in-context substitution.\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite", "Transition", "Single_summ", "Reflection", "Transition", "Narrative_cite", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Multi_summ"], "span_citation_mapping": [{"token_start": 67, "token_end": 95, "char_start": 444, "char_end": 540, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chandrasekar and Srinivas, 1997;": "19025087", "Klerke and S\u00f8gaard, 2013)": "7851317"}}}, {"token_start": 118, "token_end": 161, "char_start": 700, "char_end": 894, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Shardlow (2013a)": "17679719"}, "Reference": {}}}, {"token_start": 251, "token_end": 275, "char_start": 1389, "char_end": 1472, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Devlin and Tait, 1998;": null, "Kauchak, 2013)": "9516661"}}}, {"token_start": 305, "token_end": 351, "char_start": 1662, "char_end": 1855, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Miller, 1995)": "207846993"}}}, {"token_start": 352, "token_end": 382, "char_start": 1862, "char_end": 1991, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Thomas and Anderson (2012)": "1906570"}, "Reference": {}}}, {"token_start": 383, "token_end": 461, "char_start": 1998, "char_end": 2433, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Biran et al. (2011)": "11091984"}, "Reference": {}}}, {"token_start": 462, "token_end": 514, "char_start": 2440, "char_end": 2699, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Specia et al., 2012": "8884060"}, "Reference": {"(McCarthy and Navigli, 2007)": "126584"}}}]}
{"id": "13849868_0", "paragraph": "[BOS] Prior approaches to text simplification have addressed the task as a monolingual translation problem (Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012) .\n[BOS] The proposed models are trained on aligned sentences extracted from Wikipedia and Simple Wikipedia, a corpus that contains instances of transformation operations needed for simplification such as rewording, reordering, insertion and deletion.\n[BOS] Zhu et al. (2010) propose to use a tree-based translation model which covers splitting, dropping, reordering and substitution.\n[BOS] Coster and Kauchak (2011) employ a phrase-based Machine Translation system extended to support phrase deletion, and Wubben et al. (2012) augment a phrase-based system with a re-ranking heuristic.\n[BOS] Woodsend and Lapata (2011) view simplification as a monolingual text generation task.\n[BOS] They propose a model based on a quasi-synchronous grammar, a formalism able to capture structural mismatches and complex rewrite operations.\n[BOS] The grammar is also induced from a parallel Wikipedia corpus, and an integer linear programming model selects the most appropriate simplification from the space of possible rewrites generated by the grammar.\n[BOS] The hybrid model of Angrosh et al. (2014) combines a synchronous grammar extracted from the same parallel corpus with a set of hand crafted syntactic simplification rules.\n[BOS] In recent work, Zhang and Lapata (2017) propose a reinforcement learning-based text simplification model which jointly models simplicity, grammaticality, and semantic fidelity to the input.\n[BOS] In contrast to these methods, Narayan and Gardent (2016)'s sentence simplification approach does not need a parallel corpus for training, but rather uses a deep semantic representation as input for simplification.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Single_summ", "Multi_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 13, "token_end": 43, "char_start": 75, "char_end": 172, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhu et al., 2010;": "15636533", "Coster and Kauchak, 2011;": "4896510", "Wubben et al., 2012)": "141120"}}}, {"token_start": 85, "token_end": 112, "char_start": 430, "char_end": 556, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhu et al. (2010)": "15636533"}, "Reference": {}}}, {"token_start": 113, "token_end": 135, "char_start": 563, "char_end": 673, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Coster and Kauchak (2011)": "4896510"}, "Reference": {}}}, {"token_start": 137, "token_end": 159, "char_start": 679, "char_end": 758, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 160, "token_end": 238, "char_start": 765, "char_end": 1211, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Woodsend and Lapata (2011)": "9945908"}, "Reference": {}}}, {"token_start": 239, "token_end": 274, "char_start": 1218, "char_end": 1389, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Angrosh et al. (2014)": "18228350"}, "Reference": {}}}, {"token_start": 275, "token_end": 310, "char_start": 1396, "char_end": 1585, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhang and Lapata (2017)": "7473831"}, "Reference": {}}}, {"token_start": 311, "token_end": 351, "char_start": 1592, "char_end": 1805, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "13746789_2", "paragraph": "[BOS] Multi-task learning in NLP Neural networks are particularly well-suited for MTL allowing for parameter sharing (Caruana, 1993) .\n[BOS] Recent NLP conferences witnessed a \"tsunami\" of deep learning papers (Manning, 2015) , followed by what we call a multi-task learning \"wave\": MTL has been successfully applied to a wide range of NLP tasks (Cohn and Specia, 2013; Cheng et al., 2015; Luong et al., 2015; Plank et al., 2016; Fang and Cohn, 2016; Ruder et al., 2017; Augenstein et al., 2018) .\n[BOS] Related to it is the pioneering work on adversarial learning (DANN) (Ganin et al., 2016) .\n[BOS] For sentiment analysis we found tri-training and our MT-Tri model to outperform DANN.\n[BOS] Our MT-Tri model lends itself well to shared-private models such as those proposed recently Kim et al., 2017) , which extend upon (Ganin et al., 2016) by having separate source and target-specific encoders.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Narrative_cite", "Reflection", "Narrative_cite"], "span_citation_mapping": [{"token_start": 21, "token_end": 30, "char_start": 99, "char_end": 132, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 44, "token_end": 53, "char_start": 189, "char_end": 225, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Manning, 2015)": "33290106"}}}, {"token_start": 79, "token_end": 137, "char_start": 336, "char_end": 495, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Luong et al., 2015;": "6954272", "Plank et al., 2016;": "5632184", "Fang and Cohn, 2016;": "84842", "Ruder et al., 2017;": null, "Augenstein et al., 2018)": "3592209"}}}, {"token_start": 148, "token_end": 163, "char_start": 544, "char_end": 592, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ganin et al., 2016)": "2871880"}}}, {"token_start": 194, "token_end": 210, "char_start": 731, "char_end": 802, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Kim et al., 2017)": "1399655"}}}, {"token_start": 211, "token_end": 223, "char_start": 805, "char_end": 843, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ganin et al., 2016)": "2871880"}}}]}
{"id": "13746789_1", "paragraph": "[BOS] Neural network ensembling Related work on self-ensembling approaches includes snapshot ensembling or temporal ensembling (Laine and Aila, 2017) .\n[BOS] In general, the line between \"explicit\" and \"implicit\" ensembling , like dropout (Srivastava et al., 2014) or temporal ensembling (Saito et al., 2017) , is more fuzzy.\n[BOS] As we noted earlier our multi-task learning setup can be seen as a form of self-ensembling.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 15, "token_end": 31, "char_start": 84, "char_end": 149, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Laine and Aila, 2017)": "13123084"}}}, {"token_start": 50, "token_end": 62, "char_start": 231, "char_end": 264, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Srivastava et al., 2014)": "6844431"}}}, {"token_start": 63, "token_end": 75, "char_start": 268, "char_end": 308, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Saito et al., 2017)": "12570770"}}}]}
{"id": "13746789_0", "paragraph": "[BOS] Learning under Domain Shift There is a large body of work on domain adaptation.\n[BOS] Studies on unsupervised domain adaptation include early work on bootstrapping (Steedman et al., 2003; McClosky et al., 2006a) , shared feature representations (Blitzer et al., 2006 (Blitzer et al., , 2007 and instance weighting (Jiang and Zhai, 2007) .\n[BOS] Recent approaches include adversarial learning (Ganin et al., 2016) and fine-tuning (Sennrich et al., 2016) .\n[BOS] There is almost no work on bootstrapping approaches for recent neural NLP, in particular under domain shift.\n[BOS] Tri-training is less studied, and only recently re-emerged in the vision community (Saito et al., 2017) , albeit is not compared to classic tri-training.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite", "Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 27, "token_end": 49, "char_start": 156, "char_end": 217, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Steedman et al., 2003;": "7163980", "McClosky et al., 2006a)": "628455"}}}, {"token_start": 50, "token_end": 70, "char_start": 220, "char_end": 296, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Blitzer et al., 2006": "15978939", "(Blitzer et al., , 2007": "14688775"}}}, {"token_start": 71, "token_end": 81, "char_start": 301, "char_end": 342, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Jiang and Zhai, 2007)": "15036406"}}}, {"token_start": 86, "token_end": 98, "char_start": 377, "char_end": 418, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ganin et al., 2016)": "2871880"}}}, {"token_start": 99, "token_end": 112, "char_start": 423, "char_end": 458, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sennrich et al., 2016)": "15600925"}}}, {"token_start": 151, "token_end": 162, "char_start": 648, "char_end": 685, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Saito et al., 2017)": "12570770"}}}]}
{"id": "13896416_0", "paragraph": "[BOS] Closest to our clustering approach from Section 2 is the error-driven first-order probabilistic model of Culotta et al. (2007) .\n[BOS] Among significant differences we mention that our model is non-probabilistic, simpler and easier to understand and implement.\n[BOS] Furthermore, the update step does not stop after the first clustering error, instead the algorithm learns and uses a clustering threshold  to determine when to stop during training and testing.\n[BOS] This required the design of a method to order cluster pairs in which the clusters may not be consistent with the true coreference chains, which led to the introduction of the goodness function in Equation 1 as a new scoring measure for cluster pairs.\n[BOS] The strategy of continuing the clustering during training as long as a an adaptive threshold is met better matches the training with the testing, and was observed to lead to better performance.\n[BOS] The cluster ranking model of Rahman and Ng (2009) proceeds in a left-to-right fashion and adds the current discourse old mention to the highest scoring preceding cluster.\n[BOS] Compared to it, our adaptive clustering approach is less constrained: it uses only a weak, partial ordering between coreference decisions, and does not require a singleton cluster at every clustering step.\n[BOS] This allows clustering to start in any section of the document where coreference decisions are easier to make, and thus create accurate clusters earlier in the process.\n[BOS] The use of semantic knowledge for coreference resolution has been studied before in a number of works, among them (Ponzetto and Strube, 2006) , (Bengtson and Roth, 2008) , (Lee et al., 2011) , and (Rahman and Ng, 2011) .\n[BOS] The focus in these studies has been on the semantic similarity between a mention and a candidate antecedent, or the parallelism between the semantic role structures in which the two appear.\n[BOS] One of the earliest methods for using predicate-argument frequencies in pronoun resolution is that of Dagan and Itai (1990) .\n[BOS] Closer to our use of semantic compatibility features for pronouns are the approaches of Kehler et al. (2004) and Yang et al. (2005) .\n[BOS] The last work showed that pronoun resolution can be improved by incorporating semantic compatibility features derived from search engine statistics in the twin-candidate model.\n[BOS] In our approach, we use web-based language models to compute semantic compatibility features for neutral pronouns and show that they can improve performance over a state-of-the-art coreference resolution system.\n[BOS] The use of language models instead of search engine statistics is more practical, as they eliminate the latency involved in using search engine queries.\n[BOS] Webbased language models can be built on readily available web N-gram corpora, such as Google's Web 1T 5-gram Corpus (Brants and Franz, 2006 ).\n\n", "discourse_tags": ["Narrative_cite", "Reflection", "Transition", "Transition", "Transition", "Narrative_cite", "Reflection", "Transition", "Narrative_cite", "Transition", "Narrative_cite", "Narrative_cite", "Transition", "Reflection", "Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 18, "token_end": 30, "char_start": 88, "char_end": 132, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Culotta et al. (2007)": "6618210"}}}, {"token_start": 172, "token_end": 183, "char_start": 934, "char_end": 979, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Rahman and Ng (2009)": "7177672"}}}, {"token_start": 279, "token_end": 337, "char_start": 1505, "char_end": 1712, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ponzetto and Strube, 2006)": "1212389", "(Bengtson and Roth, 2008)": "8179642", "(Lee et al., 2011)": "618047", "(Rahman and Ng, 2011)": "2994838"}}}, {"token_start": 380, "token_end": 398, "char_start": 1955, "char_end": 2040, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Dagan and Itai (1990)": "9210393"}}}, {"token_start": 405, "token_end": 431, "char_start": 2070, "char_end": 2180, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Kehler et al. (2004)": "11647641", "Yang et al. (2005)": "2536406"}}}, {"token_start": 548, "token_end": 566, "char_start": 2836, "char_end": 2889, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Brants and Franz, 2006": null}}}]}
{"id": "14014145_2", "paragraph": "[BOS] In parsing, Pereira and Schabes (1992) proposed an extended inside-outside algorithm that infers the parameters of a stochastic CFG from a partially parsed treebank.\n[BOS] It uses partial bracketing information to improve parsing performance, but it is specific to constituency parsing, and its computational complexity makes it impractical for massive natural annotations in web text.\n[BOS] There are also work making use of word co-occurrence statistics collected in raw text or Internet n-grams to improve parsing performance (Nakov and Hearst, 2005; Pitler et al., 2010; Zhou et al., 2011; Bansal and Klein, 2011) .\n[BOS] When enriching the related work during writing, we found a work on dependency parsing (Spitkovsky et al., 2010) who utilized parsing constraints derived from hypertext annotations to improve the unsupervised dependency grammar induction.\n[BOS] Compared with their method, the strategy we proposed is formal and universal, the discriminative learning strategy and the quantitative measurement of fuzzy knowledge enable more effective utilization of the natural annotation on the Internet when adapted to parsing.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Narrative_cite", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 75, "char_start": 6, "char_end": 391, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Pereira and Schabes (1992)": "696805"}, "Reference": {}}}, {"token_start": 90, "token_end": 132, "char_start": 475, "char_end": 623, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Nakov and Hearst, 2005;": "552136", "Pitler et al., 2010;": "2076339", "Zhou et al., 2011;": "2169198", "Bansal and Klein, 2011)": "1391425"}}}, {"token_start": 148, "token_end": 160, "char_start": 699, "char_end": 743, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Spitkovsky et al., 2010)": "13184257"}}}]}
{"id": "14014145_1", "paragraph": "[BOS] In recent years, much work has been devoted to the improvement of word segmentation in a variety of ways.\n[BOS] Typical approaches include the introduction of global training or complicated features (Zhang and Clark, 2007; Zhang and Clark, 2010) , the investigation of word internal structures (Zhao, 2009; Li, 2011) , the adjustment or adaptation of word segmentation standards (Wu, 2003; Gao et al., 2004; Jiang et al., 2009) , the integrated solution of segmentation and related tasks such as part-of-speech tagging and parsing (Zhou and Su, 2003; Zhang et al., 2003; Fung et al., 2004; Goldberg and Tsarfaty, 2008) , and the strategies of hybrid or stacked modeling (Nakagawa and Uchimoto, 2007; Kruengkrai et al., 2009; Wang et al., 2010; Sun, 2011b) .\n\n", "discourse_tags": ["Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 30, "token_end": 48, "char_start": 165, "char_end": 251, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhang and Clark, 2007;": "2687347", "Zhang and Clark, 2010)": "2712419"}}}, {"token_start": 50, "token_end": 64, "char_start": 258, "char_end": 322, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhao, 2009;": "6204419", "Li, 2011)": "15126078"}}}, {"token_start": 66, "token_end": 92, "char_start": 329, "char_end": 433, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wu, 2003;": null, "Gao et al., 2004;": "15121618", "Jiang et al., 2009)": "15016194"}}}, {"token_start": 94, "token_end": 141, "char_start": 440, "char_end": 624, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhou and Su, 2003;": null, "Zhang et al., 2003;": null, "Fung et al., 2004;": "11577696", "Goldberg and Tsarfaty, 2008)": "14857072"}}}, {"token_start": 144, "token_end": 182, "char_start": 635, "char_end": 761, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Nakagawa and Uchimoto, 2007;": "11062835", "Kruengkrai et al., 2009;": "769547", "Wang et al., 2010;": "3204349", "Sun, 2011b)": "429415"}}}]}
{"id": "14014145_0", "paragraph": "[BOS] Lots of efforts have been devoted to semisupervised methods in sequence labeling and word segmentation (Xu et al., 2008; Suzuki and Isozaki, 2008; Haffari and Sarkar, 2008; Tomanek and Hahn, 2009; Wang et al., 2011) .\n[BOS] A semisupervised method tries to find an optimal hyperplane of both annotated data and raw data, thus to result in a model with better coverage and higher accuracy.\n[BOS] Researchers have also investigated unsupervised methods in word segmentation (Zhao and Kit, 2008; Johnson and Goldwater, 2009; Mochihashi et al., 2009; Hewlett and Cohen, 2011 ).\n[BOS] An unsupervised method mines the latent distribution regularity in the raw text, and automatically induces word segmentation knowledge from it.\n[BOS] Our method also needs large amounts of external data, but it aims to leverage the knowledge in the fuzzy and sparse annotations.\n[BOS] It is fundamentally different from semi-supervised and unsupervised methods in that we aimed to excavate a totally different kind of knowledge, the natural annotations implied by the structural information in web text.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Narrative_cite", "Transition", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 13, "token_end": 59, "char_start": 69, "char_end": 221, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Xu et al., 2008;": "12478551", "Suzuki and Isozaki, 2008;": "647664", "Tomanek and Hahn, 2009;": null, "Wang et al., 2011)": "13251133"}}}, {"token_start": 100, "token_end": 133, "char_start": 460, "char_end": 576, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhao and Kit, 2008;": "10102597", "Johnson and Goldwater, 2009;": "7573383", "Mochihashi et al., 2009;": "10623282", "Hewlett and Cohen, 2011": "548241"}}}]}
{"id": "1423962_3", "paragraph": "[BOS] Finally, more related to our model, Kim et al. (2017) applied their structured attention networks to a Natural Language Inference (NLI) task for learning dependency-like structures.\n[BOS] They showed that pre-training their model by a parsing dataset did not improve accuracy on the NLI task.\n[BOS] By contrast, our experiments show that such a parsing dataset can be effectively used to improve translation accuracy by varying the size of the dataset and by avoiding strong overfitting.\n[BOS] Moreover, our translation examples show the concrete benefit of learning task-oriented latent graph structures.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 10, "token_end": 62, "char_start": 42, "char_end": 298, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kim et al. (2017)": "7942973"}, "Reference": {}}}]}
{"id": "1423962_2", "paragraph": "[BOS] As for the learning of latent syntactic structure, there are several studies on learning task-oriented syntactic structures.\n[BOS] Yogatama et al. (2017) used a reinforcement learning method on shift-reduce action sequences to learn task-oriented binary constituency trees.\n[BOS] They have shown that the learned trees do not necessarily highly correlate with the human-annotated treebanks, which is consistent with our experimental results.\n[BOS] Socher et al. (2011) used a recursive autoencoder model to greedily construct a binary constituency tree for each sentence.\n[BOS] The autoencoder objective works as a regularization term for sentiment classification tasks.\n[BOS] Prior to these deep learning approaches, Wu (1997) presented a method for bilingual parsing.\n[BOS] One of the characteristics of our model is directly using the soft connections of the graph edges with the real-valued weights, whereas all of the above-mentioned methods use one best structure for each sentence.\n[BOS] Our model is based on dependency structures, and it is a promising future direction to jointly learn dependency and constituency structures in a task-oriented fashion.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 24, "token_end": 84, "char_start": 137, "char_end": 447, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Yogatama et al. (2017)": "423406"}, "Reference": {}}}, {"token_start": 85, "token_end": 130, "char_start": 454, "char_end": 676, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Socher et al. (2011)": "3116311"}, "Reference": {}}}, {"token_start": 131, "token_end": 190, "char_start": 683, "char_end": 994, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wu (1997)": "912349"}, "Reference": {}}}]}
{"id": "1423962_1", "paragraph": "[BOS] Our model learns latent graph structures in a source-side language.\n[BOS] Eriguchi et al. (2017) have proposed a model which learns to parse and translate by using automatically-parsed data.\n[BOS] Thus, it is also an interesting direction to learn latent structures in a target-side language.\n\n", "discourse_tags": ["Reflection", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 16, "token_end": 43, "char_start": 80, "char_end": 196, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "1423962_0", "paragraph": "[BOS] While initial studies on NMT treat each sentence as a sequence of words (Bahdanau et al., 2015; Sutskever et al., 2014) , researchers have recently started investigating into the use of syntactic structures in NMT models (Bastings et al., 2017; Chen et al., 2017; Eriguchi et al., 2016a Eriguchi et al., ,b, 2017 Li et al., 2017; Stahlberg et al., 2016; Yang et al., 2017) .\n[BOS] In particular, Eriguchi et al. (2016b) introduced a tree-to-sequence NMT model by building a tree-structured encoder on top of a standard sequential encoder, which motivated the use of the dependency composition vectors in our proposed model.\n[BOS] Prior to the advent of NMT, the syntactic structures had been successfully used in statistical machine translation systems (Neubig and Duh, 2014; Yamada and Knight, 2001 ).\n[BOS] These syntax-based approaches are pipelined; a syntactic parser is first trained by supervised learning using a treebank such as the WSJ dataset, and then the parser is used to automatically extract syntactic information for machine translation.\n[BOS] They rely on the output from the parser, and therefore parsing errors are propagated through the whole systems.\n[BOS] By contrast, our model allows the parser to be adapted to the translation task, thereby providing a first step towards addressing ambiguous syntactic and semantic problems, such as domain-specific selectional preference and PP attachments, in a task-oriented fashion.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Multi_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 13, "token_end": 35, "char_start": 60, "char_end": 125, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bahdanau et al., 2015;": "11212020", "Sutskever et al., 2014)": "7961699"}}}, {"token_start": 45, "token_end": 111, "char_start": 192, "char_end": 378, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Chen et al., 2017;": "3504277", "Li et al., 2017;": "12637374", "Stahlberg et al., 2016;": "11642690", "Yang et al., 2017)": "7177285"}}}, {"token_start": 113, "token_end": 165, "char_start": 387, "char_end": 629, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 174, "token_end": 270, "char_start": 664, "char_end": 1178, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Neubig and Duh, 2014;": "8004243", "Yamada and Knight, 2001": null}, "Reference": {}}}]}
{"id": "13756572_3", "paragraph": "[BOS] Finally, we note independent but closely related work by Zhao et al. (2018) , published concurrently with this paper.\n[BOS] In their work, Zhao et al. (2018) also propose a Winograd schema-like test for gender bias in coreference resolution systems (called \"WinoBias\").\n[BOS] Though similar in appearance, these two efforts have notable differences in substance and emphasis.\n[BOS] The contribution of this work is focused primarily on schema construction and validation, with extensive analysis of observed system bias, revealing its correlation with biases present in real-world and textual statistics; by contrast, Zhao et al. (2018) present methods of debiasing existing systems, showing that simple approaches such as augmenting training data with gender-swapped examples or directly editing noun phrase counts in the B&L resource are effective at reducing system bias, as measured by the schemas.\n[BOS] Complementary differences exist between the two schema formulations: Winogender schemas (this work) include gender-neutral pronouns, are syntactically diverse, and are human-validated; WinoBias includes (and delineates) sentences resolvable from syntax alone; a Winogender schema has one occupational mention and one \"other participant\" mention; WinoBias has two occupational mentions.\n[BOS] Due to these differences, we encourage future evaluations to make use of both datasets.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Transition", "Single_summ", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 62, "char_start": 6, "char_end": 272, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhao et al. (2018)": "4952494"}, "Reference": {}}}, {"token_start": 122, "token_end": 179, "char_start": 624, "char_end": 908, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhao et al. (2018)": "4952494"}, "Reference": {}}}]}
{"id": "13756572_2", "paragraph": "[BOS] Prior work also analyzes social and gender stereotyping in existing NLP and vision datasets (van Miltenburg, 2016; Rudinger et al., 2017) .\n[BOS] Tatman (2017) investigates the impact of gender and dialect on deployed speech recognition systems, while introduce a method to reduce amplification effects on models trained with gender-biased datasets.\n[BOS] Koolen and van Cranenburgh (2017) examine the relationship between author gender and text attributes, noting the potential for researcher interpretation bias in such studies.\n[BOS] Both Larson (2017) and Koolen and van Cranenburgh (2017) offer guidelines to NLP researchers and computational social scientists who wish to predict gender as a variable.\n[BOS] Hovy and Spruit (2016) introduce a helpful set of terminology for identifying and categorizing types of bias that manifest in AI systems, including overgeneralization, which we observe in our work here.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ", "Multi_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 14, "token_end": 35, "char_start": 74, "char_end": 143, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(van Miltenburg, 2016;": "18279904", "Rudinger et al., 2017)": "5310359"}}}, {"token_start": 37, "token_end": 73, "char_start": 152, "char_end": 355, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Tatman (2017)": "13997424"}, "Reference": {}}}, {"token_start": 74, "token_end": 106, "char_start": 362, "char_end": 536, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Koolen and van Cranenburgh (2017)": "11405869"}, "Reference": {}}}, {"token_start": 107, "token_end": 144, "char_start": 543, "char_end": 713, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Larson (2017)": "4117737", "Koolen and van Cranenburgh (2017)": "11405869"}, "Reference": {}}}, {"token_start": 145, "token_end": 186, "char_start": 720, "char_end": 922, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hovy and Spruit (2016)": "1083991"}, "Reference": {}}}]}
{"id": "13756572_1", "paragraph": "[BOS] how they capture implicit human biases in modern (Caliskan et al., 2017) and historical (Garg et al., 2018) text, and methods for debiasing them (Bolukbasi et al., 2016) .\n[BOS] Further work on debiasing models with adversarial learning is explored by Beutel et al. (2017) and Zhang et al. (2018) .\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 9, "token_end": 20, "char_start": 48, "char_end": 78, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Caliskan et al., 2017)": "23163324"}}}, {"token_start": 21, "token_end": 32, "char_start": 83, "char_end": 118, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Garg et al., 2018)": "4930886"}}}, {"token_start": 34, "token_end": 51, "char_start": 124, "char_end": 175, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bolukbasi et al., 2016)": "1704893"}}}, {"token_start": 61, "token_end": 84, "char_start": 222, "char_end": 302, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Beutel et al. (2017)": "24990444", "Zhang et al. (2018)": "9424845"}}}]}
{"id": "13756572_0", "paragraph": "[BOS] Here we give a brief (and non-exhaustive) overview of prior work on gender bias in NLP systems and datasets.\n[BOS] A number of papers explore (gender) bias in English word embeddings:\n\n", "discourse_tags": ["Transition", "Transition"], "span_citation_mapping": []}
{"id": "15399483_0", "paragraph": "[BOS] In the past, a significant number of techniques have been presented to reduce the hierarchical rule table.\n[BOS] He et al. (2009) most entries of the rule table by using the constraint that rules of the target-side are well-formed (WF) dependency structure, but this filtering led to degradation in translation performance.\n[BOS] They obtained improvements by adding an additional dependency language model.\n[BOS] The basic difference of our method from (Shen et al., 2008 ) is that we keep rules that both sides should be relaxed-wellformed dependency structure, not just the target side.\n[BOS] Besides, our system complexity is not increased because no additional language model is introduced.\n[BOS] The feature of head word trigger which we apply to the log-linear model is motivated by the trigger-based approach (Hasan and Ney, 2009 ).\n[BOS] Hasan and Ney (2009) introduced a second word to trigger the target word without considering any linguistic information.\n[BOS] Furthermore, since the second word can come from any part of the sentence, there may be a prohibitively large number of parameters involved.\n[BOS] Besides, He et al. (2008) built a maximum entropy model which combines rich context information for selecting translation rules during decoding.\n[BOS] However, as the size of the corpus increases, the maximum entropy model will become larger.\n[BOS] Similarly, In (Shen et al., 2009 ), context language model is proposed for better rule selection.\n[BOS] Taking the dependency edge as condition, our approach is very different from previous approaches of exploring context information.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Narrative_cite", "Reflection", "Narrative_cite", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 22, "token_end": 78, "char_start": 119, "char_end": 413, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"He et al. (2009)": "8585933"}, "Reference": {}}}, {"token_start": 79, "token_end": 93, "char_start": 420, "char_end": 478, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Shen et al., 2008": "832217"}}}, {"token_start": 154, "token_end": 166, "char_start": 800, "char_end": 843, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hasan and Ney, 2009": "12872164"}}}, {"token_start": 169, "token_end": 220, "char_start": 853, "char_end": 1120, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 221, "token_end": 266, "char_start": 1127, "char_end": 1369, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"He et al. (2008)": "586283"}, "Reference": {}}}, {"token_start": 269, "token_end": 289, "char_start": 1387, "char_end": 1473, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Shen et al., 2009": "17001645"}}}]}
{"id": "13320571_2", "paragraph": "[BOS] Another technique to reduce features is dimension reduction by matrix or tensor factorization (Argyriou et al., 2007; Lei et al., 2014) , but typically applied to supervised learning.\n[BOS] In contrast, we use word embeddings trained from unlabeled or automatically labeled corpora, bringing the aspects of semi-supervised learning or self-training.\n\n", "discourse_tags": ["Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 11, "token_end": 32, "char_start": 69, "char_end": 141, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Argyriou et al., 2007;": null, "Lei et al., 2014)": "15307333"}}}]}
{"id": "13320571_1", "paragraph": "[BOS] Another stream of research is to use word embeddings in whole neural network architectures (Collobert et al., 2011; Socher et al., 2013; Chen and Manning, 2014; Weiss et al., 2015; Dyer et al., 2015; Watanabe and Sumita, 2015 has contributed to the power of neural based approaches.\n[BOS] In this work, we conjecture that the power may partly come from the low-dimensionality of word embeddings, and this advantage can be transferred to traditional feature based systems.\n[BOS] Our experiments support this conjecture, and we expect the proposed method to help more mature, proven-towork existing systems.\n[BOS] Machine learning techniques have been proposed for reducing model size and imposing feature sparsity (Suzuki et al., 2011; Yogatama and Smith, 2014) .\n[BOS] Compared to these methods, our approach is simple, without extra twists of objective functions or learning algorithms.\n[BOS] More importantly, by using word embeddings to reduce lexical features, we explicitly exploit the inherited syntactic and semantic similarities between words.\n\n", "discourse_tags": ["Narrative_cite", "Reflection", "Reflection", "Narrative_cite", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 13, "token_end": 61, "char_start": 68, "char_end": 231, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Collobert et al., 2011;": "351666", "Socher et al., 2013;": "14687186", "Chen and Manning, 2014;": null, "Weiss et al., 2015;": "15213991", "Dyer et al., 2015;": "6278207", "Watanabe and Sumita, 2015": null}}}, {"token_start": 143, "token_end": 161, "char_start": 702, "char_end": 766, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Suzuki et al., 2011;": "16437269", "Yogatama and Smith, 2014)": "78441"}}}]}
{"id": "13320571_0", "paragraph": "[BOS] A lot of recent work has been done on training word vectors (Mnih and Hinton, 2009; Mikolov et al., 2013; Lebret and Collobert, 2014; Pennington et al., 2014) , and utilizing word vectors in various NLP tasks (Turian et al., 2010; Andreas and Klein, 2014; Bansal et al., 2014) .\n[BOS] The common approach (Turian et al., 2010; Koo et al., 2008; Bansal et al., 2014) is to use vector representations in new features, added to (near) state-of-the-art systems, and make improvement.\n[BOS] As a result, the feature space gets even larger.\n[BOS] We instead propose to reduce lexical features by word embeddings.\n[BOS] To our own surprise, though the feature space gets much smaller, the resulted system performs better.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Transition", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 12, "token_end": 48, "char_start": 53, "char_end": 164, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mnih and Hinton, 2009;": "10097073", "Mikolov et al., 2013;": "16447573", "Lebret and Collobert, 2014;": null, "Pennington et al., 2014)": "1957433"}}}, {"token_start": 50, "token_end": 82, "char_start": 171, "char_end": 282, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Turian et al., 2010;": "629094", "Andreas and Klein, 2014;": "1356465", "Bansal et al., 2014)": "7803700"}}}, {"token_start": 84, "token_end": 112, "char_start": 291, "char_end": 371, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Turian et al., 2010;": "629094", "Koo et al., 2008;": "1916754", "Bansal et al., 2014)": "7803700"}}}]}
{"id": "13267885_5", "paragraph": "[BOS] To the best of our knowledge, there have been no joint models of morphological and dependency analysis that use large-scale lexical knowledge which includes selectional preferences.\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "13267885_4", "paragraph": "[BOS] For Japanese, Morita et al. (2015) proposed a morphological analyzer that jointly performs segmentation and POS tagging using recurrent neural network language models, but does not perform dependency parsing.\n[BOS] We employ this morphological analyzer, JUMAN++ 4 , as a pre-processor to generate word lattice (described in Section 4.1).\n[BOS] Kawahara and Kurohashi (2006) proposed a probabilistic model for Japanese dependency parsing and PAS analysis based on case frames automatically compiled from a large raw corpus, which are also used as a source of selectional preferences in our model (described in Section 3.1).\n[BOS] Kudo and Matsumoto (2002) , Sassano (2004) , Iwatate (2012) and Yoshinaga and Kitsuregawa (2014) proposed supervised models for Japanese dependency parsing without using external knowledge sources.\n[BOS] These models need a 1-best output of segmentation and POS tagging as an input, and are not a joint model of morphological analysis and dependency parsing.\n[BOS] We adopt KNP 5 and CaboCha 6 as baseline dependency parsers, which are implementations of Kawahara and Kurohashi (2006) and Sassano (2004) , respectively.\n[BOS] 7 Tawara et al. (2015) proposed a joint model for Japanese morphological analysis and dependency parsing without lexical knowledge.\n[BOS] However, they failed to achieve significant improvements over conventional pipeline methods.\n\n", "discourse_tags": ["Single_summ", "Reflection", "Single_summ", "Multi_summ", "Multi_summ", "Narrative_cite", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 38, "char_start": 6, "char_end": 214, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Morita et al. (2015)": "15043144"}, "Reference": {}}}, {"token_start": 70, "token_end": 116, "char_start": 350, "char_end": 600, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kawahara and Kurohashi (2006)": "62172538"}, "Reference": {}}}, {"token_start": 126, "token_end": 205, "char_start": 635, "char_end": 993, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kudo and Matsumoto (2002)": "9404516", "Sassano (2004)": "15513499", "Iwatate (2012)": null, "Yoshinaga and Kitsuregawa (2014)": "6648997"}, "Reference": {}}}, {"token_start": 208, "token_end": 242, "char_start": 1009, "char_end": 1138, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Kawahara and Kurohashi (2006)": "62172538", "Sassano (2004)": "15513499"}}}, {"token_start": 247, "token_end": 285, "char_start": 1163, "char_end": 1391, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "13267885_3", "paragraph": "[BOS] As dependency parsing models using lexical knowledge, there have been semi-supervised approaches that use knowledge of word classes, lexical preferences or selectional preferences acquired from raw corpora (e.g., (van Noord, 2007; Koo et al., 2008; Chen et al., 2009; Zhou et al., 2011; Bansal and Klein, 2011) ).\n[BOS] However, these dependency parsing models cannot be applied to joint morphological and dependency analysis.\n\n", "discourse_tags": ["Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 21, "token_end": 76, "char_start": 125, "char_end": 316, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(van Noord, 2007;": "144544", "Koo et al., 2008;": "1916754", "Chen et al., 2009;": "14728649", "Zhou et al., 2011;": "2169198", "Bansal and Klein, 2011)": "1391425"}}}]}
{"id": "13267885_2", "paragraph": "[BOS] As a different method from lattice parsing, Qian and Liu (2012) trained separate models for Chinese word segmentation, POS tagging, and constituency parsing.\n[BOS] They proposed a unified decoding algorithm that combines the scores from these three models.\n[BOS] This is a purely supervised method that does not use lexical knowledge.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 63, "char_start": 6, "char_end": 340, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Qian and Liu (2012)": "17830435"}, "Reference": {}}}]}
{"id": "13267885_1", "paragraph": "[BOS] Lattice parsing methods have been proposed for Hebrew and Arabic (Goldberg and Tsarfaty, 2008; Goldberg et al., 2009; Green and Manning, 2010; Goldberg and Elhadad, 2011) .\n[BOS] These methods first generate a word lattice and then apply PCFG parsing to the word lattice.\n[BOS] Starting with a word lattice, the methods of Wang et al. (2013) and Zhang et al. (2015) select the best parse using dual decomposition and the randomized greedy algorithm, respectively.\n[BOS] Of these methods, Goldberg et al. (2009) incorporated an external morphological lexicon, which does not provide selectional preferences.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Multi_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 9, "token_end": 46, "char_start": 53, "char_end": 176, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Goldberg and Tsarfaty, 2008;": "14857072", "Goldberg et al., 2009;": "11183249", "Green and Manning, 2010;": "5133519", "Goldberg and Elhadad, 2011)": "10528973"}}}, {"token_start": 67, "token_end": 106, "char_start": 284, "char_end": 469, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhang et al. (2015)": "6090412"}, "Reference": {"Wang et al. (2013)": "1716683"}}}, {"token_start": 107, "token_end": 132, "char_start": 476, "char_end": 612, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Goldberg et al. (2009)": "11183249"}, "Reference": {}}}]}
{"id": "13267885_0", "paragraph": "[BOS] Some variants of transition-based parsing methods have been proposed for joint POS tagging and parsing (Bohnet and Nivre, 2012; Bohnet et al., 2013; Wang and Xue, 2014) and joint Chinese word segmentation, POS tagging, and dependency parsing (Hatori et al., 2012; Zhang et al., 2014) .\n[BOS] As an external knowledge source, Hatori et al. (2012) used a word dictionary extracted mainly from Wikipedia, but it did not provide lexical knowledge for resolving syntactic ambiguities.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ"], "span_citation_mapping": [{"token_start": 15, "token_end": 42, "char_start": 85, "char_end": 174, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bohnet and Nivre, 2012;": "1500270", "Bohnet et al., 2013;": "1992679", "Wang and Xue, 2014)": "16636082"}}}, {"token_start": 44, "token_end": 70, "char_start": 185, "char_end": 289, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hatori et al., 2012;": "10011032", "Zhang et al., 2014)": "5859332"}}}, {"token_start": 72, "token_end": 108, "char_start": 298, "char_end": 485, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hatori et al. (2012)": "10011032"}, "Reference": {}}}]}
{"id": "13747961_0", "paragraph": "[BOS] Our work focuses on modeling pairwise score functions  and is related to previous approaches in the two following aspects.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "12854977_4", "paragraph": "[BOS] 3 Hedge detection with average perceptron\n\n", "discourse_tags": ["Other"], "span_citation_mapping": []}
{"id": "12854977_3", "paragraph": "[BOS] Morante and Daelemans (2009) present their research on identifying hedge cues and their scopes.\n[BOS] Their system consists of several classifiers and works in two phases, first identifying the hedge cues in a sentence and secondly finding the full scope for each hedge cue.\n[BOS] In the first phase, they use IGTREE algorithm to train a classifier with 3 categories.\n[BOS] In the second phase, three different classifiers are trained to find the first token and last token of in-sentence scope and finally combined into a meta classifier.\n[BOS] The experiments shown that their system achieves an F1 of nearly 0.85 of identifying hedge cues in the abstracts sub corpus, while nearly 0.79 of finding the scopes with predicted hedge cues.\n[BOS] More experiments could be found in their paper (Morante and Daelemans, 2009) .\n[BOS] They also provide a detail statistics on hedge cues in BioScope corpus 2 .\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 182, "char_start": 6, "char_end": 907, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Morante and Daelemans, 2009)": "779066"}, "Reference": {}}}]}
{"id": "12854977_2", "paragraph": "[BOS] In Ganter and Strube (2009) , the same task on Wikipedia is presented.\n[BOS] In their system, score of a sentence is defined as a normalized tangent value of the sum of scores over all words in the sentence.\n[BOS] Shallow linguistic features are introduced in their experiments.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 58, "char_start": 6, "char_end": 284, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ganter and Strube (2009)": "15221179"}, "Reference": {}}}]}
{"id": "12854977_1", "paragraph": "[BOS] For speculative sentences detection, Medlock and Briscoe (2007) report their approach based on weakly supervised learning.\n[BOS] In their method, a statistical model is initially derived from a seed corpus, and then iteratively modified by augmenting the training dataset with unlabeled samples according the posterior probability.\n[BOS] They only employ bag-of-words features.\n[BOS] On the public biomedical dataset 1 , their experiments achieve the performance of 0.76 in BEP (break even point).\n[BOS] Although they also introduced more linguistic features, such as part-of-speech (POS), lemma and bigram (Medlock, 2008) , there are no significant improvements.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 2, "token_end": 96, "char_start": 6, "char_end": 503, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Medlock and Briscoe (2007)": "18343028"}, "Reference": {}}}, {"token_start": 116, "token_end": 126, "char_start": 596, "char_end": 628, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Medlock, 2008)": "2976296"}}}]}
{"id": "12854977_0", "paragraph": "[BOS] Although the concept of hedge information has been introduced in linguistic community for a long time, researches on automatic hedge detection emerged from machine learning or compu-tational linguistic perspective in recent years.\n[BOS] In this section, we give a brief review on the related works.\n\n", "discourse_tags": ["Transition", "Transition"], "span_citation_mapping": []}
{"id": "14351566_1", "paragraph": "[BOS] Apart from the datasets adopted in our experiments, the CNN/Daily Mail (Hermann et al., 2015) has been used for the task of machine reading formalized as a problem of text extraction from k (u k ) patterns on T6 of the Dialog bAbI dataset a source conditioned on a given question.\n[BOS] However, as pointed out in (Chen et al., 2016) , this dataset not only is noisy but also requires little reasoning and inference, which is evidenced by a manual analysis of a randomly selected subset of the questions, showing that only 2% of the examples call for multi-sentence inference.\n[BOS] Richardson et al. (2013) constructed an open-domain reading comprehension task, named MCTest.\n[BOS] Although this corpus demands various reasoning capabilities from multiple sentences, its rather limited size (660 paragraphs, each associated with 4 questions) renders training statistical models infeasible (Chen et al., 2016 ).\n[BOS] Children's Book Test (CBT) (Hill et al., 2015) was designed to measure the ability of models to exploit a wide range of linguistic context.\n[BOS] Despite the claim in (Sukhbaatar et al., 2015) that increasing the number of hops is crucial for the performance improvements on some tasks, which can be seen as enabling MemN2N to accommodate more supporting facts, making such performance boost particularly more pronounced on those tasks requiring complex reasoning, Hill et al. (2015) admittedly reported little improvement in performance by stacking more hops and chose a single-hop MemN2N.\n[BOS] This suggests that the ne- cessity of multi-sentence based reasoning on this dataset is not mandatory.\n[BOS] In the future, we plan to investigate into larger dialog datasets such as (Lowe et al., 2015) .\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Single_summ", "Narrative_cite", "Single_summ", "Narrative_cite", "Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 12, "token_end": 25, "char_start": 62, "char_end": 99, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hermann et al., 2015)": "6203757"}}}, {"token_start": 66, "token_end": 80, "char_start": 293, "char_end": 339, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chen et al., 2016)": "6360322"}}}, {"token_start": 127, "token_end": 147, "char_start": 589, "char_end": 682, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Richardson et al. (2013)": "2100831"}, "Reference": {}}}, {"token_start": 176, "token_end": 186, "char_start": 866, "char_end": 914, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Chen et al., 2016": "6360322"}, "Reference": {}}}, {"token_start": 189, "token_end": 222, "char_start": 924, "char_end": 1063, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Hill et al., 2015)": "14915449"}, "Reference": {}}}, {"token_start": 223, "token_end": 238, "char_start": 1070, "char_end": 1116, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sukhbaatar et al., 2015)": "1399322"}}}, {"token_start": 281, "token_end": 291, "char_start": 1370, "char_end": 1407, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hill et al. (2015)": "14915449"}, "Reference": {}}}, {"token_start": 346, "token_end": 359, "char_start": 1680, "char_end": 1723, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lowe et al., 2015)": "8379583"}}}]}
{"id": "14351566_0", "paragraph": "[BOS] This section starts with an introduction of the primary elements of MemN2N.\n[BOS] Then, we review two key elements relevant to this work, namely shortcut connections in neural networks in and memory dynamics in such models.\n\n", "discourse_tags": ["Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "14342526_4", "paragraph": "[BOS] Neural network based models are trained as language models (Vaswani et al., 2013; Auli and Gao, 2014) , translation models or joint language and translation models (Auli et al., 2013; Devlin et al., 2014) .\n[BOS] Liu et al. (2013) also introduced word embedding for source and target side of translation rule as local features.\n[BOS] In this paper we focus on enhancing the expressive power of the modeling, which is independent of the research of enhancing translation system with new designed features.\n[BOS] We believe additional improvement could be achieved by incorporating more features into our framework.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 9, "token_end": 28, "char_start": 49, "char_end": 107, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vaswani et al., 2013;": "3065236", "Auli and Gao, 2014)": "5467830"}}}, {"token_start": 29, "token_end": 54, "char_start": 110, "char_end": 210, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Auli et al., 2013;": "5552894", "Devlin et al., 2014)": "7417943"}}}, {"token_start": 56, "token_end": 79, "char_start": 219, "char_end": 333, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Liu et al. (2013)": "1163553"}, "Reference": {}}}]}
{"id": "14342526_3", "paragraph": "[BOS] The third line of research attempted to add non-linear features/components into the loglinear learning framework.\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "14342526_2", "paragraph": "[BOS] The second line of research attempted to use non-linear models instead of log-linear models, which is most similar in spirit with our work.\n[BOS] Duh and Kirchhoff (2008) used the boosting method to combine several results of MERT and achieved improvement in a re-ranking setting.\n[BOS] Liu et al. (2013) proposed an additive neural network which employed a two-layer neural network for embedding-based features.\n[BOS] To avoid local minimum, they still rely on a pre-training and posttraining from MERT or PRO.\n[BOS] Comparing to these efforts, our proposed method takes a further step that it is integrated with iterative training, instead of re-ranking, and works without the help of any pre-trained linear models.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 32, "token_end": 62, "char_start": 152, "char_end": 286, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Duh and Kirchhoff (2008)": "14707202"}, "Reference": {}}}, {"token_start": 63, "token_end": 112, "char_start": 293, "char_end": 517, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Liu et al. (2013)": "1163553"}, "Reference": {}}}]}
{"id": "14342526_1", "paragraph": "[BOS] The first line of research attempted to reinterpret original features via feature transformation or additional learning.\n[BOS] For example, Maskey and Zhou (2012) use a deep belief network to learn representations of the phrase translation and lexical translation probability features.\n[BOS] Clark et al. (2014) used discretization to transform realvalued dense features into a set of binary indicator features.\n[BOS] Lu et al. (2014) learned new features using a semi-supervised deep auto encoder.\n[BOS] These work focus on the explicit representation of the features and usually employ extra learning procedure.\n[BOS] Our proposed method only take the original feature with no transformation as input.\n[BOS] Feature transformation or combination are performed implicitly during the training of the network and integrated with the optimization of translation quality.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 25, "token_end": 50, "char_start": 146, "char_end": 291, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Maskey and Zhou (2012)": "284436"}, "Reference": {}}}, {"token_start": 51, "token_end": 74, "char_start": 298, "char_end": 417, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Clark et al. (2014)": "4994804"}, "Reference": {}}}, {"token_start": 75, "token_end": 112, "char_start": 424, "char_end": 619, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Lu et al. (2014)": "5709441"}, "Reference": {}}}]}
{"id": "14342526_0", "paragraph": "[BOS] Many research has been attempting to bring nonlinearity into the training of SMT.\n[BOS] These efforts could be roughly divided into the following three categories.\n\n", "discourse_tags": ["Transition", "Transition"], "span_citation_mapping": []}
{"id": "12637374_1", "paragraph": "[BOS] Sennrich and Haddow (2016) define a few linguistically motivated features that are attached to each individual words.\n[BOS] Their features include lemmas, subword tags, POS tags, dependency labels, etc.\n[BOS] They concatenate feature embeddings with word embeddings and feed the concatenated embeddings into the NMT encoder.\n[BOS] On the contrast, we do not specify any feature, but let the model implicitly learn useful information from the structural label sequence.\n[BOS] Shi et al. (2016) design a few experiments to investigate if the NMT system without external linguistic input is capable of learning syntactic information on the source-side as a by-product of training.\n[BOS] However, their work is not focusing on improving NMT with linguistic input.\n[BOS] Moreover, we analyze what syntax is disrespected in translation from several new perspectives.\n[BOS] Garca-Martnez et al. (2016) generalize NMT outputs as lemmas and morphological factors in order to alleviate the issues of large vocabulary and out-of-vocabulary word translation.\n[BOS] The lemmas and corresponding factors are then used to generate final words in target language.\n[BOS] Though they use linguistic input on the target side, they are limited to the word level features.\n[BOS] Phrase level, or even sentence level linguistic features are harder to obtain for a generation task such as machine translation, since this would require incremental parsing of the hypotheses at test time.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Reflection", "Single_summ", "Single_summ", "Reflection", "Single_summ", "Single_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 2, "token_end": 64, "char_start": 6, "char_end": 330, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 91, "token_end": 148, "char_start": 481, "char_end": 765, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Shi et al. (2016)": "7197724"}, "Reference": {}}}, {"token_start": 167, "token_end": 242, "char_start": 873, "char_end": 1257, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Garc\u00eda-Mart\u00ednez et al. (2016)": "6949337"}, "Reference": {}}}]}
{"id": "12637374_0", "paragraph": "[BOS] While there has been substantial work on linguistically motivated SMT, approaches that leverage syntax for NMT start to shed light very recently.\n[BOS] Generally speaking, NMT can provide a flexible mechanism for adding linguistic knowledge, thanks to its strong capability of automatically learning feature representations.\n[BOS] Eriguchi et al. (2016) propose a tree-tosequence model that learns annotation vectors not only for terminal words, but also for non-terminal nodes.\n[BOS] They also allow the attention model to align target words to non-terminal nodes.\n[BOS] Our approach is similar to theirs by using source-side phrase parse tree.\n[BOS] However, our Mixed RNN system, for example, incorporates syntax information by learning annotation vectors of syntactic labels and words stitchingly, but is still a sequenceto-sequence model, with no extra parameters and with less increased training time.\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 57, "token_end": 108, "char_start": 337, "char_end": 571, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Eriguchi et al. (2016)": "12851711"}, "Reference": {}}}]}
{"id": "12635978_0", "paragraph": "[BOS] Sentence fusion is the general label applied to tasks which take multiple sentences as input to produce a single output sentence.\n[BOS] Barzilay & McKeown (Barzilay et al., 1999; Barzilay and McKeown, 2005) first introduced fusion in the context of multidocument summarization as a way to better capture the information in a cluster of related sentences than just using the centroid.\n[BOS] The fusion task has since expanded to include other forms of sentence combination, such as the merging of overlapping sentences in a multidocument context (Marsi and Krahmer, 2005; Krahmer et al., 2008; Filippova and Strube, 2008b) and the combination of two (usually contiguous) sentences from a single document (Daum III and Marcu, 2004; Elsner and Santhanam, 2011) .\n[BOS] Variations on the fusion task include the set-theoretic notions of intersection and union (Marsi and Krahmer, 2005; McKeown et al., 2010) , which forego the problem of identifying relevance and are thus less dependent on context.\n[BOS] Query-based versions of these tasks have been studied by Krahmer et al. (2008) and have produced better human agreement in annotation experiments than generic sentence fusion (Daum III and Marcu, 2004) .\n[BOS] McKeown et al. (2010) produced an annotated fusion corpus which was employed in experiments on decoding for sentence intersection (Thadani and McKeown, 2011) .\n[BOS] While most work in the area has covered pairwise sentence combination, recent work by Filippova (2010) has also addressed fusion-referred to as multi-sentence compression-within a cluster of sentences.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Narrative_cite", "Narrative_cite", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 25, "token_end": 83, "char_start": 142, "char_end": 389, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Barzilay & McKeown (Barzilay et al., 1999;": "7031344", "Barzilay and McKeown, 2005)": "16188305"}, "Reference": {}}}, {"token_start": 107, "token_end": 139, "char_start": 529, "char_end": 627, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Marsi and Krahmer, 2005;": "2293515", "Krahmer et al., 2008;": "17303811", "Filippova and Strube, 2008b)": "14909308"}}}, {"token_start": 141, "token_end": 173, "char_start": 636, "char_end": 763, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Daum\u00e9 III and Marcu, 2004;": "7414310", "Elsner and Santhanam, 2011)": "781753"}}}, {"token_start": 182, "token_end": 209, "char_start": 814, "char_end": 909, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Marsi and Krahmer, 2005;": "2293515", "McKeown et al., 2010)": "16074941"}}}, {"token_start": 227, "token_end": 270, "char_start": 1008, "char_end": 1209, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Krahmer et al. (2008)": "17303811"}, "Reference": {}}}, {"token_start": 272, "token_end": 307, "char_start": 1218, "char_end": 1375, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"McKeown et al. (2010)": "16074941"}, "Reference": {}}}, {"token_start": 309, "token_end": 349, "char_start": 1384, "char_end": 1585, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Filippova (2010)": "14750088"}, "Reference": {}}}]}
{"id": "14623890_2", "paragraph": "[BOS] There is much work on unsupervised DA for POS tagging, including work using constraintbased methods (Subramanya et al., 2010; Rush et al., 2012) , instance weighting (Choi and Palmer, 2012) , self-training (Huang et al., 2009; Huang and Yates, 2010) , and co-training (Kbler and Baucom, 2011) .\n[BOS] All of this work uses batch learning.\n[BOS] For space reasons, we do not discuss supervised DA (e.g., Daum III and Marcu (2006) ).\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 16, "token_end": 38, "char_start": 82, "char_end": 150, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Subramanya et al., 2010;": "14000702", "Rush et al., 2012)": "9957355"}}}, {"token_start": 39, "token_end": 48, "char_start": 153, "char_end": 195, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Choi and Palmer, 2012)": "14193754"}}}, {"token_start": 49, "token_end": 67, "char_start": 198, "char_end": 255, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Huang et al., 2009;": "2183720", "Huang and Yates, 2010)": "9841638"}}}, {"token_start": 69, "token_end": 82, "char_start": 262, "char_end": 298, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(K\u00fcbler and Baucom, 2011)": "237674"}}}, {"token_start": 101, "token_end": 118, "char_start": 388, "char_end": 434, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "14623890_1", "paragraph": "[BOS] All of this work on supervised online learning is not directly relevant to this paper since we address the problem of unsupervised DA.\n[BOS] Unlike online supervised learners, we keep the statistical model unchanged during DA and adopt a representation learning approach: each unlabeled context of a word is used to update its representation.\n\n", "discourse_tags": ["Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "14623890_0", "paragraph": "[BOS] Online learning usually refers to supervised learning algorithms that update the model each time after processing a few training examples.\n[BOS] Many supervised learning algorithms are online or have online versions.\n[BOS] Active learning (Lewis and Gale, 1994; Tong and Koller, 2001; Laws et al., 2011 ) is another supervised learning framework that processes training examples -usually obtained interactively -in small batches (Bordes et al., 2005) .\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 36, "token_end": 59, "char_start": 229, "char_end": 308, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lewis and Gale, 1994;": "915058", "Tong and Koller, 2001;": null, "Laws et al., 2011": "7960617"}}}, {"token_start": 70, "token_end": 87, "char_start": 386, "char_end": 456, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bordes et al., 2005)": null}}}]}
{"id": "12295680_1", "paragraph": "[BOS] Some projects have demonstrated the superscalability of crowdsourced efforts.\n[BOS] Deng et al. (2009) used MTurk to construct ImageNet, an annotated image database containing 3.2 million that are hierarchically categorized using the WordNet ontology (Fellbaum, 1998) .\n[BOS] Because Mechanical Turk allows researchers to experiment with crowdsourcing by providing small incentives to Turkers, other successful crowdsourcing efforts like Wikipedia or Games with a Purpose (von Ahn and Dabbish, 2008) also share something in common with MTurk.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 17, "token_end": 58, "char_start": 90, "char_end": 273, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Deng et al. (2009)": "206597351"}, "Reference": {"(Fellbaum, 1998)": null}}}, {"token_start": 86, "token_end": 103, "char_start": 444, "char_end": 505, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "12295680_0", "paragraph": "[BOS] In the past two years, several papers have published about applying Mechanical Turk to a diverse set of natural language processing tasks, including: creating question-answer sentence pairs (Kaisser and Lowe, 2008) , evaluating machine translation quality and crowdsouring translations ), paraphrasing noun-noun compouds for SemEval (Butnariu et al., 2009) , human evaluation of topic models (Chang et al., 2009) , and speech transcription (McGraw et al., 2010; Marge et al., 2010a; Novotney and Callison-Burch, 2010a) .\n[BOS] Others have used MTurk for novel research directions like nonsimulated active learning for NLP tasks such as sentiment classification (Hsueh et al., 2009) or doing quixotic things like doing human-in-the-loop minimum error rate training for machine translation (Zaidan and Callison-Burch, 2009 ).\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 29, "token_end": 43, "char_start": 165, "char_end": 220, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kaisser and Lowe, 2008)": "7656873"}}}, {"token_start": 65, "token_end": 78, "char_start": 331, "char_end": 362, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Butnariu et al., 2009)": "2189362"}}}, {"token_start": 79, "token_end": 92, "char_start": 365, "char_end": 418, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chang et al., 2009)": "3441859"}}}, {"token_start": 94, "token_end": 127, "char_start": 425, "char_end": 524, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(McGraw et al., 2010;": null, "Marge et al., 2010a;": "12602706"}}}, {"token_start": 151, "token_end": 162, "char_start": 642, "char_end": 687, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hsueh et al., 2009)": "3954835"}}}, {"token_start": 163, "token_end": 196, "char_start": 691, "char_end": 826, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zaidan and Callison-Burch, 2009": "8431414"}}}]}
{"id": "15615944_6", "paragraph": "[BOS] Concurrently with this work, Mi and Huang (2015) have developed another dynamic programming for constituent shift-reduce parsing by keeping the step size for a sentence to 4n  2, instead of 2n, with an un-unary (stay) action.\n[BOS] Their final score is 90.8 F1 on WSJ.\n[BOS] Though they only experiment with beam-search, it is possible to build BFS with their transition system as well.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 91, "char_start": 6, "char_end": 392, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Mi and Huang (2015)": "9880107"}, "Reference": {}}}]}
{"id": "15615944_5", "paragraph": "[BOS] Though the framework is shift-reduce, we can notice that our system is strikingly similar to the CKY-based discriminative parser (Hall et al., 2014) because our features basically come from two nodes on the stack and their spans.\n[BOS] From this viewpoint, it is interesting to see that our system outperforms theirs by a large margin (Figure 6 ).\n[BOS] Identifying the source of this performance change is beyond the scope of this paper, but we believe this is an important question for future parsing research.\n[BOS] For example, it is interesting to see whether there is any structural advantage for shiftreduce over CKY by comparing two systems with exactly the same feature set.\n[BOS] As shown in Section 4, the previous optimal parser on shift-reduce (Sagae and Lavie, 2006) was not so strong because of the locality of the model.\n[BOS] Other optimal parsing systems are often based on relatively simple PCFGs, such as unlexicalized grammar (Klein and Manning, 2003b) or factored lexicalized grammar (Klein and Manning, 2003c) in which A* heuristics from the unlexicalized grammar guide search.\n[BOS] However, those systems are not state-of-the-art probably due to the limited context captured with a simple PCFG.\n[BOS] A recent trend has thus been extending the context of each rule (Petrov and Klein, 2007; Socher et al., 2013) , but the resulting complex grammars make exact search intractable.\n[BOS] In our system, the main source of information comes from spans as in CRF parsing.\n[BOS] This is cheap yet strong, and leads to a fast and accurate parsing system with optimality.\n\n", "discourse_tags": ["Narrative_cite", "Reflection", "Transition", "Transition", "Narrative_cite", "Narrative_cite", "Transition", "Narrative_cite", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 146, "token_end": 160, "char_start": 740, "char_end": 786, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sagae and Lavie, 2006)": "10160110"}}}, {"token_start": 189, "token_end": 203, "char_start": 931, "char_end": 979, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Klein and Manning, 2003b)": "11495042"}}}, {"token_start": 204, "token_end": 218, "char_start": 983, "char_end": 1038, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Klein and Manning, 2003c)": null}}}, {"token_start": 269, "token_end": 289, "char_start": 1275, "char_end": 1341, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Petrov and Klein, 2007;": "1123594", "Socher et al., 2013)": "14687186"}}}]}
{"id": "15615944_4", "paragraph": "[BOS] where  and f (a, p) are weight and feature vectors, respectively.\n[BOS] Note that the probability of an action sequence a under this model is the product of local probabilities, though we can cast the total score in summation form (1) by using the log of (2) as a local score (a i , p i1 ).\n[BOS] The structured perceptron is instead trained globally to select the correct action sequence given an input sentence.\n[BOS] It does not use probability and the local score is just (a i , p i1 ) =  f (a i , p i1 ).\n[BOS] In practice, this global model is much stronger than the local MaxEnt model.\n[BOS] However, training this model without any approximation is hard, and the common practice is to rely on well-known heuristics such as an early update with beam search (Collins and Roark, 2004) .\n[BOS] We are not aware of any previous study that succeeded in training a structured perceptron for parsing without approximation.\n[BOS] We will show how this becomes possible in Section 3.\n\n", "discourse_tags": ["Transition", "Reflection", "Transition", "Transition", "Transition", "Narrative_cite", "Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "15615944_3", "paragraph": "[BOS] There are two well-known models, in which the crucial difference is in training criteria.\n[BOS] The MaxEnt model is trained locally to select the correct action at each step.\n[BOS] It assigns a probability for each action a i as\n\n", "discourse_tags": ["Transition", "Transition", "Transition"], "span_citation_mapping": []}
{"id": "15615944_2", "paragraph": "[BOS] Model The model of a shift-reduce parser gives a score to each derivation, i.e., an action sequence a = (a 1 ,    , a |a| ), in which each a i is a shift or reduce action.\n[BOS] Let p = (p 1 ,    , p |a| ) be the sequence of states, where p i is the state after applying a i to p i1 .\n[BOS] p 0 is the initial state for input sentence w. Then, the score for a derivation (a) is calculated as the total score of every action:\n\n", "discourse_tags": ["Other", "Other", "Other"], "span_citation_mapping": []}
{"id": "15615944_1", "paragraph": "[BOS] Unary Action The actions above are essentially the same as those in shift-reduce dependency parsing (Nivre, 2008) , but a special action for constituent parsing UNARY(X) complicates the system and search.\n[BOS] For example, if the top element on the stack is NN, UNARY(NP) changes it to NP by applying the rule NP  NN.\n[BOS] In particular, this causes inconsistency in the numbers of actions between derivations (Zhu et al., 2013) , which makes it hard to apply the existing best first search for dependency grammar to our system.\n[BOS] We revisit this problem in Section 3.1.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 15, "token_end": 26, "char_start": 74, "char_end": 119, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 90, "token_end": 99, "char_start": 406, "char_end": 436, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhu et al., 2013)": null}}}]}
{"id": "15615944_0", "paragraph": "[BOS] We first introduce the shift-reduce algorithm for constituent structures.\n[BOS] For space reasons, our exposition is rather informal; See Zhang and Clark (2009) for details.\n[BOS] A shift-reduce parser parses a sentence through transitions between states, each of which consists of two data structures of a stack and a queue.\n[BOS] The stack preserves intermediate parse results, while the queue saves unprocessed tokens.\n[BOS] At each step, a parser selects an action, which changes the current state into the new one.\n[BOS] For example, SHIFT pops the front word from the queue and pushes it onto the stack, while RE-DUCE(X) combines the top two elements on the stack into their parent.\n[BOS] 2 For example, if the top two elements on the stack are DT and NN, RE-DUCE(NP) combines these by applying the CFG rule NP  DT NN.\n\n", "discourse_tags": ["Reflection", "Narrative_cite", "Transition", "Transition", "Transition", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 15, "token_end": 35, "char_start": 86, "char_end": 179, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Zhang and Clark (2009)": "1625811"}}}]}
{"id": "12874119_2", "paragraph": "[BOS] Input:  1 .\n[BOS] .\n[BOS] .\n[BOS]  K sequence of subgradient rates  1 .\n[BOS] .\n[BOS] .\n[BOS]  K sequence of pruning parameters Output: optimal constrained score or lower bound Apart from translation decoding, this paper is closely related to work on column generation for NLP.\n[BOS] Riedel et al. (2012) and Belanger et al. (2012) relate column generation to beam search and produce exact solutions for parsing and tagging problems.\n[BOS] The latter work also gives conditions for when beam search-style decoding is optimal.\n\n", "discourse_tags": ["Other", "Other", "Other", "Other", "Other", "Other", "Transition", "Multi_summ", "Multi_summ"], "span_citation_mapping": [{"token_start": 57, "token_end": 107, "char_start": 290, "char_end": 531, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Riedel et al. (2012)": "214915", "Belanger et al. (2012)": "214915"}, "Reference": {}}}]}
{"id": "12874119_1", "paragraph": "[BOS] There is a line of work proposing exact algorithms for machine translation decoding.\n[BOS] Exact decoders are often slow in practice, but help quantify the errors made by other methods.\n[BOS] Exact algorithms proposed for IBM model 4 include ILP (Germann et al., 2001) , cutting plane (Riedel and Clarke, 2009) , and multi-pass A* search (Och et al., 2001) .\n[BOS] Zaslavskiy et al. (2009) formulate phrase-based decoding as a traveling salesman problem (TSP) and use a TSP decoder.\n[BOS] Exact decoding algorithms based on finite state transducers (FST) (Iglesias et al., 2009 ) have been studied on phrase-based models with limited reordering (Kumar and Byrne, 2005) .\n[BOS] Exact decoding based on FST is also feasible for certain hierarchical grammars (de Gispert et al., 2010) .\n[BOS] Chang\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite", "Single_summ", "Narrative_cite", "Narrative_cite", "Other"], "span_citation_mapping": [{"token_start": 45, "token_end": 56, "char_start": 248, "char_end": 274, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Germann et al., 2001)": "90111"}}}, {"token_start": 58, "token_end": 67, "char_start": 285, "char_end": 316, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Riedel and Clarke, 2009)": "638874"}}}, {"token_start": 69, "token_end": 83, "char_start": 323, "char_end": 362, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Och et al., 2001)": "9368851"}}}, {"token_start": 85, "token_end": 116, "char_start": 371, "char_end": 488, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zaslavskiy et al. (2009)": "2980513"}, "Reference": {}}}, {"token_start": 122, "token_end": 137, "char_start": 530, "char_end": 583, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Iglesias et al., 2009": "2870777"}}}, {"token_start": 142, "token_end": 159, "char_start": 607, "char_end": 674, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kumar and Byrne, 2005)": "3112023"}}}, {"token_start": 171, "token_end": 183, "char_start": 740, "char_end": 787, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(de Gispert et al., 2010)": "2613012"}}}]}
{"id": "12874119_0", "paragraph": "[BOS] Approximate methods based on beam search and cube-pruning have been widely studied for phrasebased (Koehn et al., 2003; Tillmann and Ney, 2003; Tillmann, 2006) and syntax-based translation models (Chiang, 2007; Huang and Chiang, 2007; Watanabe et al., 2006; Huang and Mi, 2010) .\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 17, "token_end": 42, "char_start": 93, "char_end": 165, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Koehn et al., 2003;": "8884845", "Tillmann and Ney, 2003;": "7829066", "Tillmann, 2006)": "16611412"}}}, {"token_start": 43, "token_end": 74, "char_start": 170, "char_end": 283, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chiang, 2007;": null, "Huang and Chiang, 2007;": "3510512", "Watanabe et al., 2006;": "14684926", "Huang and Mi, 2010)": "16775609"}}}]}
{"id": "15214701_2", "paragraph": "[BOS] On a general level, our method bears some resemblance with (Weinberger and Saul, 2009 ) in that we perform supervised learning on a set of desired (dis)similarities and that we can think of our method as learning specialized metrics for particular subtypes of linguistic information or particular tasks.\n[BOS] Using the method of Weinberger and Saul (2009), one could learn k metrics for k subtypes of information and then simply represent a word w as the concatenation of (i) the original embedding and (ii) k representations corresponding to the k metrics.\n[BOS] 3 In case of a simple one-dimensional type of information, the corresponding representation could simply be a scalar.\n[BOS] We would expect this approach to have similar advantages for practical applications, but we view our orthogonal transformation of the original space as more elegant and it gives rise to a more compact representation.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 11, "token_end": 22, "char_start": 48, "char_end": 91, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Weinberger and Saul, 2009": "47325215"}}}, {"token_start": 61, "token_end": 73, "char_start": 316, "char_end": 362, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "15214701_1", "paragraph": "[BOS] There is also much work on incorporating the additional information into the original word embedding training.\n[BOS] Examples include (Botha and Blunsom, 2014) and (Cotterell and Schtze, 2015) .\n[BOS] However, postprocessing has several advantages.\n[BOS] DENSIFIER can be trained on a normal work station without access to the original training corpus.\n[BOS] This makes the method more flexible, e.g., when new training data or desired properties are available.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 20, "token_end": 44, "char_start": 123, "char_end": 198, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Botha and Blunsom, 2014)": "2838374", "(Cotterell and Sch\u00fctze, 2015)": "13398754"}}}]}
{"id": "15214701_0", "paragraph": "[BOS] Yih et al. (2012) also tackled the problem of antonyms having similar embeddings.\n[BOS] In their model, the antonym is the inverse of the entire vector whereas in our work the antonym is only the inverse in an ultradense subspace.\n[BOS] Our model is more intuitive since antonyms invert only part of the meaning, not the entire meaning.\n[BOS] Schwartz et al. (2015) present a method that switches an antonym parameter on or off (depending on whether a high antonym-synonym similarity is useful for an application) and learn multiple embedding spaces.\n[BOS] We only need a single space, but consider different subspaces of this space.\n[BOS] An unsupervised approach using linguistic patterns that ranks adjectives according to their intensity was presented by de Melo and Bansal (2013) .\n[BOS] Sharma et al. (2015) present a corpus-independent approach for the same problem.\n[BOS] Our results (Table 1) suggest that polarity should not be considered to be corpus-independent.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Reflection", "Single_summ", "Reflection", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 55, "char_start": 6, "char_end": 236, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 79, "token_end": 122, "char_start": 349, "char_end": 556, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Schwartz et al. (2015)": "8923308"}, "Reference": {}}}, {"token_start": 139, "token_end": 165, "char_start": 646, "char_end": 790, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Melo and Bansal (2013)": "1168687"}, "Reference": {}}}, {"token_start": 167, "token_end": 185, "char_start": 799, "char_end": 879, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Sharma et al. (2015)": "1762554"}, "Reference": {}}}]}
{"id": "13740775_0", "paragraph": "[BOS] Previous approaches to ensembling diverse models focus on model inputs.\n[BOS] Hokamp (2017) shows improvements in the quality estimation task using ensembles of NMT models with multiple input representations which share an output representation.\n[BOS] Garmash and Monz (2016) show translation improvements with multi-source-language NMT ensembles.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 15, "token_end": 44, "char_start": 84, "char_end": 251, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hokamp (2017)": "30668669"}, "Reference": {}}}, {"token_start": 45, "token_end": 67, "char_start": 258, "char_end": 353, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Garmash and Monz (2016)": "165188"}, "Reference": {}}}]}
{"id": "13728350_3", "paragraph": "[BOS] Recently there have been also studies towards leveraging word alignments from SMT models.\n[BOS] Mi et al. (2016) and use preobtained word alignments to guide the NMT attention model in the learning of favorable word pairs.\n[BOS] Arthur et al. (2016) leverage a pre-obtained word dictionary to constrain the prediction of target words.\n[BOS] Despite these approaches having a somewhat similar motivation of using pairs of translation equivalents to benefit the NMT translation, in our new bridging approach we do not use extra resources in the NMT model, but let the model itself learn the similarity of word pairs from the training data.\n[BOS] 4 Besides, there exist also studies on the learning of cross-lingual embeddings for machine translation.\n[BOS] Mikolov et al. (2013) propose to first learn distributed representation of words from large monolingual data, and then learn a linear mapping between vector spaces of languages.\n[BOS] Gehring et al. (2017) introduce source word embeddings to predict target words.\n[BOS] These approaches are somewhat similar to our source-side bridging model.\n[BOS] However, inspired by the insight of shortening the distance between source and target embeddings in the seq2seq processing chain, in the present paper we propose more strategies to bridge source and target word embeddings and with better results.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Reflection", "Transition", "Single_summ", "Single_summ", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 19, "token_end": 48, "char_start": 102, "char_end": 228, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Mi et al. (2016)": "18193214"}, "Reference": {}}}, {"token_start": 49, "token_end": 72, "char_start": 235, "char_end": 340, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Arthur et al. (2016)": "10086161"}, "Reference": {}}}, {"token_start": 149, "token_end": 184, "char_start": 761, "char_end": 938, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Mikolov et al. (2013)": "1966640"}, "Reference": {}}}, {"token_start": 185, "token_end": 203, "char_start": 945, "char_end": 1024, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Gehring et al. (2017)": "3648736"}, "Reference": {}}}]}
{"id": "13728350_2", "paragraph": "[BOS] In contrast, we did not delve into the attention model or sought to redesign it in our new bridging proposal.\n[BOS] And yet we achieve enhanced alignment quality by inducing the NMT model to capture more favorable pairs of words that are translation equivalents of each other under the effect of the bridging mechanism.\n\n", "discourse_tags": ["Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "13728350_1", "paragraph": "[BOS] The attention model plays a crucial role in the alignment quality and thus its enhancement has continuously attracted further efforts.\n[BOS] To obtain better attention focuses, propose global and local attention models; and Cohn et al. (2016) extend the attentional model to include structural biases from word based alignment models, including positional bias, Markov conditioning, fertility and agreement over translation directions.\n\n", "discourse_tags": ["Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 27, "token_end": 46, "char_start": 164, "char_end": 248, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Cohn et al. (2016)": "1964946"}}}]}
{"id": "13728350_0", "paragraph": "[BOS] Since the pioneer work of Bahdanau et al. (2015) to jointly learning alignment and translation in NMT, many effective approaches have been proposed to further improve the alignment quality.\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 6, "token_end": 17, "char_start": 24, "char_end": 54, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Bahdanau et al. (2015)": "11212020"}}}]}
{"id": "15776405_1", "paragraph": "[BOS] A rule based converter for Kashmiri language from Persio-Arabic to Devanagari script has been developed by Kak et al. (2010) and authors have claimed 90% conversion accuracy.\n[BOS] Leghari and Rehman (2010)\n\n", "discourse_tags": ["Narrative_cite", "Other"], "span_citation_mapping": [{"token_start": 12, "token_end": 34, "char_start": 56, "char_end": 130, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Kak et al. (2010)": null}}}]}
{"id": "15776405_0", "paragraph": "[BOS] The first transliteration system for a Perso-Arabic to Indic script was presented by Malik (2006) , where he described a Shahmukhi to Gurmukhi transliteration system with 98% accuracy.\n[BOS] But the accuracy was achieved only when the input text had all necessary diacritical marks for removing ambiguities, even though the process of putting missing diacritical marks is not practically possible due to many reasons like large input size, manual intervention, person having knowledge of both the scripts and so on.\n[BOS] Saini et al. (2008) developed a system, which could automatically insert the missing diacritical marks in the Shahmukhi text and convert the text to Gurmukhi.\n[BOS] The system had been implemented with various research techniques based on corpus analysis of both scripts and an accuracy of 91.37% at word level had been reported.\n[BOS] Durrani et al. (2010) presented an approach to integrate transliteration into Hindi-to-Urdu statistical machine translation.\n[BOS] They proposed two probabilistic models, based on conditional and joint probability formulations and have reported an accuracy of 81.4%.\n[BOS] Lehal and Saini (2012) presented an Urdu to Hindi transliteration system and had claimed achieving an accuracy of 97.74% at word level.\n[BOS] The various challenges such as multiple/zero character mappings, missing diacritic marks in Urdu, multiple Hindi words mapped to an Urdu word, word segmentation issues in Urdu text etc.\n[BOS] have been handled by generating special rules and using various lexical resources such as n-gram language models at word and character level and Urdu-Hindi parallel corpus.\n[BOS] Recently Malik et al. (2013) have analysed the application of statistical machine translation for solving the problem of Urdu-Hindi transliteration using a parallel lexicon.\n[BOS] The authors reported a word level accuracy of 77.8% when the input Urdu text contained all necessary diacritical marks and 77% when the input Urdu text did not contain all necessary diacritical marks, which is much below the accuracy reported in earlier works.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 111, "char_start": 6, "char_end": 521, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Malik (2006)": "6134239"}, "Reference": {}}}, {"token_start": 112, "token_end": 184, "char_start": 528, "char_end": 857, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Saini et al. (2008)": null}, "Reference": {}}}, {"token_start": 185, "token_end": 239, "char_start": 864, "char_end": 1130, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Durrani et al. (2010)": "16076435"}, "Reference": {}}}, {"token_start": 240, "token_end": 349, "char_start": 1137, "char_end": 1643, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Lehal and Saini (2012)": null}, "Reference": {}}}, {"token_start": 350, "token_end": 443, "char_start": 1650, "char_end": 2090, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Malik et al. (2013)": "8639073"}, "Reference": {}}}]}
{"id": "12889442_3", "paragraph": "[BOS] As far as the authors know, it is the first attempt to explicitly address the problem of linking linguistically-oriented and domain-oriented frames of semantics.\n[BOS] However, it has been indirectly studied through works on TM or Relation Extraction using linguistically-oriented semantic structures as features, such as in the case with (Harabagiu et al., 2005) .\n\n", "discourse_tags": ["Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 49, "token_end": 75, "char_start": 263, "char_end": 369, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Harabagiu et al., 2005)": "7965063"}}}]}
{"id": "12889442_2", "paragraph": "[BOS] The BioFrameNet (Dolbey et al., 2006 ) is an attempt to extend the FrameNet with specific frames to the bio-medical domain, and to apply the frames to corpus annotation.\n[BOS] Our attempts were similar, in that both were: 1) utilizing the FN frames or their extensions to classify mentions of biological events, and 2) relating the frames and the FEs (roles of participants) with classes in domain ontologies; e.g. the Gene Ontology (Ashburner et al., 2000) .\n\n", "discourse_tags": ["Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 44, "char_start": 6, "char_end": 175, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Dolbey et al., 2006": null}, "Reference": {}}}, {"token_start": 97, "token_end": 109, "char_start": 425, "char_end": 463, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "12889442_1", "paragraph": "[BOS] The PASbio (Wattarujeekrit et al., 2004) proposes Predicate Argument Structures (PASs), a type of linguistically-oriented semantic structures, for domain-specific lexical items, based on PASs defined in PropBank (Wattarujeekrit et al., 2004) and NomBank (Meyers et al., 2004) .\n[BOS] The PASs are defined per lexical item, and is therefore distinct from a biologically-oriented representation of events.\n[BOS] (Cohen et al., 2008) investigated syntactic alternations of verbs and their nominalized forms which occurred in the PennBioIE corpus (Kulick et al., 2004) , whilst keeping PASs of the PASBio in their minds.\n\n", "discourse_tags": ["Multi_summ", "Transition", "Multi_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 75, "char_start": 6, "char_end": 281, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Wattarujeekrit et al., 2004)": "2251578"}, "Reference": {"(Wattarujeekrit et al., 2004)": "2251578", "(Meyers et al., 2004)": "16273722"}}}, {"token_start": 99, "token_end": 148, "char_start": 416, "char_end": 622, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Cohen et al., 2008)": "15758223"}, "Reference": {"(Kulick et al., 2004)": "1207763"}}}]}
{"id": "12889442_0", "paragraph": "[BOS] Existing work on semantics approached domainoriented semantic structures from linguisticallyoriented semantics.\n[BOS] In contrast, our approach uses domain-oriented semantics to find the linguistic semantics that represent them.\n[BOS] We believe that the two different approaches could complement each other.\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "14177708_1", "paragraph": "[BOS] Other works proposed methods to produce more diverse lists of hypotheses by iteratively encouraging the decoder to produce translations that are different from the previous one (Gimpel et al., 2013) or by making small changes to the scoring function to extract k-best lists from other parts of the search space (Devlin and Matsoukas, 2012) .\n[BOS] Some useful diversity can be obtained as these hypotheses can be combined using SMT system combination or help to better train reranking systems.\n[BOS] But in spite of the introduction of more diversity, these methods do not guarantee that eventually lists containing hypotheses that are more relevant to complex features will be obtained.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 17, "token_end": 38, "char_start": 110, "char_end": 204, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Gimpel et al., 2013)": "2209694"}}}, {"token_start": 45, "token_end": 70, "char_start": 239, "char_end": 345, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Devlin and Matsoukas, 2012)": null}}}]}
{"id": "14177708_0", "paragraph": "[BOS] Recently, we proposed a rewriting system that explores in a greedy fashion the neighborhood of the one-best hypothesis found by the reranking pass using complex features, assuming that a better hypothesis can be very close to this seed hypothesis (Marie and Max, 2014) .\n[BOS] Nevertheless, this rewriting only explores a small search space, limited by the greedy search algorithm that concentrates on individual, local rewritings.\n\n", "discourse_tags": ["Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 46, "token_end": 54, "char_start": 242, "char_end": 274, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "14465500_1", "paragraph": "[BOS] Unsupervised coreference resolution.\n[BOS] Cardie and Wagstaff (1999) present an early approach to unsupervised coreference resolution based on a straightforward clustering approach.\n[BOS] Angheluta et al. (2004) build on their approach and devise more sophisticated clustering algorithms.\n[BOS] Haghighi and Klein (2007) , Ng (2008) and Charniak and Elsner (2009) employ unsupervised generative models.\n[BOS] Poon and Domingos (2008) present a Markov Logic Network approach to unsupervised coreference resolution.\n[BOS] These approaches reach competitive performance on gold mentions but not on system mentions (Ng, 2008) .\n[BOS] The multi-pass sieve approach by Raghunathan et al. (2010) can also be viewed as unsupervised.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Multi_summ", "Single_summ", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 8, "token_end": 33, "char_start": 49, "char_end": 188, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Cardie and Wagstaff (1999)": "5534672"}, "Reference": {}}}, {"token_start": 34, "token_end": 55, "char_start": 195, "char_end": 295, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Angheluta et al. (2004)": "56362275"}, "Reference": {}}}, {"token_start": 56, "token_end": 85, "char_start": 302, "char_end": 409, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Haghighi and Klein (2007)": "372666", "Ng (2008)": "4376006", "Charniak and Elsner (2009)": "10539539"}, "Reference": {}}}, {"token_start": 86, "token_end": 107, "char_start": 416, "char_end": 520, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Poon and Domingos (2008)": "7124715"}, "Reference": {}}}, {"token_start": 120, "token_end": 128, "char_start": 602, "char_end": 628, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Ng, 2008)": "4376006"}, "Reference": {}}}, {"token_start": 131, "token_end": 148, "char_start": 641, "char_end": 695, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Raghunathan et al. (2010)": null}}}]}
{"id": "14465500_0", "paragraph": "[BOS] Graph-based coreference resolution.\n[BOS] While not developed within a graph-based framework, factor-based approaches for pronoun resolution (Mitkov, 1998) can be regarded as greedy clustering in a multigraph, where edges representing factors for pronoun resolution have negative or positive weight.\n[BOS] This yields a model similar to the one presented in this paper though Mitkov's work has only been applied to pronoun resolution.\n[BOS] Nicolae and Nicolae (2006) phrase coreference resolution as a graph clustering problem: they first perform pairwise classification and then construct a graph using the derived confidence values as edge weights.\n[BOS] In contrast, work by Culotta et al. (2007) , Cai and Strube (2010) and Sapena et al. (2010) omits the classification step entirely.\n[BOS] Sapena et al. (2010) and Cai and Strube (2010) perform coreference resolution in one step using graph partitioning approaches.\n[BOS] These approaches participated in the recent CoNLL'11 shared task (Pradhan et al., 2011; Sapena et al., 2011; Cai et al., 2011b) with excellent results.\n[BOS] The approach by Cai et al. (2011b) has been modified by Martschat et al. (2012) and ranked second in the English track at the CoNLL'12 shared task (Pradhan et al., 2012) .\n[BOS] The top performing system at the CoNLL'12 shared task (Fernandes et al., 2012) also represents the problem as a graph by performing inference on trees constructed using the multi-pass sieve approach by Raghunathan et al. (2010) and Lee et al. (2011) , which in turn won the CoNLL'11 shared task.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Single_summ", "Narrative_cite", "Multi_summ", "Narrative_cite", "Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 20, "token_end": 33, "char_start": 100, "char_end": 161, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Mitkov, 1998)": "2021262"}, "Reference": {}}}, {"token_start": 85, "token_end": 124, "char_start": 447, "char_end": 657, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Nicolae and Nicolae (2006)": "6477717"}, "Reference": {}}}, {"token_start": 128, "token_end": 163, "char_start": 677, "char_end": 795, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Culotta et al. (2007)": "6618210", "Cai and Strube (2010)": "652194", "Sapena et al. (2010)": "2731424"}}}, {"token_start": 164, "token_end": 192, "char_start": 802, "char_end": 928, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Sapena et al. (2010)": "2731424", "Cai and Strube (2010)": "652194"}, "Reference": {}}}, {"token_start": 199, "token_end": 231, "char_start": 979, "char_end": 1062, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Pradhan et al., 2011;": "11898554", "Sapena et al., 2011;": "14043112", "Cai et al., 2011b)": "16608926"}}}, {"token_start": 236, "token_end": 285, "char_start": 1093, "char_end": 1262, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Cai et al. (2011b)": "16608926"}, "Reference": {"Martschat et al. (2012)": "394655", "(Pradhan et al., 2012)": "41479182"}}}, {"token_start": 293, "token_end": 309, "char_start": 1304, "char_end": 1349, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Fernandes et al., 2012)": "1884019"}}}, {"token_start": 324, "token_end": 341, "char_start": 1444, "char_end": 1498, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Raghunathan et al. (2010)": null}}}, {"token_start": 342, "token_end": 362, "char_start": 1503, "char_end": 1566, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Lee et al. (2011)": "618047"}}}]}
{"id": "1397387_5", "paragraph": "[BOS] Finally, our work adds additional features to a graph-based parser which is based on a linearmodel.\n[BOS] Recently, progress in dependency parsing has been made by introducing non-linear, neuralnetwork based models (Pei et al., 2015; Chen and Manning, 2014; Weiss et al., 2015; Dyer et al., 2015; Zhou et al., 2015) .\n[BOS] Adapting our approach to work with such models is an interesting research direction.\n\n", "discourse_tags": ["Reflection", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 39, "token_end": 80, "char_start": 194, "char_end": 321, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Pei et al., 2015;": "2102270", "Chen and Manning, 2014;": null, "Weiss et al., 2015;": "15213991", "Dyer et al., 2015;": null, "Zhou et al., 2015)": "17887856"}}}]}
{"id": "1397387_4", "paragraph": "[BOS] Another recent approach that takes into account various syntactic interactions was recently introduced by , who propose to learn to embed complex features that are being used in a graph-based parser based on other features they co-occur with in auto-parsed data.\n[BOS] Similar to our approach, the embedded features are then used as additional features in a conventional graph-based model.\n[BOS] The approaches are to a large extent complementary, and could be combined.\n\n", "discourse_tags": ["Transition", "Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "1397387_3", "paragraph": "[BOS] Works that derive features from auto-parsed data include (Sagae and Gordon, 2009; Bansal et al., 2014) .\n[BOS] Such works assign a representation (either cluster or vector) for individual word in the vocabulary based on their syntactic behavior.\n[BOS] In contrast, our learned features are designed to capture interactions between words.\n[BOS] As discussed in sections (1) and (2), most similar to ours is the work of (Chen et al., 2009; Van Noord, 2007) .\n[BOS] We extend their approach to take into account not only direct word-word interactions but also the lexical surroundings in which these interactions occur.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Reflection", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 7, "token_end": 29, "char_start": 38, "char_end": 108, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sagae and Gordon, 2009;": "797950", "Bansal et al., 2014)": "7803700"}}}, {"token_start": 70, "token_end": 104, "char_start": 350, "char_end": 460, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chen et al., 2009;": "14728649", "Van Noord, 2007)": "144544"}}}]}
{"id": "1397387_2", "paragraph": "[BOS] Among the words that use auto-parsed data, a dominant approach is self-training (McClosky et al., 2006) , in which a parser A (possibly an ensemble) is used to parse large amounts of data, and a parser B is then trained over the union of the gold data and the auto-parsed data produced by parser A.\n[BOS] In the context of dependency-parsing, successful uses of self-training require parser A to be stronger than parser B (Petrov et al., 2010) or use a selection criteria for training only on highquality parses produced by parser A (Sagae and Tsujii, 2007; Weiss et al., 2015) .\n[BOS] In contrast, our work uses the same parser (modulo the feature-set) for producing the auto-parsed data and for training the final model, and does not employ a highquality parse selection criteria when creating the auto-parsed corpus.\n[BOS] It is possible that high-quality parse selection can improve our proposed method even further.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 17, "token_end": 30, "char_start": 72, "char_end": 109, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(McClosky et al., 2006)": "628455"}}}, {"token_start": 88, "token_end": 109, "char_start": 368, "char_end": 449, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Petrov et al., 2010)": "5401473"}}}, {"token_start": 112, "token_end": 143, "char_start": 459, "char_end": 583, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sagae and Tsujii, 2007;": "2768696", "Weiss et al., 2015)": "15213991"}}}]}
{"id": "1397387_1", "paragraph": "[BOS] Among the words that use unannotated data, the dominant approach is to derive either word clusters (Koo et al., 2008) or word vectors (Chen and Manning, 2014 ) based on unparsed data, and use these as additional features for a supervised parsing model.\n[BOS] While the word representations used in such methods are not specifically designed for the parsing task, they do provide useful features for parsing, and in particular the method of (Koo et al., 2008) , relying on features derived using the Brown-clustering algorithm, provides very competitive state-of-the-art results.\n[BOS] To the best of our knowledge, we are the first to show a substantial improvement over using Brown-clustering derived features without using Brown-cluster features as a component.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 20, "token_end": 31, "char_start": 91, "char_end": 123, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Koo et al., 2008)": "1916754"}}}, {"token_start": 32, "token_end": 41, "char_start": 127, "char_end": 163, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chen and Manning, 2014": null}}}, {"token_start": 93, "token_end": 102, "char_start": 446, "char_end": 464, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Koo et al., 2008)": "1916754"}}}]}
{"id": "1397387_0", "paragraph": "[BOS] Semi-supervised approaches to dependency parsing can be roughly categorized into two groups: those that use unannotated data and those that use automatically-parsed data.\n[BOS] Our proposed method falls in the second group.\n\n", "discourse_tags": ["Transition", "Reflection"], "span_citation_mapping": []}
{"id": "13335042_0", "paragraph": "[BOS] We will first focus on bilingual embedding mappings, which are the basis of our proposals, and then on other unsupervised and weakly supervised methods to learn bilingual word embeddings.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "14610045_0", "paragraph": "[BOS] Most of previous event extraction work focused on learning supervised models based on symbolic features (Ji and Grishman, 2008; Miwa et al., 2009; Liao and Grishman, 2010; Liu et al., 2010; Hong et al., 2011; McClosky et al., 2011; Sebastian and Andrew, 2011; Chen and Ng, 2012; Li et al., 2013) or distributional features through deep learning (Chen et al., 2015; Nguyen and Grishman, 2015) .\n[BOS] They usually rely on a predefined event schema and a large amount of training data.\n[BOS] Compared with other paradigms such as Open Information Extraction (Etzioni et al., 2005; Banko et al., 2007; Banko et al., 2008; Etzioni et al., 2011; Ritter et al., 2012) , Preemptive IE (Shinyama and Sekine, 2006) , Ondemand IE (Sekine, 2006) and semantic frame based event discovery (Kim et al., 2013) , our approach can explicitly name each event type and argument role.\n[BOS] Some recent work focused on universal schema discovery (Chambers and Jurafsky, 2011; Pantel et al., 2012; Yao et al., 2012; Yao et al., 2013; Chambers, 2013; .\n[BOS] However, the schemas discovered from these methods are rather static and they are not customized for any specific input corpus.\n[BOS] Our work is also related to efforts at composing word embeddings using syntactic structures (Hermann and Blunsom, 2013; Socher et al., 2013a; Socher et al., 2013b; Bowman et al., 2014; .\n[BOS] Our trigger sense representation is similar to Word Sense Induction (Navigli, 2009; Bordag, 2006; Pinto et al., 2007; Brody and Lapata, 2009; Manandhar et al., 2010; Navigli and Lapata, 2010; Van de Cruys and Apidianaki, 2011; Wang et al., 2015b) .\n[BOS] Besides word sense, we exploit related concepts to enrich trigger representation.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Narrative_cite", "Narrative_cite", "Transition", "Narrative_cite", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 15, "token_end": 87, "char_start": 92, "char_end": 301, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ji and Grishman, 2008;": "1320606", "Miwa et al., 2009;": "1923592", "Liao and Grishman, 2010;": "11187670", "Liu et al., 2010;": "18244538", "Hong et al., 2011;": "2867611", "McClosky et al., 2011;": "2941631", "Sebastian and Andrew, 2011;": "18198203", "Chen and Ng, 2012;": null, "Li et al., 2013)": "2114517"}}}, {"token_start": 88, "token_end": 109, "char_start": 305, "char_end": 397, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chen et al., 2015;": "14339673", "Nguyen and Grishman, 2015)": "10913456"}}}, {"token_start": 134, "token_end": 180, "char_start": 534, "char_end": 667, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Etzioni et al., 2005;": "7162988", "Banko et al., 2007;": null, "Banko et al., 2008;": "6983197", "Etzioni et al., 2011;": "15515902", "Ritter et al., 2012)": "207196336"}}}, {"token_start": 181, "token_end": 193, "char_start": 670, "char_end": 711, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Shinyama and Sekine, 2006)": "8186401"}}}, {"token_start": 194, "token_end": 204, "char_start": 714, "char_end": 740, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sekine, 2006)": "648239"}}}, {"token_start": 205, "token_end": 218, "char_start": 745, "char_end": 800, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kim et al., 2013)": "7337458"}}}, {"token_start": 237, "token_end": 274, "char_start": 905, "char_end": 1033, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chambers and Jurafsky, 2011;": "12808163", "Pantel et al., 2012;": "15810061", "Yao et al., 2012;": "7826003", "Yao et al., 2013;": "2667711"}}}, {"token_start": 313, "token_end": 350, "char_start": 1248, "char_end": 1360, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hermann and Blunsom, 2013;": "17981782", "Socher et al., 2013a;": "2317858", "Socher et al., 2013b;": "990233"}}}, {"token_start": 360, "token_end": 427, "char_start": 1417, "char_end": 1616, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Navigli, 2009;": "461624", "Bordag, 2006;": "6582246", "Pinto et al., 2007;": "11878204", "Brody and Lapata, 2009;": "10171569", "Manandhar et al., 2010;": "7785234", "Navigli and Lapata, 2010;": null, "Van de Cruys and Apidianaki, 2011;": "6108278", "Wang et al., 2015b)": "14389684"}}}]}
{"id": "13273770_2", "paragraph": "[BOS] In contrast to the previous CAT research, we present a writing assistant that suggests subsequent grammar constructs with translations and interactively collaborates with learners, in view of reducing users' burden on grammar and word choice and enhancing their writing quality.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "13273770_1", "paragraph": "[BOS] More recently, interactive MT (IMT) systems have begun to shift the user's role from analyses of the source text to the formation of the target translation.\n[BOS] TransType project (Foster et al., 2002) describes such pioneering system that supports next word predictions.\n[BOS] Koehn (2009) develops caitra which displays one phrase translation at a time and offers alternative translation options.\n[BOS] Both systems are similar in spirit to our work.\n[BOS] The main difference is that we do not expect the user to be a professional translator and we provide translation hints along with grammar predictions to avoid the generalization issue facing phrase-based system.\n[BOS] Recent work has been done on using fullyfledged statistical MT systems to produce target hypotheses completing user-validated translation prefix in IMT paradigm.\n[BOS] Barrachina et al. (2008) investigate the applicability of different MT kernels within IMT framework.\n[BOS] Nepveu et al. (2004) and Ortiz-Martinez et al. (2011) further exploit user feedbacks for better IMT systems and user experience.\n[BOS] Instead of trigged by user correction, our method is triggered by word delimiter and assists in target language learning.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Reflection", "Reflection", "Transition", "Single_summ", "Multi_summ", "Reflection"], "span_citation_mapping": [{"token_start": 36, "token_end": 58, "char_start": 169, "char_end": 278, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Foster et al., 2002)": "10579939"}, "Reference": {}}}, {"token_start": 59, "token_end": 82, "char_start": 285, "char_end": 405, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Koehn (2009)": "6341545"}, "Reference": {}}}, {"token_start": 162, "token_end": 183, "char_start": 852, "char_end": 952, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Barrachina et al. (2008)": "92327"}, "Reference": {}}}, {"token_start": 184, "token_end": 218, "char_start": 959, "char_end": 1087, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Nepveu et al. (2004)": "943080", "Ortiz-Martinez et al. (2011)": "10521251"}, "Reference": {}}}]}
{"id": "13273770_0", "paragraph": "[BOS] CAT has been an area of active research.\n[BOS] Our work addresses an aspect of CAT focusing on language learning.\n[BOS] Specifically, our goal is to build a human-computer collaborative writing assistant: helping the language learner with intext grammar and translation and at the same time updating the system's segmentation /translation options through the user's word choices.\n[BOS] Our intended users are different from those of the previous research focusing on what professional translator can bring for MT systems (e.g., Brown and Nirenburg, 1990) .\n\n", "discourse_tags": ["Transition", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 93, "token_end": 109, "char_start": 516, "char_end": 560, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Brown and Nirenburg, 1990)": "8429165"}}}]}
{"id": "12436969_3", "paragraph": "[BOS] Our task is very similar to the research of SMT using MWEs (Carpuat and Diab, 2010; Ren et al., 2009 ).\n[BOS] However we are in different situation where incorrect words may be included in source sentence side, thus identifying MWEs in source side may make mistakes.\n\n", "discourse_tags": ["Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 11, "token_end": 31, "char_start": 50, "char_end": 106, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Carpuat and Diab, 2010;": "10461738", "Ren et al., 2009": "1130476"}}}]}
{"id": "12436969_2", "paragraph": "[BOS] A lot of research for identifying MWEs and constructing MWE resources have been conducted (Schneider et al., 2014; Shigeto et al., 2013) .\n[BOS] In addition, there is some research in natural language processing applications using MWEs; i.e., statistical machine translation (Carpuat and Diab, 2010; Ren et al., 2009) , information retrieval (Newman et al., 2012) and opinion mining (Berend, 2011) .\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 8, "token_end": 35, "char_start": 40, "char_end": 142, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Schneider et al., 2014;": null, "Shigeto et al., 2013)": "16247699"}}}, {"token_start": 58, "token_end": 77, "char_start": 249, "char_end": 323, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Carpuat and Diab, 2010;": "10461738", "Ren et al., 2009)": "1130476"}}}, {"token_start": 78, "token_end": 88, "char_start": 326, "char_end": 369, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Newman et al., 2012)": "8505521"}}}, {"token_start": 89, "token_end": 97, "char_start": 374, "char_end": 403, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Berend, 2011)": "8753815"}}}]}
{"id": "12436969_1", "paragraph": "[BOS] There is the work dealing with collocations, a kind of MWEs, as target of error detection (Futagi et al., 2008) .\n[BOS] Our method is different in that we are aiming at correcting not MWEs but other expressions like articles, prepositions and noun numbers as targets considering MWEs.\n\n", "discourse_tags": ["Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 14, "token_end": 31, "char_start": 61, "char_end": 117, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Futagi et al., 2008)": "61127332"}}}]}
{"id": "12436969_0", "paragraph": "[BOS] Research on grammatical error correction has recently become very popular.\n[BOS] Grammatical error correction methods are roughly divided into two types; (1) targeting few restricted types of errors (Rozovskaya and Roth, 2011; Rozovskaya and Roth, 2013; Tajiri et al., 2012) and (2) targeting any types of errors (Mizumoto et al., 2012) .\n[BOS] In the first type of error correction, classifiers like Support Vector Machines have mainly been used.\n[BOS] In the second type, statistical machine translation methods have been used.\n[BOS] The only features for grammatical error correction that have been considered in many of previous works are token, POS and syntactic information of single words, and features considering two (or more) words as a whole such as MWEs have never been used.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 28, "token_end": 64, "char_start": 164, "char_end": 280, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rozovskaya and Roth, 2011;": "14977841", "Rozovskaya and Roth, 2013;": "11168472", "Tajiri et al., 2012)": "9143507"}}}, {"token_start": 68, "token_end": 83, "char_start": 289, "char_end": 342, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mizumoto et al., 2012)": "15389327"}}}]}
{"id": "1446776_0", "paragraph": "[BOS] In recent years, many research has been done on extracting relations from free text (e.g., (Pantel and Pennacchiotti, 2006; Agichtein and Gravano, 2000; Snow et al., 2006) ); however, almost all of them require some language-dependent parsers or taggers for English, which restrict the language of their extractions to English only (or languages that have these parsers).\n[BOS] There has also been work done on extracting relations from HTML-structured tables (e.g., (Etzioni et al., 2005; Nadeau et al., 2006; Cafarella et al., 2008) ); however, they all incorporated heuristics for exploiting HTML structures; thus, they cannot handle documents written in other mark-up languages.\n[BOS] Extracting relations at character-level from semi-structured documents has been proposed (e.g., (Kushmerick et al., 1997) , (Brin, 1998) ).\n[BOS] In particular, Brin's approach (DIPRE) is the most similar to ours in terms of expanding relational items.\n[BOS] One difference is that it requires maximally-long contextual strings to bracket all seed occurrences.\n[BOS] This technique has been experimentally illustrated to perform worse than SEAL's approach on unary relations (Wang and Cohen, 2007) .\n[BOS] Brin presented five seed pairs of author names and book titles that he used in the experiment (unfortunately, he did not provide detailed results).\n[BOS] We input the top two seed pairs listed in his paper into the relational SEAL, performed ten bootstrapping iterations (took about 3 minutes), and obtained 26,000 author name/book title pairs of which the precision at 100 is perfect (100%).\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Narrative_cite", "Reflection", "Transition", "Narrative_cite", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 13, "token_end": 50, "char_start": 65, "char_end": 177, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Pantel and Pennacchiotti, 2006;": "7463996", "Snow et al., 2006)": "14680675"}}}, {"token_start": 103, "token_end": 141, "char_start": 443, "char_end": 540, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Etzioni et al., 2005;": "7162988", "Nadeau et al., 2006;": "17071982", "Cafarella et al., 2008)": "15642206"}}}, {"token_start": 172, "token_end": 207, "char_start": 719, "char_end": 831, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kushmerick et al., 1997)": "5119155", "(Brin, 1998)": "6075461"}}}, {"token_start": 213, "token_end": 222, "char_start": 856, "char_end": 879, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 264, "token_end": 279, "char_start": 1135, "char_end": 1192, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wang and Cohen, 2007)": "7567357"}}}]}
{"id": "15533677_2", "paragraph": "[BOS] Our combined parser makes the biggest contribution of this paper.\n[BOS] In contrast to the models above, it includes both graph-based and transition-based components.\n[BOS] An existing method to combine multiple parsing algorithms is the ensemble approach (Sagae and Lavie, 2006a) , which was reported to be useful in improving dependency parsing (Hall et al., 2007) .\n[BOS] A more recent approach (Nivre and McDonald, 2008 ) combined MSTParser and MaltParser by using the output of one parser for features in the other.\n[BOS] Both and Nivre and McDonald (2008) can be seen as methods to combine separately defined models.\n[BOS] In contrast, our parser combines two components in a single model, in which all parameters are trained consistently.\n\n", "discourse_tags": ["Reflection", "Reflection", "Narrative_cite", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 44, "token_end": 56, "char_start": 244, "char_end": 286, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sagae and Lavie, 2006a)": "6133066"}}}, {"token_start": 65, "token_end": 75, "char_start": 334, "char_end": 372, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hall et al., 2007)": "5625678"}}}, {"token_start": 77, "token_end": 110, "char_start": 381, "char_end": 526, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Nivre and McDonald, 2008": "9431510"}, "Reference": {}}}, {"token_start": 111, "token_end": 131, "char_start": 533, "char_end": 628, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Nivre and McDonald (2008)": "9431510"}, "Reference": {}}}]}
{"id": "15533677_1", "paragraph": "[BOS] Our transition-based parser is derived from the deterministic parser of Nivre et al. (2006) .\n[BOS] We incorporated the transition process into our beamsearch framework, in order to study the influence of search on this algorithm.\n[BOS] Existing efforts to add search to deterministic parsing include Sagae and Lavie (2006b), which applied best-first search to constituent parsing, and Johansson and Nugues (2006) and Duan et al. (2007) , which applied beamsearch to dependency parsing.\n[BOS] All three methods estimate the probability of each transition action, and score a state item by the product of the probabilities of all its corresponding actions.\n[BOS] But different from our transition-based parser, which trains all transitions for a parse globally, these models train the probability of each action separately.\n[BOS] Based on the work of Johansson and Nugues (2006) , Johansson and Nugues (2007) studied global training with an approximated large-margin algorithm.\n[BOS] This model is the most similar to our transition-based model, while the differences include the choice of learning and decoding algorithms, the definition of feature templates and our application of the \"early update\" strategy.\n\n", "discourse_tags": ["Narrative_cite", "Reflection", "Narrative_cite", "Transition", "Transition", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 11, "token_end": 22, "char_start": 54, "char_end": 97, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Nivre et al. (2006)": "7490434"}}}, {"token_start": 54, "token_end": 66, "char_start": 277, "char_end": 330, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 78, "token_end": 105, "char_start": 392, "char_end": 492, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Johansson and Nugues (2006)": "10994715", "Duan et al. (2007)": "5740154"}}}, {"token_start": 164, "token_end": 178, "char_start": 835, "char_end": 883, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Johansson and Nugues (2006)": "10994715"}}}, {"token_start": 179, "token_end": 199, "char_start": 886, "char_end": 982, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Johansson and Nugues (2007)": "10786659"}, "Reference": {}}}]}
{"id": "15533677_0", "paragraph": "[BOS] Our graph-based parser is derived from the work of McDonald and Pereira (2006) .\n[BOS] Instead of performing exact inference by dynamic programming, we incorporated the linear model and feature templates from McDonald and Pereira (2006) into our beam-search framework, while adding new global features.\n[BOS] Nakagawa (2007) and Hall (2007) also showed the effectiveness of global features in improving the accuracy of graph-based parsing, using the approximate Gibbs sampling method and a reranking approach, respectively.\n\n", "discourse_tags": ["Narrative_cite", "Reflection", "Multi_summ"], "span_citation_mapping": [{"token_start": 35, "token_end": 49, "char_start": 175, "char_end": 242, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"McDonald and Pereira (2006)": "802998"}}}, {"token_start": 63, "token_end": 105, "char_start": 315, "char_end": 529, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Nakagawa (2007)": "9310605", "Hall (2007)": "5625678"}, "Reference": {}}}]}
{"id": "15562940_2", "paragraph": "[BOS] The role pair feature has not been studied for general, broad-domain pronoun co-reference, but it has been used for other tasks: Pekar (2006) built pairs of 'templates' which share an 'anchor' argument; these correspond closely to our role pairs.\n[BOS] Association statistics of the template pairs were used to acquire verb entailments.\n[BOS] Abe et al. (2008) looked for pairs appearing in specific syntactic patterns in order to acquire finer-grained event relations.\n[BOS] Chambers and Jurafsky (2008) built narrative event chains, which are partially ordered sets of events related by a common protagonist.\n[BOS] They use high-precision hand-coded rules to get coreference information, extract predicate arguments that link the mentions to verbs, and link the arguments of the coreferred mentions to build a verb entailment model.\n[BOS] Bean and Riloff (2004) used high-precision hand-coded rules to identify coreferent mention pairs, which are then used to acquire role pairs that they refer to as Caseframe Network features.\n[BOS] They use these features to improve coreference resolution for two domain-specific corpora involving terrorism and natural disasters.\n[BOS] Their result raises the natural question as to whether the approach (which may capture domainspecific pairs such as \"kidnap-release\" in the terrorism domain) can be successfully extended to a general news corpus.\n[BOS] We address this question in the experiments reported here.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 30, "token_end": 72, "char_start": 135, "char_end": 342, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 73, "token_end": 99, "char_start": 349, "char_end": 475, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Abe et al. (2008)": "15730471"}, "Reference": {}}}, {"token_start": 100, "token_end": 174, "char_start": 482, "char_end": 840, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Chambers and Jurafsky (2008)": "529375"}, "Reference": {}}}, {"token_start": 175, "token_end": 239, "char_start": 847, "char_end": 1175, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Bean and Riloff (2004)": null}, "Reference": {}}}]}
{"id": "15562940_1", "paragraph": "[BOS] In contrast, Kehler et al. (2004) claimed that the contextual compatibility feature does not help much for pronoun coreference: existing learningbased approaches already performed well; such statistics are simply not good predictors for pronoun interpretation; data is sparse in the collected predicate-argument statistics.\n\n", "discourse_tags": ["Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 58, "char_start": 6, "char_end": 329, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kehler et al. (2004)": "11647641"}, "Reference": {}}}]}
{"id": "15562940_0", "paragraph": "[BOS] Contextual compatibility features have long been studied for pronoun coreference: Dagan and Itai (1990) proposed a heuristics-based approach to pronoun resolution.\n[BOS] It determined the preference of candidates based on predicate-argument frequencies.\n[BOS] Bean and Riloff (2004) present a system, which uses contextual role knowledge to aid coreference resolution.\n[BOS] They used lexical and syntactic heuristics to identify high-confidence coreference relations and used them as training data for learning contextual role knowledge.\n[BOS] They got substantial gains on articles in two specific domains, terrorism and natural disasters.\n[BOS] Yang et al. (2005) use statistically-based semantic compatibility information to improve pronoun resolution.\n[BOS] They use corpus-based and web-based extraction strategies, and their work shows that statistically-based semantic compatibility information can improve coreference resolution.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 14, "token_end": 46, "char_start": 88, "char_end": 259, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Dagan and Itai (1990)": "9210393"}, "Reference": {}}}, {"token_start": 47, "token_end": 115, "char_start": 266, "char_end": 647, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Bean and Riloff (2004)": null}, "Reference": {}}}, {"token_start": 116, "token_end": 165, "char_start": 654, "char_end": 944, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Yang et al. (2005)": "2536406"}, "Reference": {}}}]}
{"id": "14886962_2", "paragraph": "[BOS] But these grammar theories only give filters for excluding some elements from consideration.\n[BOS] Neither gives any preference for a particular antecedent at the sentence-level, nor do they consider text anaphora.\n\n", "discourse_tags": ["Transition", "Transition"], "span_citation_mapping": []}
{"id": "14886962_1", "paragraph": "[BOS] At first sight, grammar theories like GB (Chomsky, 1981) or I-IPSG (Pollard & Sag, 1994) , are the best choice for resolving anaphora at the sentence-level.\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 6, "token_end": 17, "char_start": 22, "char_end": 62, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 18, "token_end": 30, "char_start": 66, "char_end": 94, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Pollard & Sag, 1994)": "14500645"}}}]}
{"id": "14886962_0", "paragraph": "[BOS] Crucial for the evaluation of the centering model (Grosz et al., 1995) and its applicability to naturally occurring discourse is the lack of a specification conceming how to handle complex sentences and intrasentential anaphora.\n[BOS] Grosz et al. suggest the processing of sentences linearly one clause at a time.\n[BOS] We have shown that such an approach is not appropriate for some types of complex sentences.\n[BOS] Suri & McCoy (1994) argue in the same manner, but we consider the functional approach for languages with free word order superior to their grammatical criteria, while, for languages with fixed word order, both approaches should give the same results.\n[BOS] Hence, our approach seems to be more generally applicable.\n[BOS] Other approaches which integrate the resolution of sentenceand text-level anaphora are based on salience metrics (Haji~ov~i et al., 1992; Lappin & Leass, 1994) .\n[BOS] We consider such metrics to be a method which detracts from the exact linguistic specifications as we propose them.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Reflection", "Single_summ", "Reflection", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 8, "token_end": 20, "char_start": 40, "char_end": 76, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Grosz et al., 1995)": "11660053"}}}, {"token_start": 49, "token_end": 66, "char_start": 241, "char_end": 320, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 85, "token_end": 134, "char_start": 425, "char_end": 675, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Suri & McCoy (1994)": null}, "Reference": {}}}, {"token_start": 164, "token_end": 187, "char_start": 843, "char_end": 906, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Lappin & Leass, 1994)": "11500985"}}}]}
{"id": "1333990_5", "paragraph": "[BOS] As a whole, compared with previous works, our unsupervised trees are generated fully depending on word alignment.\n[BOS] Therefore, by using our tree structures, the incompatibility problem between tree structures and word alignment can be well resolved.\n\n", "discourse_tags": ["Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "1333990_4", "paragraph": "[BOS] The other direction is to integrate the alignment information into parsing.\n[BOS] Burkett and Klein (2008) and Burkett et al. (2010) made efforts to do joint parsing and alignment.\n[BOS] They utilized the bilingual Treebank to train a joint model and achieved better results on both parsing and word alignment.\n[BOS] Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment.\n[BOS] Our work is different from theirs in that we are pursuing better unsupervised tree structures for better translation performance.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Multi_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 15, "token_end": 63, "char_start": 88, "char_end": 316, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Burkett and Klein (2008)": "1138220", "Burkett et al. (2010)": "2707891"}, "Reference": {}}}, {"token_start": 64, "token_end": 85, "char_start": 323, "char_end": 411, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Liu et al. (2012)": "1091879"}, "Reference": {}}}]}
{"id": "1333990_3", "paragraph": "[BOS] One direction is to adapt the parse tree structure.\n[BOS] Wang et al., (2007) binarized the parse trees and adopted an EM algorithm to select the best binary tree from their parallel binarization forest.\n[BOS] Mi et al., (2008b) and Liu et al., (2009) compressed thousands of parse trees into packed forests.\n[BOS] Zhang et al. (2011a) applied a CKY binarization method on parse trees to get binary forests for forest-to-string model.\n[BOS] Burkett and Klein (2012) adopted a transformation-based method to learn a sequence of monolingual tree transformations for translation.\n[BOS] They differ from our work in that they were all based on parse trees.\n[BOS] Compared with them, we construct effective unsupervised tree structures according to the word alignment and do not need any syntactic resource.\n\n", "discourse_tags": ["Transition", "Single_summ", "Multi_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 13, "token_end": 46, "char_start": 64, "char_end": 209, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wang et al., (2007)": "17593675"}, "Reference": {}}}, {"token_start": 47, "token_end": 74, "char_start": 216, "char_end": 314, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Liu et al., (2009)": "2828132"}, "Reference": {}}}, {"token_start": 75, "token_end": 106, "char_start": 321, "char_end": 440, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhang et al. (2011a)": "16414840"}, "Reference": {}}}, {"token_start": 107, "token_end": 148, "char_start": 447, "char_end": 658, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Burkett and Klein (2012)": "12545394"}, "Reference": {}}}]}
{"id": "1333990_2", "paragraph": "[BOS] On relieving the incompatibility problem between tree structures and word alignment for translation, previous works mainly focus on two directions:\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "1333990_1", "paragraph": "[BOS] Several researchers have studied unsupervised tree structure induction for different objectives.\n[BOS] Blunsom et al. (2008 Blunsom et al. ( , 2009 Blunsom et al. ( , 2010 utilized Bayesian methods to learn synchronous context free grammar (SCFG) from a parallel corpus.\n[BOS] The obtained SCFG grammar is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007) .\n[BOS] Denero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled tree structures for syntactic pre-reordering.\n[BOS] Different from above works, we concentrate on producing effective and labeled unsupervised trees for tree-based translation models.\n[BOS] Moreover, since most of the current tree-based translation models are based on synchronous tree substitution grammar (STSG), our unsupervised trees are thus learned according to STSG, rather than SCFG.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Multi_summ", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 15, "token_end": 85, "char_start": 109, "char_end": 394, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Blunsom et al. (2008": "3079254", "Blunsom et al. ( , 2009": "1734281", "Blunsom et al. ( , 2010": "9743582"}, "Reference": {"(Chiang, 2007)": "3505719"}}}, {"token_start": 87, "token_end": 114, "char_start": 403, "char_end": 529, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Denero and Uszkoreit (2011)": "2329629"}, "Reference": {}}}]}
{"id": "1333990_0", "paragraph": "[BOS] Our work focuses on inducing effective unsupervised tree structures, and meanwhile, resolving the incompatibility problem between tree structures and word alignment for tree-based translation.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "13965916_1", "paragraph": "[BOS] A thorough such lexical semantics evaluation was created by Faruqui and Dyer (2014) 1 .\n[BOS] This website allows a user to upload a set of embeddings, and evaluates these embeddings on a series of word similarity benchmarks.\n[BOS] We follow the model presented in Faruqui and Dyer (2014) , but extend to a series of more realistic downstream tasks.\n[BOS] Schnabel et al. (2015) carried out both a thorough intrinsic evaluation of word vectors, and a limited extrinsic evaluation showing that an embedding's intrinsic performance did not necessarily correlate with its real-world performance.\n[BOS] This finding is a key motivation for this work -we aim to create a metric which does correlate with downstream performance.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Narrative_cite", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 5, "token_end": 21, "char_start": 22, "char_end": 89, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Faruqui and Dyer (2014)": "6513617"}}}, {"token_start": 53, "token_end": 66, "char_start": 252, "char_end": 294, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Faruqui and Dyer (2014)": "6513617"}}}, {"token_start": 79, "token_end": 122, "char_start": 362, "char_end": 598, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Schnabel et al. (2015)": "6197592"}, "Reference": {}}}]}
{"id": "13965916_0", "paragraph": "[BOS] Existing work on creating evaluations for word embeddings has focused on lexical semantics tasks.\n[BOS] An example of such tasks is WordSim-353 (Finkelstein et al., 2001) , in which a series of word pairs are assigned similarity judgments by human annotators, and these are compared to the similarity scores obtained from word embeddings.\n\n", "discourse_tags": ["Transition", "Single_summ"], "span_citation_mapping": [{"token_start": 24, "token_end": 69, "char_start": 138, "char_end": 344, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Finkelstein et al., 2001)": "52098500"}, "Reference": {}}}]}
{"id": "15736372_2", "paragraph": "[BOS] POS tagging with Brown clusters Brown et al. (1992) introduced the Brown clustering algorithm, which induces a hiearchy of clusters optimizing the likelihood of a hidden Markov model.\n[BOS] Each word is assigned to at most one cluster.\n[BOS] The algorithm can be used as an unsupervised POS tagger (Blunsom and Cohn, 2011) , but Brown clusters have also been used as features in discriminative sequence modeling (Turian et al., 2010) .\n[BOS] Ritter et al. (2011) and Owoputi et al. (2013) use Brown clusters induced from a large Twitter corpus to improve a POS tagger trained on a small corpus on hand-annotated tweets (Gimpel et al., 2011) .\n[BOS] Several recent papers on domain adaptation of POS taggers use discriminative taggers trained with Brown clusters as features as their baseline, e.g., .\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Narrative_cite", "Multi_summ", "Transition"], "span_citation_mapping": [{"token_start": 2, "token_end": 48, "char_start": 6, "char_end": 241, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 56, "token_end": 70, "char_start": 280, "char_end": 328, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Blunsom and Cohn, 2011)": "13341920"}}}, {"token_start": 81, "token_end": 93, "char_start": 385, "char_end": 439, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Turian et al., 2010)": "629094"}}}, {"token_start": 95, "token_end": 148, "char_start": 448, "char_end": 646, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ritter et al. (2011)": "12861120", "Owoputi et al. (2013)": "1528374"}, "Reference": {"(Gimpel et al., 2011)": null}}}]}
{"id": "15736372_1", "paragraph": "[BOS] Noeman and Madkour (2010) use FSAs for named entity transliteration, a problem which is very related to ours.\n[BOS] They learned transliteration patterns using techniques from phrasebased SMT, but formalized the transliteration grammars by composing FSAs.\n[BOS] Similarly, de Vinaspre et al. (2013) use FSAs to learn transliteration of SNOMED CT terms in Basque.\n[BOS] Spelling variations and transliteration seem to form a continuum, from nondialectal spelling variations such as Facebook/fbook, over dialectal variations such as Baltimore/Baltimaw (observed on Twitter), to cross-language variations such as Mnchen/Munich.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 2, "token_end": 58, "char_start": 6, "char_end": 261, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 59, "token_end": 88, "char_start": 268, "char_end": 368, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Vinaspre et al. (2013)": "13896830"}, "Reference": {}}}]}
{"id": "15736372_0", "paragraph": "[BOS] FSAs Many of the rules used in phonology and morphology can be analyzed as special cases of regular expressions, and many linguistic descriptions at this level can be compiled into finite state automata (FSAs) (Kaplan and Kay, 1994; Karttunen et al., 1997) .\n[BOS] Learning minimal FSAs from samples is generally NP-hard (Gold, 1978) , and most FSAs used to model phono-/morphotactic constraints have been manually constructed.\n[BOS] However, learning a minimal FSA for a fixed set of members of a Brown clusters, is obviously a much easier problem.\n[BOS] We extend the FSAs to capture spelling variations better using a simple propagation principle (see 3).\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 35, "token_end": 59, "char_start": 187, "char_end": 262, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kaplan and Kay, 1994;": "15971472", "Karttunen et al., 1997)": "18206987"}}}, {"token_start": 69, "token_end": 77, "char_start": 319, "char_end": 339, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Gold, 1978)": "8943792"}}}]}
{"id": "14847046_5", "paragraph": "[BOS] Finally, the idea of combining multiple surface renderings with the same meaning has also been explored in paraphrase generation.\n[BOS] Pang et al. (2003) propose an algorithm to align sets of parallel sentences driven entirely by the syntactic representations of the sentences.\n[BOS] The alignment algorithm outputs a merged lattice from which lexical, phrasal, and sentential paraphrases could simply be read off.\n[BOS] Barzilay and Lee (2003) cluster topically related sentences into slotted word lattices by using multiple sequence alignment for the purpose of downstream paraphrase generation from comparable corpora.\n[BOS] More recently, Zhao et al. (2010) perform round-trip translation of English sentences via different pivot languages and different off-the-shelf SMT systems to generate candidate paraphrases.\n[BOS] However, they do not combine the candidate paraphrases in any way.\n[BOS] A detailed survey of paraphrase generation techniques can be found in (Androutsopoulos and Malakasiotis, 2010) and (Madnani and Dorr, 2010) .\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 27, "token_end": 81, "char_start": 142, "char_end": 421, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Pang et al. (2003)": "11728052"}, "Reference": {}}}, {"token_start": 82, "token_end": 118, "char_start": 428, "char_end": 628, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Barzilay and Lee (2003)": "6387310"}, "Reference": {}}}, {"token_start": 119, "token_end": 175, "char_start": 635, "char_end": 898, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhao et al. (2010)": "6982885"}, "Reference": {}}}, {"token_start": 180, "token_end": 212, "char_start": 926, "char_end": 1044, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Madnani and Dorr, 2010)": null}}}]}
{"id": "14847046_4", "paragraph": "[BOS] Outside of the literature on grammatical error detection, our combination approach is directly related to the research on machine translation system combination wherein translation hypotheses produced by different SMT systems are combined to allow the extraction of a better, combined hypothesis (Bangalore et al., 2001; Rosti et al., 2007; Feng et al., 2009) .\n[BOS] However, our combination approach is different in that all the round-trip translations are produced by a single system but via different pivot languages.\n\n", "discourse_tags": ["Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 31, "token_end": 71, "char_start": 220, "char_end": 365, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bangalore et al., 2001;": "18943389", "Rosti et al., 2007;": "14570741", "Feng et al., 2009)": "3014359"}}}]}
{"id": "14847046_3", "paragraph": "[BOS] In contrast, we use multiple pivot languages to generate several round-trip translations.\n[BOS] In addition, we use a novel alignment algorithm that allows us to combine different parts of different round-trip translations and explore a whole new set of corrections that go beyond the translations themselves.\n[BOS] Finally, we do not restrict our analysis to any single type of error.\n[BOS] In fact, our test sentences contain several different types of grammatical errors.\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "14847046_2", "paragraph": "[BOS] The previous work that is most directly relevant to our approach is that of Hermet and Dsilets (2009) who focused only on sentences containing pre-marked preposition errors and generated a single round-trip translation for such sentences via a single pivot language (French).\n[BOS] They then simply posited this round-trip translation as the \"correction\" for the original sentence.\n[BOS] In their evaluation on sentences containing 133 unique preposition errors, their round-trip translation system was able to correct 66.4% of the cases.\n[BOS] However, this was outperformed by a simple method based on web counts (68.7%).\n[BOS] They also found that combining the roundtrip method with the web counts method into a hybrid system yielded higher performance (82.1%).\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 162, "char_start": 6, "char_end": 771, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hermet and D\u00e9silets (2009)": "8494338"}, "Reference": {}}}]}
{"id": "14847046_1", "paragraph": "[BOS] Park and Levy (2011) use a noisy channel model to achieve whole-sentence grammar correction; they learn a noise model from a dataset of errorful sentences but do not rely on SMT.\n[BOS] They show that the corrections produced by their model generally have higher n-gram overlap with human-authored reference corrections than the original errorful sentences.\n\n", "discourse_tags": ["Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 72, "char_start": 6, "char_end": 362, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Levy (2011)": "7784892"}, "Reference": {}}}]}
{"id": "14847046_0", "paragraph": "[BOS] To date, most work in grammatical error detection has focused on targeting specific error types (usually prepositions or article errors) by using rulebased methods or statistical machine-learning classification algorithms, or a combination of the two.\n[BOS] Leacock et al. (2010) present a survey of the common approaches.\n[BOS] However, targeted errors such as preposition and determiner errors are just two of the many types of grammatical errors present in nonnative writing.\n[BOS] One of the anonymous reviewers for this paper makes the point eloquently: \"Given the frequent complexity of learner errors, less holistic, error-type specific approaches are often unable to disentangle compounded errors of style and grammar.\"\n[BOS] Below we discuss related work that uses machine translation to address targeted errors and some recent work that also focused on whole-sentence error correction.\n[BOS] Brockett et al. (2006) use information about mass noun errors from a Chinese learner corpus to engineer a \"parallel\" corpus with sentences containing mass noun errors on one side and their corrected counterparts on the other.\n[BOS] With this parallel corpus, the authors use standard statistical machine translation (SMT) framework to learn a translation (correction) model which can then be applied to unseen sentences containing mass noun errors.\n[BOS] This approach was able to correct almost 62% of the errors found in a test set of 150 errors.\n[BOS] In our approach, we do not treat correction directly as a translation problem but instead rely on an MT system to round-trip translate an English sentence back to English.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Transition", "Reflection", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 48, "token_end": 65, "char_start": 264, "char_end": 328, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Leacock et al. (2010)": null}, "Reference": {}}}, {"token_start": 172, "token_end": 278, "char_start": 908, "char_end": 1456, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Brockett et al. (2006)": "757808"}, "Reference": {}}}]}
{"id": "15253725_0", "paragraph": "[BOS] Most of the DSL methods have a two phase architecture.\n[BOS] The first level is to determine the language group, the second level is to discriminate within the language group.\n[BOS] (Porta and Sancho, 2014) utilize maximum entropy models for the DSL task.\n[BOS] The first classifier determines the language group, the second works with empirically selected features that achieved best performance for the specific language group.\n[BOS] (Lui et al., 2014 ) also define a two phase approach involving a POS-tagger.\n[BOS] (Goutte et al., 2014) label the language group with a probabilistic model based on word co-occurrences in documents.\n[BOS] To discriminate at the language group level, SVM based classification is used.\n[BOS] (King et al., 2014) compare nave Bayes, logistic regression and SVM based classifiers.\n[BOS] They also preprocess the data with manually defined methods as named entity removal and English word removal.\n[BOS] (Purver, 2014) introduces a single-level approach, training a linear SVM on word and character ngrams of length 1-3.\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 37, "token_end": 82, "char_start": 188, "char_end": 435, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Porta and Sancho, 2014)": "18132289"}, "Reference": {}}}, {"token_start": 83, "token_end": 105, "char_start": 442, "char_end": 518, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Lui et al., 2014": "1843081"}, "Reference": {}}}, {"token_start": 106, "token_end": 148, "char_start": 525, "char_end": 726, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Goutte et al., 2014)": "15527754"}, "Reference": {}}}, {"token_start": 149, "token_end": 190, "char_start": 733, "char_end": 935, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(King et al., 2014)": "14538981"}, "Reference": {}}}, {"token_start": 191, "token_end": 221, "char_start": 942, "char_end": 1058, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Purver, 2014)": "10561748"}, "Reference": {}}}]}
{"id": "12252477_0", "paragraph": "[BOS] If we consider traditional cluster encoded word representation, e.g., Brown clusters (Brown et al., 1992) , it only uses a small number of bits to track the path on a hierarchical tree of word clusters to represent each word.\n[BOS] In fact, word embedding generalized the idea of discrete clustering representation to continuous vector representation in language models, with the goal of improving the continuous word analogy prediction and generalization ability (Bengio et al., 2003; Mikolov et al., 2013a; Mikolov et al., 2013b) .\n[BOS] However, it has been proven that Brown clusters as discrete features are even better than continuous word embedding as features for named entity recognition tasks (Ratinov and Roth, 2009 ).\n[BOS] Guo et al. (Guo et al., 2014) further tried to binarize embeddings using a threshold tuned for each dimension, and essentially used less than two bits to represent each dimension.\n[BOS] They have shown that binarization can be comparable to or even better than the original word embeddings when used as features for named entity recognition tasks.\n[BOS] Moreover, Faruqui et al. (Faruqui et al., 2015) showed that imposing sparsity constraints over the embedding vectors can further improve the representation interpretability and performance on several word similarity and text classification benchmark datasets.\n[BOS] These works indicate that, for some tasks, we do not need all the information encoded in \"standard\" word embeddings.\n[BOS] Nonetheless, it is clear that binarization loses a lot of information, and this calls for a systematic comparison of how many bits are needed to maintain the expressivity needed from word embeddings for different tasks.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Narrative_cite", "Single_summ", "Single_summ", "Single_summ", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 16, "token_end": 26, "char_start": 76, "char_end": 111, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Brown et al., 1992)": "10986188"}}}, {"token_start": 79, "token_end": 114, "char_start": 419, "char_end": 537, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bengio et al., 2003;": "2852525", "Mikolov et al., 2013a;": "16447573", "Mikolov et al., 2013b)": "7478738"}}}, {"token_start": 138, "token_end": 149, "char_start": 678, "char_end": 732, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ratinov and Roth, 2009": "1859014"}}}, {"token_start": 152, "token_end": 221, "char_start": 742, "char_end": 1089, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Guo et al., 2014)": "6540554"}, "Reference": {}}}, {"token_start": 222, "token_end": 270, "char_start": 1096, "char_end": 1355, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Faruqui et al. (Faruqui et al., 2015)": null}, "Reference": {}}}]}
{"id": "14954567_5", "paragraph": "[BOS] In this paper, we not only use all word senses disambiguated by an automatic system, but also make the semantic role labeling results to help word sense disambiguation synchronously with a joint model.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "14954567_4", "paragraph": "[BOS] Except for predicate and preposition senses, Che et al. (2010) explored all word senses for semantic role labeling.\n[BOS] They showed that all word senses can improve the semantic role labeling performance significantly.\n[BOS] However, the golden word senses were used in their experiments.\n[BOS] The results are still unknown when an automatic word sense disambiguation system is used.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 10, "token_end": 55, "char_start": 51, "char_end": 296, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Che et al. (2010)": "19754679"}, "Reference": {}}}]}
{"id": "14954567_3", "paragraph": "[BOS] Besides predicate senses, Dahlmeier et al. (2009) proposed a joint model to maximize probability of the preposition senses and the semantic role of prepositional phrases.\n\n", "discourse_tags": ["Single_summ"], "span_citation_mapping": [{"token_start": 6, "token_end": 36, "char_start": 32, "char_end": 176, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Dahlmeier et al. (2009)": "563931"}, "Reference": {}}}]}
{"id": "14954567_2", "paragraph": "[BOS] The above idea, that the predicate senses and the semantic role labeling can help each other, may be inspired by Haji et al. (2009 ), Surdeanu et al. (2008 , and Dang and Palmer (2005) .\n[BOS] They have shown that semantic role features are helpful to disambiguate verb senses and vice versa.\n\n", "discourse_tags": ["Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 8, "token_end": 51, "char_start": 31, "char_end": 190, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Haji\u010d et al. (2009": "9210201", "), Surdeanu et al. (2008": "6534839", "Dang and Palmer (2005)": null}}}]}
{"id": "14954567_1", "paragraph": "[BOS] Recently, Markov logic (Domingos and Lowd, 2009 ) became a hot framework for joint model.\n[BOS] It has been successfully used in temporal relations recognition (Yoshikawa et al., 2009 ), co-reference resolution (Poon and Domingos, 2008) , etc.\n[BOS] It is very easy to do joint modeling using Markov logic.\n[BOS] The only work is to define relevant formulas.\n[BOS] Meza-Ruiz and Riedel (2009) have joined semantic role labeling and predicate senses disambiguation with Markov logic.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Transition", "Transition", "Single_summ"], "span_citation_mapping": [{"token_start": 4, "token_end": 15, "char_start": 16, "char_end": 53, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Domingos and Lowd, 2009": "43589226"}}}, {"token_start": 31, "token_end": 42, "char_start": 135, "char_end": 189, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yoshikawa et al., 2009": "6945139"}}}, {"token_start": 44, "token_end": 58, "char_start": 193, "char_end": 242, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Poon and Domingos, 2008)": "7124715"}}}, {"token_start": 85, "token_end": 110, "char_start": 371, "char_end": 488, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Meza-Ruiz and Riedel (2009)": "1015652"}, "Reference": {}}}]}
{"id": "14954567_0", "paragraph": "[BOS] Joint models were often used in semantic role labeling community.\n[BOS] Toutanova et al. (2008) and Punyakanok et al. (2008) presented a re-ranking model and an integer linear programming model respectively to jointly learn a global optimal semantic roles assignment.\n[BOS] Besides jointly learning semantic role assignment of different constituents for one task (semantic role labeling), their methods have been used to jointly learn for two tasks (semantic role labeling and syntactic parsing).\n[BOS] However, it is easy for the re-ranking model to loss the optimal result, if it is not included in the top n results.\n[BOS] In addition, the integer linear programming model can only use hard constraints.\n[BOS] A lot of engineering work is also required in both models.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Multi_summ", "Multi_summ", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 14, "token_end": 126, "char_start": 78, "char_end": 625, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Toutanova et al. (2008)": "2243454", "Punyakanok et al. (2008)": null}, "Reference": {}}}]}
{"id": "12777818_1", "paragraph": "[BOS] Providing textual explanations for classification decisions has begun to receive attention, as part of increased interest in creating models whose decisions can be interpreted.\n[BOS] Lei et al. (2016) , jointly modeled both a classification decision, and the selection of the most relevant subsection of a document for making the classification decision.\n[BOS] Hendricks et al. (2016) generate textual explanations for visual classification problems, but in contrast to our model, they first generate an answer, and then, conditional on the answer, generate an explanation.\n[BOS] This effectively creates a post-hoc justification for a classification decision rather than a program for deducing an answer.\n[BOS] These papers, like ours, have jointly modeled rationales and answer predictions; however, we are the first to use rationales to guide program induction.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 29, "token_end": 61, "char_start": 189, "char_end": 360, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Lei et al. (2016)": "7205805"}, "Reference": {}}}, {"token_start": 62, "token_end": 127, "char_start": 367, "char_end": 711, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hendricks et al. (2016)": "12030503"}, "Reference": {}}}]}
{"id": "12777818_0", "paragraph": "[BOS] Extensive efforts have been made in the domain of math problem solving (Hosseini et al., 2014; Roy and Roth, 2015) , which aim at obtaining the correct answer to a given math problem.\n[BOS] Other work has focused on learning to map math expressions into formal languages (Roy et al., 2016 a single generative model that attempts to solve the problem while explaining the approach taken.\n[BOS] Our approach is strongly tied with the work on sequence to sequence transduction using the encoder-decoder paradigm (Sutskever et al., 2014; Bahdanau et al., 2014; Kalchbrenner and Blunsom, 2013) , and inherits ideas from the extensive literature on semantic parsing (Jones et al., 2012; Berant et al., 2013; Andreas et al., 2013; Quirk et al., 2015; Liang et al., 2016; Neelakantan et al., 2016) and program generation (Reed and de Freitas, 2016; Graves et al., 2016) , namely, the usage of an external memory, the application of different operators over values in the memory and the copying of stored values into the output sequence.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 11, "token_end": 30, "char_start": 56, "char_end": 120, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hosseini et al., 2014;": "428579", "Roy and Roth, 2015)": "560565"}}}, {"token_start": 53, "token_end": 65, "char_start": 238, "char_end": 294, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Roy et al., 2016": "10619801"}}}, {"token_start": 97, "token_end": 132, "char_start": 490, "char_end": 594, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sutskever et al., 2014;": "7961699", "Bahdanau et al., 2014;": "11212020", "Kalchbrenner and Blunsom, 2013)": "12639289"}}}, {"token_start": 142, "token_end": 193, "char_start": 649, "char_end": 795, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Jones et al., 2012;": "2493904", "Berant et al., 2013;": "6401679", "Andreas et al., 2013;": "1954146", "Quirk et al., 2015;": "2916543", "Liang et al., 2016;": "2742513", "Neelakantan et al., 2016)": "6715185"}}}, {"token_start": 194, "token_end": 213, "char_start": 800, "char_end": 867, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Reed and de Freitas, 2016;": "7034786", "Graves et al., 2016)": "205251479"}}}]}
{"id": "13972671_1", "paragraph": "[BOS] Our work takes inspiration from (Luong et al., 2013) and (Li et al., 2015) .\n[BOS] Similar to the former, we build representations for rare words on-the-fly from subword units.\n[BOS] However, we utilize recurrent neural networks with characters as the basic units; whereas Luong et al. (2013) use recursive neural networks with morphemes as units, which requires existence of a morphological analyzer.\n[BOS] In comparison with (Li et al., 2015) , our hybrid architecture is also a hierarchical sequence-to-sequence model, but operates at a different granularity level, word-character.\n[BOS] In contrast, Li et al. (2015) build hierarchical models at the sentence-word level for paragraphs and documents.\n\n", "discourse_tags": ["Reflection", "Reflection", "Single_summ", "Reflection", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 25, "char_start": 6, "char_end": 80, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Luong et al., 2013)": "14276764", "(Li et al., 2015)": "207468"}}}, {"token_start": 64, "token_end": 90, "char_start": 279, "char_end": 407, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Luong et al. (2013)": "14276764"}}}, {"token_start": 91, "token_end": 102, "char_start": 414, "char_end": 450, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Li et al., 2015)": "207468"}}}, {"token_start": 130, "token_end": 154, "char_start": 597, "char_end": 709, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Li et al. (2015)": "207468"}, "Reference": {}}}]}
{"id": "13972671_0", "paragraph": "[BOS] There has been a recent line of work on end-to-end character-based neural models which achieve good results for part-of-speech tagging (dos Santos and Zadrozny, 2014; Ling et al., 2015a) , dependency parsing (Ballesteros et al., 2015) , text classification (Zhang et al., 2015) , speech recognition (Chan et al., 2016; Bahdanau et al., 2016) , and language modeling (Kim et al., 2016; Jozefowicz et al., 2016) .\n[BOS] However, success has not been shown for cross-lingual tasks such as machine translation.\n[BOS] 1 Sennrich et al. (2016) propose to segment words into smaller units and translate just like at the word level, which does not learn to understand relationships among words.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Single_summ"], "span_citation_mapping": [{"token_start": 26, "token_end": 51, "char_start": 118, "char_end": 192, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Ling et al., 2015a)": "1689426"}}}, {"token_start": 52, "token_end": 64, "char_start": 195, "char_end": 240, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 65, "token_end": 75, "char_start": 243, "char_end": 283, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 76, "token_end": 95, "char_start": 286, "char_end": 347, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chan et al., 2016;": "14177763", "Bahdanau et al., 2016)": null}}}, {"token_start": 97, "token_end": 118, "char_start": 354, "char_end": 415, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kim et al., 2016;": null}}}, {"token_start": 140, "token_end": 175, "char_start": 521, "char_end": 692, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Sennrich et al. (2016)": "1114678"}, "Reference": {}}}]}
{"id": "15344879_1", "paragraph": "[BOS] There is also another line of research that attempts to design graph grammars such as hyperedge replacement grammar (HRG) (Chiang et al., 2013) and efficient graph-based algorithms for AMR parsing.\n[BOS] Existing work along this line is still theoretical in nature and no empirical results have been reported yet.\n\n", "discourse_tags": ["Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 17, "token_end": 34, "char_start": 92, "char_end": 149, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chiang et al., 2013)": "3545055"}}}]}
{"id": "15344879_0", "paragraph": "[BOS] Our work is directly comparable to JAMR (Flanigan et al., 2014) , the first published AMR parser.\n[BOS] JAMR performs AMR parsing in two stages: concept identification and relation identification.\n[BOS] They treat concept identification as a sequence labeling task and utilize a semi-Markov model to map spans of words in a sentence to concept graph fragments.\n[BOS] For relation identification, they adopt the graph-based techniques for non-projective dependency parsing.\n[BOS] Instead of finding maximum-scoring trees over words, they propose an algorithm to find the maximum spanning connected subgraph (MSCG) over concept fragments obtained from the first stage.\n[BOS] In contrast, we adopt a transition-based approach that finds its root in transition-based dependency parsing (Yamada and Matsumoto, 2003; Nivre, 2003; Sagae and Tsujii, 2008) , where a series of actions are performed to transform a sentence to a dependency tree.\n[BOS] As should be clear from our description, however, the actions in our parser are very different in nature from the actions used in transition-based dependency parsing.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 8, "token_end": 129, "char_start": 41, "char_end": 672, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Flanigan et al., 2014)": "5000956"}, "Reference": {}}}, {"token_start": 145, "token_end": 174, "char_start": 752, "char_end": 853, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yamada and Matsumoto, 2003;": "13163488", "Nivre, 2003;": "59829005", "Sagae and Tsujii, 2008)": "8836054"}}}]}
{"id": "1420583_3", "paragraph": "[BOS] Pathological verbosity can be an issue when tuning MERT on recall-oriented metrics such as METEOR (Lavie and Denkowski, 2009; Denkowski and Lavie, 2011) .\n[BOS] Large variance between the results obtained with MIRA has also been reported (Simianer et al., 2012) .\n[BOS] However, none of this work has focused on monsters.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 20, "token_end": 38, "char_start": 97, "char_end": 158, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lavie and Denkowski, 2009;": "207107614", "Denkowski and Lavie, 2011)": null}}}, {"token_start": 47, "token_end": 63, "char_start": 216, "char_end": 267, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Simianer et al., 2012)": "10217785"}}}]}
{"id": "1420583_2", "paragraph": "[BOS] With the emergence of new optimization techniques, there have been studies that compare stability between MIRA-MERT (Chiang et al., 2008; Chiang et al., 2009; Cherry and Foster, 2012) , PRO-MERT (Hopkins and May, 2011) , MIRA-PRO-MERT (Cherry and Foster, 2012; Gimpel and Smith, 2012; Nakov et al., 2012) .\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 18, "token_end": 47, "char_start": 112, "char_end": 189, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chiang et al., 2008;": "3506035", "Chiang et al., 2009;": "3544821", "Cherry and Foster, 2012)": "6620232"}}}, {"token_start": 48, "token_end": 59, "char_start": 192, "char_end": 224, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hopkins and May, 2011)": "4534193"}}}, {"token_start": 60, "token_end": 91, "char_start": 227, "char_end": 310, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cherry and Foster, 2012;": "6620232", "Gimpel and Smith, 2012;": "2131434", "Nakov et al., 2012)": "10107837"}}}]}
{"id": "1420583_1", "paragraph": "[BOS] The stability of MERT has been improved using regularization (Cer et al., 2008) , random restarts (Moore and Quirk, 2008) , multiple replications (Clark et al., 2011) , and parameter aggregation (Cettolo et al., 2011) .\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 11, "token_end": 20, "char_start": 52, "char_end": 85, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cer et al., 2008)": "1518309"}}}, {"token_start": 21, "token_end": 32, "char_start": 88, "char_end": 127, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Moore and Quirk, 2008)": "6809577"}}}, {"token_start": 33, "token_end": 43, "char_start": 130, "char_end": 172, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Clark et al., 2011)": "512833"}}}, {"token_start": 45, "token_end": 57, "char_start": 179, "char_end": 223, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cettolo et al., 2011)": null}}}]}
{"id": "1420583_0", "paragraph": "[BOS] We are not aware of previous work that discusses the issue of monsters, but there has been work on a different, length problem with PRO (Nakov et al., 2012) .\n[BOS] We have seen that its solution, fix the smoothing in BLEU+1, did not work for us.\n\n", "discourse_tags": ["Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 29, "token_end": 39, "char_start": 138, "char_end": 162, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Nakov et al., 2012)": "10107837"}}}]}
{"id": "13707541_0", "paragraph": "[BOS] Researchers developed many statistical methods and linguistic-rule-based methods to study automatic summarization (Banko et al., 2000; Dorr et al., 2003; Zajic et al., 2004; Cohn and Lapata, 2008) .\n[BOS] With the development of Neural Network in NLP, more and more researches have appeared in abstractive summarization since it seems possible that Neural Network can help achieve the two goals.\n[BOS] Rush et al. (2015) to-sequence model with attention mechanism to abstractive summarization and realized significant achievements.\n[BOS] Chopra et al. (2016) changed the ABS model with an RNN decoder and Nallapati et al. (2016) changed the system to a fully-RNN sequence-to-sequence model and achieved outstanding performance.\n[BOS] Zhou et al. (2017) proposed a selective gate mechanism to filter secondary information.\n[BOS] proposed a deep recurrent generative decoder to learn latent structure information.\n[BOS] Ma et al. (2018) proposed a model that generates words by querying word embeddings.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Single_summ", "Multi_summ", "Single_summ", "Other", "Single_summ"], "span_citation_mapping": [{"token_start": 16, "token_end": 53, "char_start": 96, "char_end": 202, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Banko et al., 2000;": "9952653", "Dorr et al., 2003;": "1729177", "Zajic et al., 2004;": "62613480", "Cohn and Lapata, 2008)": "2411338"}}}, {"token_start": 91, "token_end": 116, "char_start": 408, "char_end": 537, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Rush et al. (2015)": "1918428"}, "Reference": {}}}, {"token_start": 117, "token_end": 134, "char_start": 544, "char_end": 606, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Chopra et al. (2016)": "133195"}, "Reference": {}}}, {"token_start": 135, "token_end": 164, "char_start": 611, "char_end": 733, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Nallapati et al. (2016)": "8928715"}, "Reference": {}}}, {"token_start": 165, "token_end": 182, "char_start": 740, "char_end": 827, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhou et al. (2017)": "1770102"}, "Reference": {}}}, {"token_start": 196, "token_end": 214, "char_start": 924, "char_end": 1007, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ma et al. (2018)": "3708647"}, "Reference": {}}}]}
{"id": "1254267_4", "paragraph": "[BOS] Attention based methods have been successful in many application domains, such as image classification (Stollenga et al., 2014) , image caption generation , machine translation Luong et al., 2015) , and question answering (Shih et al., 2016; Chen et al., 2015; Yang et al., 2016) .\n[BOS] However, in the field of sentiment analysis, the attention is applied to only aspect-based sentiment classification (Yanase et al., 2016) .\n[BOS] To the best knowledge of ours, there is no attention-based model for a general sentiment analysis task.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 15, "token_end": 27, "char_start": 88, "char_end": 133, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Stollenga et al., 2014)": "7875983"}}}, {"token_start": 28, "token_end": 43, "char_start": 136, "char_end": 202, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Luong et al., 2015)": "1998416"}}}, {"token_start": 45, "token_end": 70, "char_start": 209, "char_end": 285, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Shih et al., 2016;": "11923637", "Chen et al., 2015;": "16566944", "Yang et al., 2016)": "8849206"}}}, {"token_start": 87, "token_end": 101, "char_start": 372, "char_end": 431, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yanase et al., 2016)": "17844515"}}}]}
{"id": "1254267_3", "paragraph": "[BOS] Two notable pioneers in using lexicon for sentiment analysis are Mohammad et al. (2013a) ; Kalchbrenner et al. (2014b) generated scores with other manually generated sentiment lexicon scores to achieved the state-of-the-art result in SemEval-2013 Twitter sentiment analysis task.\n[BOS] In general domain, Hu and Liu (2004) generated a user review lexicon that showed promising result in capturing sentiment in customer product reviews.\n\n", "discourse_tags": ["Multi_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 66, "char_start": 6, "char_end": 285, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Mohammad et al. (2013a)": "13845267", "Kalchbrenner et al. (2014b)": "1306065"}, "Reference": {}}}, {"token_start": 67, "token_end": 94, "char_start": 292, "char_end": 441, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hu and Liu (2004)": "207155218"}, "Reference": {}}}]}
{"id": "1254267_2", "paragraph": "[BOS] Endeavors to capture n-gram information bore fruits with CNN, max pooling, and softmax (Collobert et al., 2011; Kim, 2014) , which is regarded as the standard methods of the document classification problem these days.\n[BOS] Kalchbrenner et al. (2014a) extended this standard CNN model with dynamic k-max pooling, which served as an input layer to another stacked convolution layer.\n[BOS] Multichannel CNN methods (Kim, 2014; Yin and Schtze, 2015) are another branch of CNN, where assorted embeddings are considered together when convolving the input.\n[BOS] Unlike Kim (2014) 's model that relies on a single type of embedding with different mutability characteristics of the weights of embedding layer, Yin and Schtze (2015) incorporates diverse sort of embedding types using multichannel CNN.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Multi_summ", "Multi_summ"], "span_citation_mapping": [{"token_start": 14, "token_end": 35, "char_start": 63, "char_end": 128, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Collobert et al., 2011;": "351666", "Kim, 2014)": null}}}, {"token_start": 52, "token_end": 88, "char_start": 230, "char_end": 387, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kalchbrenner et al. (2014a)": "1306065"}, "Reference": {}}}, {"token_start": 89, "token_end": 127, "char_start": 394, "char_end": 556, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Kim, 2014;": null, "Yin and Sch\u00fctze, 2015)": "7146903"}, "Reference": {}}}, {"token_start": 128, "token_end": 155, "char_start": 563, "char_end": 707, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kim (2014)": null}, "Reference": {}}}, {"token_start": 156, "token_end": 176, "char_start": 709, "char_end": 799, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Yin and Sch\u00fctze (2015)": "7146903"}, "Reference": {}}}]}
{"id": "1254267_1", "paragraph": "[BOS] Since the emergence of the Convolutional Neural Networks (CNN; Collobert et al. (2011) ), conventional methods have become gradually obsolete because of the outstanding performance from the CNN variants.\n[BOS] CNN based models are distinguished from earlier methods because they do not rely on laborious feature engineering.\n[BOS] The first success of CNN in sentiment analysis was triggered by document classification research (Kim, 2014) , where CNN showed state-of-the-art results in numerous document classification datasets.\n[BOS] This success has engendered an upsurge in deep neural network research for sentiment analysis.\n[BOS] Various modified models have been proposed in the literature.\n[BOS] One of the famous deep learning methods that models a document is the generalized phrase proposed by Yin and Schtze (2014) , which represents a sentence using element-wise addition, multiplication, or recursive auto-encoder.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Single_summ", "Transition", "Transition", "Single_summ"], "span_citation_mapping": [{"token_start": 7, "token_end": 21, "char_start": 33, "char_end": 92, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Collobert et al. (2011)": "351666"}}}, {"token_start": 62, "token_end": 99, "char_start": 337, "char_end": 535, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Kim, 2014)": null}, "Reference": {}}}, {"token_start": 131, "token_end": 175, "char_start": 711, "char_end": 935, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Yin and Sch\u00fctze (2014)": "14956855"}, "Reference": {}}}]}
{"id": "1254267_0", "paragraph": "[BOS] The first attempt of sentiment analysis on text was initiated by Pang et al. (2002) who pioneered this field by using bag-of-word features.\n[BOS] This work mostly hinged on feature engineering; since then, many kinds of feature learning methods had been introduced to increase the performance (Pang and Lee, 2008; Liu, 2012; Gimpel et al., 2011; Feldman, 2013; Mohammad et al., 2013b) .\n[BOS] Aside from pure machine learning approaches, lexicon based approaches had been another trend, which relied on the manual or algorithmic creation of word sentiment scores (Hu and Liu, 2004; Kim and Hovy, 2004; Ding et al., 2008; Taboada et al., 2011) .\n\n", "discourse_tags": ["Single_summ", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 2, "token_end": 35, "char_start": 6, "char_end": 145, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Pang et al. (2002)": "7105713"}, "Reference": {}}}, {"token_start": 51, "token_end": 96, "char_start": 226, "char_end": 390, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Pang and Lee, 2008;": null, "Liu, 2012;": "114955219", "Gimpel et al., 2011;": "14113765", "Feldman, 2013;": "18397233", "Mohammad et al., 2013b)": "13845267"}}}, {"token_start": 116, "token_end": 155, "char_start": 509, "char_end": 648, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hu and Liu, 2004;": "207155218", "Kim and Hovy, 2004;": "5690545", "Ding et al., 2008;": "12442299", "Taboada et al., 2011)": "3181362"}}}]}
{"id": "13002266_3", "paragraph": "[BOS] All the works presented in this section agree that implicit arguments must be modeled as a particular case of coreference together with features that include lexical-semantic information, to build selectional preferences.\n[BOS] Another common point is the fact that these works try to solve each instance of the implicit arguments independently, without taking into account the previous realizations of the same implicit argument in the document.\n[BOS] We propose that these realizations, together with the explicit ones, must maintain a certain coherence along the document and, in consequence, the filler of an argument remains the same along the following instances of that argument until a stronger evidence indicates a change.\n[BOS] We also propose that this feature can be exploited independently from the predicate.\n\n", "discourse_tags": ["Transition", "Transition", "Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "13002266_2", "paragraph": "[BOS] On the other hand, Chai (2010, 2012) studied the implicit argument resolution on NomBank.\n[BOS] They uses a set of syntactic, semantic and coreferential features to train a logistic regres-sion classifier.\n[BOS] Unlike the dataset from SemEval-2010 (Ruppenhofer et al., 2010) , in this work the authors focused on a small set of ten predicates.\n[BOS] But for those predicates, they annotated a large amount of instances in the documents from the Wall Street Journal that were already annotated for PropBank (Palmer et al., 2005) and NomBank.\n[BOS] This allowed them to avoid the sparseness problems and generalize properly from the training set.\n[BOS] The results of this system were far better than those obtained by the systems that faced the SemEval-2010 dataset.\n[BOS] This works represent the deepest study so far of the features that characterizes the implicit arguments 2 .\n[BOS] However, many of the most important features are lexically dependent on the predicate and cannot been generalized.\n[BOS] Thus, specific annotations are required for each new predicate to be analyzed.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Multi_summ", "Narrative_cite", "Transition", "Transition", "Transition", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 2, "token_end": 46, "char_start": 6, "char_end": 211, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 51, "token_end": 82, "char_start": 242, "char_end": 350, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Ruppenhofer et al., 2010)": "12623074"}, "Reference": {}}}, {"token_start": 108, "token_end": 118, "char_start": 504, "char_end": 534, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Palmer et al., 2005)": "2486369"}}}]}
{"id": "13002266_1", "paragraph": "[BOS] Recently, the task has been taken up again around two different proposals.\n[BOS] On the one hand, Ruppenhofer et al. (2010) presented a task in SemEval-2010 that included an implicit argument identification challenge based on FrameNet (Baker et al., 1998) .\n[BOS] The corpus for this task consisted in some novel chapters.\n[BOS] They covered a wide variety of nominal and verbal predicates, each one having only a small number of instances.\n[BOS] Only two systems were presented for this subtask obtaining quite poor results (F1 below 0,02).\n[BOS] VENSES++ (Tonelli and Delmonte, 2010 ) applied a rule based anaphora resolution procedure and semantic similarity between candidates and thematic roles using WordNet (Fellbaum, 1998) .\n[BOS] The system was tuned in (Tonelli and Delmonte, 2011) improving slightly its performance.\n[BOS] SEMAFOR (Chen et al., 2010 ) is a supervised system that extended an existing semantic role labeler to enlarge the search window to other sentences, replacing the features defined for regular arguments with two new semantic features.\n[BOS] Although this system obtained the best performance in the task, data sparseness strongly affected the results.\n[BOS] Besides the two systems presented to the task, some other systems have used the same dataset and evaluation metrics.\n[BOS] Ruppenhofer et al. (2011), Laparra and Rigau (2012) , Gorinski et al. (2013) and Laparra and Rigau (2013) explore alternative linguistic and semantic strategies.\n[BOS] These works obtained significant gains over previous approaches.\n[BOS] Silberer and Frank (2012) adapted an entity-based coreference resolution model to extend automatically the training corpus.\n[BOS] Exploiting this additional data, their system was able to improve previous results.\n[BOS] Following this approach Moor et al. (2013) present a corpus of predicate-specific annotations for verbs in the FrameNet paradigm that are aligned with PropBank and VerbNet.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Narrative_cite", "Single_summ", "Transition", "Transition", "Narrative_cite", "Transition", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 17, "token_end": 118, "char_start": 87, "char_end": 547, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ruppenhofer et al. (2010)": "12623074"}, "Reference": {"(Baker et al., 1998)": "2505531"}}}, {"token_start": 119, "token_end": 159, "char_start": 554, "char_end": 736, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Tonelli and Delmonte, 2010": "10392751"}, "Reference": {"(Fellbaum, 1998)": "60033325"}}}, {"token_start": 161, "token_end": 177, "char_start": 745, "char_end": 797, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Tonelli and Delmonte, 2011)": "6235073"}, "Reference": {}}}, {"token_start": 183, "token_end": 229, "char_start": 840, "char_end": 1073, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Chen et al., 2010": "12258556"}, "Reference": {}}}, {"token_start": 272, "token_end": 311, "char_start": 1320, "char_end": 1425, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Rigau (2012)": "18398984", "Gorinski et al. (2013)": "7060672", "Laparra and Rigau (2013)": "4357435"}}}, {"token_start": 329, "token_end": 368, "char_start": 1559, "char_end": 1772, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Silberer and Frank (2012)": "9016539"}, "Reference": {}}}, {"token_start": 369, "token_end": 405, "char_start": 1779, "char_end": 1951, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "13002266_0", "paragraph": "[BOS] The first attempt for the automatic annotation of implicit semantic roles was proposed by Palmer et al. (1986) .\n[BOS] This work applied selectional restrictions together with coreference chains, in a very specific domain.\n[BOS] In a similar approach, Whittemore et al. (1991) also attempted to solve implicit 1 http://adimen.si.ehu.es/web/ImpAr arguments using some manually described semantic constraints for each thematic role they tried to cover.\n[BOS] Another early approach was presented by Tetreault (2002) .\n[BOS] Studying another specific domain, they obtained some probabilistic relations between some roles.\n[BOS] These early works agree that the problem is, in fact, a special case of anaphora or coreference resolution.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 2, "token_end": 43, "char_start": 6, "char_end": 228, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Palmer et al. (1986)": "5249956"}, "Reference": {}}}, {"token_start": 44, "token_end": 99, "char_start": 235, "char_end": 456, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Whittemore et al. (1991)": "6848252"}, "Reference": {}}}, {"token_start": 100, "token_end": 128, "char_start": 463, "char_end": 624, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Tetreault (2002)": null}, "Reference": {}}}]}
{"id": "12346560_1", "paragraph": "[BOS] The usage we make in this work of the word 'that' as an initial weak label is closely related to the idea of distant supervision (Mintz et al., 2009 ).\n[BOS] In the context of argument mining, (AlKhatib et al., 2016 ) also used noisy labels to train a classifier, albeit for a different task.\n[BOS] They exploited the manually curated idebate.org resource to define -admittedly noisy -labeled data, that were used to train an argument mining classification scheme.\n[BOS] In contrast, our approach requires no data curation and relies on a simple linguistic observation of the typical role of 'that' in argumentative text.\n[BOS] Our use of the token 'that' as a weak label to identify a relevant lexicon, is also reminiscent of the classical work by (Hearst, 1992) who suggested to use lexico-syntactic patterns to identify various lexical relations.\n[BOS] However, to the best of our knowledge, the present work is the first to use such a paradigm in the context of argument mining.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 27, "token_end": 37, "char_start": 115, "char_end": 154, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mintz et al., 2009": "10910955"}}}, {"token_start": 40, "token_end": 106, "char_start": 164, "char_end": 470, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 161, "token_end": 170, "char_start": 737, "char_end": 769, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hearst, 1992)": "15763200"}}}]}
{"id": "12346560_0", "paragraph": "[BOS] Context dependent claim detection (i.e. the detection of claims that support/contest a given topic) was first suggested by .\n[BOS] Next, (Lippi and Torroni, 2015) proposed the context independent claim detection task, in which one attempts to detect claims without having the topic as input.\n[BOS] Thus, if the texts contain claims for multiple topics, all should be detected.\n[BOS] Both works used the data in for training and testing their models.\n[BOS] have first described 'that' as an indicator for sentences containing claims.\n[BOS] Other works have identified additional indicators of claims, such as discourse markers, and have used them within a rule-based, rather than a supervised, framework (Eckle-Kohler et al., 2015; Ong et al., 2014; Somasundaran and Wiebe, 2009; Schneider and Wyner, 2012) .\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Transition", "Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 29, "token_end": 79, "char_start": 137, "char_end": 382, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Lippi and Torroni, 2015)": null}, "Reference": {}}}, {"token_start": 138, "token_end": 176, "char_start": 699, "char_end": 811, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Eckle-Kohler et al., 2015;": "88666", "Ong et al., 2014;": "16985582", "Somasundaran and Wiebe, 2009;": "2845337", "Schneider and Wyner, 2012)": "7299665"}}}]}
{"id": "12524250_5", "paragraph": "[BOS] The difference between the above methods and ours is that our word-level features are dynamically generated in the decoding stage without exhaustive or preprocessed word segmentation.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "12524250_4", "paragraph": "[BOS] Sun (2011) presents a stacked sub-word model for joint Chinese word segmentation and POS tagging.\n[BOS] By merging the outputs of the three predictors (including one word-based segmenter) into sub-word sequences, rich contextual features can be approximately derived.\n[BOS] The experiments are conducted to show the effectiveness of using word-based information.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 2, "token_end": 54, "char_start": 6, "char_end": 273, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "12524250_3", "paragraph": "[BOS] Semi-Markov Conditional Random Fields (semi-CRF) (Sarawagi and Cohen, 2004) is a model in which segmentation task is implicitly included into the decoding algorithm.\n[BOS] In this model, feature representation would be more flexible than traditional CRFs, since features can be extracted from the previous/the next segmentation within a window of variable size.\n[BOS] The problem of this approach lies in that the decoding algorithm depends on the predefined window size to exploit the boundaries of segmentations but not the real length of words.\n[BOS] Bunescu (2008) presents an improved pipeline model in which the output of the previous subtasks are considered as hidden variables, and the hidden variables together with their probabilities denoting the confidence are used as probabilistic features in the next subtasks.\n[BOS] One shortcoming of this method is inefficiency caused by the calculation of marginal probabilities of features.\n[BOS] The other disadvantages of the pipeline method are error propagation and the need of separate training of different subtasks in the pipeline.\n[BOS] Another disadvantage of pipeline method is error propagation.\n[BOS] Jiang et al. (2008) proposes a cascaded linear model for joint Chinese word segmentation and POS tagging.\n[BOS] With a character-based perceptron as the core, combined with real-valued features such as language models, the cascaded model can efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly.\n[BOS] However, they use POS tags or word information in a BruteForce way, which may suffer from the problem of time complexity.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Transition", "Transition", "Transition", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 105, "char_start": 6, "char_end": 553, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Sarawagi and Cohen, 2004)": null}, "Reference": {}}}, {"token_start": 106, "token_end": 154, "char_start": 560, "char_end": 831, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Bunescu (2008)": "8375885"}, "Reference": {}}}, {"token_start": 210, "token_end": 304, "char_start": 1172, "char_end": 1646, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Jiang et al. (2008)": "9285364"}, "Reference": {}}}]}
{"id": "12524250_2", "paragraph": "[BOS] Since we choose to use dynamic word-level features to improve the performance of POS tagging, we also review some works on word-level features.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "12524250_1", "paragraph": "[BOS] However, the analysis of Chinese-English mixed texts is rarely involved in previous literature.\n[BOS] In the aspect of the general multilingual POS tagging, most works focus on modeling cross-lingual correlations and tagging multilingual POS on respective monolingual texts, not on mixed texts (Cucerzan and Yarowsky, 2002; Yarowsky et al., 2001; Naseem et al., 2009 ).\n\n", "discourse_tags": ["Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 36, "token_end": 85, "char_start": 192, "char_end": 372, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cucerzan and Yarowsky, 2002;": "6902891", "Yarowsky et al., 2001;": "15279538", "Naseem et al., 2009": "9087527"}}}]}
{"id": "12524250_0", "paragraph": "[BOS] In recent years, POS tagging has undergone great development.\n[BOS] The mainstream method is to regard POS tagging as sequence labeling problems (Rabiner, 1990; Xue, 2003; Peng et al., 2004; Ng and Low, 2004) .\n\n", "discourse_tags": ["Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 23, "token_end": 49, "char_start": 124, "char_end": 214, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rabiner, 1990;": null, "Xue, 2003;": null, "Peng et al., 2004;": "10649571", "Ng and Low, 2004)": "11383732"}}}]}
{"id": "13748556_1", "paragraph": "[BOS] 3 The Approach\n\n", "discourse_tags": ["Other"], "span_citation_mapping": []}
{"id": "13748556_0", "paragraph": "[BOS] Several approaches have been proposed to train NMT models without direct parallel corpora.\n[BOS] The scenario that has been widely investigated is one where two languages have little parallel data between them but are well connected by one pivot language.\n[BOS] The most typical approach in this scenario is to independently translate from the source language to the pivot language and from the pivot language to the target language (Saha et al., 2016; Cheng et al., 2017) .\n[BOS] To improve the translation performance, Johnson et al. (2016) propose a multilingual extension of a standard NMT model and they achieve substantial improvement for language pairs without direct parallel training data.\n[BOS] Recently, motivated by the success of crosslingual embeddings, researchers begin to show interests in exploring the more ambitious scenario where an NMT model is trained from monolingual corpora only.\n[BOS] and Artetxe et al. (2017b) simultaneously propose an approach for this scenario, which is based on pre-trained cross lingual embeddings.\n[BOS] utilizes a single encoder and a single decoder for both languages.\n[BOS] The entire system is trained to reconstruct its perturbed input.\n[BOS] For cross-lingual translation, they incorporate back-translation into the training procedure.\n[BOS] Different from , Artetxe et al. (2017b) use two independent decoders with each for one language.\n[BOS] The two works mentioned above both use a single shared encoder to guarantee the shared latent space.\n[BOS] However, a concomitant defect is that the shared encoder is weak in keeping the uniqueness of each language.\n[BOS] Our work also belongs to this more ambitious scenario, and to the best of our knowledge, we are one among the first endeavors to investigate how to train an NMT model with monolingual corpora only.\n[BOS] is the translation in reversed direction.\n[BOS] D l is utilized to assess whether the hidden representation of the encoder is from the source or target language.\n[BOS] D g1 and D g2 are used to evaluate whether the translated sentences are realistic for each language respectively.\n[BOS] Z represents the shared-latent space.\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite", "Single_summ", "Transition", "Single_summ", "Other", "Single_summ", "Single_summ", "Single_summ", "Transition", "Transition", "Reflection", "Other", "Other", "Other", "Other"], "span_citation_mapping": [{"token_start": 75, "token_end": 93, "char_start": 423, "char_end": 478, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Saha et al., 2016;": "5983351", "Cheng et al., 2017)": null}}}, {"token_start": 95, "token_end": 133, "char_start": 487, "char_end": 704, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Johnson et al. (2016)": "6053988"}, "Reference": {}}}, {"token_start": 173, "token_end": 203, "char_start": 922, "char_end": 1054, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Artetxe et al. (2017b)": "3515219"}, "Reference": {}}}, {"token_start": 250, "token_end": 271, "char_start": 1322, "char_end": 1401, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Artetxe et al. (2017b)": "3515219"}, "Reference": {}}}]}
{"id": "14490268_1", "paragraph": "[BOS] Regarding the basic unit of transliteration, traditional systems are mostly phoneme-based (e.g. Knight and Graehl, 1998) .\n[BOS] Li et al. (2004) suggested a grapheme-based Joint SourceChannel Model within the Direct Orthographic Mapping framework.\n[BOS] Models based on characters (e.g. Shishtla et al., 2009) , syllables (e.g. Wutiwiwatchai and Thangthai, 2010) , as well as hybrid units (e.g. Oh and Choi, 2005) , are also seen.\n[BOS] In addition to phonetic features, others like temporal, semantic, and tonal features have also been found useful in transliteration (e.g. Tao et al., 2006; Li et al., 2007; Kwong, 2009) .\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 15, "token_end": 33, "char_start": 82, "char_end": 126, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 35, "token_end": 61, "char_start": 135, "char_end": 254, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Li et al. (2004)": "1693404"}, "Reference": {}}}, {"token_start": 65, "token_end": 81, "char_start": 277, "char_end": 316, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Shishtla et al., 2009)": null}}}, {"token_start": 82, "token_end": 101, "char_start": 319, "char_end": 369, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Wutiwiwatchai and Thangthai, 2010)": "81906"}}}, {"token_start": 105, "token_end": 118, "char_start": 383, "char_end": 420, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Oh and Choi, 2005)": "17161141"}}}, {"token_start": 146, "token_end": 173, "char_start": 560, "char_end": 629, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Tao et al., 2006;": "8424232", "Li et al., 2007;": "7527801", "Kwong, 2009)": "14074548"}}}]}
{"id": "14490268_0", "paragraph": "[BOS] The reports of the shared task in NEWS 2009 (Li et al., 2009 ) and NEWS 2010 (Li et al., 2010 highlighted two particularly popular approaches for transliteration generation among the participating systems.\n[BOS] One is phrase-based statistical machine transliteration (e.g. Song et al., 2010; Finch and Sumita, 2010 ) and the other is Conditional Random Fields which treats the task as one of sequence labelling (e.g. Shishtla et al., 2009) .\n[BOS] Besides these popular methods, for instance, Huang et al. (2011) used a non-parametric Bayesian learning approach in a recent study.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Single_summ"], "span_citation_mapping": [{"token_start": 9, "token_end": 18, "char_start": 40, "char_end": 66, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Li et al., 2009": "62005149"}}}, {"token_start": 20, "token_end": 29, "char_start": 73, "char_end": 99, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 47, "token_end": 74, "char_start": 225, "char_end": 321, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Song et al., 2010;": "8792705", "Finch and Sumita, 2010": "14251200"}}}, {"token_start": 79, "token_end": 106, "char_start": 341, "char_end": 446, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Shishtla et al., 2009)": null}}}, {"token_start": 116, "token_end": 136, "char_start": 500, "char_end": 587, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Huang et al. (2011)": "5852298"}, "Reference": {}}}]}
{"id": "1480182_3", "paragraph": "[BOS] There has been growing interest over the last few years in applying active learning methods to reduce the annotation burden involved in developing corpus-trained NLP modules.\n[BOS] Active learning has been applied to a variety of Information Extraction tasks, including name tagging, parsing, partial parsing, relation extraction, etc.\n[BOS] (Majidi and Crane, 2013) We have previously investigated active learning methods based on co-testing for training relation extractors for ACE relations (Fu and Grishman, 2013) .\n[BOS] We have also applied such methods for the active learning of ACE event extractors, although with a very different approach (based on the distribution of event triggers across sentences) from that proposed here (Liao and Grishman, 2011) .\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 42, "token_end": 70, "char_start": 236, "char_end": 372, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 88, "token_end": 99, "char_start": 486, "char_end": 523, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Fu and Grishman, 2013)": "7781827"}}}, {"token_start": 112, "token_end": 148, "char_start": 593, "char_end": 767, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Liao and Grishman, 2011)": "16579596"}}}]}
{"id": "1480182_2", "paragraph": "[BOS] There have been several efforts over the past decade to develop semi-supervised methods for learning such pattern sets.\n[BOS] One thread began with Riloff's observation that patterns occurring with substantially higher frequency in relevant documents than in irrelevant documents are likely to be good extraction patterns (Riloff, 1996) .\n[BOS] (Sudo et al., 2003) sorted relevant from irrelevant documents using a topic description and information retrieval engine.\n[BOS] (Yangarber et al., 2000; Yangarber, 2003 ) developed a bootstrapping approach, starting with some seed patterns, using these patterns to identify some relevant documents, using these documents to identify additional patterns, etc.\n[BOS] This approach was further refined in (Surdeanu et al., 2006) , which explored alternative pattern ranking strategies.\n[BOS] An alternative approach was adopted in (Stevenson and Greenwood, 2005) , which used Wordnet-based similarity to expand an initial set of event patterns.\n[BOS] (Huang and Riloff, 2012 ) developed a bootstrapping system to discover new triggers with selected roles.\n[BOS] For example, the word \"sniper\" is very likely to be the agent of a Die event.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Multi_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 24, "token_end": 62, "char_start": 132, "char_end": 342, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Riloff, 1996)": "15894892"}, "Reference": {}}}, {"token_start": 64, "token_end": 87, "char_start": 351, "char_end": 472, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Sudo et al., 2003)": "16749388"}, "Reference": {}}}, {"token_start": 88, "token_end": 135, "char_start": 479, "char_end": 709, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Yangarber et al., 2000;": "2344397", "Yangarber, 2003": "1398439"}, "Reference": {}}}, {"token_start": 136, "token_end": 161, "char_start": 716, "char_end": 833, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Surdeanu et al., 2006)": "1398439"}, "Reference": {}}}, {"token_start": 162, "token_end": 194, "char_start": 840, "char_end": 992, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Stevenson and Greenwood, 2005)": "6008231"}, "Reference": {}}}, {"token_start": 195, "token_end": 239, "char_start": 999, "char_end": 1187, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Huang and Riloff, 2012": "2213149"}, "Reference": {}}}]}
{"id": "1480182_1", "paragraph": "[BOS] Efforts to improve event extraction performance have focused largely on either improving the pattern-matching kernel or adding new reasonable features.\n[BOS] Most event extraction frameworks are feature-based systems.\n[BOS] Some of the featurebased systems are based on phrase or sentence level extraction.\n[BOS] Several recent studies use highlevel information to aid local event extraction systems.\n[BOS] For example, (Finkel et al., 2005) , (Maslennikov and Chua, 2007) , (Ji and Grishman, 2008) and (Patwardhan and Riloff, 2007) tried to use discourse, document, or cross-document information to improve information extraction.\n[BOS] Other research extends these approaches by introducing cross-event information to enhance the performance of multi-event-type extraction systems.\n[BOS] (Liao and Grishman, 2010) use information about other types of events to make predictions or resolve ambiguities regarding a given event.\n[BOS] (Li et al., 2013 ) implements a joint model via structured prediction with cross-event features.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Transition", "Multi_summ", "Transition", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 69, "token_end": 157, "char_start": 413, "char_end": 789, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Finkel et al., 2005)": "10977241", "(Maslennikov and Chua, 2007)": "18373718", "(Ji and Grishman, 2008)": "1320606", "(Patwardhan and Riloff, 2007)": "5749336"}, "Reference": {}}}, {"token_start": 158, "token_end": 187, "char_start": 796, "char_end": 933, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Liao and Grishman, 2010)": "11187670"}, "Reference": {}}}, {"token_start": 188, "token_end": 209, "char_start": 940, "char_end": 1036, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Li et al., 2013": "2114517"}, "Reference": {}}}]}
{"id": "1480182_0", "paragraph": "[BOS] Although there have been quite a few distinct designs for event extraction systems, most are loosely based on using patterns to detect instances of events, where the patterns consist of a predicate, event trigger, and constraints on its local syntactic context.\n[BOS] The constraints may involve specific lexical items or semantic classes.\n\n", "discourse_tags": ["Transition", "Transition"], "span_citation_mapping": []}
{"id": "14738389_16", "paragraph": "[BOS] Applying the described transformations allows us to obtain a tree representation of a tagged dependency graph that includes co-reference relations.\n\n", "discourse_tags": ["Other"], "span_citation_mapping": []}
{"id": "14738389_15", "paragraph": "[BOS] $DI arcs are depicted as dark blue links.\n[BOS] In cases where a word participates in a co-reference chain with a word that precedes it, there is no link between the word and it's $CR node.\n[BOS] Instead, its $CR node is connected to the $CR node of the first word in the co-reference chain.\n[BOS] Such arcs ($SDI) are depicted as light blue links.\n\n", "discourse_tags": ["Other", "Other", "Other", "Other"], "span_citation_mapping": []}
{"id": "14738389_14", "paragraph": "[BOS] The next problem is representing the coreferencial relations via the dependency tree.\n[BOS] In order to do this, we introduce yet another type of service node, denoted as $CR.\n[BOS] Such nodes are inserted on the right side of the corresponding wordform node, and to the left of the first POStag candidate node in the linear representation of the sentence.\n[BOS] We classify these nodes into two groups.\n[BOS] The first group consists of nodes, attached to wordforms that do not participate in a co-reference relation with another wordform that precedes them in the sentence.\n[BOS] These $CR nodes are linked to their wordform with an arc labeled $DI (discourse index), which might be linked to an entity in the discourse.\n[BOS] The second group of $CR nodes are those participating in a co-reference relation between their corresponding wordform and another wordform that precedes them in the sentence.\n[BOS] We say that such nodes share a discourse index with a word preceding them in the sentence, and assign the $SDI label to the arcs that interconnect such pairs service nodes.\n[BOS] The nodes in the second group are not connected to their corresponding wordform nodes, but are instead connected to the coreference nodes of the referenced entities.\n[BOS] This approach allows us to represent the co-reference relations as supplementary tree fragments, attached to the original tree.\n[BOS] Figure 4 presents an example of a sentence tree that contains both kinds of coreference nodes and the means through which they are connected to the graph.\n\n", "discourse_tags": ["Other", "Reflection", "Other", "Reflection", "Other", "Other", "Other", "Other", "Other", "Other", "Other"], "span_citation_mapping": []}
{"id": "14738389_13", "paragraph": "[BOS] These tags are included in the tree as service nodes.\n[BOS] In the linear representation of the sentence, they are inserted after the node for the corresponding wordform, and before the node for the next wordform to the right.\n[BOS] They are connected to the corresponding wordform with a special link $TAG.\n[BOS] In order to indicate the correct tag, we introduce another type of service node.\n[BOS] In the linear representation of the sentence, it is inserted after the last POS tag candidate node, and before the one corresponding to the next wordform to the right.\n[BOS] This node is connected to the correct tag via a special arc $CTAG (correct tag).\n[BOS] In this way, all information about the potential tags and the correct tag is represented in the form of a subtree, attached to the wordform.\n[BOS] Figure 3 depicts the encoding of a word with POS tag ambiguity.\n[BOS] The correct tag is indicated: verb, personal, perfective, transitive, finite, aorist, third person, singular.\n[BOS] The $TAG arcs are represented as red links without labels.\n[BOS] The $CTAG arc is represented as an oval.\n\n", "discourse_tags": ["Other", "Other", "Other", "Other", "Other", "Other", "Other", "Other", "Other", "Other", "Other"], "span_citation_mapping": []}
{"id": "14738389_12", "paragraph": "[BOS] As a first step of extending the tree, we assume a range of possible POS tags for each wordform in the sentence.\n[BOS] Such a range of tags has to contain the correct tag for the wordform in the given context.\n[BOS] The straightforward solution of assigning all the tags available in the tagset to each wordform makes the subsequent task of obtaining the correct tag infeasible, due to the great number of tags available in BulTreeBank.\n[BOS] In order to deal with this issue, we incorporate an inflectional lexicon (including a substantial set of entity names), which provides all possible tags for the wordforms available in it.\n[BOS] Furthermore, we enable the handling of unknown words by applying a morphological guesser that suggests up to ten possible tags per wordform.\n[BOS] Thus, we use the described components to yield a highly accurate and compact set of candidate POS tags.\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "14738389_11", "paragraph": "[BOS] We will hereafter refer to this structure as a parse graph for the sentence x.\n[BOS] Figure 2 illustrates one such parse graph.\n\n", "discourse_tags": ["Other", "Other"], "span_citation_mapping": []}
{"id": "14738389_10", "paragraph": "[BOS] is a set of undirected arcs representing the co-reference equality relation over the nodes of the dependency tree;\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "14738389_9", "paragraph": "[BOS] Let us have a set G of POS tags, and a set D of dependency tags (ROOT  D).\n[BOS] Let us have a sentence x = w 1 , ..., w n .\n[BOS] A tagged dependency graph with co-reference relations is a directed tree T = (V, A, , , C) where:\n\n", "discourse_tags": ["Other", "Other", "Other"], "span_citation_mapping": []}
{"id": "14738389_8", "paragraph": "[BOS] In Figure 1 , an HPSG-based tree of the sentence \"Vednaga odobri namerenieto na sestra si\" ('Immediately approved intention of sister his', He approved his sister's intention immediately) is shown.\n[BOS] This example illustrates the way in which the HPSG-based version of the dataset encodes dependency information (the \"NPA\" tag stands for nominal phrases of type head-adjunct).\n[BOS] Another noteworthy detail is the co-reference link between the un-expressed subject and the reflexive possessive pronoun.\n[BOS] In the HPSG-based version of the treebank, the unexpressed subject is represented explicitly only in cases when it participates in a co-reference chain, as shown in the sample sentence.\n[BOS] It is considered to be a property of the verb node, and not part of the constituent structure.\n[BOS] Figure 2 provides a view on the same sentence after its conversion to dependency format.\n[BOS] The head-adjunct relation found within the lowest NPA in the tree has been projected into a head-modifier relation.\n[BOS] Co-reference arcs have not been transferred into the dependency version of the treebank used within the CoNLL 2006 shared task.\n[BOS] We have 1 http://www.bultreebank.org/TechRep/BTB-TR03.pdf added them specially for the modeling effort reported in this paper.\n[BOS] Here, co-references are represented as secondary edges connecting the word nodes, and arc labels are represented as ovals situated between the connected word pairs.\n[BOS] The annotation of BulTreeBank complies with the definition of co-reference resolution as the identification of expressions that reference a common discourse entity (Recasens et al., 2010) .\n[BOS] From a semantic perspective, co-references include three types of relations: \"equality\", \"member-of\" and \"subset-of\".\n[BOS] Reflected linguistic phenomena include: pro-dropness (when coreferentially bound), subject and object control, secondary predication, binding, and nominalizations.\n[BOS] Co-references are found in the following set of dependency relations: coordination, subordination, complementation, adjunction and modification.\n[BOS] The annotated co-reference chains within the treebank amount to 5,312.\n[BOS] On average every third sentence contains at least one co-reference chain.\n[BOS] Thus, the impact of the co-references within Bulgarian grammar is clearly indicated.\n[BOS] analysis of a sentence as a tree that includes some new types of service nodes in addition to the nodes that represent words.\n[BOS] Service nodes connect to either words or other service nodes, in accordance with a set of rules that we describe in detail in 4.2.\n\n", "discourse_tags": ["Other", "Other", "Other", "Other", "Other", "Other", "Other", "Other", "Reflection", "Other", "Narrative_cite", "Other", "Other", "Other", "Other", "Other", "Other", "Other", "Other"], "span_citation_mapping": [{"token_start": 341, "token_end": 354, "char_start": 1608, "char_end": 1655, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "14738389_7", "paragraph": "[BOS] BulTreeBank provides rich linguistic information that goes beyond syntactic annotation.\n[BOS] It comprises the full grammatical tags, lemmas for all wordforms, syntactic relations (HPSG), named entities, as well as co-references within each sentence.\n[BOS] Since parts of speech, syntactic and coreference relations have been incorporated in our joint modeling effort, we will outline the specifics of their annotation within the dataset.\n[BOS] As we have already mentioned, Bulgarian is a morphologically rich language.\n[BOS] Morphological richness has many varieties from a typological point of view.\n[BOS] Bulgarian has a very rich verb system, and it is an inflective language, whose complete part of speech tagset comprises about 680 tags 1 .\n[BOS] As this circumstance causes sparseness and increases the modeling complexity, we opt in for filtering the input with the aid of a rich morphological lexicon and morphological guessers.\n[BOS] Besides the original HPSG-based corpus, there is a dependency version of BulTreeBank, derived from the original dataset.\n[BOS] More details regarding the types of dependency relations available in it are enlisted at http://www.bultreebank.org/dpbtb/.\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection", "Reflection", "Reflection", "Transition", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "14738389_6", "paragraph": "[BOS] 3 The Linguistic Annotation of the Bulgarian Treebank (BulTreeBank)\n\n", "discourse_tags": ["Other"], "span_citation_mapping": []}
{"id": "14738389_5", "paragraph": "[BOS] Below we describe our dataset, before we continue discussing the algorithm that handles the joint modeling task.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "14738389_4", "paragraph": "[BOS] Our approach differs in the following aspects: the third task is not identical.\n[BOS] In our case it is the addition of co-reference chains instead of the specific for Chinese word segmentation module.\n[BOS] Bulgarian is a morphologically rich language in comparison to Chinese -hence, the POS tagging model is more complex.\n[BOS] The parsing task uses dependencies instead of the CFGs used in the case of the Chinese parser.\n[BOS] Our model does not train the tasks separately, with specific models, before combining them, and the joint model is used during the development and exploitation of the proposed parser.\n[BOS] Our aim is to combine 3 closely related tasks, which have not been addressed widely in NLP, and to evaluate their impact on the processing of Bulgarian.\n[BOS] The complexity of the joint task is high not only due to the number of modules incorporated in the model, but also to the morphosyntactical richness of the language addressed in our work.\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection", "Reflection", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "14738389_3", "paragraph": "[BOS] Our work differs in the choice of an algorithm (Maximum Spanning Tree Model), and in the greater number of problems tackled by the proposed model.\n[BOS] The motivation for choosing the approach of the MSTParser is that two of the tasks that we handle can be non-local, and the algorithm may require information from distant nodes in order to find an appropriate solution.\n[BOS] Therefore, a straight adaptation of the transition-based model is not possible.\n[BOS] Qian and Liu (2012) focuses on the modelling of three tasks for Chinese -word segmentation, POS tagging and parsing.\n[BOS] The models for each task are trained separately, while the unification of predictions is performed during the decoding phase.\n[BOS] As in the previous paper, the authors report improvements over the pipeline results for Chinese.\n[BOS] The similarity is that our approach also considers three tasks in one model for one language with a modified algorithm.\n\n", "discourse_tags": ["Reflection", "Transition", "Transition", "Single_summ", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 93, "token_end": 141, "char_start": 470, "char_end": 718, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Qian and Liu (2012)": "17830435"}, "Reference": {}}}]}
{"id": "14738389_2", "paragraph": "[BOS] The reported results indicate that combining POS tagging and dependency parsing could be a successful step not only for morphologically rich languages (such as Czech and German), but also for languages where POS ambiguities are abundant (such as Chinese).\n[BOS] This work illustrates the superiority of joint models in settings rather similar to our own.\n[BOS] The authors added features for improving the POS tagging task within the combined model.\n[BOS] We also followed this strategy.\n\n", "discourse_tags": ["Transition", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "14738389_1", "paragraph": "[BOS] Our approach is inspired by works such as (Finkel and Manning, 2010) , (Bohnet and Nivre, 2012) and (Qian and Liu, 2012) .\n[BOS] Finkel and Manning (2010) report on combining NER and parsing tasks in a joint model.\n[BOS] One similarity with our task is the understanding that the separate tasks can help each other in various not-always-subsequent executions.\n[BOS] Another one is the fact that the explored algorithm is extended.\n[BOS] The difference is that the authors rely on a feature-rich CRF parser, while our algorithm is based on an online largemargin learning algorithm.\n[BOS] Bohnet and Nivre (2012) studies the combination of two tasks (POS tagging and Dependency labeled non-projective parsing) against datasets in four languages, and the reported results indicate an improvement over the pipeline-generated output for all considered languages.\n[BOS] The algorithm behind their architecture is transition-based.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Reflection", "Reflection", "Reflection", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 38, "char_start": 6, "char_end": 126, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Finkel and Manning, 2010)": "8016551", "(Bohnet and Nivre, 2012)": "1500270", "(Qian and Liu, 2012)": "17830435"}}}, {"token_start": 40, "token_end": 60, "char_start": 135, "char_end": 220, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Finkel and Manning (2010)": "8016551"}, "Reference": {}}}, {"token_start": 130, "token_end": 190, "char_start": 593, "char_end": 930, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Bohnet and Nivre (2012)": "1500270"}, "Reference": {}}}]}
{"id": "14738389_0", "paragraph": "[BOS] We are not aware of other studies that propose joint models for Bulgarian, and to the best of our knowledge, attemps at combining the three tasks (POS tagging, dependency parsing and coreference resolution) in a joint model have not been described in the literature either.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "14170579_0", "paragraph": "[BOS] There is a long thread of work aiming to improve the ability of RNN in remembering long sequences, with the long short-term memory RNN (LSTM) (Hochreiter and Schmidhuber, 1997) being the most salient examples and GRU being the most recent one.\n[BOS] Those works focus on\n\n", "discourse_tags": ["Narrative_cite", "Other"], "span_citation_mapping": [{"token_start": 25, "token_end": 48, "char_start": 114, "char_end": 182, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "15846774_3", "paragraph": "[BOS] BioCreative V CID task involved chemicaldisease relationships at the discourse level, even though they were often not specifically addressed.\n[BOS] The top-ranked system (CD-REST) (Xu et al., 2016) incorporated a discourse-level classifier, which interestingly performed better than the sentence-level classifier; however, most of the performance gain was due to features extracted from curated resources, particularly CTD.\n[BOS] Similarly, the next best system (Pons et al., 2016) used domain knowledge from various databases, and one of better performing systems, UET-CAM (Le et al., 2015) , incorporated features from coreference resolution into an intra-sentential relation classifier.\n[BOS] The present study diverges from these studies by specifically addressing implicit, discourse-level causality and focusing on textual characteristics.\n\n", "discourse_tags": ["Transition", "Single_summ", "Multi_summ", "Reflection"], "span_citation_mapping": [{"token_start": 28, "token_end": 85, "char_start": 154, "char_end": 429, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Xu et al., 2016)": "3810467"}, "Reference": {}}}, {"token_start": 86, "token_end": 114, "char_start": 436, "char_end": 570, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Pons et al., 2016)": "215553020"}, "Reference": {}}}, {"token_start": 115, "token_end": 143, "char_start": 572, "char_end": 695, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Le et al., 2015)": null}, "Reference": {}}}]}
{"id": "15846774_2", "paragraph": "[BOS] Regardless of whether they are expressed implicitly, a wide range of causal relations have also been addressed in biomedical text.\n[BOS] GE-NIA event corpus (Kim et al., 2008) and BioInfer corpus (Pyysalo et al., 2007) contain causal relationships between genes/proteins (e.g., REG-ULATION, POSITIVE REGULATION, and NEGA-TIVE REGULATION), in addition to other relation types.\n[BOS] Causal relations in these corpora were often found to be more challenging to identify than other relation types (Kim et al., 2012) .\n[BOS] In the BioCause corpus (Mihil et al., 2013) , causality was addressed as a discourse coherence relation and 850 causal discourse relations from fulltext journal articles on infectious diseases (94% of which have explicit causal triggers) were annotated.\n[BOS] In the BioDRB corpus (Prasad et al., 2011) , a larger number of discourse relation types were annotated, one of which is causality.\n[BOS] Mihil and Ananiadou (2014) focused on discourse causality in BioCause and used a semi-supervised method to recognize causal triggers and their arguments in biomedical discourse.\n[BOS] They did not address implicit discourse causality.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Multi_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 25, "token_end": 116, "char_start": 143, "char_end": 518, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Kim et al., 2008)": "5261517", "(Pyysalo et al., 2007)": "8410430"}, "Reference": {"(Kim et al., 2012)": "11216567"}}}, {"token_start": 118, "token_end": 167, "char_start": 527, "char_end": 780, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Mih\u0203il\u0203 et al., 2013)": "980182"}, "Reference": {}}}, {"token_start": 168, "token_end": 199, "char_start": 787, "char_end": 918, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Prasad et al., 2011)": "966214"}, "Reference": {}}}, {"token_start": 200, "token_end": 244, "char_start": 925, "char_end": 1159, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Mih\u0203il\u0203 and Ananiadou (2014)": "4502223"}, "Reference": {}}}]}
{"id": "15846774_1", "paragraph": "[BOS] In the biomedical domain, there is little work that specifically addresses implicit arguments.\n[BOS] Focusing on consumer health questions, Kilicoglu et al. (2013) incorporated resolution of anaphora and ellipsis to their question frame extraction pipeline and reported an 18 point improvement in F 1 score due to implicit argument resolution.\n[BOS] Coreference resolution has been studied as a strategy to recover implicit arguments and improve event extraction and varying degrees of improvement due to coreference resolution have been reported (Yoshikawa et al., 2011; Miwa et al., 2012; Kilicoglu and Bergler, 2012; Lavergne et al., 2015; .\n\n", "discourse_tags": ["Transition", "Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 18, "token_end": 63, "char_start": 107, "char_end": 349, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kilicoglu et al. (2013)": "2460667"}, "Reference": {}}}, {"token_start": 88, "token_end": 128, "char_start": 511, "char_end": 647, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yoshikawa et al., 2011;": "11992031", "Miwa et al., 2012;": "16250626", "Kilicoglu and Bergler, 2012;": "7926291"}}}]}
{"id": "15846774_0", "paragraph": "[BOS] In the general domain, Swampillai and Stevenson (2011) used an SVM-based approach to address inter-sentential relations in the MUC6 dataset.\n[BOS] Adapting structural features used for intra-sentential relation extraction (e.g., parse trees) to the inter-sentential case and addressing the data sparsity problem by hyperplane adjustment, they were able to obtain comparable performance to intra-sentential relation extraction.\n[BOS] A relevant research thread in semantic role labeling (SRL) is concerned with implicit arguments of predicates.\n[BOS] Gerber and Chai (2010) studied implicit arguments of a small number of nominal predicates, such as price and shipping.\n[BOS] Their model used a variety of features such as VerbNet classes and semantic roles for predicates and arguments, sentence distance, predicate frequency, and pointwise mutual information between arguments to identify implicit arguments.\n[BOS] The SemEval-2010 Task 10: Linking Events and their Participants in Discourse (Ruppenhofer et al., 2010) addressed the same problem on a larger set of event predicates.\n[BOS] The participating systems performed very poorly; however, more recent studies were able to improve results, by casting the problem as an anaphora resolution task (Silberer and Frank, 2012) and by using the previously identified explicit arguments of a given predicate in linking (Laparra and Rigau, 2013) .\n[BOS] Causal relations have also been studied in the general domain from a wide range of perspectives.\n[BOS] For example, Girju (2003) learned patterns indicating causal relationships between noun phrases to improve question answering.\n[BOS] Other research focused on causal relations between discourse segments (rather than individual entities) and generally reported poorer results on causal relations than other types of discourse relations (Subba and Di Eugenio, 2009 ).\n[BOS] It should be noted that most research on implicit arguments and causal relations assume the presence of explicit triggers (e.g., produce, as a result).\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Transition", "Single_summ", "Single_summ", "Single_summ", "Multi_summ", "Transition", "Single_summ", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 2, "token_end": 89, "char_start": 6, "char_end": 432, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Swampillai and Stevenson (2011)": "14846155"}, "Reference": {}}}, {"token_start": 111, "token_end": 176, "char_start": 556, "char_end": 915, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Gerber and Chai (2010)": "13804679"}, "Reference": {}}}, {"token_start": 192, "token_end": 204, "char_start": 989, "char_end": 1025, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Ruppenhofer et al., 2010)": "12623074"}, "Reference": {}}}, {"token_start": 241, "token_end": 254, "char_start": 1233, "char_end": 1284, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Silberer and Frank, 2012)": "9016539"}}}, {"token_start": 265, "token_end": 278, "char_start": 1354, "char_end": 1400, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Laparra and Rigau, 2013)": "13002266"}}}, {"token_start": 298, "token_end": 319, "char_start": 1512, "char_end": 1638, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Girju (2003)": "12440940"}, "Reference": {}}}, {"token_start": 347, "token_end": 359, "char_start": 1827, "char_end": 1874, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Subba and Di Eugenio, 2009": "784932"}}}]}
{"id": "13458891_1", "paragraph": "[BOS] Monolingual text-to-text generation research also faces many obstacles common to MT.\n[BOS] Re- Figure 3 : Relative grammaticality of BN test corpus compressions indicated by the absolute difference of RASP relation F 1 % from that of CL08.\n[BOS] cent work in MT decoding has proposed more efficient approaches than ILP to produced text optimally under syntactic and sequential models of language (Rush and Collins, 2011) .\n[BOS] We are currently exploring similar ideas for compression and other text-to-text generation problems.\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 71, "token_end": 85, "char_start": 358, "char_end": 426, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rush and Collins, 2011)": "7227918"}}}]}
{"id": "13458891_0", "paragraph": "[BOS] An early notion of compression was proposed by Dras (1997) as reluctant sentence paraphrasing under length constraints.\n[BOS] Jing and McKeown (2000) analyzed human-generated summaries and identified a heavy reliance on sentence reduction (Jing, 2000) .\n[BOS] The extraction by Knight and Marcu (2000) of a dataset of natural compression instances from the Ziff-Davis corpus spurred interest in supervised approaches to the task (Knight and Marcu, 2002; Riezler et al., 2003; Turner and Charniak, 2005; McDonald, 2006; Unno et al., 2006; Galley and McKeown, 2007; Nomoto, 2007) .\n[BOS] In particular, McDonald (2006) expanded on Knight & Marcu's (2002) transitionbased model by using dynamic programming to recover optimal transition sequences, and Clarke and Lapata (2006a) used ILP to replace pairwise transitions with trigrams.\n[BOS] Other recent work (Filippova and Strube, 2008; Galanis and Androutsopoulos, 2010 ) has used dependency trees directly as sentence representations for compression.\n[BOS] Another line of research has attempted to broaden the notion of compression beyond mere word deletion (Cohn and Lapata, 2009; Ganitkevitch et al., 2011; Napoles et al., 2011a) .\n[BOS] Finally, progress on standalone compression tasks has also enabled document summarization techniques that jointly address sentence selection and compression (Daum and Marcu, 2002; Clarke and Lapata, 2007; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012) , a number of which also rely on ILP-based inference.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Multi_summ", "Narrative_cite", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 2, "token_end": 26, "char_start": 6, "char_end": 125, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Dras (1997)": "569"}, "Reference": {}}}, {"token_start": 27, "token_end": 53, "char_start": 132, "char_end": 257, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Jing and McKeown (2000)": "800331"}, "Reference": {"(Jing, 2000)": "8934802"}}}, {"token_start": 55, "token_end": 141, "char_start": 266, "char_end": 583, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Knight and Marcu (2000)": null}, "Reference": {"(Knight and Marcu, 2002;": "7793213", "Riezler et al., 2003;": "7418660", "Turner and Charniak, 2005;": "15519576", "McDonald, 2006;": "3014198", "Unno et al., 2006;": "14729743", "Galley and McKeown, 2007;": "1762277", "Nomoto, 2007)": null}}}, {"token_start": 143, "token_end": 174, "char_start": 592, "char_end": 749, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {"Knight & Marcu's (2002)": "7793213"}}}, {"token_start": 176, "token_end": 196, "char_start": 755, "char_end": 836, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Clarke and Lapata (2006a)": "10312772"}, "Reference": {}}}, {"token_start": 197, "token_end": 219, "char_start": 843, "char_end": 923, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Filippova and Strube, 2008;": "17477341", "Galanis and Androutsopoulos, 2010": "11023355"}}}, {"token_start": 247, "token_end": 278, "char_start": 1100, "char_end": 1187, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cohn and Lapata, 2009;": "6429026", "Ganitkevitch et al., 2011;": "10616734", "Napoles et al., 2011a)": "4946068"}}}, {"token_start": 297, "token_end": 338, "char_start": 1310, "char_end": 1458, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Daum\u00e9 and Marcu, 2002;": null, "Clarke and Lapata, 2007;": "14549133", "Berg-Kirkpatrick et al., 2011;": "15467396", "Woodsend and Lapata, 2012)": "17497992"}}}]}
{"id": "15594678_0", "paragraph": "[BOS] To some extent, our idea is similar to Ma et al. (2008) , who used an anchor word alignment model to find a set of high-precision anchor links and then aligned the remaining words relying on dependency information invoked by the acquired anchor links.\n[BOS] The similarity is that both Ma et al. (2008) and this work utilize structure information to find appropriate translations for words which are difficult to align.\n[BOS] The differ-ence is that they used dependency information in the word alignment stage while our method uses syntactic information during the phrase pair extraction stage.\n[BOS] There are also many works which leverage syntax information to improve word alignments (e.g., Cherry and Lin, 2006; DeNero and Klein, 2007; Fossum et al., 2008; Hermjakob, 2009 Johnson et al., (2007) to a hierarchical PBMT model, which is built on synchronous context free grammars (SCFG).\n[BOS] Tomeh et al., (2009) described an approach for filtering phrase tables in a statistical machine translation system, which relies on a statistical independence measure called Noise, first introduced in (Moore, 2004) .\n[BOS] The difference between the above research and this work is they took advantage of some statistical measures while we use syntactic knowledge to filter phrase tables.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Narrative_cite", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 2, "token_end": 53, "char_start": 6, "char_end": 257, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ma et al. (2008)": "5584985"}, "Reference": {}}}, {"token_start": 54, "token_end": 114, "char_start": 264, "char_end": 601, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Ma et al. (2008)": "5584985"}}}, {"token_start": 122, "token_end": 169, "char_start": 649, "char_end": 807, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Cherry and Lin, 2006;": "2787289", "DeNero and Klein, 2007;": "9882011", "Fossum et al., 2008;": "2485577", "Hermjakob, 2009": "11853100", "Johnson et al., (2007)": "12131372"}}}, {"token_start": 190, "token_end": 231, "char_start": 904, "char_end": 1118, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Tomeh et al., (2009)": "10261463"}, "Reference": {"(Moore, 2004)": "16577318"}}}]}
{"id": "15852103_2", "paragraph": "[BOS] On the other hand, most existing NN-based approaches for relation classification are either based on the shortest dependency path or the raw sequence, although these two representations may complement each other.\n[BOS] In this work, we propose to combine them together based on the multi-channel CNN architecture (Kim, 2014) , aiming to capture long-distance relations without losing any information.\n\n", "discourse_tags": ["Transition", "Reflection"], "span_citation_mapping": [{"token_start": 52, "token_end": 62, "char_start": 288, "char_end": 330, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kim, 2014)": null}}}]}
{"id": "15852103_1", "paragraph": "[BOS] Although all these models have been shown to be effective, all of them only focus on learning a single representation for each relation instance.\n[BOS] Different from all previous methods, we first design a strategy to generate a mirror instance from each original relation instance and then propose a pairwise relation classification framework to learn a pair of representations for each relation instance.\n\n", "discourse_tags": ["Transition", "Reflection"], "span_citation_mapping": []}
{"id": "15852103_0", "paragraph": "[BOS] Traditional work on relation classification can be categorized into feature-based methods and kernelbased methods.\n[BOS] The former relies on a large number of human-designed features (Zhou et al., 2005; Jiang and Zhai, 2007; Li and Ji, 2014) while the latter leverages various kernels to implicitly explore a much larger feature space (Bunescu and Mooney, 2005; Nguyen et al., 2009 ).\n[BOS] However, both methods suffer from error propagation problems and poor generalization abilities on unseen words.\n[BOS] The most popular method to solve the two limitations is based on neural networks (NNs), which have been shown successful in extracting meaningful features and generalizing on unseen words for many NLP tasks (Kim, 2014) .\n[BOS] For relation classification, Socher et al. (2012) proposed a recursive matrix-vector model based on constituency parse trees.\n[BOS] Zeng et al. (2014) and dos Santos et al. (2015) respectively proposed a standard and a ranking-based CNN model based on the raw word sequences.\n[BOS] More recently, Xu et al. (2015b) and Miwa and Bansal (2016) respectively proposed a multi-channel sequential LSTM model and a bidirectional tree-LSTM model on the shortest dependency path for relation classification.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Narrative_cite", "Single_summ", "Multi_summ", "Multi_summ"], "span_citation_mapping": [{"token_start": 29, "token_end": 54, "char_start": 166, "char_end": 248, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhou et al., 2005;": "3160937", "Jiang and Zhai, 2007;": "17069935", "Li and Ji, 2014)": "20744"}}}, {"token_start": 67, "token_end": 85, "char_start": 328, "char_end": 388, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bunescu and Mooney, 2005;": "5165854", "Nguyen et al., 2009": "16028836"}}}, {"token_start": 144, "token_end": 152, "char_start": 713, "char_end": 734, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kim, 2014)": null}}}, {"token_start": 154, "token_end": 180, "char_start": 743, "char_end": 868, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Socher et al. (2012)": "806709"}, "Reference": {}}}, {"token_start": 181, "token_end": 215, "char_start": 875, "char_end": 1018, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zeng et al. (2014)": "12873739", "Santos et al. (2015)": "15620570"}, "Reference": {}}}, {"token_start": 216, "token_end": 263, "char_start": 1025, "char_end": 1241, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Xu et al. (2015b)": "5403702", "Miwa and Bansal (2016)": "2476229"}, "Reference": {}}}]}
{"id": "14234178_2", "paragraph": "[BOS] Another line of research is the attention mechanism for deep learning.\n[BOS] (Bahdanau et al., 2014) posed the attention mechanism in machine translation task, which is also the first use of it in natural language processing.\n[BOS] This attention mechanism is used to select the reference words in the original language for words in the foreign language before translation.\n[BOS] (Xu et al., 2015a) used the attention mechanism in image caption generation to select the relevant image regions when generating words in the captions.\n[BOS] Further uses of the attention mechanism included paraphrase identification (Yin et al., 2015) , document classification (Yang et al., 2016) , parsing (Vinyals et al., 2015) , natural language question answering (Sukhbaatar et al., 2015; Kumar et al., 2015; Hermann et al., 2015) and image question answering (Lin et al., 2015) .\n[BOS] (Wang et al., 2016) introduced attention mechanism into relation classification which relied on two levels of attention for pattern extraction.\n[BOS] In this paper, we will explore the word level attention mechanism in order to discover better patterns in heterogeneous contexts for the relation classification task.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Narrative_cite", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 15, "token_end": 71, "char_start": 83, "char_end": 379, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Bahdanau et al., 2014)": "11212020"}, "Reference": {}}}, {"token_start": 72, "token_end": 104, "char_start": 386, "char_end": 537, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Xu et al., 2015a)": "1055111"}, "Reference": {}}}, {"token_start": 112, "token_end": 124, "char_start": 593, "char_end": 637, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yin et al., 2015)": "5535381"}}}, {"token_start": 125, "token_end": 135, "char_start": 640, "char_end": 683, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yang et al., 2016)": "6857205"}}}, {"token_start": 136, "token_end": 147, "char_start": 686, "char_end": 716, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vinyals et al., 2015)": "14223"}}}, {"token_start": 148, "token_end": 178, "char_start": 719, "char_end": 822, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sukhbaatar et al., 2015;": null, "Kumar et al., 2015;": "2319779", "Hermann et al., 2015)": "6203757"}}}, {"token_start": 179, "token_end": 190, "char_start": 827, "char_end": 870, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lin et al., 2015)": "17014226"}}}, {"token_start": 192, "token_end": 217, "char_start": 879, "char_end": 1022, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Wang et al., 2016)": "9524495"}, "Reference": {}}}]}
{"id": "14234178_1", "paragraph": "[BOS] With the recent revival of interest in deep neural networks, many researchers have concentrated on using deep networks to learn features.\n[BOS] In NLP, such methods are primarily based on learning a distributed representation for each word, which is also called a word embedding (Turian et al., 2010) .\n[BOS] (Socher et al., 2012) presented a recursive neural network (RNN) for relation classification to learn vectors in the syntactic tree path connecting two nominals to determine their semantic relationship.\n[BOS] (Hashimoto et al., 2013 ) also employed a neural relation extraction model allowing for the explicit weighting of important phrases for the target task.\n[BOS] (Zeng et al., 2014 ) exploited a convolutional deep neural network to extract lexical and sentence level features.\n[BOS] These two levels of features were concatenated to form the final feature vector.\n[BOS] (Ebrahimi and Dou, 2015) rebuilt an RNN on the dependency path between two marked entities.\n[BOS] (Xu et al., 2015b) used the convolutional network and proposed a ranking loss function with data cleaning.\n[BOS] (Xu et al., 2015c) leveraged heterogeneous information along the shortest dependency path between two entities.\n[BOS] (Xu et al., 2016) proposed a data augmentation method by leveraging the directionality of relations.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 50, "token_end": 61, "char_start": 270, "char_end": 306, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Turian et al., 2010)": "629094"}}}, {"token_start": 63, "token_end": 102, "char_start": 315, "char_end": 517, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Socher et al., 2012)": "806709"}, "Reference": {}}}, {"token_start": 103, "token_end": 132, "char_start": 524, "char_end": 676, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Hashimoto et al., 2013": "12515181"}, "Reference": {}}}, {"token_start": 133, "token_end": 170, "char_start": 683, "char_end": 884, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Zeng et al., 2014": "12873739"}, "Reference": {}}}, {"token_start": 171, "token_end": 196, "char_start": 891, "char_end": 982, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Ebrahimi and Dou, 2015)": "14624845"}, "Reference": {}}}, {"token_start": 197, "token_end": 220, "char_start": 989, "char_end": 1095, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Xu et al., 2015b)": "12203896"}, "Reference": {}}}, {"token_start": 221, "token_end": 243, "char_start": 1102, "char_end": 1213, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Xu et al., 2015c)": "5403702"}, "Reference": {}}}, {"token_start": 244, "token_end": 266, "char_start": 1220, "char_end": 1320, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Xu et al., 2016)": "7437060"}, "Reference": {}}}]}
{"id": "14234178_0", "paragraph": "[BOS] A variety of learning paradigms have been applied to relation extraction.\n[BOS] As mentioned earlier, supervised methods have shown to perform well in this task.\n[BOS] In the supervised paradigm, relation classification is considered as a multi-classification problem, and researchers concentrate on extracting complex features, either feature-based or kernel-based.\n[BOS] (Kambhatla, 2004; Suchanek et al., 2006) converted the classification clues (such as sequences and parse trees) into feature vectors.\n[BOS] Various kernels, such as the convolution tree kernel (Qian et al., 2008) , subsequence kernel and dependency tree kernel , have been proposed to solve the relation classification problem.\n[BOS] (Plank and Moschitti, 2013) introduced semantic information into kernel methods in addition to considering structural information only.\n[BOS] However, the reliance on manual annotation, which is expensive to produce and thus limited in quantity has provided the impetus for distant-supervision (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Takamatsu et al., 2012) .\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Multi_summ", "Narrative_cite", "Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 65, "token_end": 98, "char_start": 379, "char_end": 512, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Kambhatla, 2004;": "62705866", "Suchanek et al., 2006)": "1568464"}, "Reference": {}}}, {"token_start": 105, "token_end": 117, "char_start": 548, "char_end": 591, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Qian et al., 2008)": "2010873"}}}, {"token_start": 136, "token_end": 160, "char_start": 713, "char_end": 848, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Plank and Moschitti, 2013)": "3011134"}, "Reference": {}}}, {"token_start": 185, "token_end": 223, "char_start": 987, "char_end": 1095, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mintz et al., 2009;": "10910955", "Riedel et al., 2010;": "2386383", "Hoffmann et al., 2011;": "16483125", "Takamatsu et al., 2012)": "2463401"}}}]}
{"id": "13741562_1", "paragraph": "[BOS] Limitations Recent work questioned whether supervised distributional methods actually learn the relation between x and y or only separate properties of each word.\n[BOS] Levy et al. (2015) claimed that they tend to perform \"lexical memorization\", i.e., memorizing that some words are prototypical to certain relations (e.g., that y = animal is a hypernym, regardless of x).\n[BOS] Roller and Erk (2016) found that under certain conditions, these methods actively learn to infer hypernyms based on separate occurrences of x and y in Hearst patterns (Hearst, 1992) .\n[BOS] In either case, they only learn whether x and y independently match their corresponding slots in the relation, a limitation which makes them sensitive to the training data (Shwartz et al., 2017; Sanchez and Riedel, 2017) .\n[BOS] Levy et al. (2015) claimed that the linear nature of most supervised methods limits their ability to capture the relation between words.\n[BOS] They suggested that using support vector machine (SVM) with non-linear kernels slightly mitigates this issue, and proposed KSIM, a custom kernel with multiplicative integration.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Narrative_cite", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 27, "token_end": 84, "char_start": 175, "char_end": 378, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Levy et al. (2015)": "747342"}, "Reference": {}}}, {"token_start": 85, "token_end": 125, "char_start": 385, "char_end": 566, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Roller and Erk (2016)": "14275144"}, "Reference": {"(Hearst, 1992)": "15763200"}}}, {"token_start": 156, "token_end": 175, "char_start": 742, "char_end": 795, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Shwartz et al., 2017;": "5410083", "Sanchez and Riedel, 2017)": "11603334"}}}, {"token_start": 177, "token_end": 237, "char_start": 804, "char_end": 1124, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Levy et al. (2015)": "747342"}, "Reference": {}}}]}
{"id": "13741562_0", "paragraph": "[BOS] Available Representations In supervised distributional methods, a pair of words (x, y) is represented as some combination of the word embeddings of x and y, most commonly Concat v x  v y (Baroni et al., 2012) or Diff v y  v x (Weeds et al., 2014; Fu et al., 2014) .\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 34, "token_end": 49, "char_start": 177, "char_end": 214, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Baroni et al., 2012)": "1526915"}}}, {"token_start": 50, "token_end": 71, "char_start": 218, "char_end": 269, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Weeds et al., 2014;": "11730392"}}}]}
{"id": "14568267_3", "paragraph": "[BOS] We are not aware of work that has been carried out on the data set of the BioNLP Shared Task 2009 before the task took place.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "14568267_2", "paragraph": "[BOS] As for event extraction, Yakushiji et al. (2001) present work on event extraction based on fullparsing and a large-scale, general-purpose grammar.\n[BOS] They implement an Argument Structure Extractor.\n[BOS] The parser is used to convert sentences that describe the same event into an argument structure for this event.\n[BOS] The argument structure contains arguments such as semantic subject and object.\n[BOS] Information extraction itself is performed using pattern matching on the argument structure.\n[BOS] The system extracts 23 % of the argument structures uniquely, and 24% with ambiguity.\n[BOS] Sasaki et al. (2008) present a supervised machine learning system that extracts event frames from a corpus in which the biological process E. coli gene regulation was linguistically annotated by domain experts.\n[BOS] The frames being extracted specify all potential arguments of gene regulation events.\n[BOS] Arguments are assigned domain-independent roles (Agent, Theme, Location) and domain-dependent roles (Condition, Manner).\n[BOS] Their system works in three steps: (i) CRF-based named entity recognition to assign named entities to word sequences; (ii) CRF-based semantic role labeling to assign semantic roles to word sequences with named entity labels; (iii) Comparison of word sequences with event patterns derived from the corpus.\n[BOS] The system achieves 50% recall and 20% precision.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 113, "char_start": 6, "char_end": 600, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Yakushiji et al. (2001)": "14306830"}, "Reference": {}}}, {"token_start": 114, "token_end": 266, "char_start": 607, "char_end": 1403, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Sasaki et al. (2008)": "11647190"}, "Reference": {}}}]}
{"id": "14568267_1", "paragraph": "[BOS] Most work focuses on extracting biological relations from corpora, which consists of finding associations between entities within a text phrase.\n[BOS] For example, Bundschus et al. (2008) develop a Conditional Random Fields (CRF) system to identify relations between genes and diseases from a set of GeneRIF (Gene Reference Into Function) phrases.\n[BOS] A shared task was organised in the framework of the Language Learning in Logic Workshop 2005 devoted to the extraction of relations from biomedical texts (Ndellec, 2005) .\n[BOS] Extracting protein-protein interactions has also produced a lot of research, and has been the focus of the BioCreative II competition (Krallinger et al., 2008a) .\n\n", "discourse_tags": ["Transition", "Single_summ", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 28, "token_end": 67, "char_start": 170, "char_end": 353, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Bundschus et al. (2008)": "11473206"}, "Reference": {}}}, {"token_start": 91, "token_end": 100, "char_start": 497, "char_end": 529, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(N\u00e9dellec, 2005)": "6962537"}}}, {"token_start": 122, "token_end": 138, "char_start": 645, "char_end": 698, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Krallinger et al., 2008a)": "14555187"}}}]}
{"id": "14568267_0", "paragraph": "[BOS] In recent years, research on text mining in the biomedical domain has experienced substantial progress, as shown in reviews of work done in this field (Krallinger and Valencia, 2005; Ananiadou and McNaught, 2006; Krallinger et al., 2008b) .\n[BOS] Some corpora have been annotated with event level information of different types: PropBank-style frames (Wattarujeekrit et al., 2004; Chou et al., 2006) , frame independent roles (Kim et al., 2008) , and specific roles for certain event types (Sasaki et al., 2008) .\n[BOS] The focus on extraction of event frames using machine learning techniques is relatively new because there were no corpora available.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 8, "token_end": 60, "char_start": 35, "char_end": 244, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Krallinger and Valencia, 2005;": "609407", "Ananiadou and McNaught, 2006;": "15745290", "Krallinger et al., 2008b)": "676022"}}}, {"token_start": 75, "token_end": 101, "char_start": 335, "char_end": 405, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wattarujeekrit et al., 2004;": "2251578", "Chou et al., 2006)": "13273377"}}}, {"token_start": 102, "token_end": 113, "char_start": 408, "char_end": 450, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kim et al., 2008)": "5261517"}}}, {"token_start": 115, "token_end": 130, "char_start": 457, "char_end": 517, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sasaki et al., 2008)": "11647190"}}}]}
{"id": "13856980_3", "paragraph": "[BOS] The difference of our work from these studies is that our semantic representations are learned from unlabeled bilingual (or monolingual) data and do not depend on any linguistic resources, e.g., parsers.\n[BOS] We also believe that our model is able to exploit both syntactic and semantic information for nonterminals since vector representations learned in our way are able to capture both syntactic and semantic properties (Turian et al., 2010; Socher et al., 2010) .\n\n", "discourse_tags": ["Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 75, "token_end": 96, "char_start": 396, "char_end": 472, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Turian et al., 2010;": "629094", "Socher et al., 2010)": "9923502"}}}]}
{"id": "13856980_2", "paragraph": "[BOS] On the other hand, efforts have also been directed towards attaching distributional linguistic knowledge to nonterminals.\n[BOS] Venugopal et al. (2009) propose a preference grammar to annotate nonterminals based on preference distributions of syntactic categories.\n[BOS] Huang et al. (2010) learn la-tent syntactic distributions for each nonterminal.\n[BOS] They use these distributions to decorate nonterminal Xs in SCFG rules with a real-valued feature vectors and utilize these vectors to measure the similarities between source phrases and applied rules.\n[BOS] Similar to this work, Huang et al. (2013) utilize treebank tags based on dependency parsing to learn latent distributions.\n[BOS] Cao et al. (2014) attach translation rules with dependency knowledge, which contains both dependency relations inside rules and dependency relations between rules and their contexts.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 24, "token_end": 51, "char_start": 134, "char_end": 270, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Venugopal et al. (2009)": "917915"}, "Reference": {}}}, {"token_start": 52, "token_end": 107, "char_start": 277, "char_end": 563, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Huang et al. (2010)": "8988829"}, "Reference": {}}}, {"token_start": 108, "token_end": 133, "char_start": 570, "char_end": 692, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Huang et al. (2013)": "15208097"}, "Reference": {}}}, {"token_start": 134, "token_end": 164, "char_start": 699, "char_end": 881, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Cao et al. (2014)": "14934518"}, "Reference": {}}}]}
{"id": "13856980_1", "paragraph": "[BOS] Among approaches which directly refine the single label to more fine-grained labels, syntactic and semantic knowledge are explored in various ways.\n[BOS] The syntactically augmented translation model (SAMT) proposed by Zollmann and Venugopal (2006) uses syntactic categories extracted from target-side parse trees to augment nonterminals in hierarchical rules.\n[BOS] Unfortunately, there is a data sparseness problem in this model due to thousands of extracted syntactic categories.\n[BOS] One solution to address this issue is to reduce the number of syntactic categories.\n[BOS] Zollmann and Vogel (2011) use word tags, generated by either POS tagger or unsupervised word class induction, instead of syntactic categories.\n[BOS] Hanneman and Lavie (2013) coarsen the label set by introducing a label collapsing algorithm to SAMT grammars (Zollmann and Venugopal, 2006 ).\n[BOS] Yet another solution is easing restrictions on label matching.\n[BOS] Shen et al. (2009) penalize substitution with unmatched labels while Chiang (2010) uses soft match features to model substitutions with various labels.\n[BOS] Similar to Zollmann and Venugopal (2006) , Hoang and Koehn (2010) decorate some hierarchical rules with source-side syntax information and use undecorated, decorated, and partially decorated rules in their translation model.\n[BOS] Mylonakis and Sima'an (2011) employ source-side syntax-based labels to define a joint probability synchronous grammar.\n[BOS] Combinatory Categorial Grammar (CCG) labels or CCG contextual labels are also used to enrich nonterminals (Almaghout et al., 2011; Weese et al., 2012) .\n[BOS] Li et al. (2012) incorporate head information extracted from source-side dependency structures into translation rules.\n[BOS] Besides, semantic knowledge is also used to refine nonterminals.\n[BOS] Gao and Vogel (2011) utilize target-side semantic roles to form SRL-aware SCFG rules.\n[BOS] Most of approaches introduced here explicitly require syntactic or semantic parsers trained on manually labeled data.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Transition", "Single_summ", "Multi_summ", "Transition", "Multi_summ", "Multi_summ", "Single_summ", "Multi_summ", "Single_summ", "Single_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 29, "token_end": 91, "char_start": 160, "char_end": 488, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zollmann and Venugopal (2006)": "819325"}, "Reference": {}}}, {"token_start": 108, "token_end": 137, "char_start": 585, "char_end": 727, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zollmann and Vogel (2011)": "8666976"}, "Reference": {}}}, {"token_start": 138, "token_end": 172, "char_start": 734, "char_end": 872, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hanneman and Lavie (2013)": "17952463"}, "Reference": {"(Zollmann and Venugopal, 2006": "819325"}}}, {"token_start": 187, "token_end": 201, "char_start": 951, "char_end": 1013, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Shen et al. (2009)": "17001645"}, "Reference": {}}}, {"token_start": 202, "token_end": 218, "char_start": 1020, "char_end": 1102, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Chiang (2010)": "3514217"}, "Reference": {}}}, {"token_start": 221, "token_end": 231, "char_start": 1120, "char_end": 1149, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Zollmann and Venugopal (2006)": "819325"}}}, {"token_start": 232, "token_end": 271, "char_start": 1152, "char_end": 1333, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hoang and Koehn (2010)": "2554349"}, "Reference": {}}}, {"token_start": 272, "token_end": 299, "char_start": 1340, "char_end": 1458, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Mylonakis and Sima'an (2011)": "2024558"}, "Reference": {}}}, {"token_start": 300, "token_end": 342, "char_start": 1465, "char_end": 1615, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Almaghout et al., 2011;": "14576857", "Weese et al., 2012)": "5170126"}, "Reference": {}}}, {"token_start": 344, "token_end": 379, "char_start": 1624, "char_end": 1813, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Li et al. (2012)": null}, "Reference": {}}}, {"token_start": 380, "token_end": 402, "char_start": 1820, "char_end": 1905, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Gao and Vogel (2011)": "2424684"}, "Reference": {}}}]}
{"id": "13856980_0", "paragraph": "[BOS] A variety of approaches have been explored for nonterminal refinement in hierarchical phrasebased translation.\n[BOS] These approaches can be categorized into two groups: 1) augmenting the nonterminal symbol X with informative labels, and 2) attaching distributional linguistic knowledge to each nonterminal in hierarchical rules.\n[BOS] The former only allows substitution operations with matched labels.\n[BOS] The latter normally builds an additional model as a new feature of the log-linear model to incorporate attached knowledge.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Transition"], "span_citation_mapping": []}
{"id": "52072023_0", "paragraph": "[BOS] Most systems for Neural Machine Translation are based on the sequence-to-sequence model (Seq2Seq) (Sutskever et al., 2014) , which is an encoder-decoder framework (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014) .\n[BOS] To improve NMT, a significant mechanism for the Seq2Seq model is the attention mechanism .\n[BOS] Two types of attention are the most common, which are proposed by and respectively.\n[BOS] Though the attention mechanism is powerful for the requirements of alignment in NMT, some prominent problems still exist.\n[BOS] To tackle the impact of the attention historyTu et al. Lin et al. (2018a) take the attention history into consideration.\n[BOS] An important breakthrough in NMT is that Vaswani et al. (2017) applied the fully-attention-based model to NMT and achieved the state-of-the-art performance.\n[BOS] To further evaluate the effect of our attention temperature mechanism, we will implement it to the \"Transformer\" model in the future.\n[BOS] Besides, the studies on the atSource:        Gold: growth of mobile phone users in mainland china to slow down Seq2Seq: mainland cell phone users slow down SACT: the growth of cell phone users in chinese mainland will slow down (a) Source:   12 ,        ,        Gold: since december last year , the price of crude oil on the international market has kept rising due to the general strike in venezuela and the threat of war in iraq .\n[BOS] Seq2Seq: since december last year , the international market has continued to rise in the international market and the threat of the iraqi war has continued to rise .\n[BOS] SACT: since december last year , the international market of crude oil has continued to rise because of the strike in venezuela and the war in iraq .\n[BOS] tention mechanism have also contributed to some other tasks (Lin et al., 2018b; Liu et al., 2018) Beyond the attention mechanism, there are also important methods for the Seq2Seq that contribute to the improvement of NMT.\n[BOS] Ma et al. (2018) incorporates the information about the bag-of-words of the target for adapting to multiple translations, and Lin et al. (2018c) takes the target context into consideration.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Transition", "Single_summ", "Single_summ", "Reflection", "Other", "Other", "Other", "Narrative_cite", "Multi_summ"], "span_citation_mapping": [{"token_start": 12, "token_end": 33, "char_start": 67, "char_end": 128, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sutskever et al., 2014)": "7961699"}}}, {"token_start": 37, "token_end": 63, "char_start": 143, "char_end": 225, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kalchbrenner and Blunsom, 2013;": "12639289", "Sutskever et al., 2014)": "7961699"}}}, {"token_start": 124, "token_end": 151, "char_start": 549, "char_end": 669, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Lin et al. (2018a)": "3646455"}, "Reference": {}}}, {"token_start": 160, "token_end": 192, "char_start": 717, "char_end": 832, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Vaswani et al. (2017)": "13756489"}, "Reference": {}}}, {"token_start": 383, "token_end": 409, "char_start": 1748, "char_end": 1845, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 434, "token_end": 459, "char_start": 1976, "char_end": 2096, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ma et al. (2018)": "44108216"}, "Reference": {}}}, {"token_start": 461, "token_end": 476, "char_start": 2102, "char_end": 2165, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Lin et al. (2018c)": "47018327"}, "Reference": {}}}]}
{"id": "52057510_1", "paragraph": "[BOS] Sequential QA Our work is similar to sequential question answering against knowledge bases (Iyyer et al., 2017) and the web (Talmor and Berant, 2018) , but instead of decomposing a single question into smaller questions, we rely on the curiosity of the student to generate a sequence of questions.\n[BOS] Such open information seeking was studied in semantic parsing on knowledge bases (Dahl et al., 1994 ) and more recently with modern approaches (Saha et al., 2018) , but with questions paraphrased from templates.\n[BOS] Concurrent to our work, Saeidi et al. (2018) proposed a task of generating and answering yes/no questions for rule focused text (such as traffic laws) by interacting with a user through dialog.\n[BOS] Also concurrently, Reddy et al. (2018) propose conversational question answering (CoQA) from text but allow both students and questioners to see the evidence.\n[BOS] As a result, a large percentage of CoQA answers are named entities or short noun phrases, much like those in SQuAD.\n[BOS] In contrast, the asymmetric nature of forces students to ask more exploratory questions whose answers can be potentially be followed up on.\n[BOS] 19 Dialog fits into an increasing interest in open domain dialog, mostly studied in the context of social chit-chat Ritter et al., 2011; Fang et al., 2017; Ghazvininejad et al., 2018) .\n[BOS] Most related to our effort is visual dialog (Das et al., 2017) , which relies on images as evidence instead of text.\n[BOS] More explicit goal driven scenarios, such as bargaining (Lewis et al., 2017) and item guessing (He et al., 2017) have also been explored, but the language is more constrained than in .\n[BOS] Information-seeking dialog specifically was studied in Stede and Schlangen (2004) .\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Single_summ", "Single_summ", "Single_summ", "Transition", "Narrative_cite", "Narrative_cite", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 9, "token_end": 24, "char_start": 43, "char_end": 117, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Iyyer et al., 2017)": "2623009"}}}, {"token_start": 26, "token_end": 36, "char_start": 126, "char_end": 155, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Talmor and Berant, 2018)": null}}}, {"token_start": 74, "token_end": 86, "char_start": 355, "char_end": 409, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 93, "token_end": 102, "char_start": 453, "char_end": 472, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Saha et al., 2018)": "19240019"}}}, {"token_start": 118, "token_end": 155, "char_start": 552, "char_end": 721, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Saeidi et al. (2018)": "52165754"}, "Reference": {}}}, {"token_start": 159, "token_end": 217, "char_start": 747, "char_end": 1008, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Reddy et al. (2018)": "52055325"}, "Reference": {}}}, {"token_start": 261, "token_end": 293, "char_start": 1260, "char_end": 1344, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Ritter et al., 2011;": "780171", "Fang et al., 2017;": null}}}, {"token_start": 301, "token_end": 322, "char_start": 1383, "char_end": 1469, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Das et al., 2017)": "64711781"}}}, {"token_start": 331, "token_end": 340, "char_start": 1521, "char_end": 1552, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Lewis et al., 2017)": null}}}, {"token_start": 341, "token_end": 352, "char_start": 1557, "char_end": 1588, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(He et al., 2017)": "3051772"}}}, {"token_start": 367, "token_end": 383, "char_start": 1667, "char_end": 1748, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Stede and Schlangen (2004)": "60671162"}}}]}
{"id": "52057510_0", "paragraph": "[BOS] Reading Comprehension Our work builds on span based reading comprehension (Rajpurkar et al., 2016; Joshi et al., 2017; Trischler et al., 2016) , while also incorporating innovations such as curating questions independently of supporting text to reduce trivial lexical overlap (Joshi et al., 2017; Kocisk et al., 2017) and allowing for unanswerable questions (Trischler et al., 2016; Rajpurkar et al., 2018) .\n[BOS] We handle open-ended questions like in MSMARCO (Nguyen et al., 2016) , with multiple references, but we are the first to incorporate these into information-seeking dialog.\n\n", "discourse_tags": ["Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 8, "token_end": 39, "char_start": 47, "char_end": 148, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rajpurkar et al., 2016;": "11816014", "Joshi et al., 2017;": "26501419", "Trischler et al., 2016)": "1167588"}}}, {"token_start": 46, "token_end": 76, "char_start": 196, "char_end": 323, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Joshi et al., 2017;": "26501419", "Kocisk\u00fd et al., 2017)": "2593903"}}}, {"token_start": 77, "token_end": 103, "char_start": 328, "char_end": 412, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Trischler et al., 2016;": "1167588", "Rajpurkar et al., 2018)": "47018994"}}}, {"token_start": 113, "token_end": 124, "char_start": 460, "char_end": 489, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Nguyen et al., 2016)": "1289517"}}}]}
{"id": "52070035_1", "paragraph": "[BOS] Addressing biases in NLP models/systems have recently started to gain more interest in the research community, not only because fairness in AI is important but also because bias correction can improve the robustness of the models.\n[BOS] Bolukbasi et al. (2016) is one of the first works to point out the gender stereotypes inside word2vec (Mikolov et al., 2013) and propose an algorithm to correct them.\n[BOS] Caliskan et al. (2017) also propose a method called Word Embedding Association Test (WEAT) to measure model bias inside word embeddings and finds that many of those pretrained embeddings contain problematic bias toward gender or race.\n[BOS] is one of the first works that point out existing \"unintended\" bias in abusive language detection models.\n[BOS] Kiritchenko and Mohammad (2018) compare 219 sentiment analysis systems participating in SemEval competition with their proposed dataset, which can be used for evaluating racial and gender bias of those systems.\n[BOS] Zhao et al. (2018) shows the effectiveness of measuring and correcting gender biases in co-reference resolution tasks.\n[BOS] We later show how we extend a few of these works into ours.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Single_summ", "Transition", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 44, "token_end": 89, "char_start": 243, "char_end": 409, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Bolukbasi et al. (2016)": "1704893"}, "Reference": {"(Mikolov et al., 2013)": "16447573"}}}, {"token_start": 90, "token_end": 136, "char_start": 416, "char_end": 650, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Caliskan et al. (2017)": "23163324"}, "Reference": {}}}, {"token_start": 161, "token_end": 201, "char_start": 769, "char_end": 979, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kiritchenko and Mohammad (2018)": "21670658"}, "Reference": {}}}, {"token_start": 202, "token_end": 225, "char_start": 986, "char_end": 1104, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhao et al. (2018)": "4952494"}, "Reference": {}}}]}
{"id": "52070035_0", "paragraph": "[BOS] So far, many efforts were put into defining and constructing abusive language datasets from different sources and labeling them through crowd-sourcing or user moderation (Waseem and Hovy, 2016; Waseem, 2016; Founta et al., 2018; Wulczyn et al., 2017) .\n[BOS] Many deep learning approaches have been explored to train a classifier with those datasets to develop an automatic abusive language detection system (Badjatiya et al., 2017; Park and Fung, 2017; Pavlopoulos et al., 2017) .\n[BOS] However, these works do not explicitly address any model bias in their models.\n\n", "discourse_tags": ["Narrative_cite", "Multi_summ", "Multi_summ"], "span_citation_mapping": [{"token_start": 10, "token_end": 67, "char_start": 41, "char_end": 256, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Waseem and Hovy, 2016;": "1721388", "Waseem, 2016;": "406026", "Founta et al., 2018;": "3640499", "Wulczyn et al., 2017)": "6060248"}}}, {"token_start": 69, "token_end": 136, "char_start": 265, "char_end": 572, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Badjatiya et al., 2017;": "2880908", "Park and Fung, 2017;": "687037", "Pavlopoulos et al., 2017)": "2944650"}}}]}
{"id": "52053741_2", "paragraph": "[BOS] The work of Baumel et al. (2018) and Zhang et al. (2018) are related to ours.\n[BOS] In particular, Baumel et al. (2018) propose to extend an abstractive summarization system to generate query-focused summaries; Zhang et al. (2018) add a document set encoder to their hierarchical summarization framework.\n[BOS] With these few exceptions, little research has been dedicated to investigate the feasibility of extending the encoder-decoder framework to generate abstractive summaries from multi-document inputs, where available training data are scarce.\n[BOS] This paper presents some first steps towards the goal of extending the encoder-decoder model to a multi-document setting.\n[BOS] We introduce an adaptation method combining the pointer-generator (PG) networks (See et al., 2017) and the maximal marginal relevance (MMR) algorithm (Carbonell and Goldstein, 1998 ).\n[BOS] The PG model, trained on SDS data and detailed in Section 3, is capable of generating document abstracts by performing text abstraction and sentence fusion.\n[BOS] However, if the model is applied at test time to summarize multi-document inputs, there will be limitations.\n[BOS] Our PG-MMR algorithm, presented in Section 4, teaches the PG model to effectively recognize important content from the input documents, hence improving the quality of abstractive summaries, all without requiring any training on multidocument inputs.\n\n", "discourse_tags": ["Narrative_cite", "Multi_summ", "Transition", "Reflection", "Reflection", "Reflection", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 5, "token_end": 13, "char_start": 18, "char_end": 38, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Baumel et al. (2018)": "8599292"}}}, {"token_start": 14, "token_end": 21, "char_start": 43, "char_end": 62, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Zhang et al. (2018)": "13748058"}}}, {"token_start": 30, "token_end": 53, "char_start": 105, "char_end": 215, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Baumel et al. (2018)": "8599292"}, "Reference": {}}}, {"token_start": 54, "token_end": 73, "char_start": 217, "char_end": 310, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhang et al. (2018)": "13748058"}, "Reference": {}}}, {"token_start": 145, "token_end": 160, "char_start": 739, "char_end": 789, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(See et al., 2017)": null}}}, {"token_start": 162, "token_end": 177, "char_start": 798, "char_end": 871, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Carbonell and Goldstein, 1998": "6334682"}}}]}
{"id": "52053741_1", "paragraph": "[BOS] Neural abstractive summarization utilizing the encoder-decoder architecture has shown promising results but studies focus primarily on singledocument summarization Kikuchi et al., 2016; Chen et al., 2016; Miao and Blunsom, 2016; Tan et al., 2017; Zeng et al., 2017; Zhou et al., 2017; Paulus et al., 2017; See et al., 2017; Gehrmann et al., 2018) .\n[BOS] The pointing mechanism Gu et al., 2016 ) allows a summarization system to both copy words from the source text and generate new words from the vocabulary.\n[BOS] Reinforcement learning is exploited to directly optimize evaluation metrics (Paulus et al., 2017; Kryciski et al., 2018; Chen and Bansal, 2018) .\n[BOS] These studies focus on summarizing single documents in part because the training data are abundant.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Multi_summ", "Multi_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 96, "char_start": 6, "char_end": 352, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Kikuchi et al., 2016;": "11157751", "Chen et al., 2016;": "12755643", "Miao and Blunsom, 2016;": "10480989", "Tan et al., 2017;": "26698484", "Zeng et al., 2017;": null, "Zhou et al., 2017;": "1770102", "Paulus et al., 2017;": "21850704", "See et al., 2017;": null, "Gehrmann et al., 2018)": "52144157"}}}, {"token_start": 99, "token_end": 129, "char_start": 365, "char_end": 515, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Gu et al., 2016": "8174613"}, "Reference": {}}}, {"token_start": 130, "token_end": 183, "char_start": 522, "char_end": 773, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Paulus et al., 2017;": "21850704", "Kry\u015bci\u0144ski et al., 2018;": "52091366", "Chen and Bansal, 2018)": "44129061"}}}]}
{"id": "52053741_0", "paragraph": "[BOS] Popular methods for multi-document summarization have been extractive.\n[BOS] Important sentences are extracted from a set of source documents and optionally compressed to form a summary (Daume III and Marcu, 2002; Zajic et al., 2007; Galanis and Androutsopoulos, 2010; Berg-Kirkpatrick et al., 2011; Li et al., 2013; Thadani and McKeown, 2013; Wang et al., 2013; Yogatama et al., 2015; Filippova et al., 2015; Durrett et al., 2016) .\n[BOS] In recent years neural networks have been exploited to learn word/sentence representations for single-and multi-document summarization (Cheng and Lapata, 2016; Cao et al., 2017; Isonuma et al., 2017; Yasunaga et al., 2017; Narayan et al., 2018) .\n[BOS] These approaches remain extractive; and despite encouraging results, summarizing a large quantity of texts still requires sophisticated abstraction capabilities such as generalization, paraphrasing and sentence fusion.\n[BOS] Prior to deep learning, abstractive summarization has been investigated (Barzilay et al., 1999; Carenini and Cheung, 2008; Ganesan et al., 2010; Gerani et al., 2014; Fabbrizio et al., 2014; Pighin et al., 2014; Bing et al., 2015; Liao et al., 2018) .\n[BOS] These approaches construct domain templates using a text planner or an open-IE system and employ a natural language generator for surface realization.\n[BOS] Limited by the availability of labelled data, experiments are often performed on small domain-specific datasets.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Multi_summ", "Multi_summ", "Narrative_cite", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 16, "token_end": 125, "char_start": 83, "char_end": 437, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Daume III and Marcu, 2002;": null, "Zajic et al., 2007;": null, "Berg-Kirkpatrick et al., 2011;": "15467396", "Li et al., 2013;": "8928513", "Thadani and McKeown, 2013;": "13458891", "Wang et al., 2013;": "1260503", "Yogatama et al., 2015;": "12194143", "Filippova et al., 2015;": "1992250", "Durrett et al., 2016)": "5125975"}}}, {"token_start": 130, "token_end": 226, "char_start": 462, "char_end": 917, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cheng and Lapata, 2016;": "1499080", "Cao et al., 2017;": "14651945", "Isonuma et al., 2017;": "35618061", "Yasunaga et al., 2017;": "6532096", "Narayan et al., 2018)": "3510042"}}}, {"token_start": 232, "token_end": 309, "char_start": 948, "char_end": 1172, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Barzilay et al., 1999;": "7031344", "Carenini and Cheung, 2008;": "8660413", "Ganesan et al., 2010;": "988010", "Gerani et al., 2014;": "2767900", "Fabbrizio et al., 2014;": "12682781", "Pighin et al., 2014;": "14959", "Bing et al., 2015;": "8377315", "Liao et al., 2018)": "49210924"}}}]}
{"id": "52097879_2", "paragraph": "[BOS] Independently from us Shazeer and Stern (2018) have done further exploratory work on ADAM's momentum parameters using the Transformer model (Vaswani et al., 2017)\n\n", "discourse_tags": ["Multi_summ"], "span_citation_mapping": [{"token_start": 5, "token_end": 38, "char_start": 28, "char_end": 168, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Shazeer and Stern (2018)": "4786918"}}}]}
{"id": "52097879_1", "paragraph": "[BOS] Independently from us Mao et al. (2018) extend the work of Aji and Heafield (2017) aiming to reduce gradient communication without suffering any of the negative effects we have noted.\n[BOS] In process they independently arrive to some of the methods that we use, notably tuning the momentum and applying warmup to achieve better convergence.\n\n", "discourse_tags": ["Multi_summ", "Reflection"], "span_citation_mapping": [{"token_start": 5, "token_end": 68, "char_start": 28, "char_end": 347, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Mao et al. (2018)": "38796293"}, "Reference": {}}}]}
{"id": "52097879_0", "paragraph": "[BOS] We use larger mini-batches and delay gradient updates in order to increase the speed at which the dataset is processed.\n[BOS] The principal reason why this works is because when mini-batch size is increased n (also includes delayed updates) times, communication is reduced by the same amount.\n[BOS] This aspect of our work is similar to the work of Aji and Heafield (2017) where they drop the lower 99% of the gradient updates based on absolute value thus reducing the memory traffic.\n[BOS] Compared with them we achieve faster dataset processing speed and also better model convergence as shown on Table 2 .\n\n", "discourse_tags": ["Reflection", "Reflection", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 70, "token_end": 99, "char_start": 355, "char_end": 490, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Aji and Heafield (2017)": "2140766"}, "Reference": {}}}]}
{"id": "52077283_2", "paragraph": "[BOS] In natural language processing (NLP), Mou et al. (2016) distill task-specific knowledge from word embeddings.\n[BOS] Kuncoro et al. (2016) propose to learn a single parser from an ensemble of parsers.\n[BOS] Kim and Rush (2016) investigate knowledge distillation for neural machine translation by approximately matching the sequence-level distribution of the teacher.\n[BOS] Nakashole and Flauger (2017) propose to learn bilingual mapping functions through a distilled training objective.\n[BOS] Xu and Yang (2017) tion.\n[BOS] Our work shows that the standard knowledge distillation and its novel variants can be successfully applied to the MRC task.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 11, "token_end": 29, "char_start": 44, "char_end": 115, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Mou et al. (2016)": "8350200"}, "Reference": {}}}, {"token_start": 30, "token_end": 52, "char_start": 122, "char_end": 205, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kuncoro et al. (2016)": "905294"}, "Reference": {}}}, {"token_start": 53, "token_end": 81, "char_start": 212, "char_end": 371, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kim and Rush (2016)": null}, "Reference": {}}}, {"token_start": 82, "token_end": 104, "char_start": 378, "char_end": 491, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Nakashole and Flauger (2017)": "22421874"}, "Reference": {}}}, {"token_start": 105, "token_end": 111, "char_start": 498, "char_end": 516, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Xu and Yang (2017)": "3013625"}}}]}
{"id": "52077283_1", "paragraph": "[BOS] Efficiency and Robustness in MRC.\n[BOS] Improving efficiency and robustness for reading comprehension system has attracted a lot of interest in recent years.\n[BOS] For efficiency, previous works mostly concentrate on how to scale passage-level models to large corpora such as a document without increasing computation complexity.\n[BOS] Existing approachs (Chen et al., 2017; usually first retrieve relevant passages with a ranking model and then return an answer with a reading model.\n[BOS] As for robustness, Wang and Bansal (2018) train the model with an adversarial data augmentation method.\n[BOS] Min et al. (2018) propose to selectively read salient sentences rather than the entire passage, so as to avoid looking at the adversarial sentence.\n[BOS] Our approach, however, focuses on improving efficiency and robustness by transferring knowledge from a cumbersome ensemble model to a single model.\n[BOS] Knowledge Distillation.\n[BOS] Knowledge distillation is first explored by Bucilu et al. (2006) and Hinton et al. (2014) , which attempts to transfer knowledge defined as soft output distributions from a teacher to a student.\n[BOS] Later works have been proposed to distill not only the final output but also intermediate representation from the teacher (Romero et al., 2015; Zagoruyko and Komodakis, 2017; Huang and Wang, 2017) .\n[BOS] Papernot et al. (2016) show that knowledge distillation can be used to prevent the network from adversarial attacks in image recognition.\n[BOS] Radosavovic et al. (2017) introduce data distillation that annotates large-scale unlabelled data for omni-supervised learning.\n\n", "discourse_tags": ["Other", "Transition", "Transition", "Single_summ", "Single_summ", "Single_summ", "Reflection", "Other", "Multi_summ", "Multi_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 59, "token_end": 86, "char_start": 361, "char_end": 490, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Chen et al., 2017;": null}, "Reference": {}}}, {"token_start": 89, "token_end": 109, "char_start": 504, "char_end": 600, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wang and Bansal (2018)": "4940747"}, "Reference": {}}}, {"token_start": 110, "token_end": 140, "char_start": 607, "char_end": 754, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Min et al. (2018)": "29161506"}, "Reference": {}}}, {"token_start": 182, "token_end": 218, "char_start": 989, "char_end": 1139, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Bucilu et al. (2006)": null, "Hinton et al. (2014)": "7200347"}, "Reference": {}}}, {"token_start": 219, "token_end": 266, "char_start": 1146, "char_end": 1342, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Romero et al., 2015;": "2723173", "Zagoruyko and Komodakis, 2017;": "829159", "Huang and Wang, 2017)": "30307744"}}}, {"token_start": 268, "token_end": 297, "char_start": 1351, "char_end": 1488, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Papernot et al. (2016)": "2672720"}, "Reference": {}}}, {"token_start": 298, "token_end": 329, "char_start": 1495, "char_end": 1621, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Radosavovic et al. (2017)": "7350432"}, "Reference": {}}}]}
{"id": "52077283_0", "paragraph": "[BOS] Machine Reading Comprehension.\n[BOS] Benefiting from large-scale machine reading comprehension (MRC) datasets (Hermann et al., 2015; Hill et al., 2016; Rajpurkar et al., 2016; Joshi et al., 2017) , end-to-end neural networks have achieved promising results Yu et al., 2018) .\n[BOS] Wang and Jiang (2017) combine the match-LSTM with pointer networks to predict the answer boundary.\n[BOS] match the context aginst itself to refine the passage representation.\n[BOS] Later, a variety of attention mechanisms have been proposed, such as bi-attention (Seo et al., 2017) , coattention , fully-aware attention and reattention (Hu et al., 2018) .\n[BOS] Among these works, two common traits can be summarized as: 1) compute a similary matrix between the question and the passage; 2) sequentially predict the answer start and end positions.\n[BOS] Our proposed approach is a simple and effective adaptation to existing models by taking advantage of these traits, and do not complicate previous works more than necessary.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Single_summ", "Single_summ", "Narrative_cite", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 13, "token_end": 53, "char_start": 71, "char_end": 201, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hermann et al., 2015;": "6203757", "Hill et al., 2016;": "14915449", "Rajpurkar et al., 2016;": "11816014", "Joshi et al., 2017)": "26501419"}}}, {"token_start": 54, "token_end": 72, "char_start": 204, "char_end": 279, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Yu et al., 2018)": "4842909"}}}, {"token_start": 74, "token_end": 108, "char_start": 288, "char_end": 462, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wang and Jiang (2017)": "5592690"}, "Reference": {}}}, {"token_start": 122, "token_end": 134, "char_start": 538, "char_end": 569, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Seo et al., 2017)": "8535316"}}}, {"token_start": 143, "token_end": 155, "char_start": 612, "char_end": 641, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hu et al., 2018)": "13559921"}}}]}
{"id": "102352626_5", "paragraph": "[BOS] KB (restaurant-cuisine-price range-location-rating) resto tokyo affordable vietnamese 8stars-vietnamese-affordable-tokyo-8 resto tokyo affordable vietnamese 7stars-vietnamese-affordable-tokyo-7 resto tokyo affordable vietnamese 6stars-vietnamese-affordable-tokyo-6 resto tokyo affordable vietnamese 5stars-vietnamese-affordable-tokyo-5 Table 8 : An example of responses generated by BOSSNET and baselines on bAbI dialog Task-5.\n[BOS] This example is from the KA test set with 100% unseen entities.\n\n", "discourse_tags": ["Other", "Transition"], "span_citation_mapping": []}
{"id": "102352626_4", "paragraph": "[BOS] BOSSNET pizza hut fen ditton is located at cambridge retail park newmarket road fen ditton is there anything else i can help you with Table 7 : An example of responses generated by BOSSNET and baselines on the CamRest test set.\n[BOS] Thia example has no unseen entities.\n\n", "discourse_tags": ["Transition", "Transition"], "span_citation_mapping": []}
{"id": "102352626_3", "paragraph": "[BOS] Seq2Seq+Copy their address is at Seq2Seq curry prince is at 451 newmarket road fen ditton Mem2Seq pizza hut fen ditton is located at 7 barnwell road fen ditton is 7 barnwell road fen ditton and the phone number is 01223 244955\n\n", "discourse_tags": ["Other"], "span_citation_mapping": []}
{"id": "102352626_2", "paragraph": "[BOS] Gold the address for pizza hut fen ditton is cambridge retail park newmarket road fen ditton .\n[BOS] can i help you with anything else ?\n\n", "discourse_tags": ["Other", "Other"], "span_citation_mapping": []}
{"id": "102352626_1", "paragraph": "[BOS] For encoding, some approaches represent the dialog history as a sequence .\n[BOS] Unfortunately, using a single long sequence for encoding also enforces an order over the set of KB tuples making it harder to perform inferencing over them.\n[BOS] Other approaches represent the dialog context as a bag.\n[BOS] OriginalKB (restaurant-cuisine-address-phone) pizza hut fen ditton-italian-cambridge retail park newmarket road fen ditton-01223 323737 usr-1 may i have information for an italian restaurant in the east part of town ?\n[BOS] sys-1 yes sure .\n[BOS] there is only on italian restaurant called pizza hut fen ditton in the east .\n[BOS] what else do you want to know ?\n[BOS] usr-2 what is their address please ?\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Other", "Other", "Other", "Other", "Other"], "span_citation_mapping": []}
{"id": "102352626_0", "paragraph": "[BOS] Compared to the traditional slot-filling based dialog (Williams and Young, 2007; Wen et al., 2017; Williams et al., 2017) , end-to-end training methods (e.g., (Bordes and Weston, 2017) , this work) do not require handcrafted state representations and their corresponding annotations in each dialog.\n[BOS] Thus, they can easily be adapted to a new domain.\n[BOS] We discuss end-to-end approaches along two verticals: 1) decoder: whether the response is retrieved or generated and 2) encoder: how the dialog history and KB tuples are encoded.\n[BOS] T1 T2 T3  T4  T5  T1  T2  T3 T4 T5 Table 6 : Ablation study: impact of each model element on BOSSNET Most of the existing end-to-end approaches retrieve a response from a pre-defined set (Bordes and Weston, 2017; Liu and Perez, 2017; Seo et al., 2017) .\n[BOS] These methods are generally successful when they have to provide boilerplate responsesthey cannot construct responses by using words in KB not seen during training.\n[BOS] Alternatively, generative approaches are used where the response is generated one word at a time Madotto et al., 2018) .\n[BOS] These approaches mitigate the unseen entity problem by incorporating the ability to copy words from the input (Vinyals et al., 2015; Gu et al., 2016) .\n[BOS] The copy mechanism has also found success in summarization See et al., 2017) and machine translation .\n[BOS] BOSSNET is also a copy incorporated generative approach.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Reflection", "Narrative_cite", "Transition", "Narrative_cite", "Narrative_cite", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 6, "token_end": 32, "char_start": 34, "char_end": 127, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Williams and Young, 2007;": "13903063", "Wen et al., 2017;": "10565222", "Williams et al., 2017)": "13214003"}}}, {"token_start": 33, "token_end": 88, "char_start": 130, "char_end": 360, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bordes and Weston, 2017)": "2129889"}}}, {"token_start": 174, "token_end": 207, "char_start": 696, "char_end": 803, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bordes and Weston, 2017;": "2129889", "Liu and Perez, 2017;": "14351566", "Seo et al., 2017)": "1460418"}}}, {"token_start": 246, "token_end": 263, "char_start": 1039, "char_end": 1101, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Madotto et al., 2018)": "5068596"}}}, {"token_start": 278, "token_end": 300, "char_start": 1194, "char_end": 1259, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Vinyals et al., 2015;": "5692837", "Gu et al., 2016)": "8174613"}}}, {"token_start": 310, "token_end": 319, "char_start": 1313, "char_end": 1344, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"See et al., 2017)": null}}}]}
{"id": "102351267_4", "paragraph": "[BOS] Our work is addressing the same problem, but combining both signals in a state-of-the-art neural network model, and we do not require the two datasets to have the same set of relation types.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "102351267_3", "paragraph": "[BOS] Combining Direct and Distant Supervision.\n[BOS] Despite the substantial amount of work on both directly and distantly supervised relation extraction, the question of how to combine both signals has not received the same attention.\n[BOS] Pershina et al. (2014) trained MIML-RE from (Surdeanu et al., 2012) on both types of supervision by locking the latent variables on the sentences to the supervised labels.\n[BOS] Angeli et al. (2014) and presented active learning models that select sentences to annotate and incorporate in the same manner.\n[BOS] Pershina et al. (2014) and also tried simple baseline of including the labeled sentences as singleton bags.\n[BOS] Pershina et al. (2014) did not find this beneficial, which agrees with our results in Section 4.2, while found the addition of singleton bags to work well.\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 41, "token_end": 85, "char_start": 243, "char_end": 414, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Pershina et al. (2014)": "577239"}, "Reference": {}}}, {"token_start": 86, "token_end": 112, "char_start": 421, "char_end": 548, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Angeli et al. (2014)": "6338379"}, "Reference": {}}}, {"token_start": 113, "token_end": 136, "char_start": 555, "char_end": 662, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Pershina et al. (2014)": "577239"}, "Reference": {}}}, {"token_start": 137, "token_end": 174, "char_start": 669, "char_end": 824, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Pershina et al. (2014)": "577239"}, "Reference": {}}}]}
{"id": "102351267_2", "paragraph": "[BOS] Neural Models for Distant Supervision.\n[BOS] More recently, neural models have been effectively used to model textual relations (e.g., Hashimoto et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015) .\n[BOS] Focusing on distantly supervised models, Zeng et al. (2015) proposed a neural implementation of multi-instance learning to leverage multiple sentences which mention an entity pair in distantly supervised relation extraction.\n[BOS] However, their model picks only one sentence to represent an entity pair, which wastes the information in the neglected sentences.\n[BOS] Jiang et al. (2016) addresses this limitation by max pooling the vector encodings of all input sentences for a given entity pair.\n[BOS] independently proposed to use attention to address the same limitation, and Du et al. (2018) improved by using multilevel self-attention.\n[BOS] To account for the noise in distant supervision labels, ; Wang et al. (2018) suggested different ways of using \"soft labels\" that do not necessarily agree with the distant supervision labels.\n[BOS] Ye et al. (2017) proposed a method for leveraging dependencies between different relations in a pairwise ranking framework, while arranged the relation types in a hierarchy aiming for better generalization for relations that do not have enough training data.\n[BOS] To improve using additional resources, Vashishth et al. (2018) used graph convolution over dependency parse, OpenIE extractions and entity type constraints, and used parse trees to prune irrelevant information from the sentences.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 19, "token_end": 51, "char_start": 110, "char_end": 210, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Hashimoto et al., 2013;": "12515181", "Zeng et al., 2014;": "12873739", "Nguyen and Grishman, 2015)": "12585424"}}}, {"token_start": 60, "token_end": 117, "char_start": 260, "char_end": 580, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zeng et al. (2015)": "2778800"}, "Reference": {}}}, {"token_start": 118, "token_end": 145, "char_start": 587, "char_end": 716, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Jiang et al. (2016)": "16389974"}, "Reference": {}}}, {"token_start": 146, "token_end": 173, "char_start": 723, "char_end": 860, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Du et al. (2018)": "52153793"}, "Reference": {}}}, {"token_start": 175, "token_end": 212, "char_start": 870, "char_end": 1058, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wang et al. (2018)": "53099740"}, "Reference": {}}}, {"token_start": 213, "token_end": 258, "char_start": 1065, "char_end": 1323, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ye et al. (2017)": "1516982"}, "Reference": {}}}, {"token_start": 265, "token_end": 304, "char_start": 1369, "char_end": 1559, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Vashishth et al. (2018)": "53064621"}, "Reference": {}}}]}
{"id": "102351267_1", "paragraph": "[BOS] a text corpus where two related entities are mentioned, then developed a classifier to predict the relation.\n[BOS] Researchers have since extended this approach further (e.g., Takamatsu et al., 2012; Min et al., 2013; Riedel et al., 2013; Koch et al., 2014) .\n[BOS] A key source of noise in distant supervision is that sentences may mention two related entities without expressing the relation between them.\n[BOS] Hoffmann et al. (2011) used multi-instance learning to address this problem by developing a graphical model for each entity pair which includes a latent variable for each sentence to explicitly indicate the relation expressed by that sentence, if any.\n[BOS] Our model can be viewed as an extension of Hoffmann et al. (2011) where the sentence-bound latent variables can also be directly supervised in some of the training examples.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 35, "token_end": 67, "char_start": 182, "char_end": 263, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Takamatsu et al., 2012;": "2463401", "Min et al., 2013;": "6018348", "Riedel et al., 2013;": "2687019", "Koch et al., 2014)": "6202202"}}}, {"token_start": 93, "token_end": 140, "char_start": 420, "char_end": 671, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hoffmann et al. (2011)": "16483125"}, "Reference": {}}}, {"token_start": 150, "token_end": 177, "char_start": 721, "char_end": 851, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hoffmann et al. (2011)": "16483125"}, "Reference": {}}}]}
{"id": "102351267_0", "paragraph": "[BOS] Distant Supervision.\n[BOS] The term 'distant supervision' was coined by Mintz et al. (2009) Table 2 : Weights assigned to sentences by our baseline and our best model.\n[BOS] The baseline incorrectly predicts \"no relation\", while our best model correctly predicts \"neighbourhood of\" for this bag.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 16, "token_end": 24, "char_start": 78, "char_end": 97, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Mintz et al. (2009)": "10910955"}}}]}
{"id": "102353817_1", "paragraph": "[BOS] Inter-rater agreement (IRA) is widely accepted as a metric to assess the annotation quality of a dataset.\n[BOS] The metric reflects the homogeneity of ratings which is expected to be high for a welldefined task and a qualified set of annotators.\n[BOS] For each word pair in SCWS ten scores were obtained through crowdsourcing.\n[BOS] We computed the pairwise IRA to be 0.35 (in terms of Spearman  correlation) which is a very low figure.\n[BOS] The mean IRA (between each annotator and the average of others), which can be taken as a human-level performance upperbound, is 0.52.\n[BOS] Moreover, most of the instances in SCWS have context pairs with different target words.\n[BOS] 14 This makes it possible to test context-independent models, which only considers word pairs in isolation, on the dataset.\n[BOS] Importantly, such a context-independent model can easily surpass the human-level performance upperbound.\n[BOS] For instance, we computed the performance of the Google News Word2vec pretrained word embeddings (Mikolov et al., 2013b) on the dataset to be 0.65 (), which is significantly higher than the optimistic IRA for the dataset.\n[BOS] In fact, Dubossarsky et al. (2018) showed how the reported high performance of multi-prototype techniques in this dataset was not due to an accurate sense representation, but rather to a subsampling effect, which had not been controlled for in similarity datasets.\n[BOS] In contrast, a context-insensitive word embedding model would perform no better than a random baseline on our dataset.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Reflection", "Transition", "Transition", "Transition", "Transition", "Reflection", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 202, "token_end": 222, "char_start": 973, "char_end": 1044, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mikolov et al., 2013b)": "16447573"}}}, {"token_start": 249, "token_end": 300, "char_start": 1161, "char_end": 1416, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Dubossarsky et al. (2018)": "52216759"}, "Reference": {}}}]}
{"id": "102353817_0", "paragraph": "[BOS] The Stanford Contextual Word Similarity (SCWS) dataset (Huang et al., 2012) comprises 2003 word pairs and is analogous to standard word similarity datasets, such as RG-65 (Rubenstein and Goodenough, 1965) and SimLex (Hill et al., 2015) , in which the task is to automatically estimate the semantic similarity of word pairs.\n[BOS] Ideally, the estimated similarity scores should have high correlation with those given by human annotators.\n[BOS] However, there is a fundamental difference between SCWS and other word similarity datasets: each word in SCWS is associated with a context which triggers a specific meaning of the word.\n[BOS] The unique property of the dataset makes it a suitable benchmark for multiprototype and contextualized word embeddings.\n[BOS] However, in the following, we highlight some of the limitations of the dataset which hinder its suitability for evaluating existing techniques.\n\n", "discourse_tags": ["Multi_summ", "Transition", "Transition", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 3, "token_end": 32, "char_start": 10, "char_end": 161, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Huang et al., 2012)": "372093"}, "Reference": {}}}, {"token_start": 35, "token_end": 49, "char_start": 171, "char_end": 210, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rubenstein and Goodenough, 1965)": "18309234"}}}, {"token_start": 50, "token_end": 76, "char_start": 215, "char_end": 329, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Hill et al., 2015)": "3226120"}, "Reference": {}}}]}
{"id": "13974252_2", "paragraph": "[BOS] A supervised approach to morphological disambiguation of Arabic is given by Habash and Rambow (2005) , who use two corpora of 120K words each to train several classifiers.\n[BOS] Each morphological feature is predicted separately and then combined into a full disambiguation result.\n[BOS] The accuracy of the disambiguator is 94.8%-96.2% (depending on the test corpus).\n[BOS] Note, however, the high baseline of each classifier (96.6%-99.9%, depending on the classifier) and the full disambiguation task (87.3%-92.1%, depending on the corpus).\n[BOS] We use a very similar approach below, but we experiment with more sophisticated methods for combining simple classifiers to induce a coherent prediction.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 130, "char_start": 6, "char_end": 548, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Habash and Rambow (2005)": "2216180"}, "Reference": {}}}]}
{"id": "13974252_1", "paragraph": "[BOS] Recently, Adler and Elhadad (2006) presented an unsupervised, HMM-based model for Hebrew morphological disambiguation, using a morphological analyzer as the only resource.\n[BOS] A morpheme-based model learns both segmentation and tagging in parallel from a large (6M words) un-annotated corpus.\n[BOS] Reported results are 92.32% for POS tagging and 88.5% for full morphological disambiguation.\n[BOS] We refer to this result as the state of the art and use the same data for evaluation.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 4, "token_end": 87, "char_start": 16, "char_end": 399, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Adler and Elhadad (2006)": "770625"}, "Reference": {}}}]}
{"id": "13974252_0", "paragraph": "[BOS] The idea of using short context for morphological disambiguation dates back to Choueka and Lusignan (1985) .\n[BOS] Levinger et al. (1995) were the first to apply it to Hebrew, but their work was hampered by the lack of annotated corpora for training and evaluation.\n[BOS] The first work which uses stochastic contextual information for morphological disambiguation in Hebrew is Segal (1999) : texts are analyzed using the morphological analyzer of Segal (1997) ; then, each word in a text is assigned its most likely analysis, defined by probabilities computed from a small tagged corpus.\n[BOS] In the next phase the system corrects its own decisions by using short context (one word to the left and one to the right of the target word).\n[BOS] The corrections are also automatically learned from the tagged corpus (using transformation-based learning).\n[BOS] In the last phase, the analysis is corrected by the results of a syntactic analysis of the sentence.\n[BOS] The reported results are excellent: 96.2% accuracy.\n[BOS] More reliable tests, however, reveal accuracy of 85.5% only (Lemberski, 2003, page 85) .\n[BOS] Furthermore, the performance of the program is unacceptable (the reported running time on \"\"two papers\"\" is thirty minutes).\n[BOS] Bar-Haim et al. (2005) use Hidden Markov Models (HMMs) to implement a segmenter and a tagger for Hebrew.\n[BOS] The main innovation of this work is that it models word-segments (morphemes: prefixes, stem and suffixes), rather than full words.\n[BOS] The accuracy of this system is 90.51% for POS tagging (a tagset of 21 POS tags is used) and 96.74% for segmentation (which is defined as identifying all prefixes, including a possibly assimilated definite article).\n[BOS] As noted above, POS tagging does not amount to full morphological disambiguation.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Narrative_cite", "Transition", "Single_summ", "Single_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 15, "token_end": 25, "char_start": 85, "char_end": 112, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Choueka and Lusignan (1985)": null}}}, {"token_start": 27, "token_end": 35, "char_start": 121, "char_end": 143, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Levinger et al. (1995)": "11503694"}}}, {"token_start": 64, "token_end": 211, "char_start": 278, "char_end": 1023, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Segal (1999)": null}, "Reference": {"Segal (1997)": null}}}, {"token_start": 212, "token_end": 236, "char_start": 1030, "char_end": 1116, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 265, "token_end": 380, "char_start": 1256, "char_end": 1718, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "859195_2", "paragraph": "[BOS] However, none of the above research has made use of a CBPS as a knowledge source for generating the system's contributions.\n[BOS] Previous dialogue systems research involving constraint satisfaction models includes Jordan and Di Eugenio's (1997) analysis of cooperative problem-solving dialogue as a constraint satisfaction problem.\n[BOS] However, they did not use the model to generate system responses.\n[BOS] Donaldson and Cohen (1997) propose a constraint-based model to manage a system's turn taking goals, but not as a knowledge source for helping to meet a user's information needs.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 29, "token_end": 76, "char_start": 136, "char_end": 410, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Jordan and Di Eugenio's (1997)": "59872964"}}}, {"token_start": 77, "token_end": 118, "char_start": 417, "char_end": 594, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Donaldson and Cohen (1997)": "10872371"}, "Reference": {}}}]}
{"id": "859195_1", "paragraph": "[BOS] Research on mixed-initiative dialogue has investigated techniques for deciding when a dialogue system should take initiative and techniques for content planning (e.g., see papers in Haller, Kobsa, and McRoy, 1999) .\n[BOS] Following Chu-Carroll and Brown (1999) , we distinguish dialogue initiative from task initiative (i.e., taking the lead in problem-solving).\n[BOS] In this view, a dialogue participant may take both types of initiative, dialogue initiative only, or neither.\n[BOS] Preliminary studies suggest that human-computer dialogue is more efficient when a system dynamically allocates control of task initiative (Smith and Gordon, 1997 ).\n[BOS] In COMIX, task initiative is exercised in asking for additional constraints when the user's request is under-constrained, or in suggesting constraints to relax when the request is over-constrained.\n\n", "discourse_tags": ["Multi_summ", "Reflection", "Transition", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 46, "char_start": 6, "char_end": 219, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 49, "token_end": 58, "char_start": 238, "char_end": 266, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Chu-Carroll and Brown (1999)": "11251276"}}}, {"token_start": 104, "token_end": 131, "char_start": 491, "char_end": 652, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 134, "token_end": 173, "char_start": 662, "char_end": 859, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "859195_0", "paragraph": "[BOS] Cooperative and efficient dialogue between a user and information system has been an important goal in dialogue systems research for many years.\n[BOS] Early work on cooperative response generation addressed how to generate responses explaining the reason for natural language query failures (Kaplan, 1979; Di Eugenio, 1987) .\n[BOS] However, those systems processed and responded to each query in isolation.\n[BOS] Subsequent work addressed understanding the user's query in the context of the preceding discourse (Carbonell, 1983) and inferred plans (e.g., Allen and Perrault, 1980; Carberry, 1990) .\n[BOS] Raskutti and Zukerman (1997) describe a plan-based approach that integrates understanding the user's intentions with generating queries for clarification and generating responses to under-constrained and over-constrained requests.\n[BOS] Their system does not use a constraint-based problem solver and does not provide access to data in a standard commercial database system.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Transition", "Narrative_cite", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 26, "token_end": 57, "char_start": 157, "char_end": 329, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kaplan, 1979;": "62624543", "Di Eugenio, 1987)": null}}}, {"token_start": 87, "token_end": 95, "char_start": 498, "char_end": 535, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Carbonell, 1983)": "691094"}}}, {"token_start": 96, "token_end": 118, "char_start": 540, "char_end": 603, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Allen and Perrault, 1980;": "10693016", "Carberry, 1990)": "171505657"}}}, {"token_start": 120, "token_end": 190, "char_start": 612, "char_end": 986, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Raskutti and Zukerman (1997)": "2364548"}, "Reference": {}}}]}
{"id": "6733857_6", "paragraph": "[BOS] Another technique specially designed for Brazilian Portuguese is the one proposed by (de Mendon\u00e7a Almeida et al., 2016) .\n[BOS] The work presents two approaches for dealing with spelling correction of UGC.\n[BOS] The first approach makes use of three phonetic modules, composed by the Soundex algorithm, a grapheme-to-phoneme converter and a set of language-specific phonetic rules.\n[BOS] The second one combines grapheme-to-phoneme conversion and a decision tree classifier.\n[BOS] The classifier is trained on a corpus of noisy text and employs 14 features (including string and phonetic similarity measures) to identify and correct different classes of orthographic errors.\n[BOS] The approach achieves average correction accuracy of 78%, however requires training on an annotated corpus and feature extraction -making it less scalable than an unsupervised technique.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 168, "char_start": 6, "char_end": 873, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Almeida et al., 2016)": "299948"}, "Reference": {}}}]}
{"id": "6733857_5", "paragraph": "[BOS] Since a large part of misspellings found in UGC is phonetically-motivated, (Duran et al., 2015) proposed a phonetic-based speller for correcting such errors.\n[BOS] The speller combines edit distance and several specific phonetic rules for Portuguese in order to generate correction candidates.\n[BOS] The correction of internet slang and proper name and acronyms capitalization is based on a set of lexicons.\n[BOS] Each lexicon contains many pairs of wrong-correct form of words.\n[BOS] The correction is performed by looking up the noisy word in the lexicon and substituting it by the correct version.\n[BOS] Despite this technique achieving good results in the product review domain, it is not scalable and is too restricted, since there is no form of automatic lexicon-learning.\n[BOS] Therefore, it is not suitable for a generic, domain-free normalizer.\n[BOS] The results obtained by (Duran et al., 2015) will be further discussed, as they are the main source of comparison for our work.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Transition", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 122, "char_start": 6, "char_end": 606, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Duran et al., 2015)": "11312030"}, "Reference": {}}}, {"token_start": 177, "token_end": 186, "char_start": 890, "char_end": 910, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Duran et al., 2015)": "11312030"}}}]}
{"id": "6733857_4", "paragraph": "[BOS] Regarding Brazilian Portuguese, some studies have been performed considering noises in specific domains, such as reviews of products , and some tools have been developed specifically for that same domain.\n[BOS] The normalizer described in (Duran et al., 2015) is, as far as we know, the only tool for text normalization available for Brazilian Portuguese.\n[BOS] The proposed lexicon-based normalizer considers that errors found in UGC are divided into six categories: Common misspellings: context-free orthographic errors, often phonetically-motivated.\n[BOS] Real-word misspellings: contextual orthographic errors.\n[BOS] Words that are contained in the language lexicon, but are wrong considering the context they appear.\n[BOS] Internet slang: abbreviations and expressions often used informally by internet users.\n[BOS] Case use (proper names and acronyms): proper names and acronyms wrongly or not at all capitalized.\n[BOS] Case use (start of sentence): sentences starting with a lower case word.\n[BOS] Glued words: agglutinated words that should be split.\n[BOS] Punctuation: wrong use of sentence delimiters.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Transition", "Transition", "Transition", "Transition", "Transition", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 41, "token_end": 50, "char_start": 245, "char_end": 265, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Duran et al., 2015)": "11312030"}}}]}
{"id": "6733857_3", "paragraph": "[BOS] Our technique is most similar to (Sridhard, 2015), since we implement an adaptation of the method presented in the mentioned work.\n[BOS] The method proposed by (Sridhard, 2015) aims to learn distributed representations of words to capture the notion of contextual similarity and subsequently learn normalization lexicons from these representations in a completely unsupervised manner.\n[BOS] The lexicons are represented as finite-state machines (FSMs) and the process of normalization is performed by transducing the noisy words from the FSMs.\n[BOS] Our work makes use of different distributed representation of words, different scoring function for candidate generation and hash structures (dictionaries) instead of FSMs.\n[BOS] We also introduce a method for automatically expanding the learned lexicons.\n\n", "discourse_tags": ["Reflection", "Single_summ", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 8, "token_end": 15, "char_start": 39, "char_end": 55, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 31, "token_end": 104, "char_start": 143, "char_end": 549, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Sridhard, 2015)": "10533974"}, "Reference": {}}}]}
{"id": "6733857_2", "paragraph": "[BOS] More recently, social media text normalization was tackled by using contextual graph random walks.\n[BOS] (Hassan and Menezes, 2013) proposed a method that uses random walks on a contextual similarity bipartite graph constructed from n-gram sequences on large unlabeled text corpus to build a normalization lexicon.\n[BOS] They obtained a precision of 92.43% and, using the method as a preprocessing step, improved translation quality of social media text by 6%.\n[BOS] (Han et al., 2012 ) also presented an approach for unsupervised construction of normalization lexicons based on context information.\n[BOS] Instead of a graph representation, this approach uses string similarity measures between word within a given context.\n[BOS] (Ling et al., 2013) proposed a supervised learning technique for learning normalization rules from machine translations of a parallel corpus of microblog messages.\n[BOS] They built two models that learn generalizations of the normalization process -one on the phrase level and the other on the character level.\n[BOS] The approach was shown able to improve multiple machine translation systems.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 20, "token_end": 91, "char_start": 111, "char_end": 466, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Hassan and Menezes, 2013)": "9600472"}, "Reference": {}}}, {"token_start": 92, "token_end": 136, "char_start": 473, "char_end": 729, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Han et al., 2012": "6508587"}, "Reference": {}}}, {"token_start": 137, "token_end": 205, "char_start": 736, "char_end": 1129, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Ling et al., 2013)": "15772750"}, "Reference": {}}}]}
{"id": "6733857_1", "paragraph": "[BOS] Log-linear models also have been applied as unsupervised statistical models for text normalization.\n[BOS] (Yang and Eisenstein, 2013) proposed a model in which the relationship between standard and nonstandard words may be characterized by a log-linear model with arbitrary features.\n[BOS] The weights of these features can then be trained in maximum-likelihood frameworks.\n[BOS] The use of this type of model requires a study of the problem to get the most significant features.\n[BOS] From the definition of the features, the training process in conducted to optimize the weights.\n[BOS] The advantage of these models is the easy incorporation of new features and the optimization is performed according to an objective function.\n[BOS] Although not being highly dependent of resources and context-driven, the log-linear approach requires well-defined features -which are not easily identifiable in UGC.\n[BOS] Another disadvantage is the total reliance on statistical observations on the corpus.\n[BOS] Hence, the model does not satisfactorily represents the highly semantic specificity of the noise found in UGC, which can occur with low frequency thus not having a significant statistical impact.\n[BOS] Considering these issues, this type of model is not enough to deal with generic domain and high context and semantic dependency found is UGC noise.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Transition", "Transition", "Transition", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 19, "token_end": 110, "char_start": 112, "char_end": 587, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Yang and Eisenstein, 2013)": "13890994"}, "Reference": {}}}]}
{"id": "6733857_0", "paragraph": "[BOS] Early work handled text normalization as a noisy channel model.\n[BOS] This model consists of two components: a source model and a channel model (Shannon, 1948) .\n[BOS] It assumes that a signal is transferred through a medium and gets corrupted.\n[BOS] The source model indicates the canonical form of the signal, and the channel model represents how the signal gets corrupted.\n[BOS] (Brill and Moore, 2000) defined the spelling correction problem as finding argmax w P (w|s), being s the canonical word, which was sent by the source model, and w the received corrupted word.\n[BOS] Applying Bayes' Theorem, the noisy channel model is obtained as argmax w P (s|w)P (w).\n[BOS] This model presented significant performance improvements compared to previously proposed models, achieving up to 98% correction accuracy on well-behaved noisy text.\n[BOS] However, this approach requires supervised training data for both canonical and corrupted words.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 73, "char_start": 6, "char_end": 381, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Shannon, 1948)": null}}}, {"token_start": 74, "token_end": 193, "char_start": 388, "char_end": 947, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Brill and Moore, 2000)": "472478"}, "Reference": {}}}]}
{"id": "2065400_1", "paragraph": "[BOS] The design and implementation of BRAT was informed by experience from several annotation tasks and research efforts spanning more than a decade.\n[BOS] A variety of previously introduced annotation tools and approaches also served to guide our design decisions, including the fast annotation mode of Knowtator (Ogren, 2006) , the search capabilities of the XConc tool (Kim et al., 2008) , and the design of web-based systems such as MyMiner (Salgado et al., 2010) , and GATE Teamware (Cunningham et al., 2011) .\n[BOS] Using machine learning to accelerate annotation by supporting human judgements is well documented in the literature for tasks such as entity annotation (Tsuruoka et al., 2008) and translation (Mart\u00ednez-G\u00f3mez et al., 2011) , efforts which served as inspiration for our own approach.\n[BOS] BRAT, along with conversion tools and extensive documentation, is freely available under the open-source MIT license from its homepage at http://brat.nlplab.org\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 45, "token_end": 59, "char_start": 277, "char_end": 328, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 60, "token_end": 76, "char_start": 331, "char_end": 391, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kim et al., 2008)": "5261517"}}}, {"token_start": 78, "token_end": 100, "char_start": 398, "char_end": 468, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Salgado et al., 2010)": null}}}, {"token_start": 102, "token_end": 115, "char_start": 475, "char_end": 514, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 138, "token_end": 151, "char_start": 657, "char_end": 698, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Tsuruoka et al., 2008)": "8050769"}}}, {"token_start": 152, "token_end": 164, "char_start": 703, "char_end": 744, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mart\u00ednez-G\u00f3mez et al., 2011)": "14501053"}}}]}
{"id": "2065400_0", "paragraph": "[BOS] We have introduced BRAT, an intuitive and userfriendly web-based annotation tool that aims to enhance annotator productivity by closely integrating NLP technology into the annotation process.\n[BOS] BRAT has been and is being used for several ongoing annotation efforts at a number of academic institutions and has so far been used for the creation of well-over 50,000 annotations.\n[BOS] We presented an experiment demonstrating that integrated machine learning technology can reduce the time for type selection by over 30% and overall annotation time by 15% for a multi-type entity mention annotation task.\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "2948298_6", "paragraph": "[BOS] (iii) Lack of objective evaluation metrics: The evaluation of Natural Language Generation (NLG) systems is known to be a hard problem.\n[BOS] It is further unclear whether the quality of LaVi models should be measured using metrics designed for language-only tasks.\n[BOS] Elliott and Keller (2014) performed a sentence-level correlation analysis of NLG evaluation measures against expert human judgements in the context of IC.\n[BOS] Their study revealed that most of those metrics were only weakly correlated with human judgements.\n[BOS] In the same line of research, Anderson et al. (2016) showed that the most widely-used metrics for IC fail to capture semantic propositional content, which is an essential component of human caption evaluation.\n[BOS] They proposed a semantic evaluation metric called SPICE, that measures how effectively image captions recover objects, attributes and the relations between them.\n[BOS] In this paper, we tackle this problem by proposing tasks which can be evaluated based on objective metrics for classification/detection error.\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 55, "token_end": 103, "char_start": 277, "char_end": 536, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Elliott and Keller (2014)": "10297558"}, "Reference": {}}}, {"token_start": 104, "token_end": 174, "char_start": 543, "char_end": 920, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Anderson et al. (2016)": "11933981"}, "Reference": {}}}]}
{"id": "2948298_5", "paragraph": "[BOS] Finally, proposed CLEVR, a dataset for the diagnostic evaluation of VQA systems.\n[BOS] This dataset was designed with the explicit goal of enabling detailed analysis of different aspects of visual reasoning, by minimising dataset biases and providing rich ground-truth representations for both images and questions.\n\n", "discourse_tags": ["Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 57, "char_start": 6, "char_end": 321, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "2948298_4", "paragraph": "[BOS] Most related to our paper is the work by Ding et al. (2016) .\n[BOS] Like us, they propose to extend the MS-COCO dataset by generating decoys from human-created image captions.\n[BOS] They also suggest an evaluation apparently similar to our T1, requiring the LaVi system to detect the true target caption amongst the decoys.\n[BOS] Our efforts, however, differ in some substantial ways.\n[BOS] First, their technique to create incorrect captions (using BLEU to set an upper similarity threshold) is so that many of those captions will differ from the gold description in more than one respect.\n[BOS] For instance, the caption two elephants standing next to each other in a grass field is associated with the decoy a herd of giraffes standing next to each other in a dirt field (errors: herd, giraffe, dirt) or with animals are gathering next to each other in a dirt field (error: dirt; infelicities: animals and gathering, which are both pragmatically odd).\n[BOS] Clearly, the more the caption changes in the decoy, the easier the task becomes.\n[BOS] In contrast, the foil captions we propose only differ from the gold description by one word and are thus more challenging.\n[BOS] Secondly, the automatic caption generation of Ding et al means that 'correct' descriptions can be produced, resulting in some confusion in human responses to the task.\n[BOS] We made sure to prevent such cases, and human performance on our dataset is thus close to 100%.\n[BOS] We note as well that our task does not require any complex instructions for the annotation, indicating that it is intuitive to human beings (see \u00a74).\n[BOS] Thirdly, their evaluation is a multiple-choice task, where the system has to compare all captions to understand which one is closest to the image.\n[BOS] This is arguably a simpler task than the one we propose, where a caption is given and the system is asked to classify it as correct or foil: as we show in \u00a74, detecting a correct caption is much easier than detecting foils.\n[BOS] So evaluating precision on both gold and foil items is crucial.\n\n", "discourse_tags": ["Single_summ", "Transition", "Transition", "Reflection", "Reflection", "Reflection", "Reflection", "Reflection", "Reflection", "Reflection", "Reflection", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 75, "char_start": 6, "char_end": 329, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ding et al. (2016)": "15367475"}, "Reference": {}}}]}
{"id": "2948298_3", "paragraph": "[BOS] (ii) Need for diagnostics: To overcome the bias uncovered in previous datasets, several research groups have started proposing tasks which involve distinguishing distractors from a groundtruth caption for an image.\n[BOS] Zhang et al. (2016) introduced a binary VQA task along with a dataset composed of sets of similar artificial images, allowing for more precise diagnostics of a system's errors.\n[BOS] Goyal et al. (2016a) balanced the dataset of Antol et al. (2015) , collecting a new set of complementary natural images which are similar to existing items in the original dataset, but result in different answers to a common question.\n[BOS] Hodosh and Hockenmaier (2016) also proposed to evaluate a number of state-of-the-art LaVi algorithms in the presence of distractors.\n[BOS] Their evaluation was however limited to a small dataset (namely, Flickr30K (Young et al., 2014) ) and the caption generation was based on a hand-crafted scheme using only inter-dataset distractors.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 42, "token_end": 79, "char_start": 227, "char_end": 403, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhang et al. (2016)": "6733279"}, "Reference": {}}}, {"token_start": 80, "token_end": 131, "char_start": 410, "char_end": 644, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Goyal et al. (2016a)": null}, "Reference": {"Antol et al. (2015)": "58727669"}}}, {"token_start": 132, "token_end": 215, "char_start": 651, "char_end": 987, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hodosh and Hockenmaier (2016)": "8676220"}, "Reference": {"(Young et al., 2014)": null}}}]}
{"id": "2948298_2", "paragraph": "[BOS] (i) Triviality of the LaVi tasks: Recent work has shown that LaVi models heavily rely on language priors (Ren et al., 2015; Agrawal et al., 2016; Kafle and Kanan, 2016) .\n[BOS] Even simple correlation and memorisation can result in good performance, without the underlying models truly understanding visual content Jabri et al., 2016; Hodosh and Hockenmaier, 2016) .\n[BOS] Zhang et al. (2016) first unveiled that there exists a huge bias in the popular VQA dataset by Antol et al. (2015) : they showed that almost half of all the questions in this dataset could be answered correctly by using the question alone and ignoring the image completely.\n[BOS] In the same vein, proposed a simple baseline for the task of VQA.\n[BOS] This baseline simply concatenates the Bag of Words (BoW) features from the question and Convolutional Neural Networks (CNN) features from the image to predict the answer.\n[BOS] They showed that such a simple method can achieve comparable performance to complex and deep architectures.\n[BOS] Jabri et al. (2016) proposed a similar model for the task of multiple choice VQA, and suggested a cross-dataset generalization scheme as an evaluation criterion for VQA systems.\n[BOS] We complement this research by introducing three new tasks with different levels of difficulty, on which LaVi models can be evaluated sequentially.\n\n", "discourse_tags": ["Multi_summ", "Multi_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 13, "token_end": 52, "char_start": 40, "char_end": 174, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ren et al., 2015;": "2950705", "Agrawal et al., 2016;": "12304778", "Kafle and Kanan, 2016)": "207061473"}}}, {"token_start": 54, "token_end": 92, "char_start": 183, "char_end": 370, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Jabri et al., 2016;": "11328415", "Hodosh and Hockenmaier, 2016)": "8676220"}, "Reference": {}}}, {"token_start": 94, "token_end": 222, "char_start": 379, "char_end": 1015, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhang et al. (2016)": "6733279", "Antol et al. (2015)": "58727669"}, "Reference": {}}}, {"token_start": 223, "token_end": 261, "char_start": 1022, "char_end": 1199, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Jabri et al. (2016)": "11328415"}, "Reference": {}}}]}
{"id": "2948298_1", "paragraph": "[BOS] Despite their success, it remains unclear whether state-of-the-art LaVi models capture vision and language in a truly integrative fashion.\n[BOS] We could identify three types of arguments surrounding the high performance of LaVi models:\n\n", "discourse_tags": ["Transition", "Transition"], "span_citation_mapping": []}
{"id": "2948298_0", "paragraph": "[BOS] The image captioning (IC) and visual question answering (VQA) tasks are the most relevant to our work.\n[BOS] In IC (Fang et al., 2015; Chen and Lawrence Zitnick, 2015; Donahue et al., 2015; Karpathy and Fei-Fei, 2015; Vinyals et al., 2015; , the goal is to generate a caption for a given image, such that it is both semantically and syntactically correct, and properly describes the content of that image.\n[BOS] In VQA (Antol et al., 2015; Malinowski and Fritz, 2014; Malinowski et al., 2015; Ren et al., 2015) , the system attempts to answer open-ended questions related to the content of the image.\n[BOS] There is a wealth of literature on both tasks, but we only discuss here the ones most related to our work and refer the reader to the recent surveys by (Bernardi et al., 2016; Wu et al., 2016) .\n\n", "discourse_tags": ["Transition", "Multi_summ", "Multi_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 28, "token_end": 111, "char_start": 115, "char_end": 411, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Fang et al., 2015;": "9254582", "Chen and Lawrence Zitnick, 2015;": "6785090", "Donahue et al., 2015;": "59605920", "Karpathy and Fei-Fei, 2015;": "59606130", "Vinyals et al., 2015;": "1169492"}}}, {"token_start": 112, "token_end": 169, "char_start": 418, "char_end": 606, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Antol et al., 2015;": "58727669", "Malinowski and Fritz, 2014;": null, "Malinowski et al., 2015;": "738850", "Ren et al., 2015)": "2950705"}}}, {"token_start": 198, "token_end": 217, "char_start": 747, "char_end": 805, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bernardi et al., 2016;": "47156761", "Wu et al., 2016)": "11746788"}}}]}
{"id": "6961896_1", "paragraph": "[BOS] In reranking approaches, a first-pass parser is used to enumerate a small set of candidate parses for an input sentence; the reranking model, which is a GLM, is used to select between these parses (e.g., (Ratnaparkhi et al., 1994; Johnson et al., 1999; Collins, 2000; Charniak and Johnson, 2005) ).\n[BOS] A crucial advantage of our approach is that it considers a very large set of alternatives in Y(x), and can thereby avoid search errors that may be made in the first-pass parser.\n[BOS] 1 Another approach that allows efficient training of GLMs is to use simpler syntactic representations, in particular dependency structures (McDon-ald et al., 2005) .\n[BOS] Dependency parsing can be implemented in O(n 3 ) time using the algorithms of Eisner (2000) .\n[BOS] In this case there is no grammar constant, and parsing is therefore efficient.\n[BOS] A disadvantage of these approaches is that they do not recover full, constituent-based syntactic structures; the increased linguistic detail in full syntactic structures may be useful in NLP applications, or may improve dependency parsing accuracy, as is the case in our experiments.\n[BOS] 2 There has been some previous work on GLM approaches for full syntactic parsing that make use of dynamic programming.\n[BOS] Taskar et al. (2004) describe a max-margin approach; however, in this work training sentences were limited to be of 15 words or less.\n[BOS] Clark and Curran (2004) describe a log-linear GLM for CCG parsing, trained on the Penn treebank.\n[BOS] This method makes use of parallelization across an 18 node cluster, together with up to 25GB of memory used for storage of dynamic programming structures for training data.\n[BOS] Clark and Curran (2007) describe a perceptronbased approach for CCG parsing which is considerably more efficient, and makes use of a supertagging model to prune the search space of the full parsing model.\n[BOS] Recent work Finkel et al., 2008) describes log-linear GLMs applied to PCFG representations, but does not make use of dependency features.\n\n", "discourse_tags": ["Multi_summ", "Transition", "Narrative_cite", "Narrative_cite", "Single_summ", "Transition", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 88, "char_start": 6, "char_end": 304, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ratnaparkhi et al., 1994;": "15061443", "Johnson et al., 1999;": "16005368", "Collins, 2000;": "405878", "Charniak and Johnson, 2005)": "11599080"}}}, {"token_start": 129, "token_end": 161, "char_start": 495, "char_end": 658, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 176, "token_end": 184, "char_start": 727, "char_end": 758, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Eisner (2000)": "5897173"}}}, {"token_start": 276, "token_end": 308, "char_start": 1267, "char_end": 1400, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Taskar et al. (2004)": "8313435"}, "Reference": {}}}, {"token_start": 309, "token_end": 368, "char_start": 1407, "char_end": 1682, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Clark and Curran (2004)": "6802974"}, "Reference": {}}}, {"token_start": 369, "token_end": 413, "char_start": 1689, "char_end": 1893, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Clark and Curran (2007)": "14759233"}, "Reference": {}}}, {"token_start": 414, "token_end": 445, "char_start": 1900, "char_end": 2037, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Finkel et al., 2008)": "803811"}, "Reference": {}}}]}
{"id": "6961896_0", "paragraph": "[BOS] Previous work has made use of various restrictions or approximations that allow efficient training of GLMs for parsing.\n[BOS] This section describes the relationship between our work and this previous work.\n\n", "discourse_tags": ["Transition", "Transition"], "span_citation_mapping": []}
{"id": "35266242_2", "paragraph": "[BOS] Only few text-based studies consider NEs, and if so, focus on location names using gazetteers like GeoNames, limiting the methods to the completeness of these gazetteers.\n[BOS] Since they usually also use other text-based models, it is hard to determine how much location names contribute.\n[BOS] These approaches depend on a namedisambiguation phase, using Wikipedia, DBPedia, or OpenStreetMap, since location names can refer to multiple locations (Brunsting et al., 2016) .\n[BOS] Chi et al. (2016) explicitly study the contributions of city and country names, hashtags, and user mentionings, to geolocation.\n[BOS] Their results suggested that a combination of city and country names, as well as hashtags, are good location predictors.\n[BOS] Pavalanathan and Eisenstein (2015) suggest that non-standard words are more locationspecific, and also, more likely to occur in geotagged tweets.\n[BOS] In contrast to this paper, none of the previous works study how much various NE types reveal about the user location.\n[BOS] Similarly, Salehi and S\u00f8gaard (2017) evaluate common hypotheses about language and location.\n[BOS] However, they do not explicitly study named entities.\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite", "Single_summ", "Single_summ", "Single_summ", "Reflection", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 88, "token_end": 105, "char_start": 407, "char_end": 478, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Brunsting et al., 2016)": "16137301"}}}, {"token_start": 107, "token_end": 162, "char_start": 487, "char_end": 741, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Chi et al. (2016)": "6534537"}, "Reference": {}}}, {"token_start": 163, "token_end": 196, "char_start": 748, "char_end": 893, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Pavalanathan and Eisenstein (2015)": "9048146"}, "Reference": {}}}, {"token_start": 221, "token_end": 251, "char_start": 1024, "char_end": 1176, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Salehi and S\u00f8gaard (2017)": "10874429"}, "Reference": {}}}]}
{"id": "35266242_1", "paragraph": "[BOS] All these studies require relatively large training sets to fit the models, and can be heavily biased by major events during the time of collection, such as an election or a disaster.\n[BOS] In contrast to our work, most do not consider multi-word NEs.\n\n", "discourse_tags": ["Transition", "Reflection"], "span_citation_mapping": []}
{"id": "35266242_0", "paragraph": "[BOS] Most previous studies use textual features as input.\n[BOS] Some use KL divergence between the distribution of a users words and the words used in each region (Wing and Baldridge, 2011; Roller et al., 2012) , regional topic distributions (Eisenstein et al., 2010; Ahmed et al., 2013; Hong et al., 2012) , or feature selection/weighting to find words indicative of location (Priedhorsky et al., 2014; Han et al., 2012 Han et al., , 2014 Wing and Baldridge, 2014) .\n\n", "discourse_tags": ["Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 28, "token_end": 46, "char_start": 152, "char_end": 211, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wing and Baldridge, 2011;": "1548170", "Roller et al., 2012)": "7970365"}}}, {"token_start": 47, "token_end": 73, "char_start": 214, "char_end": 307, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Eisenstein et al., 2010;": "2256302", "Ahmed et al., 2013;": "6149841", "Hong et al., 2012)": "4537898"}}}, {"token_start": 75, "token_end": 116, "char_start": 313, "char_end": 466, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Priedhorsky et al., 2014;": "16235064", "Han et al., 2012": "15859165", "Han et al., , 2014": null, "Wing and Baldridge, 2014)": "1017178"}}}]}
{"id": "129124_0", "paragraph": "[BOS] Self-training has been applied to several natural language processing tasks.\n[BOS] For event extraction, there are several studies on bootstrapping from a seed pattern set.\n[BOS] Riloff (1996) initiated the idea of using document relevance for extracting new patterns, and Yangarber et al. (2000 Yangarber et al. ( , 2003 incorporated this into a bootstrapping approach, extended by Surdeanu et al. (2006) to co-training.\n[BOS] Stevenson and Greenwood (2005) suggested an alternative method for ranking the candidate patterns by lexical similarities.\n[BOS] Liao and Grishman (2010b) combined these two approaches to build a filtered ranking algorithm.\n[BOS] However, these approaches were focused on finding instances of a scenario/event type rather than on argument role labeling.\n[BOS] Starting from a set of documents classified for relevance, Patwardhan and Riloff (2007) created a self-trained relevant sentence classifier and automatically learned domain-relevant extraction patterns.\n[BOS] Liu (2009) proposed the BEAR system, which tagged both the events and their roles.\n[BOS] However, the new patterns were boostrapped based on the frequencies of sub-pattern mutations or on rules from linguistic contexts, and not on statistical models.\n[BOS] The idea of sense consistency was first introduced and extended to operate across related documents by (Yarowsky, 1995) .\n[BOS] Yangarber et al. (Yangarber and Jokipii, 2005; Yangarber, 2006; Yangarber et al., 2007) applied cross-document inference to correct local extraction results for disease name, location and start/end time.\n[BOS] Mann (2007) encoded specific inference rules to improve extraction of information about CEOs (name, start year, end year).\n[BOS] Later, Ji and Grishman (2008) employed a rule-based approach to propagate consistent triggers and arguments across topic-related documents.\n[BOS] Gupta and Ji (2009) used a similar approach to recover implicit time information for events.\n[BOS] Liao and Grishman (2010a) use a statistical model to infer the cross-event information within a document to improve event extraction.\n\n", "discourse_tags": ["Transition", "Transition", "Multi_summ", "Single_summ", "Single_summ", "Transition", "Single_summ", "Single_summ", "Transition", "Single_summ", "Multi_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 34, "token_end": 51, "char_start": 185, "char_end": 273, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Riloff (1996)": "15894892"}, "Reference": {}}}, {"token_start": 53, "token_end": 95, "char_start": 279, "char_end": 427, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Yangarber et al. ( , 2003": "1398439"}, "Reference": {"Surdeanu et al. (2006)": "7419156"}}}, {"token_start": 96, "token_end": 117, "char_start": 434, "char_end": 556, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Stevenson and Greenwood (2005)": "6008231"}, "Reference": {}}}, {"token_start": 118, "token_end": 139, "char_start": 563, "char_end": 657, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Liao and Grishman (2010b)": "2114002"}, "Reference": {}}}, {"token_start": 163, "token_end": 200, "char_start": 794, "char_end": 996, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Patwardhan and Riloff (2007)": "5749336"}, "Reference": {}}}, {"token_start": 201, "token_end": 219, "char_start": 1003, "char_end": 1085, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Liu (2009)": "14806616"}, "Reference": {}}}, {"token_start": 252, "token_end": 275, "char_start": 1260, "char_end": 1379, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Yarowsky, 1995)": "1487550"}, "Reference": {}}}, {"token_start": 277, "token_end": 331, "char_start": 1388, "char_end": 1591, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Yangarber and Jokipii, 2005;": "2626484", "Yangarber, 2006;": null, "Yangarber et al., 2007)": "15746598"}, "Reference": {}}}, {"token_start": 332, "token_end": 358, "char_start": 1598, "char_end": 1720, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Mann (2007)": "10459581"}, "Reference": {}}}, {"token_start": 359, "token_end": 387, "char_start": 1727, "char_end": 1866, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ji and Grishman (2008)": "1320606"}, "Reference": {}}}, {"token_start": 388, "token_end": 406, "char_start": 1873, "char_end": 1965, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Gupta and Ji (2009)": "8336242"}, "Reference": {}}}, {"token_start": 407, "token_end": 436, "char_start": 1972, "char_end": 2105, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Liao and Grishman (2010a)": "11187670"}, "Reference": {}}}]}
{"id": "2627903_9", "paragraph": "[BOS] None of the revised works use as unique evidence for multilingual clustering the identification of cognate named entities between both sides of the comparable corpora.\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "2627903_8", "paragraph": "[BOS] Others works combine recognition of independent text features (numbers, dates, names, cognates) with mapping text contents to a thesaurus.\n[BOS] In ) the cross-lingual news cluster similarity is based on a linear combination of three types of input: (a) cognates, (b) automatically detected references of geographical place names, and (c) the results of a mapping process onto a multilingual classification system which maps documents onto the multilingual thesaurus Eurovoc.\n[BOS] In it is proposed to extract language-independent text features using gazetteers and regular expressions besides thesaurus and classification systems.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 31, "token_end": 100, "char_start": 151, "char_end": 481, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 101, "token_end": 127, "char_start": 488, "char_end": 638, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "2627903_7", "paragraph": "[BOS] The second approach, recognition of language independent text features, involves the recognition of elements such as: dates, numbers, and named entities.\n[BOS] In others works, for instance (Silva et. al., 2004) , the authors present a method based on Relevant Expressions (RE).\n[BOS] The RE are multilingual lexical units of any length automatically extracted from the documents using the LiPXtractor extractor, a language independent statistics-based tool.\n[BOS] The RE are used as base features to obtain a reduced set of new features for the multilingual clustering, but the clusters obtained are monolingual.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 30, "token_end": 122, "char_start": 166, "char_end": 619, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Silva et. al., 2004)": null}, "Reference": {}}}]}
{"id": "2627903_6", "paragraph": "[BOS] The first approach involves the use of existing multilingual linguistic resources, such as thesaurus, to create a text representation consisting of a set of thesaurus items.\n[BOS] Normally, in a multilingual thesaurus, elements in different languages are related via language-independent items.\n[BOS] So, two documents written in different languages can be considered similar if they have similar representation according to the thesaurus.\n[BOS] In some cases, it is necessary to use the thesaurus in combination with a machine learning method for mapping correctly documents onto thesaurus.\n[BOS] In (Steinberger et. al., 2002 ) the authors present an approach to calculate the semantic similarity by representing the document contents in a language independent way, using the descriptor terms of the multilingual thesaurus Eurovoc.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Transition", "Single_summ"], "span_citation_mapping": [{"token_start": 115, "token_end": 162, "char_start": 604, "char_end": 839, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Steinberger et. al., 2002": null}, "Reference": {}}}]}
{"id": "2627903_5", "paragraph": "[BOS] The strategies that use language-independent representation try to normalize or standardize the document contents in a language-neutral way; for example: (1) by mapping text contents to an independent knowledge representation, or (2) by recognizing language independent text features inside the documents.\n[BOS] Both approaches can be employed isolated or combined.\n\n", "discourse_tags": ["Transition", "Transition"], "span_citation_mapping": []}
{"id": "2627903_4", "paragraph": "[BOS] In (Mathieu et. al, 2004) before the clustering process, the authors perform a linguistic analysis which extracts lemmas and recognizes named entities (location, organization, person, time expression, numeric expression, product or event); then, the documents are represented by a set of terms (keywords or named entity types).\n[BOS] In addition, they use document frequency to select relevant features among the extracted terms.\n[BOS] Finally, the solution uses bilingual dictionaries to translate the selected features.\n[BOS] In (Rauber et. al., 2001 ) the authors present a methodology in which documents are parsed to extract features: all the words which appear in n documents except the stopwords.\n[BOS] Then, standard machine translation techniques are used to create a monolingual corpus.\n[BOS] After the translation process the documents are automatically organized into separate clusters using an un-supervised neural network.\n[BOS] Some approaches first carry out an independent clustering in each language, that is a monolingual clustering, and then they find relations among the obtained clusters generating the multilingual clusters.\n[BOS] Others solutions start with a multilingual clustering to look for relations between the documents of all the involved languages.\n[BOS] This is the case of (Chen and Lin, 2000) , where the authors propose an architecture of multilingual news summarizer which includes monolingual and multilingual clustering; the multilingual clustering takes input from the monolingual clusters.\n[BOS] The authors select different type of features depending on the clustering: for the monolingual clustering they use only named entities, for the multilingual clustering they extract verbs besides named entities.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Transition", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 99, "char_start": 6, "char_end": 527, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Mathieu et. al, 2004)": "7773350"}, "Reference": {}}}, {"token_start": 100, "token_end": 211, "char_start": 534, "char_end": 1153, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Rauber et. al., 2001": null}, "Reference": {}}}, {"token_start": 234, "token_end": 316, "char_start": 1295, "char_end": 1755, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Chen and Lin, 2000)": "15435394"}, "Reference": {}}}]}
{"id": "2627903_3", "paragraph": "[BOS] When the solution involves translating only some features, first it is necessary to select these features (usually entities, verbs, nouns) and then translate them with a bilingual dictionary or/and consulting a parallel corpus.\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "2627903_2", "paragraph": "[BOS] With regard to the first approach, some authors use machine translation systems, whereas others translate the document word by word consulting a bilingual dictionary.\n[BOS] In (Lawrence, 2003) , the author presents several experiments for clustering a Russian-English multilingual corpus; several of these experiments are based on using a machine translation system.\n[BOS] Columbia's Newsblaster system (Kirk et al., 2004) clusters news into events, it categorizes events into broad topic and summarizes multiple articles on each event.\n[BOS] In the clustering process non-English documents are translated using simple dictionary lookup techniques for translating Japanese and Russian documents, and the Systran translation system for the other languages used in the system.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 30, "token_end": 65, "char_start": 179, "char_end": 372, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Lawrence, 2003)": null}, "Reference": {}}}, {"token_start": 66, "token_end": 140, "char_start": 379, "char_end": 780, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Kirk et al., 2004)": "7444"}, "Reference": {}}}]}
{"id": "2627903_1", "paragraph": "[BOS] Considering the approaches based on translation technology, two different strategies are employed: (1) translate the whole document to an anchor language, and (2) translate some features of the document to an anchor language.\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "2627903_0", "paragraph": "[BOS] MDC is normally applied with parallel (Silva et. al., 2004) or comparable corpus (Chen and Lin, 2000) , (Rauber et. al., 2001) , (Lawrence, 2003) , (Steinberger et. al., 2002) , (Mathieu et. al, 2004) , .\n[BOS] In the case of the comparable corpora, the documents usually are news articles.\n\n", "discourse_tags": ["Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 8, "token_end": 18, "char_start": 35, "char_end": 65, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Silva et. al., 2004)": null}}}, {"token_start": 19, "token_end": 66, "char_start": 69, "char_end": 206, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chen and Lin, 2000)": "15435394", "(Rauber et. al., 2001)": null, "(Lawrence, 2003)": null, "(Steinberger et. al., 2002)": null, "(Mathieu et. al, 2004)": "7773350"}}}]}
{"id": "18972100_2", "paragraph": "[BOS] Early work on dictionaries in the area of psychology include the General Inquirer psychosociological dictionary (Stone and Hunt, 1963) which can be used in various applications; current work on lexical resources for identifying particular text variables -such as measuring strong/weak opinions, sentiments, subjective/objective language, etc.\n[BOS] -include the SentiWordnet resource (Esuli and Sebastiani, 2006) derived from WordNet which has been used in various opinion mining works (Devitt and Ahmad, 2007) ; other lines of research include the derivation of word-lists (semi) automatically for opinion classification (Turney, 2002) .\n[BOS] To the best of our knowledge, little research has been carried out on natural language processing for discourse interpretation in psychology.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 13, "token_end": 37, "char_start": 67, "char_end": 182, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Stone and Hunt, 1963)": "6142075"}}}, {"token_start": 70, "token_end": 108, "char_start": 364, "char_end": 516, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Esuli and Sebastiani, 2006)": "6247656", "(Devitt and Ahmad, 2007)": "6526153"}}}, {"token_start": 109, "token_end": 133, "char_start": 519, "char_end": 642, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Turney, 2002)": "484335"}}}]}
{"id": "18972100_1", "paragraph": "[BOS] There has been substantial research in the development of methods to analyze linguistic input in the field of psychotherapy in order to measure a number of psychological variables such as emotion, abstraction, referential activity, etc.\n[BOS] among them Bucci's Referential Activity (RA) non-weighted (Bucci, 2002) and weighted dictionaries (Bucci and Maskit, 2006) for the English language, or H\u00f6ltzer and others' affective dictionary (H\u00f6lzer et al., 1997) for the German language.\n[BOS] The LIWC tool has been used to detect different types of personalities in written self-descriptions (Chung and Pennebaker, 2008) .\n[BOS] This program counts meaningful words that express emotion, abstraction, verbal behavior, demographic variables, traditional personality measures, formal and informal settings, deception and honesty, emotional upheavals, social interaction, use of cognitive and emotion words, word analysis in psychotherapy, references to self and others.\n[BOS] For Spanish (Roussos and O'Connell, 2005) have developed a dictionary in the area of psychotherapy to measure referential activity.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Single_summ", "Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 45, "token_end": 81, "char_start": 260, "char_end": 396, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bucci, 2002)": null, "(Bucci and Maskit, 2006)": "206609691"}}}, {"token_start": 83, "token_end": 105, "char_start": 401, "char_end": 488, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(H\u00f6lzer et al., 1997)": "144583444"}}}, {"token_start": 106, "token_end": 194, "char_start": 495, "char_end": 970, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Chung and Pennebaker, 2008)": "5166612"}, "Reference": {}}}, {"token_start": 195, "token_end": 224, "char_start": 977, "char_end": 1108, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Roussos and O'Connell, 2005)": null}, "Reference": {}}}]}
{"id": "18972100_0", "paragraph": "[BOS] There are a number of well-established computational tools for the analysis and extraction of meaning from text in the social sciences (See (Alexa and Zuell, 2000) for an overview of tools and resources).\n[BOS] Some tools are bound to particular theoretical principles, for example the LWIC dictionary (Pennebaker et al., 2001 ) encodes specific categories to be identified in text while others follow a theory-free approach (Iker and Klein, 1974) where the theory emerges from the analysis of the data.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ"], "span_citation_mapping": [{"token_start": 27, "token_end": 35, "char_start": 146, "char_end": 169, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Alexa and Zuell, 2000)": "60782632"}}}, {"token_start": 56, "token_end": 78, "char_start": 288, "char_end": 387, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Pennebaker et al., 2001": "147748556"}, "Reference": {}}}, {"token_start": 82, "token_end": 106, "char_start": 410, "char_end": 509, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Iker and Klein, 1974)": "62748681"}}}]}
{"id": "18228350_3", "paragraph": "[BOS] Recently, there have been attempts to combine approaches.\n[BOS] Narayan and Gardent (2014) use an approach based on semantics to perform syntactic simplification, and PBMT for lexical simplifications.\n[BOS] We have also created a hybrid system, but one using linguistically sound hand written rules for syntactic simplification and automatically acquired rules for lexicalised constructs .\n[BOS] In this paper we combine this work (summarised in \u00a73) with a new method for sentence compression (described in \u00a74).\n\n", "discourse_tags": ["Transition", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 13, "token_end": 40, "char_start": 70, "char_end": 206, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Narayan and Gardent (2014)": "15489071"}, "Reference": {}}}]}
{"id": "18228350_2", "paragraph": "[BOS] Some contemporary work in text simplification has evolved from research in sentence compression, a related research area that aims to shorten sentences for the purpose of summarising the main content.\n[BOS] Sentence compression has historically been addressed in a generative framework, where transformation rules are learnt from parsed corpora of sentences aligned with manually compressed versions, using ideas adapted from statistical machine translation.\n[BOS] The compression rules learnt are typically syntactic tree-to-tree transformations (Knight and Marcu, 2000; Galley and McKeown, 2007; Riezler et al., 2003; Cohn and Lapata, 2009; Nomoto, 2008) of some variety.\n[BOS] Indeed, Woodsend and Lapata (2011) develop this line of research.\n[BOS] Their model is based on quasi-synchronous tree substitution grammar (QTSG) (Smith and Eisner, 2006) and integer linear programming.\n[BOS] Quasi-synchronous grammars aim to relax the isomorphism constraints of synchronous grammars, in this case by generating a loose alignment between parse trees.\n[BOS] Woodsend and Lapata (2011) use QTSG to generate all possible rewrite operations for a source tree, and then integer linear programming to select the most appropriate simplification.\n[BOS] Their system performs lexical and syntactic simplification as well as compression.\n\n", "discourse_tags": ["Transition", "Transition", "Multi_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 74, "token_end": 132, "char_start": 471, "char_end": 679, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Knight and Marcu, 2000;": "9363872", "Galley and McKeown, 2007;": "1762277", "Riezler et al., 2003;": "7418660", "Cohn and Lapata, 2009;": "6429026", "Nomoto, 2008)": "10425985"}}}, {"token_start": 135, "token_end": 205, "char_start": 694, "char_end": 1054, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Woodsend and Lapata (2011)": "9945908"}, "Reference": {"(Smith and Eisner, 2006)": "741354"}}}, {"token_start": 206, "token_end": 253, "char_start": 1061, "char_end": 1331, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Woodsend and Lapata (2011)": "9945908"}, "Reference": {}}}]}
{"id": "18228350_1", "paragraph": "[BOS] Hand crafted systems such as Siddharthan (2010) and Siddharthan (2011) use transformation rules that encode morphological changes as well as deletions, re-orderings, substitutions and sentence splitting, and can handle voice change correctly.\n[BOS] However, hand crafted systems are limited in scope to syntactic simplification as there are too many lexico-syntactic and lexical simplifications to enumerate manually.\n\n", "discourse_tags": ["Multi_summ", "Transition"], "span_citation_mapping": [{"token_start": 2, "token_end": 53, "char_start": 6, "char_end": 248, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Siddharthan (2010)": "6077146", "Siddharthan (2011)": "2245040"}, "Reference": {}}}]}
{"id": "18228350_0", "paragraph": "[BOS] Text simplification systems differ primarily in the level of linguistic knowledge they encode.\n[BOS] Phrase Based Machine Translation (PBMT) systems (Specia, 2010; Wubben et al., 2012; Coster and Kauchak, 2011) use the least knowledge, and as such are ill equipped to handle simplifications that require morphological changes, syntactic reordering, sentence splitting or insertions.\n[BOS] While syntax based MT approaches use syntactic knowledge, existing systems do not offer a treatment of morphology (Zhu et al., 2010; Woodsend and Lapata, 2011; Paetzold and Specia, 2013) .\n[BOS] This means that while some syntactic reordering operations can be performed well, others requiring morphological changes cannot.\n[BOS] Consider converting passive to active voice (e.g., from \"\"trains are liked by John\"\" to \"\"John likes trains\"\").\n[BOS] Besides deleting auxiliaries and reordering the arguments of the verb, there is also a requirement to modify the verb to make it agree in number with the new subject \"\"John\"\", and take the tense of the auxiliary \"\"are\"\".\n\n", "discourse_tags": ["Transition", "Multi_summ", "Multi_summ", "Transition", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 17, "token_end": 79, "char_start": 107, "char_end": 388, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Specia, 2010;": "9871276", "Wubben et al., 2012;": "141120", "Coster and Kauchak, 2011)": "4896510"}}}, {"token_start": 80, "token_end": 123, "char_start": 395, "char_end": 581, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhu et al., 2010;": "15636533", "Woodsend and Lapata, 2011;": "9945908", "Paetzold and Specia, 2013)": "17335786"}}}]}
{"id": "18930050_3", "paragraph": "[BOS] The application of text mining to information retrieval may improve precision and recall.\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "18930050_2", "paragraph": "[BOS] Data Mining is about analyzing data and finding hidden patterns using automatic or semi-automatic means.\n[BOS] Text mining is a research field of data mining which refers to the process of deriving high quality patterns and trends from text.\n[BOS] We are proposing to apply text mining techniques to finding frequent patterns in the retrieved documents in the first retrieval round which contain query terms.\n[BOS] These patterns provide us with the candidate sequences to find more terms which are relevant to the original query.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Transition"], "span_citation_mapping": []}
{"id": "18930050_1", "paragraph": "[BOS] Pseudo-relevance feedback is an important query expansion technique for improving IR performance (Qiu and Frei 1993; Sun, Ong and Chua 2006; Robertson and Jones 1976) .\n[BOS] The basic insight which motivates pseudo relevance feedback is that often the top of the initially ranked list of results contains a relatively high proportion of relevant documents.\n[BOS] The conjecture is that despite the presence of some irrelevant documents, these retrieved documents might still be used to identify relevant terms that co-occur in the relevant documents.\n[BOS] These terms are then used to modify the original query and better reflect the user's information needs.\n[BOS] With the expanded query, a second retrieval round is performed and the returned result is expected to contain more relevant documents which have been missed in the first retrieval round.\n[BOS] For pseudo relevance feedback query expansion, the most important task is to find the terms from the retrieved documents that are considered relevant to the query.\n[BOS] Therefore, relevant term selection is crucial in pseudo relevance feedback query expansion.\n[BOS] The standard criteria for selecting relevant terms have been proposed using tf/idf in vector space model (Rocchio 1997) and probabilistic model (Robertson and Jones 1976) .\n[BOS] Query length has been considered in (Kwok, Grunfeld and Chan 2000) for weighting expansion terms and some linguistic features also have been tried in (Smeaton and Rijsbergen 1983) .\n[BOS] We are proposing to use text mining techniques to find the relevant terms.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Transition", "Transition", "Transition", "Transition", "Narrative_cite", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 2, "token_end": 38, "char_start": 6, "char_end": 172, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Qiu and Frei 1993;": "11711278", "Sun, Ong and Chua 2006;": "14325011", "Robertson and Jones 1976)": "45186038"}}}, {"token_start": 213, "token_end": 227, "char_start": 1211, "char_end": 1254, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rocchio 1997)": "61859400"}}}, {"token_start": 228, "token_end": 236, "char_start": 1259, "char_end": 1305, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Robertson and Jones 1976)": "45186038"}}}, {"token_start": 238, "token_end": 259, "char_start": 1314, "char_end": 1410, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Kwok, Grunfeld and Chan 2000)": null}}}, {"token_start": 260, "token_end": 278, "char_start": 1415, "char_end": 1493, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Smeaton and Rijsbergen 1983)": "39678526"}}}]}
{"id": "18930050_0", "paragraph": "[BOS] Unlike English text in which sentences consist of words delimited by white spaces, in Chinese text, sentences are represented as strings of Chinese characters without delimiters.\n[BOS] Therefore, Chinese word segmentation is the first phase in Chinese language processing and has been widely studied for many years (Gao and Li 2005; Xue 2003; Sproat and Shih 2002; Wang, Liu and Qin 2006) .\n[BOS] Both Chinese characters and words can be used as the indexing units for Chinese IR.\n[BOS] Several approaches have shown that single character indexing can produce good results, but word and bi-gram indexing can achieve slightly better performance.\n[BOS] This however incurs greater time and space complexity with limited performance improvement (Sproat and Shih 2002; Li 1999; Kwok 1997; Peng, Huang, Schuurmans and Cercone 2002) .\n[BOS] In this paper, we propose a ranking method that combines character indexing and segmented word indexing to re-rank retrieved documents and promote relevant documents to higher positions.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition", "Multi_summ", "Multi_summ", "Transition"], "span_citation_mapping": [{"token_start": 34, "token_end": 79, "char_start": 191, "char_end": 394, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Gao and Li 2005;": null, "Xue 2003;": null, "Sproat and Shih 2002;": "17820880", "Wang, Liu and Qin 2006)": null}}}, {"token_start": 98, "token_end": 167, "char_start": 493, "char_end": 832, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sproat and Shih 2002;": "17820880", "Li 1999;": null, "Kwok 1997;": null, "Peng, Huang, Schuurmans and Cercone 2002)": "40591"}}}]}
{"id": "2924377_3", "paragraph": "[BOS] Because of the lack of prior research on this task, we are unable to compare to our results to those of other researches; but the results do seem promising.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "2924377_2", "paragraph": "[BOS] An ideal system for machine translation would take advantage of both empirical data and linguistic analysis.\n[BOS] Different authors have different objectives that they attempt to achieve high translation precision on many languages.\n[BOS] Our translation model aims is to get correct translation phrases with very limited bilingual corpus for Statistical Myanmar to English machine translation.\n\n", "discourse_tags": ["Transition", "Transition", "Reflection"], "span_citation_mapping": []}
{"id": "2924377_1", "paragraph": "[BOS] A few researches investigated the use of morphology to improve translation quality.\n[BOS] If source language is morphology rich language (such as German, Spanish, Czech), phrase-based model has limitations.\n[BOS] When a form of a word does not occur in the training data, current systems are unable to translate it.\n[BOS] Data sparseness problem can be overcome by using large training data or morphology analysis of source or/and target languages.\n[BOS] In 2005 Goldwater and McClosky used morphological analysis of Czech to improve a Czech-English statistical machine translation system.\n[BOS] This system solve data sparse problem caused by the highly inflected nature of Czech.\n[BOS] Their combine model achieved high BLEU score of development and test set.\n[BOS] Nguyen and Shimazu, 2006 proposed morphological transformational rules and Bayes' formula based transformational model to translate English to Vietnamese.\n[BOS] The score of their system is better than baseline score.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 88, "token_end": 144, "char_start": 461, "char_end": 767, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 145, "token_end": 183, "char_start": 774, "char_end": 991, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "2924377_0", "paragraph": "[BOS] In this section, previous works in Statistical machine translation on different languages are reviewed.\n[BOS] Various researchers have improved the quality of statistical machine translation system by using different methods on different language.\n[BOS] Probabilistic, models is created for simulating the translation process, in the models using bilingual corpora and then decoding a test sentence by searching (Brown et al., 1990) .\n[BOS] In 1993, they took the translation process as a noisy-channel model.\n[BOS] In terms of modeling Berger et al., 1996 appended context-based information based on the Maximum Entropy principle to enrich the word-based models.\n[BOS] Alignment model which is based on phrase structure is firstly proposed by Wang and Waible in 1998, which was automatically acquired from parallel corpus.\n[BOS] Och et al., 1999 used beam search algorithm, which could make use of pruning strategies for balancing efficiency and accuracy.\n[BOS] In 2002, Och and Ney first introduced the log-linear model into SMT.\n[BOS] In 2004 Koehn suggested using features of lexical weighting.\n[BOS] In this year, the famous phrase-based decoder, Pharaoh, was released to be a free SMT toolkit by Philipp Koehn and further updated to Moses by Koehn et al., 2007.\n[BOS] In 2003 Koehn, Och and Marcu, used noisy channel based translation model and beam search decoder.\n[BOS] They achieved fast decoding, while ensuring high quality.\n[BOS] They presented experiential result on many languages (English-German, French-English, Swedish-English, and Chinese-English).\n[BOS] Loglinear based statistical machine translation model is proposed by Zens and Ney in 2004 .\n[BOS] They solve search problem using dynamic programming and beam search with three pruning methods.\n[BOS] A comparison with Moses showed that the presented decoder is significantly faster at the same level of translation quality.\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Multi_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 39, "token_end": 89, "char_start": 260, "char_end": 515, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 90, "token_end": 120, "char_start": 522, "char_end": 669, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Berger et al., 1996": null}, "Reference": {}}}, {"token_start": 121, "token_end": 148, "char_start": 676, "char_end": 829, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 149, "token_end": 173, "char_start": 836, "char_end": 962, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 174, "token_end": 192, "char_start": 969, "char_end": 1037, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 193, "token_end": 205, "char_start": 1044, "char_end": 1104, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 206, "token_end": 251, "char_start": 1111, "char_end": 1273, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 252, "token_end": 313, "char_start": 1280, "char_end": 1572, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 314, "token_end": 370, "char_start": 1579, "char_end": 1902, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zens and Ney in 2004": "1435098"}, "Reference": {}}}]}
{"id": "6090412_2", "paragraph": "[BOS] (1) we can incorporate arbitrary features for word segmentation and parsing; (2) we demonstrate that a lattice-based approach commonly used for other languages can be effectively utilized for Chinese.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "6090412_1", "paragraph": "[BOS] In joint prediction models for Chinese, lattice structures are not typically used.\n[BOS] Commonly these models are formulated in a transition-based framework at the character level (Zhang and Clark, 2008; Zhang et al., 2014a; Wang and Xue, 2014) .\n[BOS] While this formulation can handle a large space of possible word segmentations, it can only capture features that are instantiated based on the stack and queue status.\n[BOS] Our approach offers two advantages over prior work:\n\n", "discourse_tags": ["Multi_summ", "Multi_summ", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 53, "char_start": 6, "char_end": 251, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhang and Clark, 2008;": "105219", "Zhang et al., 2014a;": "5859332", "Wang and Xue, 2014)": "16636082"}}}]}
{"id": "6090412_0", "paragraph": "[BOS] Joint Segmentation, POS tagging and Syntactic Parsing It has been widely recognized that joint prediction is an appealing alternative for pipeline architectures (Goldberg and Tsarfaty, 2008; Hatori et al., 2012; Habash and Rambow, 2005; GahbicheBraham et al., 2012; Zhang and Clark, 2008; Bohnet and Nivre, 2012) .\n[BOS] These approaches have been particularly prominent for languages with difficult preprocessing, such as morphologically rich languages (e.g., Arabic and Hebrew) and languages that require word segmentation (e.g., Chinese).\n[BOS] For the former, joint prediction models typically rely on a lattice structure to represent alternative morphological analyses (Goldberg and Tsarfaty, 2008; Tratz, 2013; Cohen and Smith, 2007) .\n[BOS] For instance, transitionbased models intertwine operations on the lattice with operations on a dependency tree.\n[BOS] Other joint architectures are more decoupled: in Goldberg and Tsarfaty (2008) , a lattice is used to derive the best morphological analysis for each part-of-speech alternative, which is in turn provided to the parsing algorithm.\n[BOS] In both cases, tractable inference is achieved by limiting the representation power of the scoring function.\n[BOS] Our model also uses a lattice to encode alternative analyses.\n[BOS] However, we employ this structure in a different way.\n[BOS] The model samples the full path from the lattice, which corresponds to a valid segmentation and POS tagging assignment.\n[BOS] Then the model improves the path and the corresponding tree via a hill-climbing strategy.\n[BOS] This architecture allows us to incorporate arbitrary features for segmentation, POS tagging and parsing.\n\n", "discourse_tags": ["Multi_summ", "Multi_summ", "Multi_summ", "Multi_summ", "Single_summ", "Transition", "Reflection", "Reflection", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 122, "char_start": 6, "char_end": 547, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Goldberg and Tsarfaty, 2008;": "2216180", "Hatori et al., 2012;": "10011032", "Zhang and Clark, 2008;": "105219", "Bohnet and Nivre, 2012)": "1500270"}}}, {"token_start": 123, "token_end": 184, "char_start": 554, "char_end": 865, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Goldberg and Tsarfaty, 2008;": "2216180", "Tratz, 2013;": "832839", "Cohen and Smith, 2007)": "905345"}}}, {"token_start": 192, "token_end": 232, "char_start": 918, "char_end": 1100, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Goldberg and Tsarfaty (2008)": "2216180"}, "Reference": {}}}]}
{"id": "8573773_1", "paragraph": "[BOS] While the mentioned syntax-based models use tree fragments for source and target (tree-to-tree), Galley et al. (2004) and Galley et al. (2006) use syntactic annotations only on the target language side (string-to-tree).\n[BOS] Further research by DeNeefe et al. (2007) revealed that adding non-minimal rules improves translation quality in this setting.\n[BOS] Here we improve statistical machine translation in this setting even further using non-minimal MBOT rules.\n\n", "discourse_tags": ["Multi_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 24, "token_end": 58, "char_start": 103, "char_end": 225, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Galley et al. (2004)": "1557806", "Galley et al. (2006)": "765547"}, "Reference": {}}}, {"token_start": 59, "token_end": 85, "char_start": 232, "char_end": 358, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"DeNeefe et al. (2007)": "2646100"}, "Reference": {}}}]}
{"id": "8573773_0", "paragraph": "[BOS] Modern statistical machine translation systems (Koehn, 2009 ) are based on different translation models.\n[BOS] Syntax-based systems have become widely used because of their ability to handle non-local reordering and other linguistic phenomena better than phrase-based models (Och and Ney, 2004) .\n[BOS] Synchronous tree substitution grammars (STSGs) of Eisner (2003) use a single source and target tree fragment per rule.\n[BOS] In contrast, an MBOT rule contains a single source tree fragment and a sequence of target tree fragments.\n[BOS] MBOTs can also be understood as a restriction of the non-contiguous STSSGs of Sun et al. (2009) , which allow a sequence of source tree fragments and a sequence of target tree fragments.\n[BOS] MBOT rules require exactly one source tree fragment.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Single_summ", "Transition", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 2, "token_end": 14, "char_start": 6, "char_end": 67, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Koehn, 2009": "32015000"}}}, {"token_start": 47, "token_end": 59, "char_start": 261, "char_end": 300, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Och and Ney, 2004)": "1272090"}}}, {"token_start": 61, "token_end": 86, "char_start": 309, "char_end": 427, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Eisner (2003)": "1542925"}, "Reference": {}}}, {"token_start": 120, "token_end": 134, "char_start": 599, "char_end": 641, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Sun et al. (2009)": "7759908"}}}]}
{"id": "13401571_4", "paragraph": "[BOS] In this paper we take a generative approach lying between PLDA and SITS.\n[BOS] In contrast to PLDA, which uses a flat topic model (i.e., LDA), we assume each text has a latent topic structure that can reflect the topic coherence pattern, and the model adapts its parameters to the segments to further improve performance.\n[BOS] Unlike SITS that targets analysing multiparty meeting transcripts, where speaker identities are available, we are interested in more general texts and assume each text has a specific topic change probability, since (1) the identity information is not always available for all kinds of texts (e.g., continuous broadcast news transcripts (Allan et al., 1998) ), (2) even for the same author, topic change probabilities for his/her different articles might be different.\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 130, "token_end": 143, "char_start": 632, "char_end": 690, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Allan et al., 1998)": "9063912"}}}]}
{"id": "13401571_3", "paragraph": "[BOS] Using a similar Markov structure, SITS (Nguyen et al., 2012) chains a set of HDP-LDAs .\n[BOS] Unlike PLDA, SITS assumes each text passage is associated with a speaker identity that is attached to the topic shift variable as supervising in-formation.\n[BOS] SITS further assumes speakers have different topic change probabilities that work as priors on topic shift variables.\n[BOS] Instead of assuming documents in a dataset share the same set of topics, Bayesseg (Eisenstein and Barzilay, 2008) treats words in a segment generated from a segment specific multinomial language model, i.e., it assumes each segment is generated from one topic, and a later hierarchical extension (Eisenstein, 2009) assumes each segment is generated from one topic or its parents.\n[BOS] Other methods using as input the output of topic models include (Sun et al., 2008) , (Misra et al., 2009) , and (Riedl and Biemann, 2012) .\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Multi_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 2, "token_end": 80, "char_start": 6, "char_end": 379, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 95, "token_end": 136, "char_start": 459, "char_end": 645, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Eisenstein and Barzilay, 2008)": "1967279"}, "Reference": {}}}, {"token_start": 138, "token_end": 160, "char_start": 651, "char_end": 765, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Eisenstein, 2009)": "6263842"}, "Reference": {}}}, {"token_start": 172, "token_end": 180, "char_start": 836, "char_end": 854, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sun et al., 2008)": "15853016"}}}, {"token_start": 181, "token_end": 190, "char_start": 857, "char_end": 877, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Misra et al., 2009)": "3063495"}}}, {"token_start": 192, "token_end": 201, "char_start": 884, "char_end": 909, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Riedl and Biemann, 2012)": "8279425"}}}]}
{"id": "13401571_2", "paragraph": "[BOS] The other branch of this work characterises the lexical cohesion using topic models, to which the model introduced in Section 3 belongs.\n[BOS] Lexical cohesion in this line of research is modelled by a probabilistic generative process.\n[BOS] PLDA presented by Purver et al. (2006) is an unsupervised topic modelling approach for segmentation.\n[BOS] It chains a set of LDAs (Blei et al., 2003) by assuming a Markov structure on topic distributions.\n[BOS] A binary topic shift variable is attached to each text passage (i.e., an utterance in (Purver et al., 2006) ).\n[BOS] It is sampled to indicate whether the j th text passage shares the topic distribution with the (j \u2212 1) th passage.\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 44, "token_end": 149, "char_start": 248, "char_end": 691, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Purver et al. (2006)": "3077921", "(Purver et al., 2006)": "3077921"}, "Reference": {"(Blei et al., 2003)": "3177797"}}}]}
{"id": "13401571_1", "paragraph": "[BOS] One branch of this work represents the lexical cohesion in a vector space by exploring the word cooccurrence patterns, e.g., TF or TF-IDF.\n[BOS] Work following this line includes TextTiling (Hearst, 1997) , which calculates the cosine similarity between two adjacent blocks of words purely based on the word frequency; C99 (Choi, 2000) , an algorithm based on divisive clustering with a matrix-ranking scheme; LSeg (Galley et al., 2003) , which uses a lexical chain to identify and weight word repetitions; U00 (Utiyama and Isahara, 2001 ), a probalistic approach using dynamic programming to find a segmentation with a minimum cost; MinCut (Malioutov and Barzilay, 2006) , which casts segmentation as a graph cut problem, and APS (Kazantseva and Szpakowicz, 2011) , which uses affinity propagation to learn clustering for segmentation.\n\n", "discourse_tags": ["Transition", "Multi_summ"], "span_citation_mapping": [{"token_start": 41, "token_end": 68, "char_start": 185, "char_end": 323, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Hearst, 1997)": "8574660"}, "Reference": {}}}, {"token_start": 69, "token_end": 90, "char_start": 325, "char_end": 414, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Choi, 2000)": "2958363"}, "Reference": {}}}, {"token_start": 91, "token_end": 114, "char_start": 416, "char_end": 511, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Galley et al., 2003)": "5509911"}, "Reference": {}}}, {"token_start": 115, "token_end": 145, "char_start": 513, "char_end": 638, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Utiyama and Isahara, 2001": "10014954"}, "Reference": {}}}, {"token_start": 146, "token_end": 170, "char_start": 640, "char_end": 727, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Malioutov and Barzilay, 2006)": "2384391"}, "Reference": {}}}, {"token_start": 172, "token_end": 197, "char_start": 733, "char_end": 842, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Kazantseva and Szpakowicz, 2011)": "9523299"}, "Reference": {}}}]}
{"id": "13401571_0", "paragraph": "[BOS] We are interested in unsupervised topic segmentation in either written or spoken language.\n[BOS] There is a large body of work on unsupervised topic segmentation of text based on lexical cohesion.\n[BOS] It can be characterised by how lexical cohesion is modelled.\n\n", "discourse_tags": ["Transition", "Transition", "Transition"], "span_citation_mapping": []}
{"id": "13970847_1", "paragraph": "[BOS] Our work, by constrast, never uses bilingual tree pairs not tree projections, and only uses word alignment alone to enhance a monolingual grammar, which learns to prefer target-side contiguity.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "13970847_0", "paragraph": "[BOS] Besides those cited in Section 1, there are some other related work on using bilingual constraints for grammar induction (rather than parsing).\n[BOS] For example, Hwa et al. (2005) use simple heuristics to project English trees to Spanish and Chinese, but get discouraging accuracy results learned from those projected trees.\n[BOS] Following this idea, Ganchev et al. (2009) and Smith and Eisner (2009) use constrained EM and parser adaptation techniques, respectively, to perform more principled projection, and both achieve encouraging results.\n\n", "discourse_tags": ["Transition", "Single_summ", "Multi_summ"], "span_citation_mapping": [{"token_start": 32, "token_end": 64, "char_start": 169, "char_end": 331, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hwa et al. (2005)": "157167"}, "Reference": {}}}, {"token_start": 69, "token_end": 109, "char_start": 359, "char_end": 552, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ganchev et al. (2009)": "11681086", "Smith and Eisner (2009)": null}, "Reference": {}}}]}
{"id": "6271137_0", "paragraph": "[BOS] The discussion of implications of the superficial processing effect will at times be limited to reading rather than listening.\n[BOS] Most of the following is of a speculative nature.\n\n", "discourse_tags": ["Other", "Other"], "span_citation_mapping": []}
{"id": "8339153_8", "paragraph": "[BOS] Feature selection models are also widely used in taxonomy learning.\n[BOS] For example, attribute selection for building lattices of concepts in [3] is done applying specific thresholds on specific information measures on the attributes extracted from corpora.\n[BOS] This models uses conditional probabilities, point-wise mutual information, and a selectional-preference-like measure as the one introduced in [25] .\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 17, "token_end": 74, "char_start": 93, "char_end": 418, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"[3]": "6168648"}, "Reference": {"[25]": "116972934"}}}]}
{"id": "8339153_7", "paragraph": "[BOS] There is a wide range of feature selection models that can be classified in two main families: supervised and unsupervised.\n[BOS] Supervised models directly exploit the class of the instances for determining if a feature is relevant or not.\n[BOS] The idea is to select features that are highly correlated with final target classes.\n[BOS] Information theoretic ranking criteria such as mutual information and information gain are often used (see [8] ).\n[BOS] Unsupervised models are instead used when the information on classes of instances is not available at the training time or it is inapplicable such as in information retrieval.\n[BOS] Straightforward and simple models for unsupervised feature selection can be derived from information retrieval weighting schemes, e.g., term frequency times inverse document frequency (tf * idf ).\n[BOS] In this case, relevant features are respectively those appearing more often or those more selective, i.e., appearing in fewer instances.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Narrative_cite", "Transition", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 77, "token_end": 80, "char_start": 451, "char_end": 454, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"[8]": "7827065"}}}]}
{"id": "8339153_6", "paragraph": "[BOS] Yet, in applications involving texts such as taxonomy learning, machine learning models are exposed to huge feature spaces.\n[BOS] This has not always positive effects.\n[BOS] A first important problem is that huge feature spaces require large computational and storage resources for applying machine learning models.\n[BOS] A second problem is that more features not always result in better accuracies of learnt classification models.\n[BOS] Many features can be noise.\n[BOS] Feature selection, i.e., the reduction of the feature space offered to machine learners, is seen as a solution (see [11] ).\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Transition", "Transition", "Single_summ"], "span_citation_mapping": [{"token_start": 81, "token_end": 112, "char_start": 479, "char_end": 602, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"[11]": "379259"}}}]}
{"id": "8339153_5", "paragraph": "[BOS] The probabilistic taxonomy learning models has at least two advantages with respect to the other models.\n[BOS] The first advantage is that it coherently uses existing taxonomies in the expansion phase.\n[BOS] Both existing and new information is modeled in the same probabilistic way.\n[BOS] The second advantage is that classification problem is binary, i.e., a word pair belongs or not to the taxonomy.\n[BOS] This allows to build a unique binary classifier.\n[BOS] This is not the case for models such as the one of [22] , where we need a multi-class classifier or a set of binary classifiers.\n[BOS] For these two reasons, we are using the probabilistic taxonomy learning setting for our study.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Transition", "Reflection", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 101, "token_end": 104, "char_start": 521, "char_end": 525, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"[22]": "6517211"}}}]}
{"id": "8339153_4", "paragraph": "[BOS] Despite the wide range of models for taxonomy learning, only very few exploit the structure of existing taxonomies.\n[BOS] The task is seen as building taxonomies from scratch.\n[BOS] In [3] , for example, lattices and the related taxonomies are the target.\n[BOS] Yet, existing taxonomies may be used to drive the process of building new taxonomies.\n[BOS] In [19] , WordNet [17] and WordNet glosses are used to drive the construction of domain specific ontologies.\n[BOS] In [22] , taxonomies are augmented exploiting their structure.\n[BOS] Inserting a new word in the network is seen as a classification problem.\n[BOS] The target classes are the nodes of the existing hierarchy.\n[BOS] The distributional description of the word as well as the existing taxonomy structure is used to make the decision.\n[BOS] This model is purely distributional.\n[BOS] In [27] , a probabilistic model exploiting existing taxonomies is introduced.\n[BOS] This model is purely based on lexicalsyntactical patterns.\n[BOS] Also in this case, the insertion of a new word in the hierarchy is seen as a binary classification problem.\n[BOS] Yet, the classification decision is taken over a pair of words, i.e., a word and its possible generalization.\n[BOS] The probabilistic classifier should decide if this pair belongs or not to the taxonomy.\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 38, "token_end": 57, "char_start": 188, "char_end": 261, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"[3]": "6168648"}, "Reference": {}}}, {"token_start": 79, "token_end": 105, "char_start": 360, "char_end": 468, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"[19]": "2453822"}, "Reference": {"[17]": "207846993"}}}, {"token_start": 106, "token_end": 175, "char_start": 475, "char_end": 847, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"[22]": "6517211"}, "Reference": {}}}, {"token_start": 176, "token_end": 270, "char_start": 854, "char_end": 1320, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"[27]": "14680675"}, "Reference": {}}}]}
{"id": "8339153_3", "paragraph": "[BOS] Lexical syntactic patterns are also a source of relevant information for deciding whether or not a particular relation holds between two words.\n[BOS] This approach has been widely used for detecting hypernymy relations such as in [13, 18] , for other ontological relations such as in [21] , or for more generic relations such as in [24, 28] .\n[BOS] These learning models generally use the hypothesis that two words are related according to a particular relation if these often appear in specific text fragments.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 33, "token_end": 46, "char_start": 195, "char_end": 244, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"[13,": "15763200", "18]": null}}}, {"token_start": 48, "token_end": 57, "char_start": 251, "char_end": 294, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"[21]": "7463996"}}}, {"token_start": 60, "token_end": 71, "char_start": 304, "char_end": 346, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"[24,": "226541", "28]": "15455102"}}}]}
{"id": "8339153_2", "paragraph": "[BOS] The distributional hypothesis is widely used in many approaches for taxonomy induction from texts.\n[BOS] For example, it is used in [3] for populating lattices, i.e. graphs of a particular class, of formal concepts.\n\n", "discourse_tags": ["Transition", "Single_summ"], "span_citation_mapping": [{"token_start": 21, "token_end": 47, "char_start": 124, "char_end": 221, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"[3]": "6168648"}, "Reference": {}}}]}
{"id": "8339153_1", "paragraph": "[BOS] The models for automatically extracting structured knowledge, such as taxonomies, from texts use variants of the distributional hypothesis [12] exploit some induced lexical-syntactic patterns (originally used in [26] ).\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 22, "token_end": 27, "char_start": 119, "char_end": 149, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"[12]": null}}}, {"token_start": 38, "token_end": 41, "char_start": 218, "char_end": 222, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"[26]": "28068242"}}}]}
{"id": "8339153_0", "paragraph": "[BOS] Extracting knowledge bases from texts is one of the major goal of NLP and KR.\n[BOS] These methods can give an important boost to knowledge-based systems.\n[BOS] In this section we want to shortly analyze some of these methods in order to motivate our choice to work within an existing probabilistic model for learning taxonomies.\n[BOS] We also review the more traditional models for super-vised and unsupervised feature selection.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Transition"], "span_citation_mapping": []}
{"id": "1292253_2", "paragraph": "[BOS] Independently of this work, Koo et al. (2007) and Smith and Smith (2007) showed that the MatrixTree Theorem can be used to train edge-factored log-linear models of dependency parsing.\n[BOS] Both studies constructed implementations that compare favorably with the state-of-the-art.\n[BOS] The work of Meil\u0203 and Jaakkola (2000) is also of note.\n[BOS] In that study they use the Matrix Tree Theorem to develop a tractable bayesian learning algorithms for tree belief networks, which in many ways are closely related to probabilistic dependency parsing formalisms and the problems we address here.\n\n", "discourse_tags": ["Multi_summ", "Multi_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 64, "char_start": 6, "char_end": 286, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Koo et al. (2007)": "11896512", "Smith and Smith (2007)": null}, "Reference": {}}}, {"token_start": 65, "token_end": 125, "char_start": 293, "char_end": 598, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "1292253_1", "paragraph": "[BOS] For grammar based models there has been limited work on empirical systems for non-projective parsing systems, notable exceptions include the work of Wang and Harper (2004) .\n[BOS] Theoretical studies of note include the work of Neuhaus and B\u00f6ker (1997) showing that the recognition problem for a mini-mal dependency grammar is hard.\n[BOS] In addition, the work of Kahane et al. (1998) provides a polynomial parsing algorithm for a constrained class of nonprojective structures.\n[BOS] Non-projective dependency parsing can be related to certain parsing problems defined for phrase structure representations, as for instance immediate dominance CFG parsing (Barton et al., 1987) and shake-and-bake translation (Brew, 1992) .\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 24, "token_end": 34, "char_start": 143, "char_end": 177, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Wang and Harper (2004)": "1669853"}}}, {"token_start": 36, "token_end": 67, "char_start": 186, "char_end": 338, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Neuhaus and B\u00f6ker (1997)": null}, "Reference": {}}}, {"token_start": 68, "token_end": 98, "char_start": 345, "char_end": 483, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kahane et al. (1998)": "499440"}, "Reference": {}}}, {"token_start": 120, "token_end": 134, "char_start": 629, "char_end": 682, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Barton et al., 1987)": "60831967"}}}, {"token_start": 135, "token_end": 149, "char_start": 687, "char_end": 726, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Brew, 1992)": "1537555"}}}]}
{"id": "1292253_0", "paragraph": "[BOS] There has been extensive work on data-driven dependency parsing for both projective parsing (Eisner, 1996; Paskin, 2001; Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; McDonald et al., 2005a) and non-projective parsing systems (Nivre and Nilsson, 2005; Hall and N\u00f3v\u00e1k, 2005; McDonald et al., 2005b) .\n[BOS] These approaches can often be classified into two broad categories.\n[BOS] In the first category are those methods that employ approximate inference, typically through the use of linear time shift-reduce parsing algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Nivre and Nilsson, 2005) .\n[BOS] In the second category are those that employ exhaustive inference algorithms, usually by making strong independence assumptions, as is the case for edge-factored models (Paskin, 2001; McDonald et al., 2005a; McDonald et al., 2005b) .\n[BOS] Recently there have also been proposals for exhaustive methods that weaken the edge-factored assumption, including both approximate methods (McDonald and Pereira, 2006) and exact methods through integer linear programming (Riedel and Clarke, 2006) or branch-and-bound algorithms (Hirakawa, 2006) .\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Narrative_cite", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 8, "token_end": 53, "char_start": 39, "char_end": 202, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Eisner, 1996;": "3262717", "Paskin, 2001;": "11950447", "Yamada and Matsumoto, 2003;": "13163488", "Nivre and Scholz, 2004;": "643522", "McDonald et al., 2005a)": "12926517"}}}, {"token_start": 54, "token_end": 83, "char_start": 207, "char_end": 309, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Nivre and Nilsson, 2005;": "17842042", "Hall and N\u00f3v\u00e1k, 2005;": "7966094", "McDonald et al., 2005b)": "6681594"}}}, {"token_start": 114, "token_end": 147, "char_start": 496, "char_end": 617, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yamada and Matsumoto, 2003;": "13163488", "Nivre and Scholz, 2004;": "643522", "Nivre and Nilsson, 2005)": "17842042"}}}, {"token_start": 173, "token_end": 200, "char_start": 774, "char_end": 857, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Paskin, 2001;": "11950447", "McDonald et al., 2005a;": "12926517", "McDonald et al., 2005b)": "6681594"}}}, {"token_start": 223, "token_end": 234, "char_start": 986, "char_end": 1034, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(McDonald and Pereira, 2006)": "802998"}}}, {"token_start": 235, "token_end": 249, "char_start": 1039, "char_end": 1113, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Riedel and Clarke, 2006)": "6270377"}}}, {"token_start": 250, "token_end": 263, "char_start": 1117, "char_end": 1161, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hirakawa, 2006)": "941952"}}}]}
{"id": "15690112_0", "paragraph": "[BOS] This section presents some of the effort pertaining to identifying NE pages in Wikipedia and some background on SVM threshold adjustment.\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "9446888_3", "paragraph": "[BOS] Learning with ambiguous labelings are previously explored for classification (Jin and Ghahramani, 2002) , sequence labeling (Dredze et al., 2009) , parsing (Riezler et al., 2002; T\u00e4ckstr\u00f6m et al., 2013; Li et al., 2014a; Li et al., 2014b) .\n[BOS] Recently, researchers derive natural annotations from web data, transform them into ambiguous labelings to supervise Chinese word segmentation models (Jiang et al., 2013; Yang and Vozila, 2014) .\n\n", "discourse_tags": ["Narrative_cite", "Multi_summ"], "span_citation_mapping": [{"token_start": 11, "token_end": 22, "char_start": 68, "char_end": 109, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Jin and Ghahramani, 2002)": "917949"}}}, {"token_start": 23, "token_end": 35, "char_start": 112, "char_end": 151, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Dredze et al., 2009)": null}}}, {"token_start": 36, "token_end": 71, "char_start": 154, "char_end": 244, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Riezler et al., 2002;": "6052790", "T\u00e4ckstr\u00f6m et al., 2013;": "2037646", "Li et al., 2014a;": "5517166", "Li et al., 2014b)": "7527306"}}}, {"token_start": 73, "token_end": 112, "char_start": 253, "char_end": 446, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Jiang et al., 2013;": "14014145", "Yang and Vozila, 2014)": "14283390"}}}]}
{"id": "9446888_2", "paragraph": "[BOS] As one reviewer kindly pointed out that our model is a factorial CRF (Sutton et al., 2004) , in the sense that the bundled tags can be factorized two connected latent variables.\n[BOS] Initially, factorial CRFs are designed to jointly model two related (and typically hierarchical) sequential labeling tasks, such as POS tagging and chunking.\n[BOS] In this work, our coupled CRF jointly models two same tasks which have different annotation schemes.\n[BOS] Moreover, this work provides a natural way to learn from incomplete annotations where one sentence only contains one-side labels.\n[BOS] The reviewer also suggests that our objective can be optimized with the latent variable structured perceptron of Sun et al. (2009) , which we leave as future work.\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 13, "token_end": 24, "char_start": 61, "char_end": 96, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sutton et al., 2004)": "6038991"}}}, {"token_start": 128, "token_end": 141, "char_start": 669, "char_end": 727, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Sun et al. (2009)": "125278"}}}]}
{"id": "9446888_1", "paragraph": "[BOS] This work is also closely related with multitask learning, which aims to jointly learn multiple related tasks with the benefit of using interactive features under a share representation (BenDavid and Schuller, 2003; Ando and Zhang, 2005; Parameswaran and Weinberger, 2010) .\n[BOS] However, according to our knowledge, multi-task learning typically assumes the existence of data with labels for multiple tasks at the same time, which is unavailable in our situation.\n\n", "discourse_tags": ["Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 9, "token_end": 59, "char_start": 45, "char_end": 278, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Ando and Zhang, 2005;": null, "Parameswaran and Weinberger, 2010)": "11319685"}}}]}
{"id": "9446888_0", "paragraph": "[BOS] This work is partially inspired by Qiu et al. (2013) , who propose a model that performs heterogeneous Chinese word segmentation and POS tagging and produces two sets of results following CTB and PD styles respectively.\n[BOS] Different from our CRFbased coupled model, their approach adopts a linear model, which directly combines two separate sets of features based on single-side tags, without considering the interacting joint features between the two annotations.\n[BOS] They adopt an approximate decoding algorithm which tries to find the best single-side tag sequence with reference to tags at the other side.\n[BOS] In contrast, our approach is a direct extension of traditional CRF, and is more theoretically simple from the perspective of modelling.\n[BOS] The use of both joint and separate features is proven to be crucial for the success of our coupled model.\n[BOS] In addition, their work indicates that their model relies on a hand-crafted loose mapping between annotations, which is opposite to our findings.\n[BOS] The naming of the \"\"coupled\"\" CRF is borrowed from the work of Qiu et al. (2012) , which treats the joint task of Chinese word segmentation and POS tagging as two coupled sequence labeling problems.\n[BOS] propose a shift-reduce dependency parsing model which can simultaneously learn and produce two heterogeneous parse trees.\n[BOS] However, their approach assumes the existence of data with annotations at both sides, which is obtained by converting phrase-structure trees into dependency trees with different heuristic rules.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Reflection", "Reflection", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 8, "token_end": 113, "char_start": 41, "char_end": 620, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Qiu et al. (2013)": "13379175"}, "Reference": {}}}, {"token_start": 161, "token_end": 236, "char_start": 881, "char_end": 1231, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Qiu et al. (2012)": "14613967"}, "Reference": {}}}, {"token_start": 237, "token_end": 289, "char_start": 1238, "char_end": 1560, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "6737447_9", "paragraph": "[BOS] The Sketch Engine [18] system provides a good comparison point to position TerminoWeb.\n[BOS] Overall, TerminoWeb's corpus analysis capabilities are simpler than the ones in Sketch Engine.\n[BOS] The purpose is quite different, as TerminoWeb's main objective is to provide an integrated platform for understanding terms related to a domain or a source text.\n[BOS] For doing so, the emphasis is on easy real-time construction and simple analysis of disposable corpora.\n[BOS] No textpreprocessing is necessary, but then, no part-of-speech analysis is available either.\n[BOS] We want the user to be able to quickly search for specialized information on the Web to understand important concepts via an integrated system for term extraction and term collocation and concordances finding.\n[BOS] This is different from studying language patterns and understanding the uses of words or phrases as can be done in a better way in Sketch Engine [18] .\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Transition", "Transition", "Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 2, "token_end": 9, "char_start": 6, "char_end": 35, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 181, "token_end": 186, "char_start": 924, "char_end": 942, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "6737447_8", "paragraph": "[BOS] For corpus analysis per se, TerminoWeb combines different modules performing term extraction, collocation extraction and concordance findings.\n[BOS] A large pool of research exists in computational terminology around the problem of term extraction.\n[BOS] Although a simple frequency based approach is implemented in TerminoWeb, there are more sophisticated algorithms being developed in the community (see [8] for a review of earlier systems and [9] for a new trend of term extraction based on comparing corpora).\n[BOS] For collocations, we refer the reader to Smadja [25] for the algorithm we implemented, and to [10] for a review of different measures.\n[BOS] Finding concordances does not require any statistical corpus linguistic knowledge, and is simply a window of text capture.\n\n", "discourse_tags": ["Transition", "Transition", "Narrative_cite", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 68, "token_end": 71, "char_start": 412, "char_end": 415, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"[8]": "14341787"}}}, {"token_start": 78, "token_end": 81, "char_start": 452, "char_end": 455, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"[9]": "50303027"}}}, {"token_start": 104, "token_end": 109, "char_start": 567, "char_end": 578, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"[25]": null}}}, {"token_start": 117, "token_end": 120, "char_start": 620, "char_end": 624, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"[10]": "2233906"}}}]}
{"id": "6737447_7", "paragraph": "[BOS] Although, systems such as WebCorp [24] and KWiCFinder [13] do not build sub-corpora, they use the Web as a large corpus to find collocations and concordances, providing user with easy-to-use realtime systems.\n\n", "discourse_tags": ["Multi_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 55, "char_start": 6, "char_end": 214, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"[24]": "106763902", "[13]": "60699329"}}}]}
{"id": "6737447_6", "paragraph": "[BOS] Our query generation module was inspired by the work of Baroni [3, 4] who suggested query combinations of common words to build a corpus of general knowledge or specialized language.\n[BOS] Earlier work by Ghani et al. [11] presented a similar idea for minority languages.\n[BOS] TerminoWeb includes a unique re-ranking of documents based on an \"\"informative score\"\" as defined in [1] .\n[BOS] It then builds informative sub-corpora from the Web.\n\n", "discourse_tags": ["Multi_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 9, "token_end": 37, "char_start": 50, "char_end": 188, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"[3,": null, "4]": "18077151"}, "Reference": {}}}, {"token_start": 38, "token_end": 57, "char_start": 195, "char_end": 277, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"[11]": "1315480"}, "Reference": {}}}, {"token_start": 58, "token_end": 97, "char_start": 284, "char_end": 449, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"[1]": null}, "Reference": {}}}]}
{"id": "6737447_5", "paragraph": "[BOS] Nevertheless our query generation module faces the same problems as those of query expansion in information retrieval [12, 27] .\n[BOS] Query expansion is a delicate task, as using general terms which tend to be polysemous can lead to off-topic documents, and using very specific terms will not help as they will not return any documents.\n[BOS] Our approach was to allow the inclusion of domain-words for restriction and then do a random selection of terms for expansion.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 14, "token_end": 24, "char_start": 83, "char_end": 132, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"[12,": null, "27]": "15558447"}}}]}
{"id": "6737447_4", "paragraph": "[BOS] Search for Web documents is usually associated to the field of information retrieval.\n[BOS] A large body of research exists within that area and we borrow from it.\n[BOS] Searching for a particular document to answer a particular question (an information retrieval task) is different than searching for domain-specific documents to \"\"augment\"\" a user's knowledge.\n[BOS] The former has a specific goal, finding an answer to a question, and the latter has a discovery purpose.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Transition"], "span_citation_mapping": []}
{"id": "6737447_3", "paragraph": "[BOS] The hit count specificity/generality approximations relate to the former view.\n[BOS] The corpus building modules gathering results from the query generation module relates to the latter view.\n\n", "discourse_tags": ["Transition", "Transition"], "span_citation_mapping": []}
{"id": "6737447_2", "paragraph": "[BOS] 7 [17] and exploiting the Web for language learners and translators.\n[BOS] In the book Corpus Linguistics and the Web [16] , a distinction is made between \"\"accessing the web as corpus\"\" and \"\"compiling corpora from the internet\"\".\n[BOS] Our system relates to both views.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 21, "token_end": 29, "char_start": 93, "char_end": 128, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"[16]": "58165312"}}}]}
{"id": "6737447_1", "paragraph": "[BOS] Our research relies mainly on the principle of \"\"Web as corpus\"\"\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "6737447_0", "paragraph": "[BOS] Our research covers a wide range of topics, uses diverse natural language processing strategies, and includes the development of multiple algorithms for all steps, from term extraction to query generation to collocation search.\n[BOS] As our purpose in this article is to present a proof of concept of an integrated system, we do not present any quantitative comparisons with other algorithms or systems, but rather highlight some research related to corpus building and analysis.\n\n", "discourse_tags": ["Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "10140027_3", "paragraph": "[BOS] In addition, neural machine translation (NMT) has shown promising results recently (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b) .\n[BOS] NMT uses a recurrent neural network to encode the whole source sentence and then produce the target words one by one.\n[BOS] These models can be trained on parallel corpora and do not need word alignments to be learned in advance.\n[BOS] There are also neural translation models that are trained on word-aligned parallel corpus (Devlin et al., 2014; Meng et al., 2015; Zhang et al., 2015; Setiawan et al., 2015) , which use the alignment information to decide which parts of the source sentence are more important for predicting one particular target word.\n[BOS] All these models are trained on plain source and target sentences without considering any syntactic information while our neural model learns rule selection for tree-based translation rules and makes use of the tree structure of natural language for better translation.\n[BOS] There is also a new syntactic NMT model (Eriguchi et al., 2016) , which extends the original sequence-to-sequence NMT model with the source-side phrase structure.\n[BOS] Although this model takes source-side syntax into consideration, it still produces target words one by one as a sequence.\n[BOS] In contrast, the tree-based translation rules used in our model can take advantage of the hierarchical structures of both source and target languages.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Narrative_cite", "Transition", "Narrative_cite", "Transition", "Transition"], "span_citation_mapping": [{"token_start": 5, "token_end": 61, "char_start": 19, "char_end": 197, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sutskever et al., 2014;": "7961699", "Bahdanau et al., 2014;": "11212020", "Luong et al., 2015a;": "1998416", "Jean et al., 2015;": "2863491", "Luong et al., 2015b)": "1245593"}}}, {"token_start": 118, "token_end": 155, "char_start": 503, "char_end": 615, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Devlin et al., 2014;": "7417943", "Meng et al., 2015;": "447679", "Setiawan et al., 2015)": "52094"}}}, {"token_start": 229, "token_end": 243, "char_start": 1063, "char_end": 1106, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Eriguchi et al., 2016)": "12851711"}}}]}
{"id": "10140027_2", "paragraph": "[BOS] There are also works that used minimal rules for modeling.\n[BOS] Vaswani et al. (2011) proposed a rule Markov model using minimal rules for both training and decoding to achieve a slimmer model, a faster decoder and comparable performance with using non-minimal rules.\n[BOS] Durrani et al. (2013) proposed a method to model with minimal translation units and decode with phrases for phrasebased SMT to improve translation performances.\n[BOS] Both of these two methods do not use distributed representations as used in our model for better generalization.\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 14, "token_end": 56, "char_start": 71, "char_end": 274, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Vaswani et al. (2011)": null}, "Reference": {}}}, {"token_start": 57, "token_end": 89, "char_start": 281, "char_end": 441, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Durrani et al. (2013)": "8555345"}, "Reference": {}}}]}
{"id": "10140027_1", "paragraph": "[BOS] There are also existing works that exploited neural networks to learn translation probabilities for translation rules used in the phrase-based translation model.\n[BOS] Namely, these methods estimated translation probabilities for phrase pairs extracted from the parallel corpus.\n[BOS] Schwenk (2012) proposed a continuous space translation model, which calculated the translation probability for each word in the target phrase and then multiplied the probabilities together as the translation probability of the phrase pair.\n[BOS] Gao et al. (2014) and Zhang et al. (2014) proposed methods to learn continuous space phrase representations and use the similarity between the source and target phrases as translation probabilities for phrase pairs.\n[BOS] All these three methods can only be used for the phrase-based translation model, not for syntaxbased translation models.\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ", "Multi_summ", "Transition"], "span_citation_mapping": [{"token_start": 45, "token_end": 84, "char_start": 291, "char_end": 530, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Schwenk (2012)": "8608051"}, "Reference": {}}}, {"token_start": 85, "token_end": 125, "char_start": 537, "char_end": 752, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Gao et al. (2014)": "10473972", "Zhang et al. (2014)": "18380505"}, "Reference": {}}}]}
{"id": "10140027_0", "paragraph": "[BOS] The rule selection problem for syntax-based SMT has received much attention.\n[BOS] proposed a lexicalized rule selection model to perform context-sensitive rule selection for hierarchical phrase-base translation.\n[BOS] Cui et al. (2010) introduced a joint rule selection model for hierarchical phrase-based translation, which also approximated the rule selection problem by a binary classification problem like our approach.\n[BOS] However, these two models adopted linear classifiers similar to those used in the MERS model , which suffers more from the data sparsity problem compared to the CSRS model.\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 40, "token_end": 76, "char_start": 225, "char_end": 430, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Cui et al. (2010)": "8996207"}, "Reference": {}}}]}
{"id": "10480989_1", "paragraph": "[BOS] Recently variational auto-encoders have been applied in a variety of fields as deep generative models.\n[BOS] In computer vision Kingma and Welling (2014), Rezende et al. (2014) , and Gregor et al. (2015) have demonstrated strong performance on the task of image generation and Eslami et al. (2016) proposed variable-sized variational auto-encoders to identify multiple objects in images.\n[BOS] While in natural language processing, there are variants of VAEs on modelling documents (Miao et al., 2016) , sentences (Bowman et al., 2015) and discovery of relations (Marcheggiani and Titov, 2016) .\n[BOS] Apart from the typical initiations of VAEs, there are also a series of works that employs generative models for supervised learning tasks.\n[BOS] For instance, Ba et al. (2015) learns visual attention for multiple objects by optimising a variational lower bound, Kingma et al. (2014) implements a semi-supervised framework for image classification and Miao et al. (2016) applies a conditional variational approximation in the task of factoid question answering.\n[BOS] Dyer et al. (2016) proposes a generative model that explicitly extracts syntactic relationships among words and phrases which further supports the argument that generative models can be a statistically efficient method for learning neural networks from small data.\n\n", "discourse_tags": ["Transition", "Multi_summ", "Multi_summ", "Transition", "Multi_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 22, "token_end": 63, "char_start": 115, "char_end": 278, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Rezende et al. (2014)": null, "Gregor et al. (2015)": "1930231"}, "Reference": {}}}, {"token_start": 64, "token_end": 89, "char_start": 283, "char_end": 393, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 104, "token_end": 114, "char_start": 478, "char_end": 507, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Miao et al., 2016)": "10796"}}}, {"token_start": 115, "token_end": 125, "char_start": 510, "char_end": 541, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bowman et al., 2015)": "61556494"}}}, {"token_start": 128, "token_end": 140, "char_start": 559, "char_end": 599, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 172, "token_end": 192, "char_start": 767, "char_end": 868, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 193, "token_end": 210, "char_start": 870, "char_end": 954, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 211, "token_end": 233, "char_start": 959, "char_end": 1068, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Miao et al. (2016)": "10796"}, "Reference": {}}}, {"token_start": 234, "token_end": 277, "char_start": 1075, "char_end": 1339, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Dyer et al. (2016)": "1992250"}, "Reference": {}}}]}
{"id": "10480989_0", "paragraph": "[BOS] As one of the typical sequence-to-sequence tasks, sentence-level summarisation has been explored by a series of discriminative encoder-decoder neural models.\n[BOS] Filippova et al. (2015) Gu et al. (2016) also apply the similar idea of combining pointer networks and softmax output.\n[BOS] However, different from all these discriminative models above, we explore generative models for sentence compression.\n[BOS] Instead of training the discriminative model on a big labelled dataset, our original intuition of introducing a combined pointer networks is to bridge the unsupervised generative model (ASC) and supervised model (FSC) so that we could utilise a large additional dataset, either labelled or unlabelled, to boost the compression performance.\n[BOS] Dai and Le (2015) also explored semi-supervised sequence learning, but in a pure deterministic model focused on learning better vector representations.\n\n", "discourse_tags": ["Transition", "Single_summ", "Reflection", "Reflection", "Single_summ"], "span_citation_mapping": [{"token_start": 34, "token_end": 64, "char_start": 170, "char_end": 288, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Gu et al. (2016)": "8174613"}, "Reference": {}}}, {"token_start": 146, "token_end": 173, "char_start": 765, "char_end": 916, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "10997670_0", "paragraph": "[BOS] The English language belongs to the Germanic languages branch of the Indo-European language family, widely spoken on six continents.\n[BOS] The HOO shared task is organized to help authors with writing tasks.\n[BOS] Identifying grammatical and linguistic errors in text is an open challenge to researchers.\n[BOS] In recent times, researchers (Heidorn, 2000) have provided quite a benchmark for spell checker and grammar checkers, which is commonly available.\n[BOS] In this task it is aimed to correct errors beyond the scope of these commonly available checkers i.e. detection and correction of jarring errors at part-ofspeech (POS) level, syntax level and semantic level.\n[BOS] Earlier Heidorn (1975) developed augmented phrase structure grammar.\n[BOS] (Tetreault et. al., 2008) has dealt with error pattern with preposition by non-native speakers.\n[BOS] Meurers and Wunsch (2010) showed a surface based state-of-the-art machine learning technique, which deals with some frequently used prepositions.\n[BOS] (Elghafari et al., 2010) worked on Data-Driven Prediction of Prepositions in English.\n[BOS] Boyd et al. (2011) used an n-gram based machine-learning approach.\n[BOS] Last year we have also participated in this shared task; our system report was reported in (Bhaskar et. al., 2011 ).\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 63, "token_end": 136, "char_start": 334, "char_end": 676, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Heidorn, 2000)": null}, "Reference": {}}}, {"token_start": 137, "token_end": 150, "char_start": 683, "char_end": 751, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Heidorn (1975)": null}, "Reference": {}}}, {"token_start": 151, "token_end": 176, "char_start": 758, "char_end": 853, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Tetreault et. al., 2008)": "17110547"}, "Reference": {}}}, {"token_start": 177, "token_end": 212, "char_start": 860, "char_end": 1005, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Meurers and Wunsch (2010)": "10591477"}, "Reference": {}}}, {"token_start": 213, "token_end": 237, "char_start": 1012, "char_end": 1097, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Elghafari et al., 2010)": "10591477"}, "Reference": {}}}, {"token_start": 238, "token_end": 257, "char_start": 1104, "char_end": 1170, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Boyd et al. (2011)": "5953389"}, "Reference": {}}}, {"token_start": 275, "token_end": 285, "char_start": 1268, "char_end": 1290, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bhaskar et. al., 2011": null}}}]}
{"id": "11023319_1", "paragraph": "[BOS] There are many differences between these ngram based methods and our approach.\n[BOS] In ngram approach, a sentence is viewed as a collection of n-grams with different length without differentiating the specific linguistic phenomena.\n[BOS] In our approach, a sentence is viewed as a collection of check-points with different types and depth, conforming to a clear linguistic taxonomy.\n[BOS] Furthermore, in n-gram approach, only one general score at the system level is provided which make it not suitable for system diagnoses, while in our approach we can give scores of linguistic categories and provide much richer information to help developers to find the concrete strength and flaws of the system, in addition to the general score.\n[BOS] The n-gram based metric is not very effective when comparing the systems with different architectures or systems with similar general score, while our approach is more effective in both cases by digging into the multiple linguistic levels and disclosing the latent differences of the systems.\n\n", "discourse_tags": ["Reflection", "Transition", "Reflection", "Transition", "Reflection"], "span_citation_mapping": []}
{"id": "11023319_0", "paragraph": "[BOS] This work is inspired by (Yu, 1993) with many extensions.\n[BOS] (Yu, 1993) proposed MTE evaluation system based on check-points for EnglishChinese machine translation systems with human craft linguistic taxonomy including 3,200 pairs of sentences containing 6 classes of check-points.\n[BOS] Their check-points were manually constructed by human experts, therefore it will be costly to build new test corpus while the check-points in our approach are constructed automatically.\n[BOS] Another limitation of their work is that only binary score is used for credits while we use n-gram matching rate which provides a broader coverage of different levels of matching.\n[BOS] There are many recent work motivated by ngram based approach.\n[BOS] (Callison-Burch et al., 2006) criticized the inadequate accuracy of evaluation at the sentence level.\n[BOS] (Lin and Och, 2004) used longest common subsequence and skipbigram statistics.\n[BOS] (Banerjee and Lavie, 2005) calculated the scores by matching the unigrams on the surface forms, stemmed forms and senses.\n[BOS] (Liu et al., 2005) used syntactic features and unlabeled head-modifier dependencies to evaluate MT quality, outperforming BLEU on sentence level correlations with human judgment.\n[BOS] (Gimenez and Marquez, 2007) showed that linguistic features at more abstract levels such as dependency relation may provide more reliable system rankings.\n[BOS] (Yang et al., 2007) formulates MT evaluation as a ranking problems leading to greater correlation with human assessment at the sentence level.\n\n", "discourse_tags": ["Reflection", "Single_summ", "Single_summ", "Single_summ", "Transition", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 17, "token_end": 129, "char_start": 70, "char_end": 668, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Yu, 1993)": "17911526"}, "Reference": {}}}, {"token_start": 143, "token_end": 167, "char_start": 743, "char_end": 844, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Callison-Burch et al., 2006)": "7647892"}, "Reference": {}}}, {"token_start": 168, "token_end": 185, "char_start": 851, "char_end": 929, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Lin and Och, 2004)": "1586456"}, "Reference": {}}}, {"token_start": 186, "token_end": 215, "char_start": 936, "char_end": 1057, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Banerjee and Lavie, 2005)": "7164502"}, "Reference": {}}}, {"token_start": 216, "token_end": 252, "char_start": 1064, "char_end": 1242, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Liu et al., 2005)": null}, "Reference": {}}}, {"token_start": 253, "token_end": 282, "char_start": 1249, "char_end": 1403, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Gimenez and Marquez, 2007)": null}, "Reference": {}}}, {"token_start": 283, "token_end": 311, "char_start": 1410, "char_end": 1552, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Yang et al., 2007)": null}, "Reference": {}}}]}
{"id": "11633224_2", "paragraph": "[BOS] In recent years many works have been devoted to the word segmentation task.\n[BOS] For example, the introduction of global training or complicated features (Zhang and Clark, 2007; Zhang and Clark, 2010) ; the investigation of word structures (Li, 2011) ; the strategies of hybrid, joint or stacked modeling (Nakagawa and Uchimoto, 2007; Kruengkrai et al., 2009; Wang et al., 2010; Sun, 2011) , and the semisupervised and unsupervised technologies utilizing raw text (Zhao and Kit, 2008; Johnson and Goldwater, 2009; Mochihashi et al., 2009; Hewlett and Cohen, 2011) .\n[BOS] We estimate that the annotation transformation technologies can be adopted jointly with complicated features, system combination and semisupervised/unsupervised technologies to further improve segmentation performance.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 23, "token_end": 41, "char_start": 121, "char_end": 207, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhang and Clark, 2007;": "2687347", "Zhang and Clark, 2010)": "2712419"}}}, {"token_start": 43, "token_end": 52, "char_start": 214, "char_end": 257, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Li, 2011)": "15126078"}}}, {"token_start": 54, "token_end": 90, "char_start": 264, "char_end": 389, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Nakagawa and Uchimoto, 2007;": "11062835", "Kruengkrai et al., 2009;": "769547", "Wang et al., 2010;": "3204349"}}}, {"token_start": 96, "token_end": 136, "char_start": 407, "char_end": 570, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhao and Kit, 2008;": "10102597", "Johnson and Goldwater, 2009;": "7573383", "Mochihashi et al., 2009;": "10623282", "Hewlett and Cohen, 2011)": "548241"}}}]}
{"id": "11633224_1", "paragraph": "[BOS] The iterative training procedure proposed in this work shares some similarity with the co-training algorithm in parsing (Sarkar, 2001) , where the training procedure lets two different models learn from each other during parsing the raw text.\n[BOS] The key idea of co-training is utilize the complementarity of different parsing models to mine additional training data from raw text, while iterative training for annotation transformation emphasizes the iterative optimization of the parellelly annotated corpora used to train the transformation models.\n[BOS] The predictself methodology is implicit in many unsupervised learning approaches, it has been successfully used by (Daum III, 2009 ) in unsupervised dependency parsing.\n[BOS] We adapt this idea to the scenario of annotation transformation to improve transformation accuracy.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 15, "token_end": 27, "char_start": 93, "char_end": 140, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Sarkar, 2001)": "2682462"}}}, {"token_start": 98, "token_end": 128, "char_start": 566, "char_end": 734, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Daum\u00e9 III, 2009": "13366727"}, "Reference": {}}}]}
{"id": "11633224_0", "paragraph": "[BOS] Researches focused on the automatic adaptation between different corpora can be roughly classified into two kinds, adaptation between different domains (with different statistical distribution) (Blitzer et al., 2006; Daum III, 2007) , and adaptation between different annotation guidelines (Jiang et al., 2009; Zhu et al., 2011) .\n[BOS] There are also some efforts that totally or partially resort to manual transformation rules, to conduct treebank conversion (Cahill and Mccarthy, 2002; Hockenmaier and Steedman, 2007; Clark and Curran, 2009) , and word segmentation guideline transformation (Gao et al., 2004; Mi et al., 2008) .\n[BOS] This work focuses on the automatic transformation between annotation guidelines, and proposes better annotation transformation technologies to improve the transformation accuracy and the utilization rate of human-annotated knowledge.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 22, "token_end": 44, "char_start": 150, "char_end": 238, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Blitzer et al., 2006;": "15978939", "Daum\u00e9 III, 2007)": "5360764"}}}, {"token_start": 50, "token_end": 66, "char_start": 285, "char_end": 334, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Jiang et al., 2009;": "15016194", "Zhu et al., 2011)": "7416323"}}}, {"token_start": 85, "token_end": 116, "char_start": 447, "char_end": 550, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cahill and Mccarthy, 2002;": "2432889", "Hockenmaier and Steedman, 2007;": "1331239", "Clark and Curran, 2009)": "5637441"}}}, {"token_start": 118, "token_end": 137, "char_start": 557, "char_end": 635, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Gao et al., 2004;": "15121618", "Mi et al., 2008)": null}}}]}
{"id": "1091879_1", "paragraph": "[BOS] Instead of modifying the parse tree to improve machine translation performance, many methods were proposed to modify word alignment by taking syntactic tree into consideration, including deleting incorrect word alignment links by a discriminative model (Fossum et al., 2008) , re-aligning sentence pairs using EM method with the rules extracted with initial alignment (Wang et al., 2010) , and removing ambiguous alignment of functional words with constraint from chunk-level information during rule extraction (Wu et al., 2011) .\n[BOS] Unlike all these pursuits, to generate a consistent word alignment, our method modifies the popularly used IDG symmetrization method to make it suitable for string-to-tree rule extraction, and our method is much simpler and faster than the previous works.\n\n", "discourse_tags": ["Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 37, "token_end": 48, "char_start": 238, "char_end": 280, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Fossum et al., 2008)": "2485577"}}}, {"token_start": 64, "token_end": 73, "char_start": 364, "char_end": 393, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wang et al., 2010)": null}}}, {"token_start": 89, "token_end": 99, "char_start": 501, "char_end": 534, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wu et al., 2011)": "1937469"}}}]}
{"id": "1091879_0", "paragraph": "[BOS] There are a lot of attempts in improving word alignment with syntactic information (Cherry and Lin, 2006; DeNero and Klein, 2007; Hermjackob, 2009) and in improving parser with alignment information (Burkett and Klein, 2008 parser/aligner itself rather than the translation model.\n[BOS] To improve the performance of syntactic machine translation, Huang and Knight (2006) proposed a method incorporating a handful of relabeling strategies to modify the syntactic trees structures.\n[BOS] Ambati and Lavie (2008) restructured target parse trees to generate highly isomorphic target trees that preserve the syntactic boundaries of constituents aligned in the original parse trees.\n[BOS] Wang et al., (2010) proposed to use re-structuring and re-labeling to modify the parser tree.\n[BOS] The restructuring method uses a binarization method to enable the reuse of sub-constituent structures, and the linguistic and statistical re-labeling methods to handle the coarse nonterminal problem, so as to enhance generalization ability.\n[BOS] Different from the previous work of modifying tree structures with post-processing methods, our methods try to learn a suitable grammar for string-to-tree SMT models, and directly produce trees which are consistent with word alignment matrices.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ", "Single_summ", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 13, "token_end": 36, "char_start": 67, "char_end": 153, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cherry and Lin, 2006;": "2787289", "DeNero and Klein, 2007;": null}}}, {"token_start": 41, "token_end": 50, "char_start": 183, "char_end": 229, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Burkett and Klein, 2008": "1138220"}}}, {"token_start": 62, "token_end": 97, "char_start": 293, "char_end": 486, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Huang and Knight (2006)": "6089287"}, "Reference": {}}}, {"token_start": 98, "token_end": 132, "char_start": 493, "char_end": 683, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Ambati and Lavie (2008)": "14006844"}, "Reference": {}}}, {"token_start": 133, "token_end": 157, "char_start": 690, "char_end": 783, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wang et al., (2010)": null}, "Reference": {}}}]}
{"id": "11918447_2", "paragraph": "[BOS] Multi-task learning involves sharing parameters between related tasks, whereby each task benefits from extra information in the training signals of the related tasks, and also improves its generalization performance.\n[BOS] Luong et al. (2016) showed improvements on translation, captioning, and parsing in a shared multi-task setting.\n[BOS] Recently, Pasunuru and Bansal (2017) extend this idea to video captioning with two related tasks: video completion and entailment generation.\n[BOS] We demonstrate that abstractive text summarization models can also be improved by sharing parameters with an entailment generation task.\n\n", "discourse_tags": ["Transition", "Single_summ", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 37, "token_end": 64, "char_start": 229, "char_end": 340, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Luong et al. (2016)": "6954272"}, "Reference": {}}}, {"token_start": 67, "token_end": 77, "char_start": 357, "char_end": 383, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Pasunuru and Bansal (2017)": null}}}]}
{"id": "11918447_1", "paragraph": "[BOS] Recognizing textual entailment (RTE) is the classification task of predicting whether the relationship between a premise and hypothesis sentence is that of entailment (i.e., logically follows), contradiction, or independence (Dagan et al., 2006) .\n[BOS] The SNLI corpus Bowman et al. (2015) allows training accurate end-to-end neural networks for this task.\n[BOS] Some previous work (Mehdad et al., 2013; Gupta et al., 2014) has explored the use of textual entailment recognition for redundancy detection in summarization.\n[BOS] They label relationships between sentences, so as to select the most informative and non-redundant sentences for summarization, via sentence connectivity and graphbased optimization and fusion.\n[BOS] Our focus, on the other hand, is entailment generation and not recognition, i.e., to teach summarization models the general natural language inference skill of generating a compressed sentence that logically entails the original longer sentence, so as to produce more effective short summaries.\n[BOS] We achieve this via multi-task learning with entailment generation.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Multi_summ", "Multi_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 53, "char_start": 6, "char_end": 251, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Dagan et al., 2006)": null}, "Reference": {}}}, {"token_start": 55, "token_end": 81, "char_start": 260, "char_end": 363, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 82, "token_end": 151, "char_start": 370, "char_end": 728, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Mehdad et al., 2013;": "5843073", "Gupta et al., 2014)": "5891659"}, "Reference": {}}}]}
{"id": "11918447_0", "paragraph": "[BOS] Earlier summarization work focused more towards extractive (and compression) based summarization, i.e., selecting which sentences to keep vs discard, and also compressing based on choosing grammatically correct sub-sentences having the most important pieces of information (Jing, 2000; Knight and Marcu, 2002; Clarke and Lapata, 2008; Filippova et al., 2015) .\n[BOS] Bigger datasets and neural models have allowed the addressing of the complex reasoning involved in abstractive summarization, i.e., rewriting and compressing the input document into a new summary.\n[BOS] Several advances have been made in this direction using machine translation inspired encoder-aligner-decoder models, convolution-based encoders, switching pointer and copy mechanisms, and hierarchical attention models (Rush et al., 2015; Nallapati et al., 2016; See et al., 2017) .\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Narrative_cite"], "span_citation_mapping": [{"token_start": 52, "token_end": 81, "char_start": 279, "char_end": 364, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Jing, 2000;": "8934802", "Knight and Marcu, 2002;": "7793213", "Clarke and Lapata, 2008;": "3004447", "Filippova et al., 2015)": "1992250"}}}, {"token_start": 154, "token_end": 182, "char_start": 764, "char_end": 855, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rush et al., 2015;": "1918428", "Nallapati et al., 2016;": "8928715", "See et al., 2017)": null}}}]}
{"id": "11022639_0", "paragraph": "[BOS] Several recent deep neural network architectures (Hermann et al., 2015; Hill et al., 2015; Chen et al., 2016; Kobayashi et al., 2016 ) were applied to the task of text comprehension.\n[BOS] The last two architectures were developed independently at the same time as our work.\n[BOS] All of these architectures use an attention mechanism that allows them to highlight places in the document that might be relevant to answering the question.\n[BOS] We will now briefly describe these architectures and compare them to our approach.\n\n", "discourse_tags": ["Narrative_cite", "Reflection", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 4, "token_end": 37, "char_start": 21, "char_end": 138, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hermann et al., 2015;": "6203757", "Hill et al., 2015;": "14915449", "Chen et al., 2016;": "6360322", "Kobayashi et al., 2016": "10239453"}}}]}
{"id": "10328681_3", "paragraph": "[BOS] 1 Also known as nut-paragraph.\n\n", "discourse_tags": ["Other"], "span_citation_mapping": []}
{"id": "10328681_2", "paragraph": "[BOS] Our event extraction approach ( 4.1) closely resembles the Open-IE event extraction approach (Fader et al., 2011; Hu et al., 2013; Do et al., 2011) which views events as sentence-level relations.\n[BOS] Events are extracted via syntactic and lexical constraints, which are imposed on sentence level structure, such as dependency parse.\n[BOS] For example, Sun et al. (2015) use the nsubj and dobj relations to identify relation pairs, which are then merged if they share the same predicate to form a (Subj,Pred,Obj) tuple expressing an event.\n[BOS] Unlike traditional event paradigms like ACE (NIST, 2004) and ERE (ERE, 2013) , the Open-IE event paradigm enjoys portability and domain-independence.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 14, "token_end": 43, "char_start": 65, "char_end": 153, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Fader et al., 2011;": "10318045", "Hu et al., 2013;": "13840496", "Do et al., 2011)": "3030259"}}}, {"token_start": 79, "token_end": 125, "char_start": 360, "char_end": 546, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Sun et al. (2015)": "7213770"}, "Reference": {}}}, {"token_start": 131, "token_end": 137, "char_start": 593, "char_end": 609, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(NIST, 2004)": null}}}, {"token_start": 138, "token_end": 144, "char_start": 614, "char_end": 629, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "10328681_1", "paragraph": "[BOS] The study of dominant elements of discourse has been formally studied in linguistics as a part of centering theory (Grosz et al., 1995) , a broader theory of attention and coherence in discourse, both of which were analyzed on a document-level basis (i.e. local discourse).\n[BOS] The authors suggested the use of centering constructs to keep track of the key entities, which change with discourse.\n[BOS] Document-level importance of entities (which include events) was explored by Gamon et al. (2013) .\n[BOS] The authors use the term salience to denote entity importance and graded entities into 3 categories -most salient, less salient, not salient.\n[BOS] They extracted supervision from web-search logs to semiautomatically obtain noisy salience judgments for a large web corpus.\n[BOS] Salient entities in a web document were then identified using graph centrality measures.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Transition"], "span_citation_mapping": [{"token_start": 2, "token_end": 85, "char_start": 6, "char_end": 403, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Grosz et al., 1995)": "11660053"}, "Reference": {}}}, {"token_start": 86, "token_end": 159, "char_start": 410, "char_end": 787, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Gamon et al. (2013)": "12090677"}, "Reference": {}}}]}
{"id": "10328681_0", "paragraph": "[BOS] Attempts to distinguish foreground and background regions in text date back to the 1980s.\n[BOS] Decker (1985) generated summaries from newspaper reports, where they used deterministic syntactic rules to label foreground events.\n[BOS] These rules were based on predictable reporting styles in journalism such as the inverted pyramid and block paragraph 1 , and drew heavily on the syntactic correlation between grounding and information content.\n[BOS] We analyze the performance of these rules for news-peg identification in our experiments ( 6).\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 19, "token_end": 78, "char_start": 102, "char_end": 450, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Decker (1985)": "15271310"}, "Reference": {}}}]}
{"id": "10619801_1", "paragraph": "[BOS] Equation parsing takes as input a sentence x describing a single mathematical equation, comprising one or two variables and other quantities mentioned in x.\n[BOS] Let N be the set of noun phrases in the sentence x.\n[BOS] The output of the task is the mathematical equation described in x, along with a mapping of each variable in the equation to its corresponding noun phrase in N .\n[BOS] We refer to this mapping as the \"grounding\" of the variable; the noun phrase represents what the variable stands for in the equation.\n[BOS] Table 1 gives an example of an input and output for the equation parsing of the text in example 2.\n[BOS] Since an equation can be written in various forms, we use the form which most agrees with text, as our target output.\n[BOS] So, for example 1, we will choose V 1 = 3  V 2 and not V 2 = V 1  3.\n[BOS] In cases where several equation forms seem to be equally likely to be the target equation, we randomly choose one of them, and keep this choice consistent across the dataset.\n\n", "discourse_tags": ["Other", "Other", "Other", "Other", "Other", "Other", "Other", "Other"], "span_citation_mapping": []}
{"id": "10619801_0", "paragraph": "[BOS] The work most related to this paper is (Madaan et al., 2016) , which focuses on extracting relation triples where one of the arguments is a number.\n[BOS] In contrast, our work deals with multiple variables and complex equations involving them.\n[BOS] There has been a lot of recent work in automatic math word problem solving Roy et al., 2015; Hosseini et al., 2014; Roy and Roth, 2015) .\n[BOS] These solvers cannot handle sentences individually.\n[BOS] They require the input to be a complete math word problem, and even then, they only focus on retrieving a set of answer values without mentioning what each answer value corresponds to.\n[BOS] Our work is also conceptually related to work on semantic parsing -mapping natural language text to a formal meaning representation (Wong and Mooney, 2007; Clarke et al., 2010; Cai and Yates, 2013; Kwiatkowski et al., 2013; Goldwasser and Roth, 2011) .\n[BOS] However, as mentioned earlier, there are some significant differences in the task definition that necessitate the development of a new approach.\n\n", "discourse_tags": ["Narrative_cite", "Reflection", "Narrative_cite", "Transition", "Transition", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 10, "token_end": 20, "char_start": 45, "char_end": 66, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Madaan et al., 2016)": null}}}, {"token_start": 63, "token_end": 89, "char_start": 305, "char_end": 391, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Roy et al., 2015;": "8559332", "Hosseini et al., 2014;": "428579", "Roy and Roth, 2015)": "8559332"}}}, {"token_start": 144, "token_end": 196, "char_start": 698, "char_end": 899, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Clarke et al., 2010;": null, "Cai and Yates, 2013;": null, "Kwiatkowski et al., 2013;": null, "Goldwasser and Roth, 2011)": null}}}]}
{"id": "10738628_2", "paragraph": "[BOS] Hashimoto and Kawahara (2008) (HK08) is the first large scale study to our knowledge that addressed token classification into idiomatic versus literal for Japanese MWEs of all types.\n[BOS] They apply a supervised learning framework using support vector machines based on TinySVM with a quadratic kernel.\n[BOS] They annotate a web based corpus for training data.\n[BOS] They identify 101 idiom types each with a corresponding 1000 examples, hence they had a corpus of 102K sentences of annotated data for their experiments.\n[BOS] They experiment with 90 idiom types only for which they had more than 50 examples.\n[BOS] They use two types of features: word sense disambiguation (WSD) features and idiom features.\n[BOS] The WSD features comprised some basic syntactic features such as POS, lemma information, token n-gram features, in addition to hypernymy information on words as well as domain information.\n[BOS] For the idiom features they were mostly inflectional features such as voice, negativity, modality, in addition to adjacency and adnominal features.\n[BOS] They report results in terms of accuracy and rate of error reduction.\n[BOS] Their overall accuracy is of 89.25% using all the features.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 241, "char_start": 6, "char_end": 1206, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "10738628_1", "paragraph": "[BOS] In Katz and Giesbrecht (2006) (KG06) the authors carried out a vector similarity comparison between the context of an MWE and that of the constituent words using LSA to determine if the expression is idiomatic or not.\n[BOS] The KG06 is similar in intuition to work proposed by , however the latter work was unsupervised.\n[BOS] KG06 experimented with a tiny data set of only 108 sentences corresponding to one MWE idiomatic expression.\n\n", "discourse_tags": ["Single_summ", "Transition", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 51, "char_start": 6, "char_end": 223, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Katz and Giesbrecht (2006)": "10987468"}, "Reference": {}}}]}
{"id": "10738628_0", "paragraph": "[BOS] Several researchers have addressed the problem of MWE classification (Baldwin et al., 2003; Katz and Giesbrecht, 2006; Schone and Juraksfy, 2001; Hashimoto et al., 2006; Hashimoto and Kawahara, 2008) .\n[BOS] The majority of the proposed research has been using unsupervised approaches and have addressed the problem of MWE type classification irrespective of usage in context Cook et al., 2007) .\n[BOS] We are aware of two supervised approaches to the problem: work by (Katz and Giesbrecht, 2006) and work by (Hashimoto and Kawahara, 2008) .\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 9, "token_end": 55, "char_start": 56, "char_end": 205, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Baldwin et al., 2003;": "1695436", "Katz and Giesbrecht, 2006;": "10987468", "Schone and Juraksfy, 2001;": "17089673", "Hashimoto et al., 2006;": "1339735", "Hashimoto and Kawahara, 2008)": "14175753"}}}, {"token_start": 83, "token_end": 90, "char_start": 382, "char_end": 400, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Cook et al., 2007)": "235425"}}}, {"token_start": 105, "token_end": 115, "char_start": 475, "char_end": 502, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Katz and Giesbrecht, 2006)": "10987468"}}}, {"token_start": 118, "token_end": 127, "char_start": 515, "char_end": 545, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hashimoto and Kawahara, 2008)": "14175753"}}}]}
{"id": "10324034_3", "paragraph": "[BOS] In the MCTest text comprehension challenge (Richardson et al., 2013) , the leading models use complex engineered features ensembling multiple traditional semantic NLP approaches (Wang and McAllester, 2015) .\n[BOS] The best deep model so far (Yin et al., 2016) uses convolutional neural networks for sentence representations, and attention on multiple levels to pick evidencing sentences.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 4, "token_end": 17, "char_start": 13, "char_end": 74, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Richardson et al., 2013)": "2100831"}}}, {"token_start": 29, "token_end": 42, "char_start": 160, "char_end": 211, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 50, "token_end": 58, "char_start": 247, "char_end": 265, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Yin et al., 2016)": "1310204"}}}]}
{"id": "10324034_2", "paragraph": "[BOS] In the entailment context, (Bowman et al., 2015) introduced a large dataset with single-evidence sentence pairs (Stanford Natural Language Inference, SNLI), but a larger vocabulary and slightly more complicated (but still conservatively formed) pedigree chart model is used to show the pattern of traits that are passed from one generation to the next in a family?\n[BOS] A pedigree is a chart which shows the inheritance of a trait over several generations.\n[BOS] Figure 51 .14 In a pedigree, squares symbolize males, and circles represent females.\n[BOS] energy pyramid model is used to show the pattern of traits that are passed from one generation to the next in a family?\n[BOS] Energy is passed up a food chain or web from lower to higher trophic levels.\n[BOS] Each step of the food chain in the energy pyramid is called a trophic level.\n[BOS] sentences.\n[BOS] They also proposed baseline recurrent neural model for modeling sentence representations, while word-level attention based models are being studied more recently (Rocktschel et al., 2015) (Cheng et al., 2016) .\n\n", "discourse_tags": ["Single_summ", "Other", "Other", "Other", "Other", "Other", "Other", "Narrative_cite"], "span_citation_mapping": [{"token_start": 2, "token_end": 76, "char_start": 6, "char_end": 369, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Bowman et al., 2015)": "14604520"}, "Reference": {}}}, {"token_start": 191, "token_end": 220, "char_start": 966, "char_end": 1078, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Rockt\u00e4schel et al., 2015)": "2135897", "(Cheng et al., 2016)": "6506243"}}}]}
{"id": "10324034_1", "paragraph": "[BOS] Sentence-level representations in the retrieval + inference context have been popularly proposed within the Memory Network framework (Weston et al., 2014) , but explored just in the form of averaged word embeddings; the task includes only very simple sentences and a small vocabulary.\n[BOS] Much more realistic setting is introduced in the Answer Sentence Selection context (Wang et al., 2007) (Baudi et al., 2016a) , with state-of-art models using complex deep neural architectures with attention (dos Santos et al., 2016), but the selection task consists of only retrieval and no inference (answer prediction).\n[BOS] A more indirect retrieval task regarding news summarization was investigated by (Cao et al., 2016) .\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 19, "token_end": 31, "char_start": 114, "char_end": 160, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 64, "token_end": 86, "char_start": 346, "char_end": 421, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wang et al., 2007)": null, "(Baudi\u0161 et al., 2016a)": "14965173"}}}, {"token_start": 100, "token_end": 110, "char_start": 494, "char_end": 529, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 140, "token_end": 148, "char_start": 705, "char_end": 723, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cao et al., 2016)": "8244856"}}}]}
{"id": "10324034_0", "paragraph": "[BOS] Our primary concern when integrating natural language query with textual evidence is to find sentence-level representations suitable both for relevance weighing and answer prediction.\n\n", "discourse_tags": ["Transition"], "span_citation_mapping": []}
{"id": "10011032_14", "paragraph": "[BOS] Although SH(t) is similar to the one used in Hatori et al. (2011) , now it shifts the first character in the queue as a new word, instead of shifting a word.\n[BOS] Following Zhang and Clark (2010) , the POS tag is assigned to the word when its first character is shifted, and the word-tag pairs observed in the training data and the closed-set tags (Xia, 2000) are used to prune unlikely derivations.\n[BOS] Because 33 tags are defined in the CTB tag set (Xia, 2000) , our model exploits a total of 36 actions.\n[BOS] To train the model, we use the averaged perceptron with the early update (Collins and Roark, 2004) .\n[BOS] In our joint model, the early update is invoked by mistakes in any of word segmentation, POS tagging, or dependency parsing.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Narrative_cite", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 14, "token_end": 22, "char_start": 51, "char_end": 71, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Hatori et al. (2011)": "5404235"}}}, {"token_start": 45, "token_end": 51, "char_start": 180, "char_end": 202, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Zhang and Clark (2010)": "2712419"}}}, {"token_start": 80, "token_end": 89, "char_start": 339, "char_end": 366, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Xia, 2000)": null}}}, {"token_start": 105, "token_end": 114, "char_start": 448, "char_end": 471, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Xia, 2000)": null}}}, {"token_start": 140, "token_end": 148, "char_start": 595, "char_end": 620, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Collins and Roark, 2004)": "10366378"}}}]}
{"id": "10011032_13", "paragraph": "[BOS]  RL/RR: reduce the top two trees on the stack,\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "10011032_12", "paragraph": "[BOS]  SH(t): shift the first character in the input queue as a new word onto the stack, with POS tag t.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "10011032_11", "paragraph": "[BOS]  A: append the first character in the queue to the word on top of the stack.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "10011032_10", "paragraph": "[BOS] The incremental framework of our model is based on the joint POS tagging and dependency parsing model for Chinese (Hatori et al., 2011) , which is an extension of the shift-reduce dependency parser with dynamic programming (Huang and Sagae, 2010) .\n[BOS] They specifically modified the shift action so that it assigns the POS tag when a word is shifted onto the stack.\n[BOS] However, because they regarded word segmentation as given, their model did not consider the interaction between segmentation and POS tagging.\n[BOS] Based on the joint POS tagging and dependency parsing model by Hatori et al. (2011) , we build our joint model to solve word segmentation, POS tagging, and dependency parsing within a single framework.\n[BOS] Particularly, we change the role of the shift action and additionally use the append action, inspired by the character-based actions used in the joint segmentation and POS tagging model by Zhang and Clark (2010) .\n[BOS] The list of actions used is the following:\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 12, "token_end": 30, "char_start": 61, "char_end": 141, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hatori et al., 2011)": "5404235"}}}, {"token_start": 43, "token_end": 53, "char_start": 209, "char_end": 252, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Huang and Sagae, 2010)": "1153327"}}}, {"token_start": 106, "token_end": 121, "char_start": 548, "char_end": 612, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Hatori et al. (2011)": "5404235"}}}]}
{"id": "10011032_9", "paragraph": "[BOS] Among the many recent works on joint segmentation and POS tagging for Chinese, the linear-time incremental models by Zhang and Clark (2008) and Zhang and Clark (2010) largely inspired our model.\n[BOS] Zhang and Clark (2008) proposed an incremental joint segmentation and POS tagging model, with an effective feature set for Chinese.\n[BOS] However, it requires to computationally expensive multiple beams to compare words of different lengths using beam search.\n[BOS] More recently, Zhang and Clark (2010) proposed an efficient character-based decoder for their word-based model.\n[BOS] In their new model, a single beam suffices for decoding; hence, they reported that their model is practically ten times as fast as their original model.\n[BOS] To incorporate the word-level features into the character-based decoder, the features are decomposed into substring-level features, which are effective for incomplete words to have comparable scores to complete words in the beam.\n[BOS] Because we found that even an incremental approach with beam search is intractable if we perform the wordbased decoding, we take a character-based approach to produce our joint model.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ", "Single_summ", "Single_summ", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 17, "token_end": 36, "char_start": 89, "char_end": 172, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Zhang and Clark (2008)": "105219", "Zhang and Clark (2010)": "2712419"}}}, {"token_start": 42, "token_end": 86, "char_start": 207, "char_end": 466, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhang and Clark (2008)": "105219"}, "Reference": {}}}, {"token_start": 87, "token_end": 141, "char_start": 473, "char_end": 743, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zhang and Clark (2010)": "2712419"}, "Reference": {}}}]}
{"id": "10011032_8", "paragraph": "[BOS] Another line of work exists on lattice-based parsing for Semitic languages (Cohen and Smith, 2007; Goldberg and Tsarfaty, 2008) .\n[BOS] These methods first convert an input sentence into a lattice encoding the morphological ambiguities, and then conduct joint morphological segmentation and PCFG parsing.\n[BOS] However, the segmentation possibilities considered in those studies are limited to those output by an existing morphological analyzer.\n[BOS] In addition, the lattice does not include word segmentation ambiguities crossing boundaries of space-delimited tokens.\n[BOS] In contrast, because the Chinese language does not have spaces between words, we fundamentally need to consider the lattice structure of the whole sentence.\n[BOS] Therefore, we place no restriction on the segmentation possibilities to consider, and we assess the full potential of the joint segmentation and dependency parsing model.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Transition", "Transition", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 13, "token_end": 32, "char_start": 63, "char_end": 133, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cohen and Smith, 2007;": "905345", "Goldberg and Tsarfaty, 2008)": "14857072"}}}]}
{"id": "10011032_7", "paragraph": "[BOS] In Chinese, Luo (2003) proposed a joint constituency parser that performs segmentation, POS tagging, and parsing within a single character-based framework.\n[BOS] They reported that the POS tags contribute to segmentation accuracies by more than 1%, but the syntactic information has no substantial effect on the segmentation accuracies.\n[BOS] In contrast, we built a joint model based on a dependency-based framework, with a rich set of structural features.\n[BOS] Using it, we show the first positive result in Chinese that the segmentation accuracies can be improved using the syntactic information.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 2, "token_end": 62, "char_start": 6, "char_end": 342, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Luo (2003)": "1742176"}, "Reference": {}}}]}
{"id": "10011032_6", "paragraph": "[BOS] We perform experiments using the Chinese Treebank (CTB) corpora, demonstrating that the accuracies of the three tasks can be improved significantly over the pipeline combination of the state-of-the-art joint segmentation and POS tagging model, and the dependency parser.\n[BOS] We also perform comparison experiments with partially joint models, and investigate the tradeoff between the running speed and the model performance.\n\n", "discourse_tags": ["Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "10011032_5", "paragraph": "[BOS] Second, although the feature set is fundamentally a combination of those used in previous works (Zhang and Clark, 2010; Huang and Sagae, 2010) , to integrate them in a single incremental framework is not straightforward.\n[BOS] Because we must perform decisions of three kinds (segmentation, tagging, and parsing) in an incremental framework, we must adjust which features are to be activated when, and how they are combined with which action labels.\n[BOS] We have also found that we must balance the learning rate between features for segmentation and tagging decisions, and those for dependency parsing.\n\n", "discourse_tags": ["Narrative_cite", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 18, "token_end": 32, "char_start": 102, "char_end": 148, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Zhang and Clark, 2010;": "2712419", "Huang and Sagae, 2010)": "1153327"}}}]}
{"id": "10011032_4", "paragraph": "[BOS] Two major challenges exist in formalizing the joint segmentation and dependency parsing task in the character-based incremental framework.\n[BOS] First, we must address the problem of how to align comparable states effectively in the beam.\n[BOS] Because the number of dependency arcs varies depending on how words are segmented, we devise a step alignment scheme using the number of character-based arcs, which enables effective joint decoding for the three tasks.\n\n", "discourse_tags": ["Reflection", "Reflection", "Reflection"], "span_citation_mapping": []}
{"id": "10011032_3", "paragraph": "[BOS] Based on these observations, we aim at building a joint model that simultaneously processes word segmentation, POS tagging, and dependency parsing, trying to capture global interaction among these three tasks.\n[BOS] To handle the increased computational complexity, we adopt the incremental parsing framework with dynamic programming (Huang and Sagae, 2010) , and propose an efficient method of character-based decoding over candidate structures.\n\n", "discourse_tags": ["Other", "Reflection"], "span_citation_mapping": [{"token_start": 52, "token_end": 62, "char_start": 320, "char_end": 363, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Huang and Sagae, 2010)": "1153327"}}}]}
{"id": "10011032_2", "paragraph": "[BOS] Syntactic information is also considered beneficial to improve the segmentation of out-ofvocabulary (OOV) words.\n[BOS] Unlike languages such as Japanese that use a distinct character set (i.e. katakana) for foreign words, the transliterated words in Chinese, many of which are OOV words, frequently include characters that are also used as common or function words.\n[BOS] In the current systems, the existence of these characters causes numerous oversegmentation errors for OOV words.\n\n", "discourse_tags": ["Other", "Other", "Other"], "span_citation_mapping": []}
{"id": "10011032_1", "paragraph": "[BOS] the only difference is the existence of the last word S; however, whether or not this word exists changes the whole syntactic structure and segmentation of the sentence.\n[BOS] This is an example in which word segmentation cannot be handled properly without considering long-range syntactic information.\n\n", "discourse_tags": ["Other", "Other"], "span_citation_mapping": []}
{"id": "10011032_0", "paragraph": "[BOS] The current peace is awarded to peace-operation-related groups.\n\n", "discourse_tags": ["Other"], "span_citation_mapping": []}
{"id": "10046354_0", "paragraph": "[BOS] Most of the tools available for automatic keyphrase extraction only implement one approach, and are often outdated with respect to the current state-of-the-art.\n[BOS] These tools also rely on in-house text preprocessing and candidate selection/filtering pipelines, which makes it difficult to compare results across several approaches.\n[BOS] One notable exception to this is the DKPro Keyphrases Java framework (Erbs et al., 2014) , which provides a UIMA-based workbench for developing and evaluating new keyphrase extraction approaches.\n[BOS] However, this framework requires users to learn UIMA before they can get started, and does not provide supervised approaches that are known to perform better (Hasan and Ng, 2014) .\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 69, "token_end": 115, "char_start": 348, "char_end": 543, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Erbs et al., 2014)": "7559566"}, "Reference": {}}}, {"token_start": 144, "token_end": 152, "char_start": 708, "char_end": 728, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Hasan and Ng, 2014)": "11777996"}}}]}
{"id": "1144632_0", "paragraph": "[BOS] Many methods have been proposed for learning word representations.\n[BOS] Earlier work learns embeddings using a recurrent language model (Collobert et al., 2011) , while several simpler and more lightweight adaptations have been proposed (Huang et al., 2012; Mikolov et al., 2013) .\n[BOS] While most of the learnt vectors are semantically oriented, work has been done in order to extend the model to learn syntactically oriented embeddings (Ling et al., 2015a) .\n[BOS] Attention models are common in vision related tasks (Tang et al., 2014) , where models learn to pay attention to certain parts of a image in order to make accurate predictions.\n[BOS] This idea has been recently introduced in many NLP tasks, such as machine translation (Bahdanau et al., 2014) .\n[BOS] In the area of word representation learning, no prior work that uses attention models exists to our knowledge.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite", "Narrative_cite", "Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 19, "token_end": 31, "char_start": 118, "char_end": 167, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Collobert et al., 2011)": "351666"}}}, {"token_start": 42, "token_end": 59, "char_start": 244, "char_end": 286, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Huang et al., 2012;": "372093", "Mikolov et al., 2013)": "16447573"}}}, {"token_start": 87, "token_end": 97, "char_start": 435, "char_end": 466, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ling et al., 2015a)": "14800090"}}}, {"token_start": 104, "token_end": 115, "char_start": 506, "char_end": 546, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Tang et al., 2014)": "16440891"}}}, {"token_start": 150, "token_end": 162, "char_start": 724, "char_end": 767, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bahdanau et al., 2014)": "11212020"}}}]}
{"id": "10161320_0", "paragraph": "[BOS] From the previous explorations of online translation model combination, we see the work of Liu et al. (2009) proposing an unconstrained combination of hiero and tree-to-string models as a special configuration of our framework, and we also replicate it.\n[BOS] Denero et al. (2010) combine translation models even with different search paradigms.\n[BOS] Their approach is different, since their component systems do not interact at decoding time, instead, each of them provides its weighted translation forest first, the forests are then combined to infer a new combination model.\n\n", "discourse_tags": ["Narrative_cite", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 18, "token_end": 25, "char_start": 97, "char_end": 114, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Liu et al. (2009)": "2815754"}}}, {"token_start": 54, "token_end": 112, "char_start": 266, "char_end": 584, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Denero et al. (2010)": "798995"}, "Reference": {}}}]}
{"id": "10828403_0", "paragraph": "[BOS] As paraphrase acquisition from a corpus, a study with a parallel corpus and comparable corpus has been performed.\n[BOS] Barzilay and McKeown paraphrase text using plural English translations made from the same document (Barzilay and McKeown, 2001 ).\n[BOS] In addition, Shinyama and Sekine paraphrase using plural newspaper articles that report the same event (Shinyama and Sekine, 2003) .\n[BOS] In a text sim-\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite", "Other"], "span_citation_mapping": [{"token_start": 44, "token_end": 55, "char_start": 216, "char_end": 252, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Barzilay and McKeown, 2001": "9842595"}}}, {"token_start": 78, "token_end": 88, "char_start": 359, "char_end": 392, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Shinyama and Sekine, 2003)": "12547848"}}}]}
{"id": "12108307_0", "paragraph": "[BOS] Most of previous event extraction work focused on supervised learning paradigm and trained event extractors on human-annotated data which yield relatively high performance.\n[BOS] (Ahn, 2006; Ji and Grishman, 2008; Hong et al., 2011; McClosky et al., 2011; Li et al., 2013 Li et al., , 2014 Nguyen and Grishman, 2015; Nguyen et al., 2016) .\n[BOS] However, these supervised methods depend on the quality of the training data and labeled training data is expensive to produce.\n[BOS] Unsupervised methods can extract large numbers of events without using labeled data (Chambers and Jurafsky, 2011; Cheung et al., 2013; Huang et al., 2016) .\n[BOS] But extracted events may not be easy to be mapped to events for a particular knowledge base.\n[BOS] Distant supervision have been used in relation extraction for automatically labeling training data (Mintz et al., 2009; Hinton et al., 2012; Krause et al., 2012; Krishnamurthy and Mitchell, 2012; Berant et al., 2013; Surdeanu et al., 2012; .\n[BOS] But DS for RE cannot directly use for EE.\n[BOS] For the reasons that an event is more complicated than a relation and the task of EE is more difficult than RE.\n[BOS] The best reported supervised RE and EE system got a F1-score of 88.0% (Wang et al., 2016) and 55.4% (Nguyen et al., 2016) respectively.\n[BOS] Reschke et al. (2014) extended the distant supervision approach to fill slots in plane crash.\n[BOS] However, the method can only extract arguments of one plane crash type and need flight number strings as input.\n[BOS] In other words, the approach cannot extract whole event with different types automatically.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite", "Transition", "Narrative_cite", "Transition", "Narrative_cite", "Transition", "Transition", "Narrative_cite", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 30, "token_end": 88, "char_start": 185, "char_end": 343, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ahn, 2006;": null, "Ji and Grishman, 2008;": "1320606", "Hong et al., 2011;": "2867611", "McClosky et al., 2011;": "2941631", "Li et al., 2013": "2114517", "Li et al., , 2014": "15552794", "Nguyen and Grishman, 2015;": "10913456", "Nguyen et al., 2016)": "6452487"}}}, {"token_start": 123, "token_end": 149, "char_start": 557, "char_end": 640, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Chambers and Jurafsky, 2011;": "12808163", "Cheung et al., 2013;": "278288", "Huang et al., 2016)": "14610045"}}}, {"token_start": 181, "token_end": 235, "char_start": 833, "char_end": 986, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Mintz et al., 2009;": "10910955", "Hinton et al., 2012;": "14832074", "Krause et al., 2012;": "2605890", "Krishnamurthy and Mitchell, 2012;": "5633240", "Berant et al., 2013;": "6401679"}}}, {"token_start": 292, "token_end": 300, "char_start": 1232, "char_end": 1251, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Wang et al., 2016)": "9524495"}}}, {"token_start": 305, "token_end": 313, "char_start": 1262, "char_end": 1283, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 316, "token_end": 375, "char_start": 1304, "char_end": 1613, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Reschke et al. (2014)": "2889796"}, "Reference": {}}}]}
{"id": "11118320_0", "paragraph": "[BOS] Our work essentially follows a similar procedure as reported in [Klementiev and Roth, 2006] paper, but applied to English-Tamil language pair.\n[BOS] Earlier works, such as [Cucerzan and Yarowsky, 1999] and [Collins and Singer, 1999] addressed identification of NEs from untagged corpora.\n[BOS] They relied on significant contextual and morphological clues.\n[BOS] [Hetland, 2004] outlined methodologies based on time distribution of terms in a corpus to identify NEs, but only in English.\n[BOS] While a large body of literature exists on transliteration, we merely point out that the focus of this work (based on [Klementiev and Roth, 2006] ) is not on transliteration, but mining transliteration pairs, which may be used for developing a transliteration system.\n\n", "discourse_tags": ["Reflection", "Multi_summ", "Multi_summ", "Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 12, "token_end": 22, "char_start": 70, "char_end": 97, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"[Klementiev and Roth, 2006]": "30985549"}}}, {"token_start": 35, "token_end": 79, "char_start": 155, "char_end": 362, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 80, "token_end": 106, "char_start": 369, "char_end": 493, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"[Hetland, 2004]": null}, "Reference": {}}}, {"token_start": 132, "token_end": 142, "char_start": 618, "char_end": 645, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"[Klementiev and Roth, 2006]": "30985549"}}}]}
{"id": "11074199_2", "paragraph": "[BOS] Among multi-hop reasoning systems: Hill et al. (2015) apply attention on window-based memory, by extending multi-hop end-to-end memory network (Sukhbaatar et al., 2015) .\n[BOS] Dhingra et al. (2016) extend attention-sum reader to multi-turn reasoning with an added gating mechanism.\n[BOS] The Iterative Alternative (IA) reader (Sordoni et al., 2016) produces query glimpse and document glimpse in each iterations and uses both glimpses to update recurrent state in each iteration.\n[BOS] Shen et al. (2017) propose a multi-hop attention model that used reinforcement learning to dynamically determine when to stop digesting intermediate information and produce an answer.\n\n", "discourse_tags": ["Multi_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 9, "token_end": 23, "char_start": 41, "char_end": 98, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Hill et al. (2015)": "14915449"}, "Reference": {}}}, {"token_start": 24, "token_end": 47, "char_start": 100, "char_end": 174, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Sukhbaatar et al., 2015)": "1399322"}, "Reference": {}}}, {"token_start": 49, "token_end": 74, "char_start": 183, "char_end": 288, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 75, "token_end": 119, "char_start": 295, "char_end": 486, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Sordoni et al., 2016)": "14500125"}, "Reference": {}}}, {"token_start": 120, "token_end": 153, "char_start": 493, "char_end": 676, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Shen et al. (2017)": "6300274"}, "Reference": {}}}]}
{"id": "11074199_1", "paragraph": "[BOS] Most papers on single-pass reasoning systems propose novel ways to use the attention mechanism: Wang and Jiang (2016) propose match-LSTM to model the interaction between context and query, as well as introducing the use of a pointer network (Vinyals et al., 2015) to extract the answer span from the context.\n[BOS] Xiong et al. (2017) propose the Dynamic Coattention Network, which uses co-dependent representations of the question and the context, and iteratively updates the start and end indices to recover from local maxima and to find the optimal answer span.\n[BOS] propose the Multi-Perspective Context Matching model that matches the encoded context with query by combining various matching strategies, aggregates matching vector with bidirectional LSTM, and predict start and end positions.\n[BOS] In order to merge the entity score during its multiple appearence, Kadlec et al. (2016) propose attention-sum reader who computes dot product between context and query encoding, does a softmax operation over context and sums the probability over the same entity to favor the frequent entities over rare ones.\n[BOS] Chen et al. (2016) propose to use a bilinear term to calculate the attentional alignment between context and query.\n\n", "discourse_tags": ["Multi_summ", "Single_summ", "Other", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 19, "token_end": 38, "char_start": 102, "char_end": 193, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Wang and Jiang (2016)": "5592690"}, "Reference": {}}}, {"token_start": 42, "token_end": 68, "char_start": 206, "char_end": 314, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Vinyals et al., 2015)": null}, "Reference": {}}}, {"token_start": 69, "token_end": 118, "char_start": 321, "char_end": 570, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Xiong et al. (2017)": "3714278"}, "Reference": {}}}, {"token_start": 156, "token_end": 216, "char_start": 811, "char_end": 1119, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Kadlec et al. (2016)": "11022639"}, "Reference": {}}}, {"token_start": 217, "token_end": 240, "char_start": 1126, "char_end": 1241, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Chen et al. (2016)": "6360322"}, "Reference": {}}}]}
{"id": "11074199_0", "paragraph": "[BOS] Recently, both QA and Cloze-style machine comprehension tasks like CNN/Daily Mail have seen fast progress.\n[BOS] Much of this recent work has been based on end-to-end trained neural network models, and within that, most have used recurrent neural networks with soft attention (Bahdanau et al., 2015) , which emphasizes one part of the data over the others.\n[BOS] These models can be coarsely divided into two categories: single-pass and multi-pass reasoners.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Transition"], "span_citation_mapping": [{"token_start": 51, "token_end": 67, "char_start": 236, "char_end": 305, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bahdanau et al., 2015)": "11212020"}}}]}
{"id": "11331864_1", "paragraph": "[BOS] Finally, neural networks have been introduced into ED very recently with the early work on convolutional neural networks (Nguyen and Grishman, 2015b; Chen et al., 2015) .\n[BOS] The other work includes: (Nguyen et al., 2016a) who employ bidirectional recurrent neural networks to perform event trigger and argument labeling jointly, (Jagannatha and Yu, 2016) who extract event instances from health records with recurrent neural networks and (Nguyen et al., 2016b) who propose a two-stage training algorithm for event extension with neural networks.\n\n", "discourse_tags": ["Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 18, "token_end": 38, "char_start": 97, "char_end": 174, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Nguyen and Grishman, 2015b;": "10913456", "Chen et al., 2015)": "14339673"}}}, {"token_start": 45, "token_end": 67, "char_start": 208, "char_end": 328, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 69, "token_end": 90, "char_start": 338, "char_end": 442, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 91, "token_end": 115, "char_start": 447, "char_end": 554, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"(Nguyen et al., 2016b)": null}, "Reference": {}}}]}
{"id": "11331864_0", "paragraph": "[BOS] There have been three major approaches to event detection in the literature.\n[BOS] First, the pattern-based approach explores the application of patterns to identify the instances of events, in which the patterns are formed by predicates, event triggers and constraints on the syntactic context (Grishman et al., 2005; Cao et al., 2015a; Cao et al., 2015b) .\n[BOS] Second, the feature-based approach relies on linguistic intuition to design effective feature sets for statistical models for ED, ranging from the local sentence-level representations (Ahn, 2006; Li et al., 2013) , to the higher level structures such as the cross-sentence or cross-event information (Ji and Grishman, 2008; Gupta and Ji, 2009; Patwardhan and Riloff, 2009; Liao and Grishman, 2011; Hong et al., 2011; McClosky et al., 2011; Li et al., 2015) .\n[BOS] Some recent work on the feature-based approach has also investigated event trigger detection in the joint inference with event argument prediction (Riedel et al., 2009; Poon and Vanderwende, 2010; Li et al., 2013; Venugopal et al., 2014) to benefit from their inter-dependencies.\n\n", "discourse_tags": ["Transition", "Narrative_cite", "Narrative_cite", "Narrative_cite"], "span_citation_mapping": [{"token_start": 50, "token_end": 78, "char_start": 283, "char_end": 362, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Grishman et al., 2005;": null, "Cao et al., 2015a;": "11409285", "Cao et al., 2015b)": "1480182"}}}, {"token_start": 106, "token_end": 123, "char_start": 524, "char_end": 583, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ahn, 2006;": null, "Li et al., 2013)": "2114517"}}}, {"token_start": 132, "token_end": 197, "char_start": 629, "char_end": 827, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Ji and Grishman, 2008;": "1320606", "Gupta and Ji, 2009;": "8336242", "Patwardhan and Riloff, 2009;": "2524712", "Liao and Grishman, 2011;": "15865939", "Hong et al., 2011;": "2867611", "McClosky et al., 2011;": "2941631", "Li et al., 2015)": "14117526"}}}, {"token_start": 222, "token_end": 256, "char_start": 983, "char_end": 1073, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Riedel et al., 2009;": "475213", "Poon and Vanderwende, 2010;": "1160159", "Li et al., 2013;": "2114517", "Venugopal et al., 2014)": "8247565"}}}]}
{"id": "10672840_2", "paragraph": "[BOS] 3 System Architecture\n\n", "discourse_tags": ["Other"], "span_citation_mapping": []}
{"id": "10672840_1", "paragraph": "[BOS] Our SMT system for correcting preposition and determiner errors is based on Mizumoto et al. (2012) .\n[BOS] They constructed a translation model from the data of the language-exchange social network service Lang-8 1 and evaluated its performance for 18 error types, including preposition and determiner errors in the Konan-JIEM Learner Corpus.\n[BOS] On preposition error correction, they showed that their SMT system outperformed a system using a maximum entropy model.\n[BOS] The main difference with this work is that our new corpus collection here is about three times larger.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Reflection"], "span_citation_mapping": [{"token_start": 17, "token_end": 98, "char_start": 82, "char_end": 474, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Mizumoto et al. (2012)": "15389327"}, "Reference": {}}}]}
{"id": "10672840_0", "paragraph": "[BOS] For the noun number errors, we improved the system proposed by Izumi et al. (2003) .\n[BOS] In Izumi et al. (2003) , a noun number error detection method is a part of an automatic error detection system for transcribed spoken English by Japanese learners.\n[BOS] They used a maximum entropy method whose features are unigrams, bigrams and trigrams of surface words, of POS tags and of the root forms.\n[BOS] They trained a classifier on only a learner corpus.\n[BOS] The main difference between theirs and ours is a domain of the training corpus and features we used.\n[BOS] We trained a classifier on the mixed corpus of the leaner corpus and the native corpus.\n[BOS] We employ a treepath feature in our system.\n\n", "discourse_tags": ["Reflection", "Single_summ", "Single_summ", "Single_summ", "Reflection", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 14, "token_end": 22, "char_start": 69, "char_end": 88, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"Izumi et al. (2003)": "18202204"}}}, {"token_start": 24, "token_end": 101, "char_start": 97, "char_end": 462, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Izumi et al. (2003)": "18202204"}, "Reference": {}}}]}
{"id": "1136358_3", "paragraph": "[BOS] Then in the second stage, we apply one heuristic rule to extract extra words from the dependency tree.\n[BOS] Specifically, if a word is in a particular dependency relation 2 , aux, auxpass or cop, with a word that is already in the chain after the first stage, then we include this word in the chain as well.\n[BOS] For the example (1), the word \"will\" is inserted into the dependency chain in the second stage.\n[BOS] The reason we perform this additional step is that context words identified with one of the above three dependency relations usually indicate\n\n", "discourse_tags": ["Other", "Other", "Other", "Other"], "span_citation_mapping": []}
{"id": "1136358_2", "paragraph": "[BOS] 3 Dependency Chain Extraction Figure 1 shows the dependency parse tree 1 for the example (1).\n[BOS] To extract the dependency chain for the target event, we have used a two-stage approach to create the chain.\n[BOS] In the first stage, we start from the target event, traverse the dependency parse tree, identify all its direct or indirect governors and dependents and include these words in the chain.\n[BOS] For the example (1), a list of words [launch, describing, protest, their] are included in the dependency chain after the first stage.\n\n", "discourse_tags": ["Other", "Other", "Other", "Other"], "span_citation_mapping": []}
{"id": "1136358_1", "paragraph": "[BOS] Similar to our dependency chains, dependency paths between two nodes in a dependency tree have been widely used as features for various NLP tasks and applications, including relation extraction (Bunescu and Mooney, 2005) , temporal relation identification (Choubey and Huang, 2017) semantic parsing (Moschitti, 2004) and question answering (Cui et al., 2005) .\n[BOS] Differently, our dependency chains are generated with respect to an event word and include words that govern or depend on the event, which therefore are not bounded by two pre-identified nodes in a dependency tree.\n\n", "discourse_tags": ["Narrative_cite", "Reflection"], "span_citation_mapping": [{"token_start": 32, "token_end": 44, "char_start": 180, "char_end": 226, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Bunescu and Mooney, 2005)": "5165854"}}}, {"token_start": 45, "token_end": 57, "char_start": 229, "char_end": 287, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 57, "token_end": 67, "char_start": 288, "char_end": 322, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Moschitti, 2004)": null}}}, {"token_start": 68, "token_end": 78, "char_start": 327, "char_end": 364, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Cui et al., 2005)": "13246172"}}}]}
{"id": "1136358_0", "paragraph": "[BOS] Constituency-based and dependency-based parse trees have been explored and applied to improve performance of neural nets for the task of sentiment analysis and semantic relation extraction (Socher et al., 2013; Bowman and Potts, 2015; Tai et al., 2015) .\n[BOS] The focus of these prior studies is on designing new neural network architectures (e.g., tree-structured LSTMs) corresponding to the parse tree structure.\n[BOS] In contrast, our method aims at extracting appropriate event-centered data representations from dependency trees so that the neural net models can effectively concentrate on relevant regions of contexts.\n\n", "discourse_tags": ["Narrative_cite", "Transition", "Reflection"], "span_citation_mapping": [{"token_start": 27, "token_end": 57, "char_start": 143, "char_end": 258, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Socher et al., 2013;": "990233", "Bowman and Potts, 2015;": "15770688", "Tai et al., 2015)": "3033526"}}}]}
{"id": "11858650_2", "paragraph": "[BOS] Our work is most closely related to Yoshikawa et al. (2009)\n\n", "discourse_tags": ["Narrative_cite"], "span_citation_mapping": [{"token_start": 9, "token_end": 17, "char_start": 42, "char_end": 65, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "11858650_1", "paragraph": "[BOS] While most data-driven techniques model temporal relations as local pairwise classifiers, this approach has the limitation that there is no systematic mechanism to ensure global consistencies among predicted temporal relations (e.g., if event A happens before event B and event B happens before event C, then A should happen before C).\n[BOS] To avoid this drawback, a line of research has explored techniques for the global optimization of local classifier decisions.\n[BOS] Chambers and Jurafsky (2008) add global constraints over local classifiers using Integer Linear Programming.\n[BOS] Yoshikawa et al. (2009) jointly model related temporal classification tasks using ML.\n[BOS] These approaches are shown to improve the accuracy of temporal relation models.\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 88, "token_end": 107, "char_start": 480, "char_end": 588, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Chambers and Jurafsky (2008)": "2199359"}, "Reference": {}}}, {"token_start": 108, "token_end": 139, "char_start": 595, "char_end": 766, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "11858650_0", "paragraph": "[BOS] Recently, data-driven approaches to modeling temporal relations for written text have been gaining momentum.\n[BOS] Boguraev and Ando (2005) apply a semi-supervised learning technique to recognize events and to infer temporal relations between time expressions and their anchored events.\n[BOS] Mani et al. (2006) model temporal relations between events as well as between events and time expressions using maximum entropy classifiers.\n[BOS] The participants of TempEval-1 investigate a variety of techniques for temporal analysis of text (Verhagen et al., 2007) .\n\n", "discourse_tags": ["Transition", "Single_summ", "Single_summ", "Narrative_cite"], "span_citation_mapping": [{"token_start": 21, "token_end": 53, "char_start": 121, "char_end": 292, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Boguraev and Ando (2005)": null}, "Reference": {}}}, {"token_start": 54, "token_end": 80, "char_start": 299, "char_end": 439, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 98, "token_end": 108, "char_start": 538, "char_end": 566, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Verhagen et al., 2007)": "39011"}}}]}
{"id": "10151247_2", "paragraph": "[BOS] Transfer learning has been recently investigated by Mou et al. (2016) , who distinguish two settings: semantically equivalent transfer (where both source and target tasks are natural language inference) and semantically different transfer (where the source task is natural language inference and the target task is paraphrase detection).\n[BOS] They report increased performance only in the former setting.\n[BOS] Zoph et al. (2016) train a parent model on a highresource language pair (such as English-French) in order to improve low-resource language pairs.\n[BOS] They manage to improve the baseline with an average 5.6 BLEU points.\n\n", "discourse_tags": ["Single_summ", "Single_summ", "Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 73, "char_start": 6, "char_end": 411, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Mou et al. (2016)": "11866664"}, "Reference": {}}}, {"token_start": 74, "token_end": 126, "char_start": 418, "char_end": 638, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Zoph et al. (2016)": "16631020"}, "Reference": {}}}]}
{"id": "10151247_1", "paragraph": "[BOS] Our work is different in that we focus on transfer learning to improve performance, using state of the art neural models employed mainly for machine translation.\n\n", "discourse_tags": ["Reflection"], "span_citation_mapping": []}
{"id": "10151247_0", "paragraph": "[BOS] Paraphrase generation has been recently explored as a statistical machine translation problem in a neural setting.\n[BOS] Prakash et al. (2016) used a stacked-LSTM (Long Short-Term Memory) SEQ2SEQ network with residual connections and demonstrated strong performance over the simple and attentionenhanced SEQ2SEQ models.\n[BOS] They report superior scores on several datasets: the Paraphrase Database corpus (Ganitkevitch et al., 2013, PPDB) , captions from Common Objects in Context (Lin et al., 2014, MSCOCO) , and question pairs from WikiAnswers (Fader et al., 2013) .\n[BOS] Mallinson et al. (2017) adapt the NMT architecture to incorporate bilingual pivoting and report improvements over the baseline in simi-larity prediction, paraphrase identification as well as paraphrase generation.\n\n", "discourse_tags": ["Transition", "Single_summ", "Narrative_cite", "Single_summ"], "span_citation_mapping": [{"token_start": 22, "token_end": 65, "char_start": 127, "char_end": 325, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Prakash et al. (2016)": "9385494"}, "Reference": {}}}, {"token_start": 75, "token_end": 95, "char_start": 385, "char_end": 445, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 99, "token_end": 114, "char_start": 462, "char_end": 514, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}, {"token_start": 119, "token_end": 132, "char_start": 541, "char_end": 573, "span_type": "Reference", "span_citation_mapping": {"Dominant": {}, "Reference": {"(Fader et al., 2013)": "8893912"}}}, {"token_start": 134, "token_end": 178, "char_start": 582, "char_end": 795, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Mallinson et al. (2017)": "17246494"}, "Reference": {}}}]}
{"id": "11659554_2", "paragraph": "[BOS] Fraser and Marcu (2007) take a semi-supervised approach to word alignment, using a small amount of gold data to further tune parameters of a headword-aware generative model.\n[BOS] They show a significant improvement over a Model-4 union baseline on a very large corpus.\n\n", "discourse_tags": ["Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 59, "char_start": 6, "char_end": 275, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {}, "Reference": {}}}]}
{"id": "11659554_1", "paragraph": "[BOS] DeNero and Klein (2007) refine the distortion model of an HMM aligner to reflect tree distance instead of string distance.\n[BOS] Fossum et al. (2008) start with the output from GIZA++ Model-4 union, and focus on increasing precision by deleting links based on a linear discriminative model exposed to syntactic and lexical information.\n\n", "discourse_tags": ["Single_summ", "Single_summ"], "span_citation_mapping": [{"token_start": 2, "token_end": 27, "char_start": 6, "char_end": 128, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Klein (2007)": "9882011"}, "Reference": {}}}, {"token_start": 28, "token_end": 71, "char_start": 135, "char_end": 341, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Fossum et al. (2008)": "2485577"}, "Reference": {}}}]}
{"id": "11659554_0", "paragraph": "[BOS] Recent work has shown the potential for syntactic information encoded in various ways to support inference of superior word alignments.\n[BOS] Very recent work in word alignment has also started to report downstream effects on BLEU score.\n[BOS] Cherry and Lin (2006) introduce soft syntactic ITG (Wu, 1997) constraints into a discriminative model, and use an ITG parser to constrain the search for a Viterbi alignment.\n[BOS] Haghighi et al. (2009) confirm and extend these results, showing BLEU improvement for a hierarchical phrasebased MT system on a small Chinese corpus.\n[BOS] As opposed to ITG, we use a linguistically motivated phrase-structure tree to drive our search and inform our model.\n[BOS] And, unlike ITG-style approaches, our model can generate arbitrary alignments and learn from arbitrary gold alignments.\n\n", "discourse_tags": ["Transition", "Transition", "Single_summ", "Single_summ", "Reflection", "Reflection"], "span_citation_mapping": [{"token_start": 43, "token_end": 83, "char_start": 250, "char_end": 423, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Cherry and Lin (2006)": "2787289", "(Wu, 1997)": "912349"}, "Reference": {}}}, {"token_start": 84, "token_end": 116, "char_start": 430, "char_end": 579, "span_type": "Dominant", "span_citation_mapping": {"Dominant": {"Haghighi et al. (2009)": "1319915"}, "Reference": {}}}]}
{"id": "11872486_0", "paragraph": "[BOS] It is common for machine learning approaches to begin learning of any new task from scratch, for example by randomly initializing the parameters of a neural network.\n[BOS] This disregards any knowledge gained by similar algorithms when solving previous tasks.\n[BOS] Transfer learning approaches, on the other hand, store the knowledge gained in one context and apply it to different, related problems.\n[BOS] This type of approach is particularly appealing when one lacks sufficient quantity of in-domain labeled training data, such as when there are only a few hundred known examples of a target.\n\n", "discourse_tags": ["Transition", "Transition", "Transition", "Transition"], "span_citation_mapping": []}
