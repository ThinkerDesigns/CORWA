[BOS] We are interested in a particularly harsh zero-shot learning scenario: given labeled examples for N relation types during training, extract relations of a new type R N +1 at test time.
[BOS] The only information we have about R N +1 are parametrized questions.

[BOS] This setting differs from prior art in relation extraction.
[BOS] Bronstein et al. (2015) explore a similar zero-shot setting for event-trigger identification, in which R N +1 is specified by a set of trigger words at test time.
[BOS] They generalize by measuring the similarity between potential triggers and the given seed set using unsupervised methods.
[BOS] We focus instead on slot filling, where questions are more suitable descriptions than trigger words.

[BOS] Open information extraction (open IE) (Banko et al., 2007 ) is a schemaless approach for extracting facts from text.
[BOS] While open IE systems need no relation-specific training data, they often treat different phrasings as different relations.
[BOS] In this work, we hope to extract a canonical slot value independent of how the original text is phrased.

[BOS] Universal schema (Riedel et al., 2013) represents open IE extractions and knowledge-base facts in a single matrix, whose rows are entity pairs and columns are relations.
[BOS] The redundant schema (each knowledge-base relation may overlap with multiple natural-language relations) enables knowledge-base population via matrix completion techniques.
[BOS] Verga et al. (2017) predict facts for entity pairs that were not observed in the original matrix; this is equivalent to extracting seen relation types with unseen entities (see Section 6.1).
[BOS] Rocktschel et al. (2015) and Demeester et al. (2016) use inference rules to predict hidden knowledge-base relations from observed naturallanguage relations.
[BOS] This setting is akin to generalizing across different manifestations of the same relation (see Section 6.2) since a natural-language description of each target relation appears in the training data.
[BOS] Moreover, the information about the unseen relations is a set of explicit inference rules, as opposed to implicit natural-language questions.

[BOS] Our zero-shot scenario, in which no manifestation of the test relation is observed during training, is substantially more challenging (see Section 6.3).
[BOS] In universal-schema terminology, we add a new empty column (the target knowledgebase relation), plus a few new columns with a single entry each (reflecting the textual relations in the sentence).
[BOS] These columns share no entities with existing columns, making the rest of the matrix irrelevant.
[BOS] To fill the empty column from the others, we match their descriptions.
[BOS] Toutanova et al. (2015) proposed a similar approach that decomposes natural-language relations and computes their similarity in a universal schema setting; however, they did not extend their method to knowledge-base relations, nor did they attempt to recover out-of-schema relations as we do.

[BOS] We consider the slot-filling challenge in relation extraction, in which we are given a knowledgebase relation R, an entity e, and a sentence s. For example, consider the relation occupation, the entity "Steve Jobs", and the sentence "Steve Jobs was an American businessman, inventor, and industrial designer".
[BOS] Our goal is to find a set of text spans A in s for which R(e, a) holds for each a  A.
[BOS] In our example, A = {businessman, inventor, industrial designer}.
[BOS] The empty set is also a valid answer (A = ) when s does not contain any phrase that satisfies R(e, ?).
[BOS] We observe that given a natural-language question q that expresses R(e, ?)
[BOS] (e.g. "What did Steve Jobs do for a living?
[BOS] "), solving the reading comprehension problem of answering q from s is equivalent to solving the slot-filling challenge.

[BOS] The challenge now becomes one of querification: translating R(e, ?)
[BOS] into q.
[BOS] Rather than querify R(e, ?)
[BOS] for every entity e, we propose a method of querifying the relation R. We treat e as a variable x, querify the parametrized query R(x, ?)
[BOS] (e.g. occupation(x, ?))
[BOS] as a question template q x ("What did x do for a living?
[BOS] "), and then instantiate this template with the relevant entities, creating a tailored natural-language question for each entity e ("What did Steve Jobs do for a living?").
[BOS] This process, schema querification, is by an order of magnitude more efficient than querifying individual instances because annotating a relation type automatically annotates all of its instances.

[BOS] Applying schema querification to N relations from a pre-existing relation-extraction dataset converts it into a reading-comprehension dataset.
[BOS] We then use this dataset to train a readingcomprehension model, which given a sentence s and a question q returns a set of text spans A within s that answer q (to the best of its ability).

[BOS] In the zero-shot scenario, we are given a new relation R N +1 (x, y) at test-time, which was neither specified nor observed beforehand.
[BOS] For example, the deciphered(x, y) relation, as in "Turing and colleagues came up with a method for efficiently deciphering the Enigma", is too domainspecific to exist in common knowledge-bases.
[BOS] We then querify R N +1 (x, y) into q x ("Which code did x break?")
[BOS] or q y ("Who cracked y?
[BOS] "), and run our reading-comprehension model for each sentence in the document(s) of interest, while instantiating the question template with different entities that might participate in this relation.
[BOS] 1 Each time the model returns a non-null answer a for a given question q e , it extracts the relation R N +1 (e, a).

[BOS] Ultimately, all we need to do for a new relation is define our information need in the form of a question.
[BOS] 2 Our approach provides a naturallanguage API for application developers who are interested in incorporating a relation-extraction component in their programs; no linguistic knowledge or pre-defined schema is needed.
[BOS] To implement our approach, we require two components: training data and a reading-comprehension model.
[BOS] In Section 4, we construct a large relationextraction dataset and querify it using an efficient crowdsourcing procedure.
[BOS] We then adapt an existing state-of-the-art reading-comprehension model to suit our problem formulation (Section 5).

