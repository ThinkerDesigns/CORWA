[BOS] Our work draws on recent work in attention-based VQA and human studies in saliency prediction.

[BOS] We work with the free-form and open-ended VQA dataset released by (Antol et al., 2015) .
[BOS] VQA Models.
[BOS] Attention-based models for VQA typically use convolutional neural networks to highlight relevant regions of image given a question.
[BOS] Stacked Attention Networks (SAN) proposed in (Yang et al., 2015) use LSTM encodings of question words to produce a spatial attention distribution over the convolutional layer features of the image.
[BOS] Hierarchical Co-Attention Network (Lu et al., 2016) generates multiple levels of image attention based on words, phrases and complete questions, and is the top entry on the VQA Challenge 1 as of the time of this submission.
[BOS] Another interesting approach uses question parsing to compose the neural network from modules, attention being one of the sub-tasks addressed by these modules (Andreas et al., 2016) .
[BOS] Note that all these works are unsupervised attention models, where "attention" is simply an intermediate variable (a spatial distribution) that is produced by the model to optimize downstream loss (VQA cross-entropy).
[BOS] The fact that some (it's unclear how many) of these spatial distributions end up being interpretable is simply fortuitous.
[BOS] In contrast, we study where humans choose to look to answer visual questions.
[BOS] These human attention maps can be used to evaluate unsupervised maps.
[BOS] Human Studies.
[BOS] There's a rich history of work in collecting eye tracking data from human subjects to gain an understanding of image saliency and visual perception (Jiang et al., 2014; Judd et al., 2009; Fei-Fei et al., 2007; Yarbus, 1967) .
[BOS] Eye tracking data to study natural visual exploration (Jiang et al., 2014; Judd et al., 2009 ) is useful but difficult and expensive to collect on a large scale.
[BOS] (Jiang et al., 2015) established mouse tracking as an accurate ap- Figure 3 : Deblurring procedure to collect attention maps.
[BOS] We present subjects with a blurred image and ask them to sharpen regions of the image that will help them answer the question correctly, in a smooth, click-and-drag, 'coloring' motion with the mouse.

[BOS] proach to collecting attention maps.
[BOS] They collected large-scale attention annotations for MS COCO (Lin et al., 2014) on Amazon Mechanical Turk (AMT).
[BOS] While (Jiang et al., 2015) studies natural exploration and collects task-independent human annotations by asking subjects to freely move the mouse cursor to anywhere they wanted to look on a blurred image, our approach is task-driven.
[BOS] Specifically, as described in 3, we collect ground truth attention annotations by instructing subjects to sharpen parts of a blurred image that are important for answering the questions accurately.
[BOS] Section 4 covers evaluation of unsupervised attention maps generated by VQA models against our human attention maps.

