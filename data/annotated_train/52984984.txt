[BOS] Both Rios et al. (2017) and Liu et al. (2018) propose some techniques to improve the translation of ambiguous words.
[BOS] Rios et al. (2017) use sense embeddings and lexical chains as additional input features.
[BOS] Liu et al. (2018) introduce an additional context vector.
[BOS] There is an apparent difference in evaluation between these two studies.
[BOS] Rios et al. (2017) design a constrained WSD task.
[BOS] They create well-designed test sets to evaluate the performance of NMT models in distinguishing different senses of ambiguous words, rather than evaluating the translations of ambiguous words directly.
[BOS] By contrast, Liu et al. (2018) evaluate the translations of ambiguous words but on a common test set.
[BOS] Scoring the contrastive translations is not evaluating the real output of NMT models.
[BOS] In this paper, we directly evaluate the translations generated by NMT models, using ContraWSD as the test set.

[BOS] In NMT, the encoder may encode contextual information into the hidden states.
[BOS] Marvin and Koehn (2018) explore the ability of hidden states at different encoder layers in WSD, while we focus on exploring the attention mechanisms that connect the encoder and the decoder.
[BOS] Koehn and Knowles (2017) and Ghader and Monz (2017) investigate the relation between attention mechanisms and the traditional word alignment.
[BOS] They find that attention mechanisms not only pay attention to the aligned source tokens but also distribute attention to some unaligned source tokens.
[BOS] In this paper, we perform a more fine-grained investigation of attention mechanisms, focusing on the task of translating ambiguous nouns.
[BOS] We also explore the advanced attention mechanisms in Transformer models (Vaswani et al., 2017) .

[BOS] The encoder-decoder attention mechanisms differ in NMT models.
[BOS] Tang et al. (2018b) evaluate different NMT models, but focusing on NMT architectures.
[BOS] Tang et al. (2018a) ; Domhan (2018) compare different attention mechanisms.
[BOS] However, there is no detailed analysis on attention mechanisms.

[BOS] In this paper, we mainly investigate the encoderdecoder attention mechanisms.
[BOS] More specifically, we explore how attention mechanisms work when translating ambiguous nouns.

