[BOS] A variety of approaches have been explored for nonterminal refinement in hierarchical phrasebased translation.
[BOS] These approaches can be categorized into two groups: 1) augmenting the nonterminal symbol X with informative labels, and 2) attaching distributional linguistic knowledge to each nonterminal in hierarchical rules.
[BOS] The former only allows substitution operations with matched labels.
[BOS] The latter normally builds an additional model as a new feature of the log-linear model to incorporate attached knowledge.

[BOS] Among approaches which directly refine the single label to more fine-grained labels, syntactic and semantic knowledge are explored in various ways.
[BOS] The syntactically augmented translation model (SAMT) proposed by Zollmann and Venugopal (2006) uses syntactic categories extracted from target-side parse trees to augment nonterminals in hierarchical rules.
[BOS] Unfortunately, there is a data sparseness problem in this model due to thousands of extracted syntactic categories.
[BOS] One solution to address this issue is to reduce the number of syntactic categories.
[BOS] Zollmann and Vogel (2011) use word tags, generated by either POS tagger or unsupervised word class induction, instead of syntactic categories.
[BOS] Hanneman and Lavie (2013) coarsen the label set by introducing a label collapsing algorithm to SAMT grammars (Zollmann and Venugopal, 2006 ).
[BOS] Yet another solution is easing restrictions on label matching.
[BOS] Shen et al. (2009) penalize substitution with unmatched labels while Chiang (2010) uses soft match features to model substitutions with various labels.
[BOS] Similar to Zollmann and Venugopal (2006) , Hoang and Koehn (2010) decorate some hierarchical rules with source-side syntax information and use undecorated, decorated, and partially decorated rules in their translation model.
[BOS] Mylonakis and Sima'an (2011) employ source-side syntax-based labels to define a joint probability synchronous grammar.
[BOS] Combinatory Categorial Grammar (CCG) labels or CCG contextual labels are also used to enrich nonterminals (Almaghout et al., 2011; Weese et al., 2012) .
[BOS] Li et al. (2012) incorporate head information extracted from source-side dependency structures into translation rules.
[BOS] Besides, semantic knowledge is also used to refine nonterminals.
[BOS] Gao and Vogel (2011) utilize target-side semantic roles to form SRL-aware SCFG rules.
[BOS] Most of approaches introduced here explicitly require syntactic or semantic parsers trained on manually labeled data.

[BOS] On the other hand, efforts have also been directed towards attaching distributional linguistic knowledge to nonterminals.
[BOS] Venugopal et al. (2009) propose a preference grammar to annotate nonterminals based on preference distributions of syntactic categories.
[BOS] Huang et al. (2010) learn la-tent syntactic distributions for each nonterminal.
[BOS] They use these distributions to decorate nonterminal Xs in SCFG rules with a real-valued feature vectors and utilize these vectors to measure the similarities between source phrases and applied rules.
[BOS] Similar to this work, Huang et al. (2013) utilize treebank tags based on dependency parsing to learn latent distributions.
[BOS] Cao et al. (2014) attach translation rules with dependency knowledge, which contains both dependency relations inside rules and dependency relations between rules and their contexts.

[BOS] The difference of our work from these studies is that our semantic representations are learned from unlabeled bilingual (or monolingual) data and do not depend on any linguistic resources, e.g., parsers.
[BOS] We also believe that our model is able to exploit both syntactic and semantic information for nonterminals since vector representations learned in our way are able to capture both syntactic and semantic properties (Turian et al., 2010; Socher et al., 2010) .

