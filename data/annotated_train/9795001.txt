[BOS] A large body of work is devoted to extractive sentence compression.
[BOS] Here, we mention a few.
[BOS] (Knight and Marcu, 2002) propose two models to generate a short sentence by deleting a subset of words: the decision tree model and the noisy channel model, both based on a synchronous context free grammar.
[BOS] (Turner and Charniak, 2005) and (Galley and McKeown, 2007) build upon this model reporting improved results.

[BOS] (McDonald, 2006) develop a system using largemargin online learning combined with a decoding algorithm that searches the compression space to produce a compressed sentence.
[BOS] Discriminative learning is used to combine the features and weight their contribution to a successful compression.
[BOS] (Cohn and Lapata, 2007) cast the sentence compression problem as a tree-to-tree rewriting task.
[BOS] For this task, they train a synchronous tree substitution grammar, which dictates the space of all possible rewrites.
[BOS] By using discriminative training, a weight is assigned to each grammar rule.
[BOS] These grammar rules are then used to generate compressions by a decoder.

[BOS] In contrast to the large body of work on extractive sentence compression, work on abstractive sentence compression is relatively sparse.
[BOS] propose an abstractive sentence compression method based on a parse tree transduction grammar and Integer Linear Programming.
[BOS] For their abstractive model, the grammar that is extracted is augmented with paraphrasing rules obtained from a pivoting approach to a bilingual corpus (Bannard and Burch, 2005) .
[BOS] They show that the abstractive model outperforms an extractive model on their dataset.
[BOS] (Cohn and Lapata, 2013) follow up on earlier work and describe a discriminative tree-to-tree transduction model that can handle mismatches on the structural and lexical level.

[BOS] There has been some work on the related task of sentence simplification.
[BOS] (Coster and Kauchak, 2011; Zhu et al., 2010) develop models using data from Simple English Wikipedia paired with English Wikipedia.
[BOS] Their models were able to perform rewording, reordering, insertion and deletion actions.
[BOS] (Woodsend and Lapata, 2011) use Simple Wikipedia edit histories and an aligned WikipediaSimple Wikipedia corpus to induce a model based on quasi-synchronous grammar and integer linear programming.
[BOS] (Wubben et al., 2012) propose a model for simplifying sentences using monolingual Phrase-Based Machine Translation obtaining state of the art results.

[BOS] Recently, significant advances have been made in sequence to sequence learning.
[BOS] The paradigm has shifted from traditional approaches that are more focused on optimizing the parameters of several subsystems, to a single model that learns mappings between sequences by learning fixed representations end to end.
[BOS] This approach employs large recurrent neural networks (RNNs) and has been successfully applied to machine translation Sutskever et al., 2014) , image captioning (Vinyals et 42 al., 2015) and extractive summarization (Filippova et al., 2015) .

[BOS] This encoder-decoder approach encodes a source sequence into a vector with fixed length, which the decoder decodes into the target sequence.
[BOS] The model is trained as a whole to maximize the probability of a correct transduction given the source sentence.
[BOS] While normal RNNs can have difficulties with long term dependencies, the Long ShortTerm Memory (LSTM) is an extension that can handle these dependencies well and which can avoid vanishing gradients (Hochreiter and Schmidhuber, 1997 RNN encoders create a single representation of the entire source sequence from which the target sequence is generated by the decoder.
[BOS] claim that this fixed-length vector prevents improving the performance of encoder-decoder systems.
[BOS] This is particularly the case when the RNN needs to deal with long sentences.
[BOS] They propose an extension that allows a model to automatically search for parts of a source sentence that are relevant to predicting a target word.
[BOS] So, each time a target word is generated by the decoder, the model tries to find the places in the source sentence where the most relevant information is concentrated.
[BOS] This architecture differs from the basic encoder-decoder in that it encodes the input sentence into a sequence of vectors and chooses a subset of these vectors while decoding.
[BOS] This means that not all information needs to be stored in one fixed-length vector, allowing for better performance on for instance longer sentences.
[BOS] In this way the model can learn soft alignments between source and target segments.
[BOS] This approach is called soft attention and the resulting model is an attention-based Recurrent Neural Network (aRNN).
[BOS] For a more detailed description of the model, see .

[BOS] A similar model is used by (Rush et al., 2015) to generate headlines.
[BOS] They train the model on a data set compiled from the GigaWord corpus, where longer sentences from news articles are paired with the corresponding headline of the article.
[BOS] They compare the performance of an attention-based RNN with a collection of other systems.
[BOS] They find that the vanilla attention-based RNN is unable to outperform a Moses system.
[BOS] Only after additional tuning on extractive compresssions do they get better ROUGE scores.
[BOS] This can be attributed to the fact that additional extractive features bias the system towards retaining more input words, which is beneficial for higher ROUGE scores.

[BOS] Following this work, we employ an attentive Recurrent Network as described in to the task of abstractive summarization of scene descriptions.

