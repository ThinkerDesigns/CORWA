[BOS] The related work to this paper can be broken into two groups:

[BOS] Analysis Several approaches have been devised to analyze MT models and the linguistic properties that are learned during training.
[BOS] A common approach has been to use activations from a trained model to train an external classifier to predict some relevant information about the input.
[BOS] Khn (2015) and Qian et al. (2016b) analyzed linguistic information learned in word embeddings, while Qian et al. (2016a) went further and analyzed linguistic properties in the hidden states of a recurrent neural network.
[BOS] Adi et al. (2016) looked at the overall information learned in a sentence summary vector generated by an RNN using a similar approach.
[BOS] Our approach closely aligns with that of Shi et al. (2016) and Belinkov et al. (2017a) , where the activations from various layers in a trained NMT system are used to predict linguistic properties.

[BOS] Integrating Morphology Some work has also been done in injecting morphological or more general linguistic knowledge into an NMT system.
[BOS] Sennrich and Haddow (2016) proposed a factored model that incorporates linguistic features on the source side as additional factors.
[BOS] An embedding is learned for each factor, just like a source word, and then the word and factor embeddings are combined before being passed on to the encoder.
[BOS] Aharoni and Goldberg (2017) proposed a method to predict the target sentence along with its syntactic tree.
[BOS] They linearize the tree in order to use the existing sequence-to-sequence model.
[BOS] Nadejde et al. (2017) also evaluated several methods of incorporating syntactic knowledge on both the source and target.
[BOS] While they used factors on the source side, their best method for the target side was to linearize the information and interleave it between the target words.
[BOS] Garca-Martnez et al. (2016) used a neural MT model with multiple outputs, like in our case of Multi-task learning.
[BOS] Their model predicts two properties at every step, the lemma of the target word and its morphological information.
[BOS] They then use an external tool to use this information to generate the actual target word.
[BOS] Dong et al. (2015) presented multi-task learning to translate a language into multiple target languages, and Luong et al. (2015) did experiments involving several levels of source and target language information.
[BOS] There have been previous efforts to integrate morphology into MT systems by learning factored models Durrani et al., 2014b) over POS and morphological tags.

