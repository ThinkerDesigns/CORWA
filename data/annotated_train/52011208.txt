[BOS] Memory Networks There are variety of studies proposed to increase the LSTM memory capacity by using memory networks.
[BOS] The two most salient examples are Neural Turing Machine (NTM) (Graves et al., 2014) and Memory Network (Weston et al., 2014) .
[BOS] Cheng et al. (2016) propose a machine reading simulator which processes text incrementally from left to right.
[BOS] In the NMT task, present a decoder enhanced decoder with an external shared memory which extends the capacity of the network and has the potential to read, write, and forget information.
[BOS] In fact DHEA can be viewed as a special case of memory networks, with only reading mechanism for the translation task.
[BOS] Quite remarkably DHEA incorporates two different types of memory (source memory and decoding history memory) and significantly improves upon state-of-the-arts.

[BOS] Attention Mechanism Attention in neural networks (Bahdanau et al., 2014; Luong et al., 2015) is designed to assign weights to different inputs instead of threating all input sequences equally as original neural networks do.
[BOS] A number of efforts have been made to improve the attention mechanism (Tu et al., 2016b; Mi et al., 2016; .
[BOS] Some of them incorporated the previous attention history into the current attention for better alignment, but none of them are based on the decoding history.

[BOS] The application of self-attention mechanisms in RNNs have been previously studied, and in general, it appears to capture syntactic dependencies among distant words (Liu and Lapata, 2017; Kim et al., 2017; Lin et al., 2017) .
[BOS] Vaswani et al. (2017) resort to self-attention mechanism and showed outstanding performance.
[BOS] Our approach is diffrent from their work in two aspect.
[BOS] First, our method can be viewed as a variant of RNN decoder which allows a form of memory, thus has the the potential to better handle sentences of arbitrary length.
[BOS] Second, we forcus on controlling the information flow between the source side memory and the target side memory and design a gate to balance the contribution of the two-sides.

[BOS] Recurrent Residual Networks Our work is also related to residual connections, which have been shown to improve the learning process of deep neural networks by addressing the vanishing gradient problem (He et al., 2015; Szegedy et al., 2016) .
[BOS] Recently, several architectures using residual connections with LSTMs have been proposed (Kim et al., 2017; Wang, 2017; for sequence prediction.
[BOS] These connections create a direct path from previous layers, helping the transmission of information.
[BOS] Related to our work, Miculicich et al. (2018) propose a target sideattentive residual recurrent network for decoding, where attention over previous words contributes directly to the prediction of the next word.
[BOS] Comparatively, DHEA attends to the previous hidden state and make a combination with the source context.

[BOS] Exploiting Contextual Information A thread of work in sequence to sequence learning attempts to exploit auxiliary context information (Wang and Cho, 2016; Li et al., 2017; Zhang and Zong, 2016) .
[BOS] Recently Tu et al. (2016a) propose using context gates in NMT to dynamically control the contributions from the source contexts and the RNN hidden state.
[BOS] Our approach focuses on integrating the decoding history and the source side context to NMT architecture.
[BOS] In addition, we have a multi-layer approach to better utilize the contextual information.
[BOS] Experiments in Section 4.3 show the superiority of DHEA.

[BOS] In the same period of our work, Lin et al. (2018) and Xia et al. (2017) first turn eyes to the target side attention of NMT architecture.
[BOS] Our approach share the similar idea with these work.
[BOS] The diffrence lies in that we concernd more about the integrating of the souce side context and the target side context and designed three types of combination functions.
[BOS] In addition, we approached in a multi-layer way which is more effective.

