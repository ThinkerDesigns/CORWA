[BOS] Dependency-based syntactic representations of sentences have been found to be useful for various NLP tasks, especially for those involving natural language understanding in some way.
[BOS] We briefly review prior work both on graph-based and transition-based neural dependency parsers.

[BOS] In transition-based parsing, we learn a model for scoring transitions from one state to the next, conditioned on the parse history, and parse a sentence by taking the highest-scoring transition out of every state until a complete dependency graph has been derived.
[BOS] Chen and Manning (2014) made the first successful attempt at introducing deep learning into a transition-based dependency parser.
[BOS] At each step, the feed-forward neural network assigns a probability to every action the parse can take from certain state (words on the stack and buffer).
[BOS] Some researchers have attempted to address the limitations of (Chen and Manning, 2014) by augmenting it with additional complexity.

[BOS] A beam search and a conditional random field loss function were incorporated into the transitionbased neural network models (Weiss et al., 2015; Zhou et al., 2015; Andor et al., 2016) , which allow the parsers to keep the top-k partial parse trees and revoke previous actions once it finds evidence that they may have been incorrect by locally greedy choices.
[BOS] Dyer et al (2015) used three LSTMs to represent the buffer, stack, and parsing history, getting state-of-the-art results on Chinese and English dependency parsing tasks.

[BOS] Graph-based parsers use machine learning for scoring each possible edge for a given sentence, typically by factoring the graphs into their component arcs, and constructing the parse tree with the highest score from these weighted edges.
[BOS] Kiperwasser and Goldberg (2016b) presented a neural graph-based parser in which the bi-directional L-STM's recurrent output vector for each word is concatenated with each possible head's vector (also produced by the same biLSTM), and the result is used as input to a multi-layer perceptron (MLP) for scoring this modifier-head pair.
[BOS] Given the scores of the arcs, the highest scoring tree is constructed using Eisner's decoding algorithm (Eisner, 1996) .
[BOS] Labels are predicted similarly, with each word's recurrent output vector and its head's vector being used in a multi-class MLP.

[BOS] Kiperwasser and Goldberg (2016a) also proposed a hierarchical tree LSTM to model the dependency tree structures in which each word is represented by the concatenation of its left and right modifier (child) vectors, and the modifier vectors are generated by two (leftward or rightward) recurrent neural networks.
[BOS] The tree representations were produced in a bottom-up recursive way with the (greedy) easy-first parsing algorithm (Goldberg and Elhadad, 2010) .
[BOS] Similarly, Cheng et al (2016) proposed a graph-based neural dependency parser that is able to predict the scores for the next arc, conditioning on previous parsing decisions.
[BOS] In addition to using one bi-directional recurrent network that produces a recurrent vector for each word, they also have uni-directional recurrent neural networks (left-to-right and right-toleft) that keep track of the probabilities of each previous parsing actions.

[BOS] In their many-task neural model, Hashimoto et al (2016) included a graph-based dependency parse in which the traditional MLP-based method that Kiperwasser and Goldberg (2016b) used was replaced with a bilinear one.
[BOS] Dozat and Manning (2017) modified the neural graph-based approach of (Kiperwasser and Goldberg, 2016b ) in a few ways to improve the performance.
[BOS] In addition to building a network that is larger and uses more regularization, they replace the traditional MLPbased attention mechanism and affine label classifier with biaffine ones.

[BOS] This work is most closely related to the graphbased parsing approaches with multiple high-order refinements (Rush and Petrov, 2012; Zhang et al., 2014) , although the neural networks were not used in their parsers.
[BOS] Rush and Petrov (2012) proposed a multi-pass coarse-to-fine approach in which a coarse model was used to prune the search space in order to make the inference with up to thirdorder features practical.
[BOS] They start with a lineartime vine pruning pass and build up to high-order models.
[BOS] Zhang et al (2014) introduced a randomized greedy algorithm for dependency parsing in which they begin with a tree drawn from the uniform distribution and use hill-climbing strategy to find the optimal parse tree.
[BOS] Although they reported that drawing the initial tree randomly results in the same performance as when initialized from a trained first-order distribution, but multiple random restarts are required to avoid getting stuck in a locally optimal solution.
[BOS] Their greedy algorithm breaks the parsing into a sequence of local steps, which correspond to choosing the head for each modifier word (one arc at a time) in the bottom-up order relative to the current tree.
[BOS] In contrast, we employed the global inference algorithm to change the entire tree (all at a time) in each refinement step, which makes the improvement more efficient.

