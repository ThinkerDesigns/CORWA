[BOS] Non-Autoregressive Neural Machine Translation Schwenk (2012) proposed a continuousspace translation model to estimate the conditional distribution over a target phrase given a source phrase, while dropping the conditional dependencies among target tokens.
[BOS] The evaluation was however limited to reranking and to short phrase pairs (up to 7 words on each side) only.
[BOS] Kaiser and Bengio (2016) investigated neural GPU (Kaiser and Sutskever, 2015) , for machine translation.
[BOS] They evaluated both non-autoregressive and autoregressive approaches, and found that the nonautoregressive approach significantly lags behind the autoregressive variants.
[BOS] It however differs from our approach that each iteration does not output a refined version from the previous iteration.
[BOS] The recent paper by Gu et al. (2017) is most relevant to the proposed work.
[BOS] They similarly introduced a sequence of discrete latent variables.
[BOS] They however use supervised learning for inference, using the word alignment tool (Dyer et al., 2013) .
[BOS] To achieve the best result, Gu et al. (2017) stochastically sample the latent variables and rerank the corresponding target sequences with an external, autoregressive model.
[BOS] This is in contrast to the proposed approach which is fully deterministic during decoding and does not rely on any extra reranking mechanism.

[BOS] Parallel WaveNet Simultaneously with Gu et al. (2017) , Oord et al. (2017) presented a nonautoregressive sequence model for speech generation.
[BOS] They use inverse autoregressive flow (IAF, Kingma et al., 2016) to map a sequence of independent random variables to a target sequence.
[BOS] They apply the IAF multiple times, similarly to our iterative refinement strategy.
[BOS] Their approach is however restricted to continuous target variables, while the proposed approach in principle could be applied to both discrete and continuous variables.
[BOS] Novak et al. (2016) proposed a convolutional neural network that iteratively predicts and applies token substitutions given a translation from a phasebased translation system.
[BOS] Unlike their system, our approach can edit an intermediate translation with a higher degree of freedom.
[BOS] QuickEdit (Grangier and Auli, 2017) and deliberation network (Xia et al., 2017) incorporate the idea of refinement into neural machine translation.
[BOS] Both systems consist of two autoregressive decoders.
[BOS] The second decoder takes into account the translation generated by the first decoder.
[BOS] We extend these earlier efforts by incorporating more than one refinement steps without necessitating extra annotations.
[BOS] Bordes et al. (2017) proposed an unconditional generative model for images based on iterative refinement.
[BOS] At each step l of iterative refinement, the model is trained to maximize the log-likelihood of target Y given the weighted mixture of generated samples from the previous iteration l1 and a corrupted target .
[BOS] That is, the corrupted version of target is "infused" into generated samples during training.
[BOS] In the domain of text, however, computing a weighted mixture of two sequences of discrete tokens is not well defined, and we propose to stochastically mix denoising and lowerbound maximization objectives.

