[BOS] Prior approaches to text simplification have addressed the task as a monolingual translation problem (Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012) .
[BOS] The proposed models are trained on aligned sentences extracted from Wikipedia and Simple Wikipedia, a corpus that contains instances of transformation operations needed for simplification such as rewording, reordering, insertion and deletion.
[BOS] Zhu et al. (2010) propose to use a tree-based translation model which covers splitting, dropping, reordering and substitution.
[BOS] Coster and Kauchak (2011) employ a phrase-based Machine Translation system extended to support phrase deletion, and Wubben et al. (2012) augment a phrase-based system with a re-ranking heuristic.
[BOS] Woodsend and Lapata (2011) view simplification as a monolingual text generation task.
[BOS] They propose a model based on a quasi-synchronous grammar, a formalism able to capture structural mismatches and complex rewrite operations.
[BOS] The grammar is also induced from a parallel Wikipedia corpus, and an integer linear programming model selects the most appropriate simplification from the space of possible rewrites generated by the grammar.
[BOS] The hybrid model of Angrosh et al. (2014) combines a synchronous grammar extracted from the same parallel corpus with a set of hand crafted syntactic simplification rules.
[BOS] In recent work, Zhang and Lapata (2017) propose a reinforcement learning-based text simplification model which jointly models simplicity, grammaticality, and semantic fidelity to the input.
[BOS] In contrast to these methods, Narayan and Gardent (2016)'s sentence simplification approach does not need a parallel corpus for training, but rather uses a deep semantic representation as input for simplification.

[BOS] The above-mentioned systems support the full range of transformations involved in text simplification.
[BOS] Other works address specific subtasks, such as syntactic or lexical simplification, which involve identifying grammatical or lexical complexities in a text and rewriting these using simpler words and structures.
[BOS] Syntactic simplification might involve operations such as sentence splitting, rewriting of sentences including passive voice and anaphora resolution (Chandrasekar and Srinivas, 1997; Klerke and Sgaard, 2013) .
[BOS] 1 Lexical simplification involves complex word identification, substitute generation, context-based substitute selection and simplicity ranking.
[BOS] To identify the words to be simplified, Shardlow (2013a) proposes to use a Support Vector Machine (SVM) that exploits several lexical features, such as fre-quency, character and syllable length.
[BOS] Our approach also uses a SVM classifier for identifying complex words, but complements this set of features with context-related features that have not been exploited in previous work.
[BOS] 2 In the lexical simplification subtask, existing methods differ in their decision to include a word sense disambiguation (WSD) step for substitute selection and in the ranking method used.
[BOS] Ranking is often addressed in terms of word frequency in a large corpus since it has been shown that frequent words increase a text's readability (Devlin and Tait, 1998; Kauchak, 2013) .
[BOS] Models that include a semantic processing step for substitute selection aim to ensure that the selected substitutes express the correct meaning of words in specific contexts.
[BOS] WSD is often carried out by selecting the correct synset (i.e. set of synonyms describing a sense) for a target word in WordNet (Miller, 1995) and retrieving the synonyms describing that sense.
[BOS] Thomas and Anderson (2012) use WordNet's tree structure (hypernymy relations) to reduce the size of the vocabulary in a document.
[BOS] Biran et al. (2011) perform disambiguation in an unsupervised manner.
[BOS] They learn simplification rules from comparable corpora and apply them to new sentences using vector-based context similarity measures to select words that are the most likely candidates for substitution in a given context.
[BOS] This process does not involve an explicit WSD step, and simplification is addressed as a context-aware lexical substitution task.
[BOS] The SemEval 2012 English Lexical Simplification task (Specia et al., 2012 ) also addresses simplification as lexical substitution (McCarthy and Navigli, 2007) , allowing systems to use external sense inventories or to directly perform in-context substitution.

[BOS] In our work, we opt for an approach which addresses lexical substitution in a direct way and does not include an explicit disambiguation step.
[BOS] Lexical substitution systems perform substitute ranking in context using vector-space models (Thater et al., 2011; Kremer et al., 2014; Melamud et al., 2015) .
[BOS] Recently, Apidianaki (2016) showed that a syntax-based substitution model can successfully filter the paraphrases available in the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013) to select the ones that are adequate in specific contexts.
[BOS] In the same line, Cocos et al. (2017) used a word embedding-based substitution model (Melamud et al., 2015) for ranking PPDB paraphrases in context.
[BOS] We extend this work and adapt the Melamud et al. (2015) model to the simplification setting by using candidate paraphrases extracted from the Simple PPDB resource (Pavlick and Callison-Burch, 2016) , a subset of the PPDB that contains complex words and phrases, and their simpler counterparts that can be used for incontext simplification.

