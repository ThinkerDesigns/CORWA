[BOS] Previous works developed in the literature have tackled cross-lingual dependency parsing by using cross-lingual annotation projection methods, multilingual model learning methods, and crosslingual representation learning methods.

[BOS] Cross-lingual annotation projection methods use parallel sentences to project the annotations from the source language side to the target language side and then train dependency parsers on the target data with projected annotations (Hwa et al., 2005; Smith and Eisner, 2009; Zhao et al., 2009) .
[BOS] For cross-lingual annotation projection methods, both the word alignment training step and the annotation projection step can introduce errors or noise.
[BOS] Thus much work developed in the literature has focused on designing robust projection algorithms such as graph-based projection with label propagations (Das and Petrov, 2011) , improving projection performance by using auxiliary resources such as Wikipedia metadata (Kim and Lee, 2012) or WordNet (Khapra et al., 2010) , or boosting projection performance by heuristically modifying or correcting the projected annotations (Hwa et al., 2005; Kim et al., 2010) .
[BOS] Some work has also proposed to project the discrete dependency arc instances instead of treebank as the training set .
[BOS] Moreover, besides cross-lingual dependency parsing, cross-lingual annotation projection methods have also demonstrated success in various other sequence labeling tasks including POS tagging (Das and Petrov, 2011; Yarowsky and Ngai, 2001) , relation extraction , named entity recognition (Kim et al., 2010; Kim and Lee, 2012) , constituent syntax parsing (Jiang et al., 2011) , and word sense disambiguation (Khapra et al., 2010) .

[BOS] Multilingual model learning methods train cross-lingual dependency parsers with parameter constraints obtained from parallel data Ganchev et al., 2009) or linguistic knowledges (Naseem et al., 2010; Naseem et al., 2012) .
[BOS] Among these methods, some proposed to train a joint dependency parsing system with parameters shared across the dependency parsing models in individual languages .
[BOS] Other works used posterior regularization techniques to encode the linguistic constraints in learning dependency parsing models (Ganchev et al., 2009; Naseem et al., 2010; Naseem et al., 2012) .
[BOS] The linguistic constraints may either come from manually constructed universal dependency parsing rules (Naseem et al., 2010) or manually specified typological features (Naseem et al., 2012) , or be learned from parallel sentences (Ganchev et al., 2009) .
[BOS] Besides cross-lingual dependency parsing, multilingual model learning methods have also achieved good empirical results for other multilingual NLP tasks, including named entity recognition (Burkett et al., 2010; Che et al., 2013; Wang and Manning, 2014) , syntactic parsing (Burkett et al., 2010) , semantic role labeling (Zhuang and Zong, 2010; Kozhevnikov and Titov, 2012) , and word sense disambiguation (Guo and Diab, 2010) .

[BOS] Cross-lingual representation learning methods induce language-independent features to bridge the cross-lingual difference in the original wordlevel representation space and build connections across different languages.
[BOS] They train a dependency parser in the induced representation space by exploiting labeled data from the source language and apply it in the target language (Dur-rett et al., 2012; Tckstrm et al., 2012; Zhang et al., 2012) .
[BOS] A variety of auxiliary resources have been used to induce interlingual features, including bilingual lexicon (Durrett et al., 2012) , and unlabeled parallel sentences (Tckstrm et al., 2013) .
[BOS] Based on different learning mechanisms (whether or not using labeled data) for inducing language-independent features, cross-lingual representation learning methods can be categorized into unsupervised representation learning (Tckstrm et al., 2013) and supervised representation learning (Durrett et al., 2012) .
[BOS] The language-independent features include bilingual word clusters (Tckstrm et al., 2012) , language-independent projection features (Durrett et al., 2012) , and automatically induced languageindependent POS tags (Zhang et al., 2012) .
[BOS] Besides cross-lingual dependency parsing, in the literature cross-lingual representation learning methods have also demonstrated efficacy in different NLP applications such as cross language named entity recognition (Tckstrm et al., 2012) and cross language semantic role labeling .
[BOS] Our work shares similarity with these cross-lingual representation learning methods on inducing new language-independent features, but differs from them in that we learn cross-lingual word embeddings.
[BOS] Though multilingual word embeddings have been employed in the literature, they are developed for other NLP tasks such as cross-lingual sentiment analysis , and machine translation (Zou et al., 2013) .
[BOS] Moreover, the method in requires parallel sentences with observed word-level alignments, and the method in (Zou et al., 2013) first learns language-specific word embeddings in each language separately and then transforms representations from one language to another language with machine translation alignments, while we jointly learn crosslingual word embeddings in the two languages by only exploiting a small set of bilingual word pairs.

[BOS] From the perspective of applying deep networks in natural language processing systems, there are a number of works in the literature (Collobert and Weston, 2008; Collobert et al., 2011; Henderson, 2004; Socher et al., 2011; Titov and Henderson, 2010; Turian et al., 2010) .
[BOS] Socher et al. (2011) applied recursive autoencoders to address sentencelevel sentiment classification problems.
[BOS] Collobert and Weston (2008) and Collobert et al. (2011) employed a deep learning framework for jointly multi-task learning and empirically evaluated it with four NLP tasks, including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling.
[BOS] Henderson (2004) proposed discriminative training methods for learning a neural network statistical parser.
[BOS] Titov and Henderson (2010) extended the incremental sigmoid Belief networks (Titov and Henderson, 2007) to a generative latent variable model for dependency parsing.
[BOS] Turian et al. (2010) employed neural networks to induce word representations for sequence labeling tasks such as named entity recognition.

