[BOS] Text Summarization.
[BOS] Before the successful application of neural generative models, most of the existing works on text summarization (Dorr et al., 2003; Durrett et al., 2016) have focused on extractive methods.
[BOS] While some of the early approaches have used a rich set of heuristic rules or sparse features to select textual units to include in the summary, more recent works (Cheng and Lapata, 2016; Nallapati et al., 2017) leverage neural models to select words and sentences from the original text.
[BOS] With the emergence of sequenceto-sequence models (Sutskever et al., 2014 ) and large-scale datasets like CNN/Daily Mail (Hermann et al., 2015; Nallapati et al., 2016) and NYT (Paulus et al., 2018) , abstractive summarization of longer text have become a more feasible and popular task.
[BOS] Several recent approaches have been proposed to tackle abstractive summarization problem, where Nallapati et al. (2016) exploits hierarchical encoders, See et al. (2017) proposes pointer-generator network and coverage mechanism to overcome OOV and repetition problems, Tan et al. (2017) introduces a graphbased attention mechanism and hierarchical beam search strategy, and (Paulus et al., 2018) proposes to optimize for ROUGE metric via reinforcement learning.
[BOS] Although impressive progress has been achieved for sentence-level summarization, attempts on abstractive document summarization task are still in early stages where the simple LEAD-3 baseline performance is only very recently matched (Paulus et al., 2018) .
[BOS] Neural Machine Translation.
[BOS] With the recent success of encoder-decoder architectures (Sutskever et al., 2014; Bahdanau et al., 2015) , neural machine translation systems has gained a a lot of attention both from academia (Cho et al., 2014; Luong et al., 2015; Luong and Manning, 2016) and industry Vaswani et al., 2017; Ahmed et al., 2018 ) over statistical machine translation, which has been the dominating translation paradigm for years.
[BOS] Most of these works has focused more on enhancing the architecture design aspect to tackle with various challenges such as different attention mechanisms (Bahdanau et al., 2015; Luong et al., 2015) , a character-level decoder (Chung et al., 2016) , a translation coverage mechanism (Tu et al., 2016) , and so on.
[BOS] However, only very recently, a few works Ranzato et al., 2016; Norouzi et al., 2016; Shen et al., 2016; Bahdanau et al., 2017; Zhukov and Kretov, 2017; Casas et al., 2018) have investigated sequence-level optimization by training to maximize BLEU score.
[BOS] Neural Sequence Generation with RL.
[BOS] Most neural sequence generation models are trained with the objective of maximizing the probability of the next correct word.
[BOS] However, this results in a major discrepancy between training and test settings of these models because they are trained with cross-entropy loss at word-level, but evaluated based on sequence-level discrete metrics such as ROUGE (Lin and Och, 2004) or BLEU (Papineni et al., 2002) .
[BOS] On the other hand, directly optimizing for such evaluation metrics is hard due to non-differentiable nature of the exact objective (Rosti et al., 2011) .
[BOS] Recent works (Ranzato et al., 2016; Bahdanau et al., 2017; Paulus et al., 2018) address the difficulty of differentiating with respect to rewards based on such discrete metrics using variants of reinforcement learning.
[BOS] These methods essentially propose to mitigate the problem by optimizing the reward weighted log-likelihood of the hypothesis sequences generated by the model distribution.
[BOS] In this paper, we propose an alternative solution to tackle this problem by introducing a differentiable approximation to exact LCS metric that can be directly optimized by standard gradient-based methods without RL, while still addressing the exposure bias problem.

