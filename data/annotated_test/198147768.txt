[BOS] Before the adoption of neural models, early approaches to AEG involved identifying error statistics and patterns in the corpus and applying them to grammatically correct sentences (Brockett et al., 2006; Rozovskaya and Roth, 2010) .
[BOS] Inspired by the back-translation approach, recent AEG approaches inject errors into grammatically correct input sentences by adopting methods from neural machine translation (Felice and Yuan, 2014; Kasewa et al., 2018) .
[BOS] Xie et al. (2018) propose an approach that adds noise to the beam-search phase of an back-translation based AEG model to generate more diverse errors.
[BOS] They use the synthesized parallel data generated by this method to train a multi-layer convolutional GEC model and achieve a 5 point F 0.5 improvement on the CoNLL-2014 test data (Ng et al., 2014) .
[BOS] Ge et al. (2018) propose a fluency-boosting learning method that generates less fluent sentences from correct sentences and pairs them with correct sentences to create new error-correct sentence pairs during training.
[BOS] Their GEC model trained with artificial errors approaches human-level performance on multiple test sets.

