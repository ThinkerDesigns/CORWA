[BOS] Text summarization has long been an active area of research and query-based summarization has gained momentum more recently.
[BOS] Classical summarization models usually identify salient parts of a text by encapsulating manually crafted rules into linear functions (Lin and Bilmes, 2011) which are solved using integer linear programming (ILP) (Nayeem and Chali, 2017; Boudin et al., 2015) , conditional random fields (CRF) (Shen et al., 2007) , or graph algorithms (Parveen and Strube, 2015; Erkan and Radev, 2004) .
[BOS] More recently, neural networks, mostly with an encoder-decoder framework (Bahdanau et al., 2014) , have been used to learn the underlying features (Jadhav and Rajan, 2018; Nallapati et al., 2016) trained by minimizing the cross-entropy loss (Nallapati et al., 2017) or reinforcement learning (Narayan et al., 2018; Paulus et al., 2017) .
[BOS] Our baseline models for query-based summa-rization (Nema et al., 2017; Hasselqvist et al., 2017) are both implemented on the encoderdecoder framework with the former incorporating a diversity function in their model aimed at minimizing the problem of repetitive word generation inherent in encoder-decoder models.
[BOS] However our approach is similar to neither, as our goal is not to train a query-based summarizer from scratch but rather to investigate the competitiveness of using pre-trained models for closely related tasks-i.e., MRC and MT-on query-based summarization.

