[BOS] Sentential context has been successfully applied in SMT (Meng et al., 2015; .
[BOS] In these works, sentential context representation which is generated by the CNNs is exploited to guided the target sentence generation.
[BOS] In broad terms, sentential context can be viewed as a sentence abstraction from a specific aspect.
[BOS] From this point of view, domain information (Foster and Kuhn, 2007; Hasler et al., 2014; Wang et al., 2017b) and topic information (Xiao et al., 2012; Xiong et al., 2015; Zhang et al., 2016) can also be treated as the sentential context, the exploitation of which we leave for future work.

[BOS] In the context of NMT, several researchers leverage document-level context for NMT (Wang et al., 2017a; Choi et al., 2017; , while we opt for sentential context.
[BOS] In addition, contextual information are used to improve the encoder representations Lin et al., 2018) .
[BOS] Our approach is complementary to theirs by better exploiting the encoder representations for the subsequent decoder.
[BOS] Concerning guiding the NMT generation with source-side context, Zheng et al. (2018) split the source content into translated and untranslated parts, while we focus on exploiting global sentence-level context.

