[BOS] Factoid QA data sets involving question-answer pairs as well as their corresponding Freebase matches have been created in the past.
[BOS] In (Berant et al., 2013) , factoid QA over knowledge graphs are formulated as semantic parsing problems, where each natural language question is first converted into a logic form to retrieve the answer with traditional symbolic approaches.
[BOS] In (Berant et al., 2013) , a small-scale data set of several thousands of question-answer pairs, called WebQuestions, is created by human annotators.
[BOS] In (Yih et al., 2016) , the WebQuestions set is further refined by providing human-annotated semantic parses for some questions that are answerable using Freebase, which is called WebQuestionsSP (WebQSP).
[BOS] Recently, deep learning approaches have become popular in the field of NLP.
[BOS] Neural networks require far more training data than a small data set of several thousands of samples.
[BOS] In (Bordes et al., 2015) , a much larger QA data set of about 100K question/answer pairs, called SimpleQuestions, is created.
[BOS] In this work, some randomly chosen Freebase triples are shown to human annotators.
[BOS] For each given triple, an annotator is asked to manually compose a question to reflect the relation in the triple.
[BOS] The issues with SimpleQuestions lie in that most constructed questions are quite simple in linguistic structure and many questions even directly use the keywords in the Freebase predicates since human annotators may be greatly limited in composition when a particular triple is shown.
[BOS] According to (Petrochuk and Zettlemoyer, 2018) , SimpleQuestions is nearly solved with only standard neural network methods if its linguistic ambiguity is taken into account.
[BOS] In (Vlad Serban et al., 2016) , a large QA data set is automatically generated by neural networks but it obviously lacks rich linguistic variations.
[BOS] Additionally, many similar factoid QA data sets are also released for other non-English languages, e.g. WebQA in .
[BOS] Meanwhile, another direction of data collection efforts involve QA in various reading comprehension tasks, e.g. SQuAD in (Rajpurkar et al., 2016) , MS-MARCO in (Nguyen et al., 2016) , TriviaQA in (Joshi et al., 2017) .
[BOS] However, we believe question answering over structured knowledge graphs remains a viable NLP task for the promising research direction to combine neural computing methods with the traditional symbolic processing approaches.

