[BOS] RL-based summarisation.
[BOS] Most existing RLbased summarisers fall into two categories: crossinput systems and input-specific systems .
[BOS] Cross-input systems learn a summarisation policy at training time by letting the RL agent interact with a ROUGE-based reward function.
[BOS] At test time, the learned policy is used to generate a summary for each input document.
[BOS] Most RL-based summarisers fall into this category (Chen and Bansal, 2018; Narayan et al., 2018b; Dong et al., 2018; Kryscinski et al., 2018; Pasunuru and Bansal, 2018; .
[BOS] As an alternative, input-specific RL (Rioux et al., 2014; Ryang and Abekawa, 2012) does not require reference summaries: for each input document at test time, a summarisation policy is trained specifically for the input, by letting the RL summariser interact with a heuristic-based reward function, e.g. ROUGE between the generated summary and the input document (without using any reference summaries).
[BOS] However, the performance of inputspecific RL falls far behind the cross-input counterparts.

[BOS] In 7 we use our learned reward to train both cross-input and input-specific RL systems.
[BOS] A similar idea has been explored by , but unlike their work that learns the reward from ROUGE scores, we learn our reward directly from human ratings.
[BOS] Human evaluation experiments suggest that our reward can guide both kinds of RL-based systems to generate human-appealing summaries without using reference summaries.

[BOS] The reward learning idea is also related to inverse RL (IRL) (Ng and Russell, 2000) .
[BOS] By observing some (near-)optimal sequences of actions, IRL algorithms learn a reward function that is consistent with the observed sequences.
[BOS] In the case of summarisation, human-written reference summaries are the (near-)optimal sequences, which are expensive to provide.
[BOS] Our method only needs human ratings on some generated summaries, also known as the bandit feedback (Kreutzer et al., 2017) , to learn the reward function.
[BOS] Furthermore, when employing certain loss functions (see 4 and Eq.
[BOS] (2)), our method can even learn the reward function from preferences over generated summaries, an even cheaper feedback to elicit (Kreutzer et al., 2018; Gao et al., 2018) .

[BOS] Heuristic-based rewards.
[BOS] Prior work proposed heuristic-based reward functions to train crossinput RL summarisers, in order to strengthen certain properties of the generated summaries.
[BOS] Arumae and Liu (2019) propose four reward functions to train an RL-based extractive summariser, including the question-answering competency rewards, which encourage the RL agent to generate summaries that can answer cloze-style questions.
[BOS] Such questions are automatically created by removing some words in the reference summaries.
[BOS] Experiments suggest that human subjects can answer the questions with high accuracy by reading their generated summaries; but the human judgement scores of their summaries are not higher than the summaries generated by the stateof-the-art supervised system.
[BOS] Kryscinski et al. (2018) propose a simple heuristic that encourages the RL-based abstractive summariser to generate summaries with more novel tokens, i.e. tokens that do not appear in the input document.
[BOS] However, both ROUGE and human evaluation scores of their system are lower than the state-of-the-art summarisation systems (e.g. See et al., 2017) .
[BOS] In addition, the above rewards require reference summaries, unlike our reward that only takes a document and a generated summary as input.

[BOS] Rewards learned with extra data.
[BOS] Pasunuru and Bansal (2018) propose two novel rewards for training RL-based abstractive summarisers: RougeSal, which up-weights the salient phrases and words detected via a keyphrase classifier, and Entail reward, which gives high scores to logically-entailed summaries using an entailment classifier.
[BOS] RougeSal is trained with the SQuAD reading comprehension dataset (Rajpurkar et al., 2016) , and Entail is trained with the SNLI (Bowman et al., 2015) and Multi-NLI (Williams et al., 2018) datasets.
[BOS] Although their system achieves new state-of-the-art results in terms of ROUGE, it remains unclear whether their system generates more human-appealing summaries as they do not perform human evaluation experiments.
[BOS] Additionally, both rewards require reference summaries.
[BOS] Louis and Nenkova (2013) , Peyrard et al. (2017) and build featurerich regression models to learn a summary evaluation metric directly from the human judgement scores (Pyramid and Responsiveness) provided in the TAC'08 and '09 datasets 1 .
[BOS] Some features they use require reference summaries (e.g. ROUGE metrics); the others are heuristic-based and do not use reference summaries (e.g. the JensenShannon divergence between the word distributions in the summary and the documents).
[BOS] Their experiments suggest that with only non-referencesummary-based features, the correlation of their learned metric with human judgements is lower than ROUGE; with reference-summary-based features, the learned metric marginally outperforms ROUGE.
[BOS] In 6, we show that our reward model does not use reference summaries but outperforms the feature-based baseline by as well as ROUGE.

[BOS] Unlike the above work that attempts to learn a summary evaluation metric, the target of our work is to learn a good reward, which is not necessarily a good evaluation metric.
[BOS] A good evaluation metric should be able to correctly rank summaries of different quality levels, while a good reward function focuses more on distinguishing the best summaries from the mediocre and bad summaries.
[BOS] Also, an evaluation metric should be able to evaluate summaries of different types (e.g. extractive and abstractive) and from different genres, while a reward function can be specifically designed for a single task.
[BOS] We leave the learning of a generic summarisation evaluation metric for future work.

