[BOS] Cross-lingual summarization has been proposed to present the most salient information of a source document in a different language, which is very important in the field of multilingual information processing.
[BOS] Most of the existing methods handle the task of CLS via simply applying two typical translation schemes, i.e., early translation (Leuski et al., 2003; Ouyang et al., 2019) and late translation (Orasan and Chiorean, 2008; Wan et al., 2010) .
[BOS] The early translation scheme first translates the original document into target language and then generates the summary of the translated document.
[BOS] The late translation scheme first summarizes the original document into a summary in the source language and then translates it into target language.
[BOS] Leuski et al. (2003) translate the Hindi document to English and then generate the English headline for it.
[BOS] Ouyang et al. (2019) present a robust abstractive summarization system for low resource languages where no summarization corpora are currently available.
[BOS] They train a neural abstractive summarization model on noisy English documents and clean English reference summaries.
[BOS] Then the model can learn to produce fluent summaries from disfluent inputs, which allows generating summaries for translated documents.
[BOS] Orasan and Chiorean (2008) summarize the Romanian news with the maximal marginal relevance method (Goldstein et al., 2000) and produce the English summaries for English speakers.
[BOS] Wan et al. (2010) adopt the late translation scheme for the task of English-to-Chinese CLS.
[BOS] They extract English sentences considering both the informativeness and translation quality of sentences and automatically translate the English summary into the final Chinese summary.
[BOS] The above researches only make use of the information from only one language side.
[BOS] Some methods have been proposed to improve CLS with bilingual information.
[BOS] Wan (2011) proposes two graph-based summarization methods to leverage both the English-side and Chineseside information in the task of English-to-Chinese CLS.
[BOS] Inspired by the phrase-based translation models, Yao et al. (2015) introduce a compressive CLS, which simultaneously performs sentence selection and compression.
[BOS] They calculate the sentence scores based on the aligned bilingual phrases obtained by MT service and perform compression via deleting redundant or poorly translated phrases.
[BOS] Zhang et al. (2016) propose an abstractive CLS which constructs a pool of bilingual concepts represented by the bilingual elements of the source-side predicate-argument structures (PAS) and the target-side counterparts.
[BOS] The final summary is generated by maximizing both the salience and translation quality of the PAS elements.

[BOS] However, all these researches belong to the pipeline paradigm which not only relies heavily on hand-crafted features but also causes error propagation.
[BOS] End-to-end deep learning has proven to be able to alleviate these two problems, while it has been absent due to the lack of largescale training data.
[BOS] Recently, Ayana et al. (2018) present zero-shot cross-lingual headline generation based on existing parallel corpora of translation and monolingual headline generation.
[BOS] Similarly, Duan et al. (2019) propose to use monolingual abstractive sentence summarization system to teach zero-shot cross-lingual abstractive sentence summarization on both summary word generation and attention.
[BOS] Although great efforts have been made in cross-lingual summarization, how to automatically build a high-quality large-scale cross-lingual summarization dataset remains unexplored.

[BOS] In this paper, we focus on English-to-Chinese and Chinese-to-English CLS and try to automatically construct two large-scale corpora respectively.
[BOS] In addition, based on the two corpora, we perform several end-to-end training methods noted as Neural Cross-Lingual Summarization.

