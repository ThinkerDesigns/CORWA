[BOS] Earlier work on data-driven, end-to-end approaches to conversational response generation treated the task as statistical machine translation, where the goal is to generate a response given the previous dialogue turn (Ritter et al., 2011; Vinyals and Le, 2015) .
[BOS] While these studies resulted in a paradigm change compared to earlier work, they do not include mechanisms to represent conversation context.
[BOS] To tackle this problem and have a better representation of conversation context as input to generation, (Serban et al., 2016) proposed hierarchical recurrent encoder-decoder (HRED) networks.
[BOS] HRED combines two RNNs, one at the token level, modeling individual turns, and one at the dialogue level, inputting turn representations from the tokenlevel RNNs.
[BOS] However, utterances generated by such neural response generation systems are often generic and contentless (Vinyals and Le, 2015) .
[BOS] To improve the diversity and content of generated responses, HRED was later extended with a latent variable that aims to model the higher level aspects (such as topic) of the generated responses, resulting in the VHRED approach (Serban et al., 2017) .
[BOS] Another challenge for dialogue response generation is the integration of knowledge into the generated responses.
[BOS] (Liu et al., 2018) extracted facts relevant to a dialogue from knowledge using string matching, named entity recognition and linking, found additional entities from knowledge that are most relevant to the facts by a neural similarity scorer, and used these as input context features for the dialogue generation RNN.
[BOS] (Ghazvininejad et al., 2018) used end-to-end memory networks to base the generated responses on knowledge, where an attention over the knowledge relevant to the conversation context is estimated, and multiple knowledge representations are included as input during the decoding of responses.
[BOS] In this work, we use end-to-end memory networks as a baseline.

[BOS] Although much research has focused on response generation in a chit-chat setting, models trained on large datasets of human-human interactions of diverse speaker characteristics often tend to generate responses which are too vague and generic (common for most speakers) or inconsistent in personality (switching between different speakers' characteristics).
[BOS] Recently, (Zhang et al., 2018) presented the CONVAI2 challenge containing persona descriptions and over 10K real human chit-chats where speakers were required to converse based on their assigned persona.
[BOS] (Li et al., 2016a ) learned speaker persona embeddings from a single-speaker setting (e.g. Twitter posts) or a speaker-address style (human-human conversations) to generate personalized responses given a single utterance input.
[BOS] Another related work (Raghu et al., 2018) applies hierarchical memory network for task oriented dialog problem.
[BOS] In this work, we compare our model with (Zhang et al., 2018) which uses a memoryaugmented sequence-to-sequence response generator grounded on the dialogue history and persona.

