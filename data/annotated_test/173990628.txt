[BOS] Content selection is integral to any summarization system.
[BOS] Neural approaches to abstractive summarization often perform content selection jointly with surface realization using an encoder-decoder architecture (Rush et al., 2015; Nallapati et al., 2016; Chen et al., 2016b; Tan et al., 2017; See 1 We make our code and models publicly available at https: Paulus et al., 2017; Celikyilmaz et al., 2018; Narayan et al., 2018) .
[BOS] Training these models end-to-end means learning to perform both tasks simultaneously and can require a massive amount of data that is unavailable and unaffordable for many summarization tasks.

[BOS] Recent approaches emphasize the importance of separating content selection from summary generation for abstractive summarization.
[BOS] Studies exploit extractive methods to identify content words and sentences that should be part of the summary and use them to guide the generation of abstracts (Chen and Bansal, 2018; Gehrmann et al., 2018; .
[BOS] On the other hand, surface lexical features have been shown to be effective in identifying pertinent content (Carenini et al., 2006; Wong et al., 2008; Galanis et al., 2012) .
[BOS] Examples include sentence length, position, centrality, word frequency, whether a sentence contains topic words, and others.
[BOS] The surface cues can also be customized for new domains relatively easily.
[BOS] This paper represents a step forward in this direction, where we focus on developing lightweight models to select summary-worthy sentence singletons and pairs and use them as the basis for summary generation.

[BOS] A succinct sentence can be generated by shortening or rewriting a lengthy source text.
[BOS] Recent studies have leveraged neural encoder-decoder models to rewrite the first sentence of an article to a title-like summary (Nallapati et al., 2016; Zhou et al., 2017; Li et al., 2017; Guo et al., 2018; Cao et al., 2018a) .
[BOS] Compressive summaries can be generated in a similar vein by selecting important source sentences and then dropping inessential sentence elements such as prepositional phrases.
[BOS] Before the era of deep neural networks it has been an active area of research, where sentence selection and compression can be accomplished using a pipeline or a joint model (Daum III and Marcu, 2002; Zajic et al., 2007; Gillick and Favre, 2009; Wang et al., 2013; Li et al., 2013 Li et al., , 2014 Filippova et al., 2015) .
[BOS] A majority of these studies focus on selecting and compressing sentence singletons only.

[BOS] A sentence can also be generated through fusing multiple source sentences.
[BOS] However, many aspects of this approach are largely underinvestigated, such as determining the set of source sentences to be fused, handling its large cardinality, and identifying the sentence relationships for per-

[BOS] Merged Sentence: (A) The bombing killed 58 people.

[BOS] Pakistan denies its spy agency helped plan bombing that (B) Wajid Shamsul Hasan, Pakistan's high commissioner to Britain, and Hamid Gul, killed 58. former head of the ISI, firmly denied the agency's involvement in the attack.

