[BOS] Our work mainly builds on the context window approach to artificial error generation.
[BOS] In this approach, all the possible error fragments (errors and their surrounding context) and their corresponding correct fragments are first extracted from GEC annotated corpora.
[BOS] For example, I follows his and I follow his are the fragments extracted from the example sentences in the first paragraph.
[BOS] With these correct-incorrect fragments, for each errorfree sentence, if we find the same correct fragment in the sentence, we can inject errors by replacing that fragment with the incorrect one.
[BOS] Felice (2016) has shown that a context window size of one, that is, one token before and after the error words or phrases, is able to generate a decent amount of error sentences while maintaining the plausibility of the errors.
[BOS] Thus, the current study also adopts this context window size in extracting fragments.

[BOS] The current work is also inspired by the fluency boost learning proposed in Ge et al. (2018) .
[BOS] In their study, sentence fluency is defined as the inverse of the sentence's cross entropy.
[BOS] During fluency boost training, the fluency of candidate sentences generated by their GEC seq2seq model is monitored.
[BOS] Candidate sentences with less than perfect fluency compared to the correct ones are appended as additional error-contained data for subsequent training.
[BOS] Fluency is also used during multi-round GEC inference, in that inference continues as long as the fluency of the output sentences keeps improving.
[BOS] The present study uses fluency measure in an opposite way.
[BOS] We examine how the decrease of fluency in artificial error sentences influences the performance of grammatical error correction.

