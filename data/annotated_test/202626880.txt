[BOS] The generic response problem has been actively studied.
[BOS] Yao et al. (2016) and Nakamura et al. (2019) proposed models that constrain decoders to directly suppress generation of frequent words.
[BOS] Yao et al. (2016) diversified the response by a loss function in which words with high inverse document frequency values are preferred.
[BOS] Nakamura et al. (2019) proposed a loss function that adds weights based on the inverse of the word frequency.
[BOS] Xing et al. (2017) proposed a model using topic words extracted from utterances.
[BOS] Their model ensembles words predicted using the topic words and the words predicted by the decoder.
[BOS] All of the methods described above only focus on the amount of a information in a response.
[BOS] Therefore, generated responses tend to lack relevance to input utterances.
[BOS] MMI-bidi (Li et al., 2016) solves this problem by approximating the PMI between the utterance Q and the generated

[BOS] (1) Here, both P (R|Q) and P (Q|R) are computed by independent Seq2Seq models.
[BOS] Specifically, the N -best candidate responses generated by the former model are re-ranked by Equation (1).
[BOS] MMIbidi exhibited a strong performance for diversifying responses while preserving relevance to an input utterance.
[BOS] However, its effects depend on the diversities of the N-best candidate responses.
[BOS] If these responses are diverse, MMI-bidi can improve futher.

[BOS] 3 Proposed Model Figure 1 shows the outline of the proposed model.
[BOS] It first identifies keywords that strongly co-occur between utterances and their responses in a training corpus using PPMI (section 3.1).
[BOS] The decoder then uses Gumbel-Softmax to sample words in the output layer (section 3.3).
[BOS] Finally, it computes the proportion of output words matching the keywords, and add weights to the loss function (section 3.4).

