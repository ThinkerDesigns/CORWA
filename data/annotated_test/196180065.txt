[BOS] There are many models for text summarization such as rule-based models (Dorr et al., 2003) and statistical models (Banko et al., 2000; Zajic et al., 2004; Filippova and Strube, 2008; Woodsend et al., 2010; Filippova and Altun, 2013) .

[BOS] Recently, abstractive summarization models based on neural encoder-decoders have been proposed (Rush et al., 2015; Chopra et al., 2016; Zhou et al., 2017; Paulus et al., 2018) .
[BOS] There are mainly two research directions: model architectures and optimization methods.

[BOS] Pointer networks (Vinyals and Le, 2015; Gulcehre et al., 2016; See et al., 2017) and copy mechanisms (Gu et al., 2016; Zeng et al., 2016) have been proposed for overcoming the unknown word problem.
[BOS] Other methods for the improvement of abstractive summarization models include use of existing summaries as soft templates with a source text and extraction of actual fact descriptions from a source text .
[BOS] Although summary length control of abstractive summarization has been studied, previous studies focus on incorporation of a length controlling method to neural abstractive summarization models (Kikuchi et al., 2016; Fan et al., 2018; Liu et al., 2018; Fevry and Phang, 2018; Schumann, 2018) .
[BOS] In contrast, our research focuses on a global optimization method.

[BOS] Optimization methods for optimizing a model with respect to evaluation scores, such as reinforcement learning (Ranzato et al., 2015; Paulus et al., 2018; Chen and Bansal, 2018; Wu and Hu, 2018) and minimum risk training (Ayana et al., 2017) , have been proposed for summarization models based on neural encoder-decoders.
[BOS] Our method is similar to that of Ayana et al. (2017) in terms of applying MRT to neural encoder-decoders.
[BOS] There are two differences between our method and Ayana et al. 's: (i) our method uses only the part of the summary generated by a model within the length constraint for calculating the ROUGE score and (ii) it penalizes summaries that exceed the length of the reference regardless of its ROUGE score.

