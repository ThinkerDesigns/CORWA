[BOS] The existing state-of-the-art NMT model known as the Transformer (Vaswani et al., 2017) works well on different scenarios (Lakew et al., 2018; Imankulova et al., 2019) .
[BOS] MultiNMT using the artificial token approach (Johnson et al., 2017) is known to help the language pairs with relatively lesser data (Lakew et al., 2018; Rikters et al., 2018) and outperform bi-directional and uni-directional translation approaches (Imankulova et al., 2019) .
[BOS] Similarly, we exploit MultiNMT approach with Transformer architecture.
[BOS] Our work is heavily based on Imankulova et al. (2019) .
[BOS] They proposed a multi-stage fine-tuning approach that combines multilingual modeling and domain adaptation.
[BOS] They utilize out-of-domain pivot parallel corpora to perform domain adaptation on in-domain pivot parallel corpora and then perform multilingual transfer for a language pair of interest.
[BOS] However, instead of utilizing out-ofdomain pivot parallel corpora, we investigate the impact of other in-domain pivot parallel corpora.

[BOS] Pseudo-parallel data can be used to augment existing parallel corpora for training, and previous work has reported that such data generated by so-called back-translation can substantially improve the quality of NMT (Sennrich et al., 2016) .
[BOS] However, this approach requires base MT systems that can generate somewhat accurate translations (Imankulova et al., 2017) .
[BOS] Therefore, instead of creating noisy pseudo-parallel corpora, we take advantage of other in-domain pivot parallel corpora.

