[BOS] Extractive summarization has seen growing popularity in the past decades (Nenkova and McKeown, 2011) .
[BOS] The methods focus on selecting representative sentences from the document(s) and optionally deleting unimportant sentence constituents to form a summary (Knight and Marcu, 2002; Radev et al., 2004; Zajic et al., 2007; Martins and Smith, 2009; Gillick and Favre, 2009; Lin and Bilmes, 2010; Wang et al., 2013; Li et al., 2013 Li et al., , 2014 Hong et al., 2014; Yogatama et al., 2015) .
[BOS] A majority of the methods are unsupervised.
[BOS] They estimate sentence importance based on the sentence's length and position in the document, whether the sentence contains topical content and its relationship with other sentences.
[BOS] The summarization objective is to select a handful of sentences to maximize the coverage of important content while minimizing summary redundancy.
[BOS] Although unsupervised methods are promising, they cannot benefit from the large-scale training data harvested from the Web (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018) .
[BOS] Neural extractive summarization has focused primarily on extracting sentences (Nallapati et al., 2017; Cao et al., 2017; Isonuma et al., 2017; Tarnpradab et al., 2017; Zhou et al., 2018; Kedzie et al., 2018) .
[BOS] These studies exploit parallel training data consisting of source articles and story highlights (i.e., human abstracts) to create ground-truth labels for sentences.
[BOS] A neural extractive summarizer learns to predict a binary label for each source sentence indicating if it is to be included in the summary.
[BOS] These studies build distributed sentence representations using neural networks (Cheng and Lapata, 2016; Yasunaga et al., 2017) and use reinforcement learning to optimize the evaluation metric (Narayan et al., 2018b) and improve summary coherence (Wu and Hu, 2018) .
[BOS] However, sentence extraction can be coarse and in many cases, only a part of the sentence is worthy to be added to the summary.
[BOS] In this study, we perform finer-grained extractive summarization by allowing the system to select consecutive sequences of words rather than sentences to form a summary.

[BOS] Interestingly, studies reveal that summaries generated by recent neural abstractive systems are, in fact, quite "extractive."
[BOS] Abstractive systems often adopt the encoder-decoder architecture with an attention mechanism (Rush et al., 2015; Nallapati et al., 2016; Paulus et al., 2017; Guo et al., 2018; Gehrmann et al., 2018; Lebanoff et al., 2018; Celikyilmaz et al., 2018) .
[BOS] The encoder condenses a source sequence to a fixed-length vector and the decoder takes the vector as input and generates a summary by predicting one word at a time.
[BOS] See, Liu, and Manning (2017) suggest that about 35% of the summary sentences occur in the source documents, and 90% of summary n-grams appear in the source.
[BOS] Moreover, the summaries may contain inaccurate factual details and introduce new meanings not present in the original text (Cao et al., 2018; .
[BOS] It thus raises concerns as to whether such systems can be used in realworld scenarios to summarize materials such as legal documents.
[BOS] In this work, we choose to focus on extractive summarization where selected word sequences can be highlighted on the source text to avoid change of meaning.

[BOS] Our proposed method is inspired by the work of Lei et al. (2016) who seek to identify rationales from textual input to support sentiment classification and question retrieval.
[BOS] Distinct from this previous work, we focus on generating generic document summaries.
[BOS] We present a novel supervised framework encouraging the selection of consecutive sequences of words to form an extractive summary.
[BOS] Further, we leverage reinforcement learning to explore the space of possible extractive summaries and promote those that are fluent, adequate, and competent in question answering.
[BOS] We seek to test the hypothesis that successful summaries can serve as document surrogates to answer important questions, and moreover, ground-truth questionanswer pairs can be derived from human abstracts.
[BOS] In the following section we describe our proposed approach in details.

