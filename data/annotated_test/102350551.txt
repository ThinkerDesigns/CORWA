[BOS] Text simplification has often been addressed as a monolingual translation process, which generates a simplified version of a complex text.
[BOS] Zhu et al. (2010) employ a tree-based translation model and consider sentence splitting, deletion, reordering, and substitution.
[BOS] Coster and Kauchak (2011) use a Phrase-Based Machine Translation (PBMT) system with support for deleting phrases, while Wubben et al. (2012) extend a PBMT system with a reranking heuristic (PBMT-R).
[BOS] Woodsend and Lapata (2011) propose a model based on a quasisynchronous grammar, a formalism able to capture structural mismatches and complex rewrite operations.
[BOS] Narayan and Gardent (2014) combine a sentence splitting and deletion model with PBMT-R.
[BOS] This model has been shown to perform competitively with neural models on automatic metrics, though it is outperformed using human judgments (Zhang and Lapata, 2017) .

[BOS] In recent work, Seq2Seq models are widely used for sequence transduction tasks such as machine translation (Sutskever et al., 2014; Luong et al., 2015) , conversation agents (Vinyals and Le, 2015) , summarization (Nallapati et al., 2016) , etc.
[BOS] Initial Seq2Seq models consisted of a Recurrent Neural Network (RNN) that encodes the source sentence x to a hidden vector of a fixed dimension, followed by another RNN that uses this hidden representation to generate the target sentence y.
[BOS] The two RNNs are then trained jointly to maximize the conditional probability of the target sentence given the source sentence, i.e. P (y|x).
[BOS] Other works have since extended this framework to include attention mechanisms (Luong et al., 2015) and transformer networks (Vaswani et al., 2017) .
[BOS] 2 Nisioi et al. (2017) was the first major application of Seq2Seq models to text simplification, applying a standard encoder-decoder approach with attention and beam search.
[BOS] Vu et al. (2018) extended this framework to incorporate memory augmentation, which simultaneously performs lexical and syntactic simplification, allowing them to outperform standard Seq2Seq models.

[BOS] There are two main Seq2Seq models we will compare to in this work, along with the statistical model from Narayan and Gardent (2014) .
[BOS] Zhang and Lapata (2017) proposed DRESS (Deep REinforcement Sentence Simplification), a Seq2Seq model that uses a reinforcement learning framework at training time to reward the model for producing sentences that score high on fluency, adequacy, and simplicity.
[BOS] This work showed stateof-the-art results on human evaluation.
[BOS] However, the sentences generated by this model are in general longer than the reference simplifications.
[BOS] Zhao et al. (2018) proposed DMASS (Deep Memory Augmented Sentence Simplification), a multilayer, multi-head attention transformer architecture which also integrates simplification rules.
[BOS] This work has been shown to get state-of-the-art results in an automatic evaluation, training on the WikiLarge dataset introduced by Zhang and Lapata (2017) .
[BOS] Zhao et al. (2018) , however, does not perform a human evaluation, and restricting evaluation to automatic metrics is generally insufficient for comparing simplification models.
[BOS] Our model, in comparison, is able to generate shorter and simpler sentences according to Flesch-Kincaid grade level (Kincaid et al., 1975) and human judgments, and provide a comprehensive analysis using human evaluation and a qualitative error analysis.
[BOS] Standard Seq2Seq models use cross entropy as the loss function at training time.
[BOS] This only takes into account how similar our generated tokens are to those in the reference simple sentence, and not the complexity of said tokens.
[BOS] Therefore, we first develop a model to predict word complexities, and incorporate these into a custom loss function.

