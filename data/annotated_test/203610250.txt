[BOS] Sentence fusion aims to produce a single summary sentence by fusing multiple source sentences.
[BOS] Dependency graphs and discourse structure have proven useful for aligning and combining multiple sentences into a single sentence (Barzilay and McKeown, 2005; Marsi and Krahmer, 2005; Filippova and Strube, 2008; Cheung and Penn, 2014; Gerani et al., 2014) .
[BOS] Mehdad et al. (2013) construct an entailment graph over sentences for sentence selection, then fuse sentences together using a word graph.
[BOS] Abstract meaning representation and other graph-based representations have also shown success in sentence fusion (Liu et al., 2015; Nayeem et al., 2018) .
[BOS] Geva et al. (2019) fuse pairs of sentences together using Transformer, focusing on discourse connectives between sentences.

[BOS] Recent summarization research has put special emphasis on faithfulness to the original text.
[BOS] Cao et al. (2018a) use seq-to-seq models to rewrite templates that are prone to including irrelevant entities.
[BOS] Incorporating additional information into a seq-to-seq model, such as entailment and dependency structure, has proven successful (Li et al., 2018; Song et al., 2018) .
[BOS] The closest work to our human evaluation seems to be from Falke et al. (2019) .
[BOS] Similar to our work, they find that the PG model is more faithful than Fast-Abs-RL and Bottom-Up, even though it has lower ROUGE.
[BOS] They show that 25% of outputs from these stateof-the-art summarization models are unfaithful to the original article.
[BOS] Cao et al. (2018b) reveal a similar finding that 27% of the summaries generated by a neural sequence-to-sequence model have errors.
[BOS] Our study, by contrast, finds 38% to be unfaithful, but we limit our study to only summary sentences created by fusion.
[BOS] Our work examines a wide variety of state-of-the-art summarization systems, and perform in-depth analysis over other measures including grammaticality, coverage, and method of merging.

